 Welcome back. Today is our lab session of week 6. The topic of this week was Generative Adversarial Networks. In the theory session on Monday, we covered two kinds of GANs. First we covered the classic GAN described in the original paper of 2014 literally with the name generative adversarial networks and the second thing that we did was something called DC GAN. The DC GAN brought about a sort of the same kind of vector vector arithmetic in the space of images as we have in the space of words in words we have word to wake if you remember word embeddings whether we have glove or word to wake up word contextual embeddings and more and so on and so forth the end result of all of that is words can be represented as vectors, either context-free or contextually. In the world of images, the surprising thing is people didn't expect that images could be represented as vectors. And sure enough, the DC GAN opened the doors to that. There are many, many GANs that we didn't talk about. I have on my shelf a book here called GANs in Action. I mean, I have a pre-production copy, I believe, which I printed out because of my licensing from Manning. Very good, very nice. And and i think it where is my printed copy or has it not yet come out i'll have to see i would encourage you it's a good book it's a good book and if you want to pursue that certainly please pursue that now uh gans in action actually let me show you guys the book. Let's go to Amazon and see the book. Or go to Manning. If you go to manning.com, let me sign in. So Manning is the publisher of the book. If you sign in, it will give you an idea of that. It's logging me in. will give you an idea of that it's logging me in so somewhere in here is Gann in action this is a lovely book actually I like it very much and if you look in the introduction to Gann's generative models and so on and so forth it's actually now it is published so it's a very good book you may consider buying it it's available these days as you know there's a lot of sale going on one way to purchase Manning books is to just look for coupons sooner or later you'll be lucky and you'll get a 40% or a 50% discount coupon and then you'll get both the printed copy the e-book and and everything the line book and everything all sorts of versions of the books become available and if you have a account with Dropbox as I have I find it rather convenient as these books you purchase even before they are published as the chapters keep coming they keep showing up in the Dropbox account and so they keep showing up on your iPad and so forth and your cell phone whatever it is I usually use my iPad Pro for it it begins to show up there so that's sort of a convenience thing and it's also obviously available in Amazon so you can see that is a book dedicated to it action so how relevant it is it is a good book it's a good book. It's an easy read though, but it is really worth it. See, all of these books are, how should I say it? Gants are gants. There are so many gants out there. Just having a book that you can quickly refer to is worth the 30 but 30 dollars actually when you get it with coupon and all that you'll get it for 25 it's certainly worth it so today i'm going to do something interesting i want to show you the power of gans in a very direct way so we have here gans uh i don know, did any one of you watch this video by Ali Ghazi? If you haven't watched the video, I would encourage, is there anybody here who has watched it, the speaker? Started, but not completed it. Yeah, how did you like it so far? I like that. Yeah, I like the the reading he is one of the people who explains slivian very clearly uh is known for his explanation now there are few medium articles that i put now this is one thing that i so first of all if you want to see the pytorch various implementations of ganon pytorch there is a There is a dedicated GitHub which shows quite literally that PyTorch GAN and this library PyTorch GAN contains implementations of all this long list of GANs and just looking at it you realize that this is how much activity is happening in this field. That simply the list of GANs is so long. And so of these I taught you only the classic one and the DC GAN. When we are doing image processing, we'll come back and do a few more, three, four more GANs we will cover at that time, important ones, the latest breakthroughs that are happening we are going to cover. So that is one thing the other thing I wanted to show you is a fun stuff so is this and let's start by looking at this for a moment if you're used to tensorflow playground this is very similar to that what we will do is remember what do GANs do they imagine that the data is like real currency the the generator is like the thief the the person who is going to counterfeit it and he cannot see the data the thief cannot see the data and yet even without seeing the data in due course of time that he figures out what the actual data looks like it's the most remarkable thing imagine a counterfeiter who has never seen a dollar bill but who only knows what the cop is saying is this a dollar or not is this a valid dollar or counterfeit and based knows what the cop is saying? Is this a dollar or not? Is this a valid dollar or counterfeit? And based on what the police is using to distinguish between the two, just looking at how many mistakes the police is making, it sort of learns what the dollar is. The whole idea is quite remarkable. So we are going to do, let's do this little experiment here so let me go here so you look at these data sets you see this data so suppose data is in a circle or maybe something simple imagine that the data is in a straight line right let us see how long it takes the GAN to figure it out so I want you to pay attention at two places do you see the fake here box of fake this shows what the generator is producing the real data so at any given moment the discriminator will get a bunch of samples from the real and a bunch of samples from the fake Do we remember that guys from the from the Monday session? Yeah, so and the discriminator to decide is it real or fake and then it has to propagate the gradients back For the real and for the fake it has to propagate the gradients this way. So let's do that. Let's run this and See what happens do you see the decision boundary between fake and real is uh going on is changing how and now it is drawing it is making points here the fake is so if you look at it the fake is would you say that the fake is already coming beginning to figure out the already coming beginning to figure out the thief is beginning to figure out what the data should look like and you see this little um i don't know if you can see the hair growing from the bottom of the purple dots or the pink dots do you see those little lines that is the direction of the gradient descent so if you if you look at those lines carefully they are tilting these data points towards this line can you see that towards the actual green points so let's see what happens if you go a few more steps do you notice how much closer it is now it is making big gradients you know is those it's figured out that there is a big arrow pointing towards this data let's run a few more epochs now do you see it is zooming in towards it and you can see the decision boundary change at this moment the decision boundary is still wrong but it's getting there. Keep looking at the background green and purple also. So at this moment, what do you think guys? It is getting pretty close isn't it yep definitely yes and let's see how further how much further it goes if you run it for a few thousand epochs more and look at the fake it is drawing you see that the fake and the real they're big into at least to the casual viewer is beginning to look fairly accurate isn't it it could have fooled you except at the bottom where it's a bit harder and this is where you're training the model from scratch. It takes a few thousand epochs. It will move and gradually stretch and do things. Asif, what's the discriminator doing here in the meantime? Discriminator is just telling if a data point is fake, is it green or purple? Okay. Because the discriminator is not told that, right? Yeah. So yes whenever it's given a fake data it's told that it's actually real data so the generator is trying to fool the discriminator the thief passes on a data point or a currency and says this is real is the job of the police to say no this is not real okay so now you see that it has sort of gotten stuck in a place at which it cannot bounce out of and that shows that when you train these gans sometimes they can be a little bit uns like you know they can get caught up in a situation from which they don't easily recover now one more, if you look at it, do you notice the discriminant loss and the generative loss are very close to each other now? Right? And if you keep looking at this, you'll see that something quite interesting, the symmetric KL divergence, but the KL divergence is, I didn't explain what KL divergence is. I did that in the math class, but not here. So KL divergence basically tells you how different is one probability or one distribution from another. So a simple way of thinking is it is a measure of how much the shapes differ. Are we together? KL divergence is a measure of how much the shapes differ. Now KL divergence is asymmetric so a symmetric version of it is called a J's divergence. It is just KL divergence of AB plus KL divergence of BA halved. That's what it is. How's that spelled? K-A-L-E? No, no, no. Look at this word here. K, capital K, capital L. Oh, I see it over there. Sorry. Yes, that's what it is, right? And now let's try, this is, it seems to have gotten stuck. What happens if just starting from scratch, we use a pre-trained model. Now we are going to do a different game, right? We are going to start with a pre-trained model now we are going to do a different game right we are going to start with a pre-trained model and see how fast this one can go and do its job let's go to another data set i think there's a little bit of a bug here okay we'll start with the pre trained model. And a pre trained model has pretty much already figured it out. So it doesn't take long for it to do that. So pre training is again a sign of the power of transfer learning. Do you see if you look at the if you look at the kl divergence graph here and especially the js divergence which you should look at it is having a pretty strong drop so that you make it a bit bigger on this on this side you'll see that it's continuously dropping and it can get into a bit of a mess now so this also refers to the fact that it's sometimes unstable did you notice that now it's all over the place so then it will again converge after some time so you have to watch out at what point you want to stop let's take a different distribution you take a distribution like this and you say you notice that this is a ellipsoidal structure standing up like a flame and this is like a circular structure if you use a pre-trained one let's see how far a pre-trained gan can go Gann can go. Something is preventing this site from making progress. Okay. I think you need to check the generator. There's a box on the bottom. On the bottom there's a box on the bottom on the bottom there is a generator yeah yeah that is okay no generator oh right generator and this yes did i accidentally click on it and reset it okay let's try this use a pre-trained model for this let's try this this. All right. I don't know what I did to screw up. So I'll just refresh this page and do it again. All right. Let's try this guy and let's start it. Now, do you see how fake data is just in the beginning clustered at one point and then it's sort of being stretched out. and if you look very carefully you can see the you can literally see the way it's transforming the manifold from noise to this and you remember we talked about those manifold transformations in all as paper reading on Sunday it was a first paper reading and so we won't go too too much into it but you can clearly see that it's beginning to fall in place kale divergence is that smoothing method come again kale divergence is it first smoothing for the model no no no it's a measure it's like entropy it's literally across entropy measure very close to cross entropy okay it's a measure of how much two things differ the prediction and the reality differ if you if you want to have it that way okay that's all it is okay let's look at a circle now try a luck with the circle and see how it goes okay let's look at a circle now try a luck with the circle and see how it goes and it's worth playing around with it Do you see how those points have gotten scattered away from each other gradually? And there are regions in which it is not producing any points at all. And look at the discriminator, how the discriminator is deciding whether it's fake or real. The fake points, it is if you look where my mouse is You can see that this is what the discriminator things and the generator is not trying to fool this. And so after a little while, it will become pretty close. Right. Now, the best part is you could do your own picture. You can go here and let's make a picture. My coffee mug of sorts. Well, this is a reasonably good coffee mug, I guess. Let's see what happens. so the point is would you get fooled seeing data from the generator and say it could be real data coming from the action thing just looking at this you would agree that this data except for a few mistakes in the center pretty much has the dots are placing themselves where the green dots are isn't it would you agree guys most of this pink or purple dots are placing themselves where the green dots are in the vicinity of the green dots so well that is it that is it and this is the last one I'd like to try out and see what happens you notice it's not able to make up its mind repeatedly it's so do you think it will converge face I think it will just sometimes not looking good well yes take really a long time or maybe never I think it was playing baseball remember I told you that when I one that you can have it gan is that it may zero in it may you know what happens is that it starts producing one one one one one because it figures it has figured out how to produce one then the discriminator will try to distinguish between the ones once the discriminator succeeds generator will abandon one and it will go to two right and then once discriminator figures that out then it will go to three right so pretty much it's a cat-and-mouse game and it keeps happening and you see that happening here in some sense so with that I hope this makes this whole thing quite real for you. I would encourage you to go play with this on your own. And put your own data, visualize, draw your own figures and so forth. And this page, can I please give it to you as reading material for this week? Yeah. Which is about the Ghana articles and so forth. Also read this. It's a, people have put in a lot of effort to create this site. It was a joint effort, I believe, between Georgia Tech and Google to produce this, or Facebook, I forget which one is too. I think Facebook or Google. It's a joint collaboration that produced this. So let's give some time to it. So today, what will I do? I would like to show you a notebook and this notebook, a GAN notebook is actually something I did not write from scratch. It is essentially taken from the PyTorch website itself. And it is so well written that I didn't see much that I could improve upon it so I left it as that so we'll go over this carefully today and I want you guys to do that so before you run this notebook what you'll have to do is you'll have to follow one instruction there if you want to run it locally on your machine data follow this you go to this website and you go to this Google Drive like if you go to this Google Drive because go there I must warn you that it's a huge data file not too huge it is about a little more than a gig or one and a half gig or something So it will take a bit of time to download Do you see this so of these files Get this guy you see IMG align celebrate celebra This file guy is the third one the one that has a download link built to it, download this file and unzip it on your machine in whichever location that you like. Then what you have to do is come to your code and in the code, we'll come to that code, but let me go to that place a little early on do you see this put that value here in the third cell here is the code visible guys yes so just put your directory here and then increase the number of workers based on how many how much GPU cores and so on and so forth that you have right so two is obviously meant for pretty underpowered machines but we can make it more so we will we'll carefully walk through this now we know the theory of gang it's a cat-and-mouse game between the cop and the thief so mad plot like do we by now is this becoming very easy to remember what is this line doing anybody remembers the graph from the code yes it just inlines the graphs then you don't need to do from future import print this is for people who are using a python 2.7 it's unnecessary then the this is do you notice that these are all torch imports torch and torch vision and then numpy and so forth that's all it is. They have deliberately seeded the random number generator so that we get reproducible results. Now, there are a few arguments or parameters you need to set. Data root is where your data will be, how many workers you want. A batch size. So because the DCGAN paper uses a batch size of 128, they're sort of sticking to it. They're trying to reproduce the DC GAN paper exactly. And this is one of the things we'll do guys, as we move forward, we'll take the latest paper and we'll try to reproduce it in code. It's surprisingly easy to do that with PyTorch. So image size here images are 64 by 64. number of color channels there are three color channels right so it is not monochrome and it is not a four channel privilege nz is the length of the latent vector the underlying representation we can ignore that depth of feature map so these are things that are there in the dcgan paper i didn't go into that but let's see the things that we do understand number of epochs sorry what does the number of epochs refer to guys how many times through the data are you going to go exactly times through the data you're going to go exactly familiar with it learning rate we are certainly very familiar with learning rate a beta one it is a hyper parameter of the atom optimizer right and this number should be about half and so forth and gpu is the number of gpus available right so if you set the number of gpu available to let's say one it means that you have one video card, two means two video cards, and so forth. Why is that important, guys? Why is that important? You can take advantage of the graphical processing unit. Otherwise what will happen? Instead of just CPU. Yes, otherwise the whole learning will run very, very slow. So now let's go here and read this. This is simply an image folder. Now, do we remember these data sets we produce using some code? Here, this comes from torch vision image in which you point out the image folder. And do we remember these transformations? This one will ensure and resize the image to 64 by 64. it is just a precaution this image already comes as 64 by 64. i suppose it's just a way to ensure that just in case it wasn't and you're applying this code to some others some other images that you have downloaded they will get resized to 64 by 64 a center crop what does Center crop do guys remember we talked about it in the first session you just take the center of it you just leave a few pixels and the edges away then to tensize obviously you want to convert it to tensor because that's what I taught you things of data as and normalize. Normalize all the values to have an average of 0.5, which again makes sense. You do that and then after that you create a data. So once you have the dataset, this is PyTorch language. What do you do? You create a data loader to which you give the data set and the batch size shuffle is equal to true and number of workers so why shuffle true what does shuffle true do guys uh the randomness yes every epoch it will reshuffle the images right so that the neural network is not catching on to specific patterns in the sequence now this line of code you all are familiar with if the GPU is there now plot some training images what is it doing it is just picking from the data loader using the iterator in some images you can do any which way like you could just say data a for a park just take one batch of data in that but here you're taking it and you're reducing the figures in an 8x8 grid you're putting all these pictures this is standard bad plot link now now let's start implementing this DCGAN paper. So, one of the things is there is a particular way to initialize the weights. The paper says that it is better to initialize it with a normal distribution. So, this is not like, like for example people do all sorts of initialization Xavier initialization this that but here they are saying, let's start with the normal distribution as an initialization and it has some benefits. Sajeevan G. And they say that this that these, you know, this mostly comes by trial and error. They're saying that if you take a normal distribution with this sort of a zero, mean zero and standard deviation 0.2, it will work very well for you. So that's all it is, it's just initialization. Now let's look at the generator code. So the generator guys, what is the input to the generator? Do we remember? The random number? Yes. You just feed it noise, any kind of noise, any kind of random input. You can give it a linear random input. You can give it a normal random input. You can give it a binomial or whatever it is, log normal. It is your choice. You can feed it whatever random input and it will produce an output which is now in this case what is the output that it should produce images image and what should be the size of that image 3 cross 64 cross 64 isn't it three channels 64 height 64 width so we are going to do that now remember that the generator is very peculiar it is the way we think of continent it runs it backwards in some sense so the way to read the transformer i mean the generator is actually to read it Raja Ayyanar?rk?a?z out of this, which is being produced out of just a small 8 cross 8. And this is being produced out of 4 cross 4. And that itself is being produced by noise that we will inject, which is being scattered into a, it is being projected into a massive number of filters, number of channels. And so it's almost like you're running a congulation network backwards remember that in typical con your sizes decrease as you go from left to right and number of channels increase so here is the opposite number of channels are high in the beginning and as you go along They keep on decreasing ultimately you ended and with only three channels, but the size of the image keeps on increasing So how does the generator work you give it some input? Right and let's say indeed con transpose and let's say indeed conv transpose uh stuff like that so it's not there's nothing very specifically and what is the forward it just says forward is easy you just apply the sequential thing to it by the way guys you notice that this way of writing it it is very uh aeras way of writing things. In one long statement, it has weaved all of this forward. It is okay, I like it better laid out with the init and a forward, but some people like it just like this. They like it to be declared using the sequential because they like the Keras syntax so i think in the somewhere in the code you'll see i have given an example of the sequential also now so now that we have defined the generator let us see how do we generate so what you do is what are we doing here network g is the generator you're instantiating a generator over those number of GPU and then you are moving it to the device, which device? The GPU. So now your generator will be loaded onto the GPU. Right? And so on and so forth. And what if you have multiple GPUs? This is something I didn't teach you, but maybe this is a good occasion to introduce. If you have more than one GPU, the problem is to which GPU are you putting this device? What you do is you give it to the device and you say GPU, okay? But then you need to do a form of map reduce. Your batch will be broken up into a parallel sub-batches and they'll be fed into the different number of GPUs and each GPU will work on a fraction of a batch, a fraction of that mini-batch. And then in the end, they will merge their results when you're computing the loss. So it's sort of a MapReduce kind of thinking. So that that is it so when you say data parallel you're getting it ready to get into that mode right and you apply the initialization of weights you know weight initialization if you remember all right so you're just saying use these weights and print g so when you print it you notice something very interesting you start out with a compilation layer which is a hundred so it is 512 channels which is a fairly massive number of channels but as you go down at the end what is the number of channels guys three why is it only three at the end because we are trying to produce a three-dimensional I mean RGB image that's a channel color channels now what about the discriminator if we look at the discriminator actually it is in some sense the generator run backwards and so you'll see that you notice just look at the last line here does this line look a lot like the first line of the generator so you look at the first line of the generator 8410 bias is equal to false and GF times 844 right right? And you look at the last line here. And if, are we together, guys? Do you see that in many ways, the same thing run backwards, right? And there are actually, there are a small bunch of subtleties you use because it's cons you do batch norm 2D typically. And then there's another subtlety here. Look, in the generator you get away by using ReLU. It will give you speed. But in the, generally in the, in the discriminator, this is a bit of a subtlety, you tend to keep it as leaky ReLU. What's the difference between leaky ReLU and ReLU? Anyone there? When it's a negative, it's not just zero. Yes, it doesn't disappear. It's a negative value. It's a small gradient. that's all it is. And the reason you do that is at the end of it, then you give it a sigmoid. And why in the world are we putting sigmoid at the end of the discriminator. Sajeevan G. You just wanted to do a Boolean classification. Sajeevan G. In the classification. Vaidhyanathan Exactly. So if we go back to that picture that we had, where did that thing go? Oh, I got rid of it. Let's go back to that image can close this now. This now. YouTube and GAN lab. Okay. So when you go to the GAN lab, do you notice that the discriminator is either saying it's real or it's saying it's fake that's all it is so uh with that in mind let's now go back to the dc gun we understand that for binary classification a sigmoid is quite effective you don't need multiple uh nodes that So now what do we do? Now that you create the discriminator, you go and instantiate the discriminator. This code is exactly the same as for the generator. So do you notice that guys, if you really think about it, it's a very interesting idea. You take approximately the mirror image of a convolation net. You take a convolation net for classification and you call it the discriminator just as you would I mean imagine that you have a convolation network classification and then you do something interesting you you invert the convolation that almost you do a lateral inversion and you use that to generate an image a fake image and. And then, so now into the classifier, you feed both real and fake images and you try to train the generator. That is the whole game. Once you get used to it, it looks very simple, but it takes a little bit of a time to get used to it, which is why I started with code that actually works. And the thing is, this has some exercises at the end of it. I make sure that those exercises are easy to do and really worth time. So here we go. We set it up. This is the whole theory of the loss function associated with this. So you guys will all remember that the across the binary cross entropy losses this right Y and minus in our language we don't call it X and we call it a y hat y log y hat plus 1 minus y times log 1 minus y hat that is how I would write in the language in which we are used to or the way i've taught you initialize the bc bc stands for a binary cross entropy in other words the entropy that we are used to right so you do that now do you notice that they're using two different optimizers one for the discriminator and one for the Generator why is it a good idea to use two two optimizers? Because one should hold the parameters of the discriminator and one should be bound one should work on the parameters of the generator and you don't want to mix these two up isn't it guys you don't want to use one and the same generator in one in the i mean both for generator and discriminator that wouldn't be very good if if nothing else the number of parameters are different right a generator architecture theoretically could be very different from this discriminator architecture. So you use two of them. With that, it's straightforward now. Training loop is interesting. See the magic of GAN is not in creating two neural networks, but it is in the training loop. So let's look at the training loop a little bit more carefully. When we look at this training loop, you run for a certain number of epochs. And by the way, guys, I must warn you that when you do run on your machine, it takes a while to run. So beware. Let me see if it has finished running on my machine. Probably it has. Yeah, it did. And it finished in 30 minutes, executed in 30 minutes, 31 seconds on this laptop. But this laptop is no ordinary laptop. It is GPU powered. Then, but if you run it on the workstation, it runs in about five, six minutes. So here it is, the generator discriminator loss during this. so you can see that the discriminator pretty much the police is always ahead of the cop ahead of the thief right it will have much less loss than the generator because the generator the thief is just trying to guess what the police is thinking the police of of course has an advantage, it's trying to, it is really trying to distinguish between the real and the fake. So for that reason, you'll see this common pattern with gangs, right? The losses of the thief will always be a little bit more than the police. And I suppose that's true for real life too because there's a saying in law enforcement all thieves eventually get caught so something like that so how do you visualize the G in progress so when you do this you can see this all sorts of animation here let me do that well it will execute and it will show well this isn't a big but yeah let's play this very much Lou or once okay let's go yeah do you notice guys that the faces how it is how the generator is trying to come up with better and better faces more realistic faces i don't know if you can make out from there but the faces are significantly improving and after a little while it becomes a little hard to tell for example this place this face of this pretty girl here to me looks fairly realistic it could easily fool us into believing this is real likewise the faith the face of this very scholarly person who say wait is it all begins to look pretty real and here is all of these begin to look pretty real now this is we haven't trained it enough we have trained it only for a limited number of epochs. When you train it for more epochs, or you use a bigger network, remember this code is using a network that you can fairly easily train on your laptops. But if you really want to play with it, increase the number of layers, do funny things, that is literally your homework. Now, let's compare let's take a batch of real images and a batch of fake images and see how it goes between the two groups you can see the real images and the fake images I don't know if you can easily tell the difference I think it's pretty hard isn't it some pictures are easy maybe this one was a bit easy to tell it easy maybe this one was a bit easy to tell it's fake this one is a bit easy to tell some of them you can see the edges are blurry and so forth do you see that if you look very carefully at the edge you'll see blurriness one easy way to catch fake is look at look at this and the general resolution of the image real images tend to have higher resolution. Now what do you do after that? Now think about it. What happens if you train it for much longer? And then change the model architecture. It is very hard to get a lot of images because, I mean, you can try different image data sets, but generally images are a bit of a hard thing to get. So instead I would suggest, especially because with human faces, there is this model release, you need permission to take or gather a lot of photographs of people. So the best that you can do is you can Change the architecture of this congelation. Let's both of them a play around with it guys So for example, what happens if you deliberately try to simplify the cop? Simplify the discriminator remove a few layers from it con blairs from it. Try that see see if that helps a few layers from it, gone layers from it. Try that, see if that helps. And obviously run it for more epochs. Play with the learning rate, see what the learning rate does. It doesn't mention it here, but play with the learning rate. Yes. I have a question that, so how come it matches the, you know, the shading in the picture, the it matches the you know the shading in the picture the uh illumination all these things it seems uh you know so uh it's kind of impossible how it does it all this yeah see there's a thing about computers they're just absolutely blind to them it is just a mathematical vector of numbers and it is trying to fake that vector to it is just a sequence of numbers isn't it one dimensional vector ultimately can you go over the training loop again yes i'll go over the training now um that's the long one and that's the one that we'll go over this one i'll go over in a bit guys hold on but before I do that let me just show you that the results are sufficiently impressive now one more thing you can generate music not just images so if you go here and I want to go here for a moment and let me share the sound let me share the sound very much share computer sound i think the fake ones are seeming more realistic in the images yes that's what it is yeah that's the day these days the state of the art is way ahead this again is now all technology ahead. DC GAN is now old technology. So do you notice this way, it can generate all sorts of sounds it can produce and it can imitate. So for example, English, here it is. Let's see how well. The Blue Lagoon is a 1980 American romance and adventure film directed by Randall Kleiser. And then? Aspects of the Sublime in English Poetry and Painting, 1770 to 1850. Yes, so can you tell which of these two is real and which is fake? I'll play this again. The Blue Lagoon is a 1980 American romance and adventure film directed by Randall Kleiser. Aspects of the Sublime in English Poetry and Painting, 1770 to 1850. Again one is fake. The second one is a little bit nonsensical. Is it obvious that it's fake? No, not exactly. Not at all, right? Yeah, if you weren't paying close attention. The Blue Lagoon is a 1980 American romance and adventure film directed by Randall Kleiser. Yeah, so this is from a different accent. Aspects of the sublime in English poetry and painting, 1770 to 1850. sublime in English poetry and painting 1770 to 1850 let's try a different one the Blue Lagoon is a 1980 American romance and adventure film directed by Randall Kleiser so these are using sort of different aspects of the aspects of the sublime in English poetry and painting 1770 to 1850 and then you can not only can you do it in English, just in case you thought it's just English, let's try something in Mandarin. 第一班商業航班在1919年來往於美國和加拿大。 Now let's listen to, oh this one we can't listen to for some reason. I don't know what happened. So all right, we'll try a different one. 第一班商育航班在1919年來往於美國和加拿大。 This is the real one. And now let's try this. 有眾多小運河通過該鎮,碼頭邊狀數環繞。 On casual observation, you would say it's the same person, isn't it? Right. This is it, guys. And so there are many, many samples and you can play around with it. Even if you cook up words, it will learn to imitate. So it also shows that, you know, how powerful these GANs have become. So now that we know all about it, it certainly behooves us to go and try to understand the meat of the matter which is the main training group let's go look at it so remember in the training loop what did I say in one epoch what you do is and for each step what you do is you train the the discriminant first you give the cop an advantage by the way doesn't I don't think it makes that much of a difference. But you give the discriminant, you train the discriminator first. Let me just use the word cop. You train the cop first. So when you do that, you do a gradient descent only on the cop, only on the discriminator. Don't that so let's read that this is the net d what was net d guys it was the adam optimizer associated with the discriminator with the cop so you why do we do zero grad by now you must be familiar with this code whenever we do the training loop we always zero grad or reset the gradient zero exactly this is just pushing it on to the uh to the cpu and all of that i wouldn't do that let's not worry about that at the end of it what you get is this is to do with the fact that you know you have multiple gpus and so on and so forth a little bit of twiddling with the code but we can blast over it it doesn't do anything special it just sort of breaks it up onto the CPU GPUs a forward pass how do we do the forward pass so remember net D was your network do you remember guys that net D was the the variable net D was the network look at this the discriminant network are we together and net G is the generative network these are the variables this programmer has used so remember that net D is that is the cop net G is the thief so net G makes the prediction yeah net D makes the prediction the cop makes the prediction and it produces an output what does minus 1 stand for it says as many whatever the batch sizes respect that get us a view a single dimensional view with minus with whatever number of columns there are there are calculate the loss on each so here it is we calculate the loss on the output in the label now uh just something to be aware of it's always better don't put label and output always put output and label right so you put y hat y. If you remember in scikit-learn, traditionally we, it doesn't matter by convention, at least I used to do y y hat. So here you have to remember to do output and label. Output is your y hat of course in the language that we were used to. So now that we have computed the output, what is the next thing we should do? What is the criteria doing? It is a is a loss function right it is a binary cross entropy loss you get the loss now what do you do with the loss now you need to back propagate the loss are we together guys so all the gradients will get of whose all the gradients will get, of whose? All the gradients of the COPs will be updated. Weight gradients. So that COPs neural network is primed to make a gradient descent step. And that is exactly what you do backward. That is what you will do at the very end. So you have all of these gradients, then you get output mean item you figure out what it is. Now you move forward. Jay Shah, Dr. Anand Oswal, MD, Once you have trained it with real data all real badge. Now what you do is you say, all right, I trained with all fake batch. Jay Shah, Dr. Anand Oswal, MD, How do you get fake data you can go to the generator and get a lot of fake data from it the thief will produce you as many counterfeits as you want so it's very simple what do you feed to the thief what do you feed to the generator you're feeding random noise in fact random normal noise rand n will produce a bell curve like you know a noise from a bell curve data from a bell curve right it doesn't matter whether you feed it random or random normal or log normal or whatever the point is it should be random of some kind right so it doesn't terribly matter which kind you do that and now what do you do if you feed random noise into the generator what will it come out with it will come out with an image would you agree it will basically come out with an image yes and so what we do and now here's the thing that when you are training the put cop you are being honest when you give this data that the thief has produced to the cock you tell the cop that this is fake right so you the label that you give to that data when you give it to the cop is you say this is all fake in other was zero let's say right and then you let the cop do its prediction but before you do it is do you notice that there is one crucial line here fake fake.detach. Why are we detaching the output from the generator? It's a technicality. The reason you do that is that if you, that tensor is associated with the generator network, neural network, thief neural network. Yeah, we don't want to do the back. We don't want to calculate the gradient on that. Exactly. You have to be careful. Otherwise you might end up giving the thief a hint. So you want to detach it. And then once you get the same thing, minus one will make it into a column vector results because ultimately you're just doing sigmoid. All you want is the probability for each input. There is a probability, right right that you are producing well no sorry the net D will produce a probability that this is a real or a fake and then so you have a error now in those predictions again over the false images the cop will have certain number of errors and even that you used to back when you do error D dot back or who is who is learning from this step who is going to learn from this step these gradients the discriminator the copy of the copy discriminator exactly and so now you have a total error which is the error coming from the real data and the fake data and then both of these errors are together right the total energy and now you ask the optimizer to actually take a step which optimizer the the cop optimizer to actually take a gradient descent step so at by the time the execution reaches line 48 you have taken one step of learning one mini batch of learning for the cop isn't it but the thief hasn't learned anything so now you move on to the thief so let's move on to the thief ah this line then G is the team this line looks familiar that G zero bad down no Why did you say one mini batch? Because it is wrapped in an outer circle. For I data, this is a batch of data for each batch. Or the technical word is mini batch, but you use the word batch. It means your batch size will be 128 images. That's all. Out of millions of images, only 128 images. Okay. That's it it it's a small batch and so you are inside a double for loop for a for given an epoch given a mini batch you are doing one step of learning for the uh for the cop and one step of learning for the thief so you just just count the step function, this function step, right? This is your gradient descent function and see that it is inside a double for loop. And that gives you a clue of what's happening. So, yes. So in the fake.detach that line is crucial because you don't want the the see there's a generator to to have adjustable weights when you pass the forward and that's wrong okay that is it that is a crucial thing and so now what happens is a training this guy is actually much easier right training the G network what do you do you give it some the thief some data you fill it with real labor like you pretend when when when you have to train the thief what you do is you a successful thief will produce a counterfeit and claim that it is real isn't it so this is what you do when you fill the labels for it you give it real levels real level means one you say it is real it is real data a real data and then it will give it to the cop and it will try to fool the cop now it will look at the output that the cop that the cop produces and it will use the loss function between the this is your last function, the output in there. This is the last function produced by who this is the last function still produced by the cop. Jay Shah, Right, it is the cop who is learning. I mean, sorry, who's who is the ultimate arbiter. So you see how many mistakes that cop make arbiter so you see how many mistakes that cop make and based on that mistake you do a back propagation in the thief and you take a one gradient descent step for the thief and that is it that makes one cycle one step of learning and then every 50 rows this part you must be familiar with guys now, right? Every 50 rows just produce some statistics, right? As if one question, that day you said that during when the generator feeds in data, the discriminator doesn't learn, right? Are we doing that here? Yeah, yeah. See, what happens is that this step is two steps. The first step is where, if you notice, right, only the discriminator is making a step. So only the discriminator is learning, right? See, unless you call the step on a thing, it won't learn. So part A, the discriminator learns. And now look at part B. In this case, learn so party that the discriminator learns and now look at Part B in this case you notice that the it would have learned if right here right after this I had also said optimizer D dot step we are not doing that okay so so where does the discriminator get its fake data from it gets it from the generator. So let's go over that again. No, no, no. Because generator is going to claim that everything is true, right? It will always claim everything is true. So then where does the discriminator get the labeled fake data? Okay. So let me, maybe this is time that we explain it with a little bit of a, let me go there. It calls for a new page, I suppose, or maybe at the end of this Gantt, let me explain this. And this is perhaps good review. So see, here's a thing this is the thief this is actual data right the pile of truth now what happens is that this cop discriminator it doesn't know where the data is coming from. All it knows is it is just a simple classifier that is fed X, Y, a batch of capital X and the labels, right? Data labels. and it produces an answer either it will say that this is real or it will say it's fake usually more commonly as 1 0 or something like that it will it will choose between one of these two so this is a basic architecture now the question is how do we connect the dots so the way we connect the dots is now I'll redraw this picture in the two steps first the step in which the the cop is learning so let me use this color to say that the cop is learning this is terrible maybe I'll use this data. So you take, let's take a mini batch of 128 images, right? So you create X real, and of course the label will always say one. Would you agree? A column vector of ones, one, one, one, one, one, right? Then what you do is you take this generator and from generator you produce again 128 fakes and once again you will have X fake and this time around zero zero zero zero zeros right this is the data that you have so now what you do is you go to the you go to the discriminator you first pass this data number one pass real data data right and so it will take a real data it it will produce a real law, it will produce a why had real right and so from why had real you'll get a loss real Right. And from loss real you wait there. You don't you don't do anything further. Then you go You, you, you pass in now the fake data. This data also goes in. Step number two, fake data. But this time you're being honest with the police when the police is learning. So the cop is learning. So once again, this will produce the why hack, the predictions on the fake data. And so it will lead to its own fake loss. So now once you have the loss, what do you do on the losses on the losses you say backward so all the gradients will backwards so all the gradients will get computed right and the gradients will accumulate from these two losses like if you're doing them together and then what happens is now that the gradients have accumulated now is the great time to take this network b and d dot step so when you do b d dot step remember that g has not learned at this stage d has learned something so far so good yeah yeah i got it i got what i think now i understood i was not realizing that there is a step where you have only zeros going in. Yes. And so let me complete this story in case some other people will benefit. So this is the step when cop learns. Now what does it look like when the thief learns? The same thing. When the thief learns, you play a different game when thief learns thief is blind Steve has never seen the data so the architecture is very simple in that particular moment you generate let's say 128 fakes but what you will do is you will produce the image, but you will claim that these are all real because that's what thieves do, right? So you will send this as real data and you will feed it into the police, D, right? So now you're fooling the cop. So what do you want to see? You want to see how well you fooled it. So the answer that you hope the cop will give is close to one. So when you are beginning to win, when the thief is beginning to do well, it will produce an answer closer to one. But if it produces an answer closer to zero, you know that you're screwed. mean you need to you'll get caught so what happens is you pass this but with once and notice the difference that you're passing it this time with ones and not zeros so once again this is once it will only produce the y hat fix that will lead to a loss function the D now what Now, what you do is you take that loss, right? And once you have that loss, you call the backwards here. So by the way, the loss that you're doing is you compare it to this guy, right? And then you come up with your loss. And once you have your loss, the rest of it is simple you call the backwards and then you call the net d g dot step and now both the cop and the thief have taken a step okay so that is essentially it there isn't anything else yes i got it uh asif can you go back there I think this is the question that I was asking yes on Monday yes and I think we we kind of reasoned it saying that the loss has to be propagated through D because that it's only understood at that sigmoid in the original paper yes in the original paper we did that but now what is what we are doing is so dc gans and all of these people what they do is because the output is so like the output here of these is just a label here we are just producing true fall like you know probability right and you have a number here as a fake or real. So you feed directly into it. You don't need to actually. You take the loss directly. You don't go through this. So, okay. So essentially you're not passing, you're not doing a back propagation through D. You're essentially taking the... You're taking the loss and you're directly going to G and updating G. Okay. So this is it. This is all that you're doing here so then and you see this here by the way this is also in this figure if you look at this so literally you know there's always refinements and improvements and so you look at this yeah yeah generator loss you're feeding it directly into this and getting an update you're getting this your generator laws you're feeding it directly into this and getting an update you're getting the gradients and you're getting the updates whereas when the discriminator laws you're feeding it in here and improving this the discriminator so two-step process so this is it guys so the rest of it now you realize is very straightforward. So just some bookkeeping and thus we come to the end of a complicated training loop, which is not intuitive. You know, this is not a code you'll understand unless you know Gantz. Asif? Yes. Just one more minute. Can we go to the loss function plotting which you have done over here? Function plotting that came out of that lab? Yeah. Okay. No, back in the code itself. Come again? Back in the code itself. There's this graph which you have plotted for the loss function. Oh, one second oh where did the notebook disappear the third one yeah the fourth one yeah yes uh at the bottom yes results of the log yes uh as if shouldn't be the loss function for discriminator be decreasing and for the generator be increasing because initially the discriminator was getting the crab data from the generator but then the generator starts learning and produces like you know the proper copy things if you can say and now you see that see what happens is generator always learns better than the i mean sorry the discriminator always learns better than the generator the I mean sorry the discriminator always learns better than the generator generator has one see the discriminator has one advantage at least half the time it's seeing the real data isn't it where is the discriminator, the generator never gets to see the real data. So think about it this way, suppose that all you're trying to tell whether the fruit is real or not and one of the column attributes is the color of the fruit and you know we are talking about apples so it will be somewhere between green and red but the generator, the thief doesn't know so he's producing continuously fruits like pink purple brown black whatnot gradually it figures out that something greenish and something reddish will do right but no matter what you do the loss function the net loss of the discriminator tends to be lower than the net loss of the generator simply because see the generator is deforming noise into this into this output right okay that's that Okay. That's that. And so that is that guys. And can you believe that what we do as simple mathematics, just with a few matrices, just basic vectors and matrices is all we are talking about some basic matrix multiplication and matrix calculus leads to such wonderful results. Let's play this training loop all over again and see what happens. Sorry, what is happening here? The key, why is it not running? Real versus fake, where do we go? No, I would like to run this again. Yeah. So now we do this. Did you notice they had Aseem Premji as one of the pictures? Oh, it did. The beginning. The third picture on top was Asim Premji. Oh, interesting. So that is today's lab guys. That's all I have for today. I deliberately wanted to keep it a little bit brief so that I walk through what we have covered. Now I see this GAN, the training loop, the rest of it is easy. By the way, you don't have to use a fully fried DC GAN. So I'll post some simpler examples. I feel that this code is very good. By the way, where is this code? Let me first show you where this code is. So this code I did not write. This code comes from the PyTorch tutorial. PyTorch. If you just do Gann tutorial it will come up is the thing I taught it's on the I taught website so this is this it looks much better here this is it so how do you bring it into your notebook all you have to do is do you see this thing here shortcuts to it right download to notebook download to github a view on github download to notebook running if you have a Google collab account just directly of course click here but the only thing to remember is when you do this so let me do this when you do this do not forget one crucial step what is that step guys the data yes do not please forget to download the data otherwise you'll keep scratching your head on why it's not working so you must follow this step of downloading the data and if you want to see what this architecture is so in this main thing there's a nice visual image of the architecture yeah this is it so do you notice that this looks the CNN's as we are used to it's running backwards you take a random input of random number of thousand thousand I mean a hundred size vector 100 dimensional vector you project it look into how many channels you project you project into 4x4 and 1024 channels that's a whole lot of channels and then you start decreasing 512 256 128 and finally only 64 channels and finally only 64 channels I mean 64 by 64 but the number of channels keeps decreasing keeps decreasing 128 and then finally 3 and the image size keeps increasing image size went from 4 8 16 32 64 whereas number of channels kept kept on collapsing because at the end of it you have to come up with something some random vector you have to morph this through some process into a 64 by 64 by 3 and that's what you do then then this is it and so going back to collab remember that remember to do this make sure you download the data that's what it is so guys I'd like to take a 10-minute break and then let's come back and let's wind down this course and let's see what we learned and didn't learn and what's the road ahead. Any questions? One more statement I'd like to make. See guys, this GAN is good. It's a very good, but it's a little bit intimidating because it's a, I mean, I explained the code to you, but perhaps it's a little bit intimidating. So I have been in the pursuit of the simplest GAN one can write. I will post a couple of simple examples, right? Some very basic examples. For example, suppose I produce a GAN that only produces, let's say numbers between 10 and 20. Like, you know, the real data is any number, any floating point number between 10 and 20. So can a generator catch on to that trick? Things like that. But we can do actually all of that with a very simple, we just see the discriminator is just a classifier. So nothing prevents us from just putting a single logistic unit there, one neuron. with a very simple we just seek the discriminator is just a classifier so nothing prevents us from just putting a single logistic unit there one neuron one layer one neuron as simple as that right and likewise you can keep your generator to be just one hidden layer right and producer to produce a number and nothing prevents us from doing that. Surprising thing is that itself will learn and I feel that somehow most of the examples of GANs on the internet they tend to be more leaning towards real-life examples which is very good but perhaps a little bit intimidating when you're learning it for the first time. so I'll post some simple examples okay I have a question in this lab when we are training the the cop mechanism we said we will take a mini batch of real and the mini batch of fake but when we were studying the theory on Monday we talked about mini batch of half and half yeah yeah that is it so how do you make half and half if I take 128 images let's say mini batch size is 128 I take 128 real and 128 fake and pass it through. What do I get? Half and half, right? Okay. But in this case, when we look at the program, it is sequentially running. Oh, yes, yes. So in other words, the thing that you're concerned about is when we did not mix it up. Correct. But mixing doesn't help you. The reason is that at this particular moment, we are just doing matrix multiplication. And the order in which you pass this data through the network in the forward direction really doesn't matter. Because remember, it's not learning from that order. The order, it is not that, see, it would be trouble if we said uh here is 128 real data learn from it so then the pop would say okay i learned something from it now here is 128 fake data and learn from it because the moment you learn your weights change but if you don't learn your weights don't change so then it doesn't matter you you can mix them up or you can give them in whichever order you like it simply doesn't matter because at the end of it what you'll end up with is loss a number there will be a number let's say 47 coming from the mistakes you made in the real data and maybe there's another number 56 coming from the mistakes you made in the fake data the loss is essentially a marks you get in the exam the cop is made in the fake data the loss is essentially a marks you get in the exam the cop is getting in the exam right so in what order you feed in the numbers with the additive it will still come to the same numbers added up the total of the two right okay understood that's pretty much it That's pretty much it. So, all right, so I'll post a few examples and maybe I'll create some more, I'll give some more homeworks associated with those examples. In case this code, I mean, I personally feel that first time you encounter a GAN, most of the GAN examples on the internet are rather complicated. And I was sitting and writing it, I didn't finish it. I was hoping I'd finish it before this class. So I would have started you guys with that very simple example. It just occurred to me very late that why start with DCGAN when I can start with a very primitive GAN, a one node classifier and just a single hidden layer as a generator. And it works, of course, it completely conforms to the theory. And so in principle, it completely works. So I'll post that code. And I'll also post detailed notes in the notebook for that code. So guys, what do we need to do? What we need to do here is do these things guys run it for more epochs see what happens right then you can and one more thing that you couldn't do here is that you couldn't do the vector part like you know man with glasses minus this so there are labels there in the data you can search for those labels they will find a few labels with man with glasses. And you can try out that man with glasses minus just man and then plus woman and then so forth. but maybe that's a bit of a challenge the code is there guys one of the sites that I want to show you guys that's very useful is I don't know if you notice that it is there in your class resources which is somewhere at the bottom are the resources isn't it um okay let me go and search for resources important resources uh yeah useful resources so if you look here there's a lot of very useful resources i have right and one of the sites that you will find is a paper with code. It is actually quite an influential site, paper with code, and I would encourage you to go there. It contains all the trending research, what is latest and greatest and so forth. Literally it has those words, latest and greatest. So for example, all the landmark things, scikit-learn, nobody would argue that this is a very influential project. TensorFlow, models, auto differentiation, PyTorch, transformers, they are all here. More than that, what is trending at this moment? Here we go. How do you train a GAN with limited data? Do you notice that almost all the time, half the things are talking about GANs or they're talking about Transformers. So look at this. This is GAN. What is this about transformer isn't it right then google landmark recognition 2020 third place solution okay uh this is it then oh once again style gan right uh then so on and so forth you can go this will be about transfer learning texture synthesis transformers and so forth. You can go this will be about transfer learning, texture synthesis, transformers and so forth. So you can go and read the important papers that are emerging. For example, this paper, which is a number three from the top, one of these days I'll cover I think multiple of you have pointed out that we should cover this paper, it certainly is worth covering. But the beautiful thing with this paper says this site will only host a paper if the code is available so you can just go straight to the code do you see the code is right here and you can go and read the code. The different implementations that people have provided, somebody has provided a PyTorch implementation already of the research paper, means you don't have to wait. You can just right away, you see a research that you like, you can right away start using it. And that is how open and collaborative and vibrant the AI community is. It's very vibrant. People put in a lot of effort to quickly go implement it and then post it and so forth. So please do use that. Thank you.