 Yes, we can hear you loud and clear. So, Asif, I'm presenting on behalf of Homi Baba. And just wanted to start. We have three pillars that we enforced that you wanted us to do. And one of that is most of our code is single responsibility principle. We use the single responsibility principle. We use the single responsibility principle. And the second part was we also had branding. So we got logos and stuff like that. So our application has that stuff. And the third part was, the work was distributed and everybody contributed, even though we had a very big team, all of us contributed. This is awesome. Great team spirit and beautiful logo. Okay, thank you. So used Shopify to get it. Nice. So then this is the technology stack, front-end ReactJS and Tailwind, fast API for the microservices for the APIs, database Postgres, infra Docker, Apache Ticker, and the AI ML sentence transformers, FIAS, BERT, cross encoders, and search elastic. Nice. One more thing I wanted to add was who did what, so that, just so that you know. So Madhu did the downloader and the RAID POCs. We are still not added that to this application as yet, but it's ready to go. Jai did the chunker. And he was really passionate about it. He made a lot of changes and he knows that code inside out. But he's offline now, so we may not be able to explain that part of the code. And Hari Priya did the database aditya did the parser the cross encoder and he dockerized a lot of the components and prashan did the semantic search elastic search microservices and built the entire solution together. Okay. So this was the high level architecture you provided us, Asif. And this was our database design. It slightly differs from this, but this is based on what we did. There were some changes uh the we had a composite key in the chunks table between chunk ID and document ID so this is the high level architecture just put it in in in in a flow diagram any questions on this and this is standard of what you wanted and this is exactly what you gave us to so the semantic surface does both the index building as well as adding to the index as well as searching the index that's correct yes and it manages one index file or many index files one one index one yeah excellent okay and how often do you save the index file when there is a add to it suppose a record is added to the index file there's a add to it suppose a record is added to the index file when do you save it so right now so as if this is prashant so the way we are doing it right now is we have a separation between offline services and online services and offline services just do a one-time job so we the flow we have is there is a downloader which will download all the documents, PDFs, texts, JSONs, whatnot. And then there would be a processor which will, like the Tika basically, would process it, convert it to text. And then this index creation would, chunking and then index creation would be done. And that's it. So we won't be we are not dynamically checking for any new documents added. Okay, good. Nice. So this is the inference architecture. That is right. Yeah. Can I ask question? Go ahead. Hey, the files index, are you storing it in files or you're storing it in database? We are storing it in file. Okay. Actually in DB as well actually. Why would you store it in a DB? Now one question, why would you store the index in a DB? Big binary objects shouldn't be stored in the database. Yeah. So avoid the database. Databases give you the facilities like that, like Postgres gives you. Don't do that. First of all, you'll have a vendor lock. And secondly, as a principle a principle large big binary objects don't belong to the database they belong in object stores sorry so keep it in the file system or object stores okay all right please continue so these were the challenges environment but luckily for us once we built the entire Docker script and the standing up Docker script, that really helped. So now we can deploy that on any environment. And the integration was another difficult part. But with the Dockerization, we kind of worked around that and the disperse team that that was sometimes a challenge because sometimes we worked in zoom calls and sometimes they went away and we went we worked together so we had some challenges there but it's all working out well now yeah your team has a lot of remote people right and it must have been quite a challenge coordinating different time zones yeah sometimes we got into conversations and uh we forgot about the people remote We learned from those lessons too. So we can, we'll go to a live demo and I'll hand over to Prashant. So let me share my screen. Does everybody see? Yes. Yes, we can see that. Cool. Okay. Yeah, so this is the GUI that Albert referred to earlier. And so basically right now our corp... Just hold on for a moment. And so basically right now our core. Just hold on for a moment. Just bear with us for a few minutes. Okay, yes. So basically our corpus as required basically does not include documents all from the same domain so we have a few pdfs that we gathered from data.gov is in essentially it's the same set of pdfs that we had even during like the openheimer uh presentation right as we were part of the same team. And beyond that, we have like some more mixed documents that have different formats like JSON or XML or PDF, but they don't all belong to the same domain. So we can like put in a few queries here. So let's say state of business as the first query and we'll see what it outputs so these are basically the three results that it picked out of the documents and these results as we've discussed over the past few weeks are an accumulation of results returned by elastic search and semantic search. And what we are doing here is we actually remove the duplicates. So we are just showing the unique results here. Are there other queries as if you want to i know uh just say so here's the thing for ai search now it's best to write percent it says so that's right are you able to hear me hello yeah okay go ahead please yeah so I'm trying to see. Okay, just say something like, how would I start a small business? Yeah, so your corpus is probably not big, but the search results are right. In the sense that I think right now it's hardly 10 documents as of now. Right. Okay, so no, this is good. I think your UI looks very nice. You're simple and nice. Your architecture, your inference architecture is right. And everything has come together. It's all working, isn't it? Yes. And that in, I mean, a big thanks to actually Opp I was architect basically we did I we got like inspiration from the way open I was had designed the architecture and we just extended it. R. Vijay Mohanaraman, Ph.D.: So basically what we described earlier about this offline processing and online inference pipeline all of that was similar to how it was done. R. Vijay Mohanaraman, Ph. was done during that phase of project. But to the extent that it is a hybrid AI search engine, you wouldn't see its power or validate its effectiveness unless you give it a good corpus. If you're having difficulty picking a corpus, then you can start with some simpler ones, like start with the whole Wikipedia, which is very easy to download if you want they are available as parquet file files from kegel for example you can try the wikipedia itself and search through it apply your technology to that sure thing yeah yeah certainly wonderful and do you want to talk about your inference the producer pipeline Sure thing. Yeah. Yeah. Certainly do. Wonderful. Do you want to talk about your inference, the producer pipeline? Sure. So give me a moment. I'll share another screen. Yeah, so essentially this is the entire setup. So we have basically offline services, which as I was mentioning earlier, has the downloader, which is essentially the download.py file, a doc processor which uses the pachetika to convert it to text and a chunker which as Albert mentioned jay did a lot of work on which create semantically creates chunks appropriate chunks show me any one thing the any one implementation in python one implementation in python okay let's go to the chunker okay oh this is a pretty extensive implementation very good i think so jay has put up a good effort over here in terms of researching how he can make Chunker better over time as well. And he's still not stopping. He's still going through and figuring out what thresholds or what kind of numbers which we are putting in over here, how much percentage of data we are losing, and what it does to our chunks as well uh so kudos to Jay on that front yeah we wish he was playing this code yeah yeah I can see I can get I got the idea of the code because I I think I have a sense it's coming from this is excellent work by the way uh yeah using clustering you're also using data cleaning and you're looking at the cluster sizes and sub clusters if needed which is all very good this is really good approach continue down see there are only a few good well-established approaches and this code that you're showing i think i know which approach it belongs to it is well done the couple of minor feedback i will give you guys on the code first of all it's very good it's a lot of very good work throughout the couple of feedback which is more or less the same to all of you uh three feedback i'll give you see two feedback actually uh one is that when three one of the three events happen you do io you do model inference or you do rpc remote procedure call all of those actions should should have what do you remember, they should all have logging and they should belong to try except block exception handling blocks. So that is essential. That is quite essentially one one zero. How do you know that file exists and file is available? You wouldn't know. So if it fails, it will throw a not very clean exception. So you can do exception formatting and beautiful things and print very useful messages. In production, it matters. The tying to root cause analysis matters a lot in production. So it is important to do that. Please bring in exception handling and log in. See, most companies, most people don't do that, which is why they suffer. A code is like, you know, the time to build a car is finite, but a car lives for 20 years or 15 years. The same is true with code. Code has a decade worth of life, but it is written in a matter of weeks you need to as a courtesy to the people who are going to own it for the next decade have a lot of comments uh type hinting exception handling logging it makes everybody's life easy uh one consideration that i would advise add type hinting. So my second feedback would be add type hinting. For example, PDB. If you look at line 103 PDB, what it refers to, or first. So from the context people can infer, but it is much better if it is declaratively stated. But other than that, this is very good code. Please continue this. It's very well written. Nice job. Thanks for the input, Asif. Anything that you want to show? I mean, yeah. So, I mean, this is basically the offline services. Beyond that, we, so I mean, this is basically the offline services beyond that we have the online services, which has AI service, which is essentially the microservice. Is your microservice written in FastAPI or is it written in Ray? Right now FastAPI. We have a position to do things with Ray, uh we gotta get handle on it then we'll explore more with ray on this front as well yeah ray is almost a deregulation see one ai framework whether it's ray mlops it is a cube flow etc etc a big one ray is a pretty good choice, but pick one and use that. When you just use a general purpose microservices framework like for example, Flask or FastAPI, then you forego many of the powerful features that AI-served frameworks bring. So I would suggest that. And I know that you guys are already doing, I think you just mentioned that Madhu already has a proof of concept. But in the coming weeks, please go full blown with anyone. Choose Ray because Ray seems to be the choice of everybody here. Pick one, you can get help from other teams. Yeah, we have Sachin here too and he's ready to kind of assist for rain. Okay. All right, please continue. If you have anything else to show. Yeah, so I mean, that's essentially microservice essentially handles. I mean, what it does is it gets the queries from the UI. It forwards it to the individual module. So Elasticsearcher, the Semanticsearcher, which get their individual outcomes. it forwards it to the individual module so elastic searcher the semantic searcher which get their individual outcomes and microservice then forwards that to the cross encoder and essentially we pass it through mini lm model and where do you keep your configuration information the location of the services and things like that where do these how do these services know where the other services are how does that for example your main microservice now where the cross yeah main microservice yes yeah that yeah it's all happening or you have hardwired it in the microservice as of now yes yes externalize it to a configuration file pick your format any format the ml format json format or key value format whatever you like but externalize all of your configuration it should never be in code but for now it's good because see your team started late so they are practically you have a disadvantage of almost 10 days. So it will take you a little bit of time to catch up, but you guys have done an admirable job rising up to the occasion and putting it all together. But in the next time, let's say in a week or two, I try to externalize it. Absolutely. Yeah, that's, and front end is basically the UI, the beautiful UI. And all of them are Dockerized. So as you would see, each of them have their own Docker file. And basically, we just use a Docker compose to fire up each of the containers and create that sequence, get it going since you guys are getting into docker if it is a new experience if you already are gurus of docker i won't say anything but if you're getting into containerization do give podman a look i think sachin is uh experimenting with podman as well so uh yeah We will try to see how we can implement that. One more thing, bring up Cockpit. I didn't do it on your machines. Bring up Cockpit. Cockpit is a Linux application. You will realize why I'm asking you to do Cockpit and Podman. To manage Podman, you'll realize why I'm asking you to do Cockpit and Podman. And when you manage Podman, you may look at Portena. Portena manages both Docker and Podman very well on the machine, on the server. It just makes it very easy for you to see what's running, what's not running. Gives you a dashboard. Cockpit is for that. Yeah, Cockpit. Cockpit gives you cockpit you can look at the entire cluster and see it and portena again helps you manage containers and see what's going on so as if the box that we got is it a gpu thing or a cpu box no this is a pure cpu thing because at this moment you're not fine tuning any model but soon you will have the access to the inference server the moment you start fine tuning. Okay, thank you. All right guys. So yeah, that's all that we had. So I will...