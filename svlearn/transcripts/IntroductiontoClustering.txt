 All right, so today's topic is clustering. Clustering is very different from classifiers and regressors. The clusters solve a completely different problem. Clusters are your introduction to something called unsupervised learning. So let me mention this. There's some descriptors to it. Clusters. Clusters are unsupervised learners. What does that mean? It's a rather old term supervised versus unsupervised. It has stuck to the industry though the meaning is I suppose supervised unsupervised learners what does that mean now you realize that for supervised learning supervised learning the X vector goes in and what And what comes out is a Y hat. And the data data, so this is supervised learning. Could you guys give an example of supervised learning? That we have learned so far what are some supervised learners. Cows and ducks. Yes, that's an example, classifiers and regresses. Those are your classifiers, regresses. The defining quality is you have to give a training data which contains the x1 x2 x3 etc etc and then you also have to provide what does it map to what is the target value right so the training data gives you a mapping from the input space to the output space output space is the y space of the target space and so all of supervised learning is to learn a function y is equal to some function fx plus epsilon this fx is close approximation approx to to the ground truth. The ground truth that generated the data. So this data that you have that generated the data. So I hope this is a summary of what we have been doing, classifiers and regressors. In the case of regressor, you're trying to predict the sale of ice cream on a beach. Well, you need some data. You need for a given temperature, for a given wind speed, for a given day of the week, how much ice cream, what is the historic data? How much ice cream what is the historic data how much ice cream did you send and one can use that now to create a generalize a function that generalizes the behavior between input and output learns the behavior the relationship or learns the function F which provides you a generalization on the other hand unsupervised learning has a completely different goal. Unsupervised learning learning there is only the x-vector only we only get the entire data space and there are no target variables these are unsupervised learning examples. So I'll explain to you what that means because this seems pretty mysterious. How can you just get that data set that gives you the input and there is no output. So the way you think in unsupervised learning is you do not think in terms of input and output. You say there is data and there is some pattern in the data. You have to recognize the pattern. So one word that you may use for unsupervised learning that I think would be good is pattern recognition and I always think of it like that. This is pattern recognition. The goal is to recognize some pattern in the data whereas the goal in predictive modeling is predictive modeling so in supervised learning is predictive so you will often find textbooks in machine learning since machine learning has become such a vast field we just which just deal with either unsupervised learning or supervised learning. So their entire books devoted only to predictive modeling, namely classification and regression. And some things in between, like semi-supervised, like recommended systems and so forth. And there are certain books which are related only to pattern recognition. It says, give me the data and we'll find some patterns. And I'll give you an example of what those patterns are. And why is recognizing pattern in the data a form of learning in itself? We'll come to that in a few moments now consider this example and so i'll just mention some examples of pattern recognition and then i'll explain them plus string dimensionality reduction. Later time and so forth. Anomaly detection. Anomaly and outlier detection. These are examples of pattern recognition. So today we are going to and I'll explain what those are but at some point today, but today we'll start with just clustering. What are clusters? So this is the scope of things. Typically books call this clustering and sometimes people call these topics or the things that do it, the algorithms that can do it, as clusters. yes like classification and classifiers are there so what is this topic clustering and clut and clusters to illustrate this imagine that you are looking at this data that you create some data points for you. Are you guys able to see the data on the screen? Yes. Suppose you see data like this and I've simplified it. What do you see data? What can you recognize about the way data is distributed in the feature space? Let's say that this is your feature space. This is x1, this is x2. There are two distinct clusters. Yes, what you notice is the data is not uniformly spread in the feature space. It tends to be centered around centered around these two clusters or chunks, the two chunks of clusters of data. And recognizing these clusters in the data space is one of the core goals in machine learning because you see pattern in the data and that pattern means something for example if it is a medical situation one cluster present people who are healthy if you have done some lab work and there are certain biological metrics biometrics then the other cluster might represent markers or may signify people who are healthy and another one may signify people who carry a particular disease. So data tends to segregate itself out like this. That would be an example. I'll take the example of this. Suppose you are a person who's never been to San Francisco. And somebody just drops you in San Francisco in the middle of the night or at night. Or maybe even in the daytime. You feel completely lost. Well, let's assume that you don't have Google Maps you feel completely lost and what do you do you start taking a notebook and you start walking around the streets and keeping track of which street intersects with street and suppose I tell you that I drop you in some area and I say go and find go find yourself a good restaurant to eat you're hungry generally it's very hard for you to determine which direction you should go in order to find food or the restaurants or things like that you don't know where the residential areas are you don't know where the parks are and so forth because you have a very local view of the city but suppose now that you are flying over san francisco and we will do that when we arrive on flights uh into the san francisco airport we often see the whole city lit up or in broad daylight you see and when you see the city what you see are very specific patterns you notice that the financial district all the commercial buildings are a large number of them are clustered around the financial district the waterfront isn't it the tip of the peninsula you also notice that there are residential areas slightly back into the peninsula deeper into the peninsula and towards the towards the waterfront towards the sea and then you notice that you notice the golden gate park lots of greenery lots of, which are all clustered together. So when you see these clusters, I suppose if I were to oversimplify San Francisco, you would say it is like this. This is the Golden Gate Bridge. and obviously this is an oversimplification you would say that the financial district seems to be something like this. This is the financial or commercial area. Then you realize that there is a lot of greenery which I'll just oversimplify and make it like this. Greenery. It's called the Golden Gate Park. And then you see these houses and you see those houses and you see that the houses form roughly speaking a region of their own this is a very rough approximation of San Francisco. But would you agree that this has some semblance to what it is? Isn't it guys? And so suppose you're looking for, you're dropped in some place and you have to go find your aunt or your best friend who lives in a house. And let's say that we drop you right at the Golden Gate Bridge suppose this is where you had dropped this has Suppose you're dropped at this location, the Golden Gate Bridge, and you want to go to the park. You know where to go or how to go. If you have to go to the residential area, you have an idea of how far you have to go to reach the residential area to get a sense. So in other words, recognizing patterns about a city has a lot of practical value. It's a form of learning because it expedites or it facilitates a lot of things you would want to do with the data. So this is the purpose of finding patterns in data, in this particular case, finding clusters in data. So we will limit access to clusters, but then it raises the question, how do we know, does data always come clustered? It may or may not come clustered. So for example, it is perfectly possible data won't be clustered so you could have a situation in which data is all like this So this is your rather homogeneous, mostly lacking, mostly lacking in clustering. Mostly lacking in clusters, isn't it? You can't discern or you can't see any well-defined clusters in the data. And sometimes it is so. For example, if somebody has just combed the beach, the sand on the beach, it's all uniformly set in a small box, in a child's play box. You don't see clusters of sand. But then after some time, children come and play, and once they are done playing, what you notice is, or as they play, you notice that all sorts of sand hills have been created clusters of sand are being created isn't it so so it raises the question that when we look for clusters how often do we find clusters in data so those of you who are new I want want you to guess, most of the data, like how often do you find clusters in datasets? So imagine the space of a million different datasets, something to do with automobiles, something to do with atmosphere, something to do with health, financial data, astrophysical data, chemical data, social data, political data data how often do you find genuine clusters in the data 10% of the time 12% of the time 1% of the time some of you who are new to this class give me a guess I would say 10% of the time. You think approximately how often do we see clusters? So let me pose this question. Let's say 10% of the time. Anybody else has a different guess? 98. 98. Somebody says 98% of the time Dennis 100% of the time anybody else 70 70 percent of the time. Anybody else guys. I'll give you the interesting thing about nature. And it has to do with the way data is generated it's in fact a very fascinating subject that relates to things like kiosk theory and so forth see whenever data is generated by any sort of equation that has even the least bit of non-linearity. Generally, but most of the time data when you produce, when the forces that produce data end up producing clustered data. So let me illustrate this with a very obvious example. You lie down in your backyard because that's I guess these days that's all you can do at night. In your go to your backyard and in your patio just lay down and look up. If you're lucky enough to see stars or if you remember seeing stars what do you notice are the stars uniformly spread in the sky or are they clustered mostly clustered in fact those clusters are what we call the galaxies and the constellations and so forth so that is a very important point the sky would be very boring if it was just a uniform distribution of stars but in all stars belong to clusters that we call galaxies our galaxy has a particular shape another cluster has a different shape and the galaxy is very different looking we belong to a spiral galaxy the galaxies which are different the more popular and things like that so they come across in shape if you just stand if you look obviously at any city you'll find clustering by design you know residential houses are clustered the commercial houses are just and so on and so forth. But go back 100 years and you see people, when they settle down on a land, on a new piece of land, it is human tendency to form clusters, their little communities, their little villages, and so forth in the towns. Cities are quite literally clusters on the map, of population clusters on the map of population clusters in the map it's everywhere if you look at the way the mail comes whether it is email or it is a physical mail into your mailbox you would agree that means come to us from a fairly independent agents across the world some marketer sends you an email there's a lot of that then spammers send you email and your friends may have a reason to occasionally send you emails and then your colleagues send you emails family sends you news if you look at the number of emails that you receive in unit time let's say a day or six hour hourly or something depending on the intensity of things you do you'll observe a very interesting phenomenon you'll observe that mail comes in bunches even though these are independent agents sending the email some days you will get a lot of emails or it could be Twitter messages or whatever it is you'll get a lot of them another day or another unit of time you may feel that not many people are remembering you are sending you emails you look at your mailbox one day your mailman will suddenly deposit the load physical mails another day you might not get any mail at all or you may get a very small number of mails or why is that so ultimately these mails are all coming from independent agents one would one would intuitively think that every day you should get a more or less uniform bunch of emails and yet it is not mails come in bunches you should get a more or less uniform bunch of emails and yet it is not. Mails come in bunches. You stand on a highway, on the overpass of a highway, and let us say that you don't have a congestion of the traffic is on the highway is free-flowing. If it is free-flowing one would say that you know cars should be uniformly distributed across the lanes and equally separated in any lane but actually it's not you if you ever stand in the overpass and look at traffic pre-framed traffic you notice that cars come in waves they come in bunches they come in clusters all the time in fact whenever you look look around you you'll see clustering in nature, in physical data, in biological data, chemical data, financial data. Everywhere you'll see clustering. So clustering is pretty much a given that most of the time it is there, unless there is a deliberate reason why clusters should not be there. is a deliberate reason why clusters should not be there. So for example if you take a complete still bottle of water, one can argue that inside it the molecules are not clustered, are not very noticeably clustered, they are fairly uniformly spread in the liquid or in a gas at steady state all the molecules are essentially not clustered. They are all uniformly bouncing around. So there are situations where clusters are not there in data but quite often they are there in data. With the same, for example, gas molecules, if you look at, and you plot the kinetic energies across, if you look at and you plot the kinetic energies across if you look at the space of kinetic energies before you'll again suddenly start seeing that they are clustered around certain values so that depends upon how you look at the data one way or the other you'll end up noticing some or the other form of clustering in the region in fact absence of clustering is one of the things that should always surprise you. It's fairly hard to detect a situation where there is no clustering. So for example, when people would tune their radio the old-fashioned way, if you are not tuned into a station, you would hear what is called static or white noise. Do you guys have that experience? I don't know if anybody still uses a radio. But if you use the car radio and you tune into a channel that doesn't have a station associated with it, a frequency that has no station associated with it, broadcasting, you seem to hear static and for the longest time people thought that that is white noise that is absolutely uniform noise there are no frequencies special discovered that actually it's not what we are hearing in that static is a very subtle a signal every subtle pattern in fact we we are directly hearing the proof of the Big Bang. So when the universe was created, the Big Bang that happened, the reverberations of the Big Bang as a microwave, as microwaves, is still there. And we are seeing the microwave background signature of the Big Bang so quite literally you are listening to the reverberation so in some sense the the aftermath of the Big Bang and it's very clear in front of you the proof that we had a Big Bang that created the universe it's very interesting so finding situations where there is no question is hard in one TV show which was numbers episode was that there was a crime situation in which the criminal was doing committing crime crimes with no pattern like locations had no pattern to it it was not localized in one place or the other and so on and so forth and one of the things that uh the way they solved the crime when the mathematician genius was there and he solved the crime by saying you know what there has to be a pattern number one and secondly the fact that he is doing it like this means he is trying to create a situation of no patterns the cl cannot and he used that information to dig deeper and then And he used that information to dig deeper and then posited that there is actually an underlying pattern. Discovered the pattern and then discovered the error and so forth. So patterns, that's the value of patterns in clusters. So with that sort of a background, let us now launch into understanding how would you find clusters? So we all, I hope hope agree that clusters are everywhere like you know with some rare exceptions data comes clusters so clusters are algorithms sir one question is there is that always a given that data always comes with clusters what if on the case of you should use the word almost always comes cluster so far you should use the word almost always comes cluster like an example if you're looking at the location of a molecules in a gas in a steady-state gas and then there won't be clusters or location of molecules in a Hello, are you guys hearing Asif's voice? No. Okay, so it's not on my end. I just want to confirm I keep losing his voice. Okay, so it's not on my end. I just want to confirm. I keep losing his voice. No audio, Asif. Hello? Hello Asif? I think he has dropped out. Hurry and join barbara he seems to be hello Thank you. Hi everyone. I just called him. He says there's some poverty issue going on and he'll be back soon. Okay. Thanks, Dennis. back soon. Thank you. It looks like Omkar is the host now. By the way, we have compiled some quiz questions and posted them on Slack. If you guys have time, just... Thank you. Okay. you Thank you. I think the attendance is lower today, right, Prachi? Sorry, Hari, what? The attendance is much lower today, I see. Yeah, I think we have what? 18 people? I said, what would be the percentage? The question that you had asked earlier, what would be close to 99.5%? Most of the time, we have clustering in data. In fact, if you don't see it in one way, if you change the situation, look at different parameters of the phenomena, you'll see clustering. Like, for example, if you look at the kinetic energy of the molecules then suddenly you'll start seeing different clusters one cluster for oxygen one for nitrogen and so on and so forth in the same uniform gas so clustering is there one way or the other it depends on what you're looking at rarely data presents itself without clustering. So generally your assumption should be there is clustering. Now it's a question of how poor or how rich the clustering is. Absence of clustering is very, very poor clustering. It's another way to put it. You should assume there is clustering and we'll learn the techniques, how to know the quality of the clustering that is a topic for today now comes clustering and detecting clusters so let's go back and take this example again suppose I have data points like this and you have data points like this in how many ways can you detect clusters? So the first method that you can think of, and we'll do three kinds of methods. One is k-means clustering and its relatives. So k-means is the original clustering method, very simple, the most often used method because of its share simplicity. It is to clustering. Clustering and we will also learn about a third method, third class of clustering methods called density based clustering. Asif, I'm sorry, has the recording been started? Yes it has been. Let me verify that. Yes, it started. So we will learn about three different types of clustering. We might not learn about all of them today, but we'll learn about them. So K-means clustering. K-means clustering actually solves half the problem. If you ask, you cannot ask how many clusters are there in the data. K-means clustering will say, I don't know, you tell me how many clusters are there and then I will go and find it. If you are wrong about the number of clusters that you expect to see in the data, it will essentially nonetheless partition the data into as many clusters as you want. So here for example, you notice that a good partitioning would be two clusters, isn't it? So you give k is equal to 2, it will would be two clusters isn't it so you give K is equal to 2 it will find the two clusters very nicely but if you give K is equal to 3 then it will start somehow forcing the data into three different buckets or three different clusters and you don't get a good quality clusters so a K-means clustering has the limitation. You must specify specify k, the number of clusters, the number of clusters to detect. It turns out that it's not as bad as it looks because there are techniques that we can use to even with k-means clustering by doing it again and again and see which value of K gives us good clusters we can actually guess what should the value what is a good number of clusters to see in the data but for now we will just say this is a situation so how do you solve it Kim's testing actually starts with these assumptions it says pick in the simplest form pick okay initial points as cluster centers or centroids the word is central and you'll see this word centroid used in the literature so let me also use the word sin lester saying as clusters and droids and then be a sign all points to their nearest nearest centroid this is and the C part that comes up is quite interesting find the center of gravity find the new centroids each in each cluster. But before I go there, let's understand what is a centroid. So suppose you have a bunch of points here, and you ask what is a centroid here? Centroid is the center of gravity, you can say, or center of mass. So for example, if all of these points, you think of them as marbles, and they all weigh something, then you intuitively realize that there is a center location, a point, or it may or may not be a point, but it's somewhere there, right in the dead center, which is the center of gravity. So that if you were to put your finger there and hold all these points up, it would stay at equilibrium, sort of like that. If you think of all of these points as a sheet of paper or the marbles on a sheet of paper. Is the centroid is the center of gravity? theta. Is the centroid is the center of gravity? Centroid, centroid is the center of mass, if you think of each point as having some mass point sorry as having a finite uniform mass uniform uniform mass right so that's the meaning of this. So now that we have set the stage, let's do it. And the last stage is repeat. Repeat B and C over and over again. That's the algorithm. So it is very intuitive when I explain it to you and you'll see that. So let me start with some points. So suppose I randomly pick two points. Which two points would you like to pick? Suppose, you know, if you're lucky, we'll end up picking a point. One point will belong to one cluster. Let's say that if you're lucky, we might end up with a situation like one random point we might pick up as this and another random point we pick up as this. They belong to two different clusters. But let us say that it is not the case. So let me go back and say that actually you ended up picking accidentally two points that are actually fairly bad. Let's say that you ended up picking these two points. I'll just circle circle them suppose these are your initial guesses let me just mark them as the initial guesses at centroid C 0 these are your initial centroids These are your initial centroids. And now what you do is you assign to each point the nearest centroid. So you ask yourself which of these centroids should it belong to. So if you look at these points, this point is closer to C1, this is C2. So if I were to draw a decision boundary for C2, it would be not a decision boundary for C2 it would be not a decision boundary a clustering boundary I would say that this is the space for C2 and this is the space for C1 is this a good belonging like assigning each point to the nearest centroid? Would this be a good separation guys? Would you all agree? Is it obvious what I did? Anyone? Yes, sir. It's pretty obvious, right? So this is a step number one. Step one is pick step number one. Just pick some centroid step two is you assign it now we come to step three the crucial step thing which says that for each of the clusters find the new centroid so let me use a different color now yeah let me pick a different color what is the centroid of the c1 cluster would you agree that the centroid is probably this this guy so this is c1 one use the word C0 C1 would you agree with that and if you look at this perhaps the centroid is something like would you say this is the person approximately C1 2 in the second iteration in the first iteration here this is the centroid of the cluster here would you agree guys yes this is it and now you play this game again this time when you play this game you'll realize that all of these points belong to c1 C1 prime and all of these points belong to C2 prime so that's beginning to look pretty good there now let's do this game and so the whole idea is that was step 2 now repeat this process what is the centroid of all of this where the centroid of all of C1 prime cluster, green cluster, is pretty much I guess not likely to move much. It will remain more or less the same. But for C2, so let me just say that this is still the same. This one probably will become this. Let me call it c prime prime two and this is also c prime prime one right and now you realize that your cluster now what is the journey of the cluster if you look at the journey of the cluster this give me a moment so journey of the cluster is this cluster it started from here it went to this and from this it hopped to this and pretty much stopped. C1 didn't migrate much. It went here and it sort of stayed here. But then what happens is after a few iterations, here about three, you notice notice that the centroids are not moving much and and the other observation that you have is at points data points are not migrating from one cluster to another do you notice that it's a very simple observation with a we just went through two three iterations and then we notice that but we have actually found the central and the clusters and once you have done that you say okay we are going to stop so in this particular case the stopping point would be a third iteration and so you would say the blue cluster is it this cluster is it and this point is your main point. And this point is where you stop. And this is your cluster. And thus, you have discovered clusters. Are we together, guys? Was that easy? Yeah. I hope that was easy. That was straightforward. So this algorithm is very important. It's k-means clustering. And it is surprising that we can find clusters with such a simple algorithm, especially in view of a different result, which is that you ask yourself this question. When we take computer science, we sort data. You know, we search for things in data, we sort data. And we search for things in data we saw a sort data and we learn the bigger notation that says okay you can do sorting and searching in order and you know order n square for bubble sort order n log n for quick sort and sometimes linear search order n and so forth impressive but you always find a fully sorted order of elements. So people wonder that can we not find clusters in a very straightforward way? Why is it an exercise in machine learning? Then comes the result which says that actually finding clusters in an absolute way deterministically is a nearly impossible problem. In fact it is considered to be at least NP-hard. Now when a problem is NP-hard means it can only be solved in non-polynomial times. At least it is that hard. It's a computer science or algorithmic sort of language so in that language you say it's a very very hard problem to solve and there is a reason for it because what you call clusters it depends upon how you look at the data so I will illustrate this with a point suppose you have data here And then you have another gesture here. Let's say another gesture here. And to add to the noise, we can add a situation like this now imagine that obviously there's some outlier points here and there so the question is how many clusters do you see if you look at this data how many clusters do you see well it's hard to say you can argue that the entire thing is one cluster so this is your case equal to one there is just one cluster you can alternatively argue that there are actually two clusters. They are this and sorry they are this and this. Do you see that k is equal to 2? You can continue to argue that no actually there are not two clusters you see actually five clusters, right? one Two sorry two three Five right. Do you see this guy's K is equal to five Are we together right but you see this guy's K is equal to 5 are we together so there is a interpretational aspect and the resolution aspect at what resolution are you looking at the data are you looking at it macroscopically are you looking at it microscopically and but then you can go even further if you so choose you can go even further if you so choose you can go and argue that within clusters are clusters so this is one cluster or within the cluster right and things like that and sub cluster why is this a sub cluster a sub cluster cluster so therefore because because there is further clustering inside a cluster it is not determinate it is hard to it is fix a number you see that and that is what makes this problem computationally NP-hard. You can't do it. You have to use probabilistic methods. In other words, machine learning methods. K-means is like that. It will randomly start with two points and it will do clustering. And if you start from different points, you might occasionally end up with completely different shape of clusters. It's the nature of the beast. So that is something to know that clustering is a machine learning. It's being non intuitive. See the human eye looks at a beach and immediately spots the clusters of people. It looks up into the sky and immediately spots the galaxies. The human eye is a marvelous computation machine. It instantly detects clusters. We look at data and we don't have to be told. We just look at it and even without education, without any training, we can tell, oh, we are looking at clusters. This must have been evolutionary even when our great ancestors were fishes in the sea. You know that fishes roam around in schools, clusters. We would notice a cluster of fishes, a school of fishes, one species of fish, another species, and a big whale cluster and so forth, the family of whales will die. So cluster is something we learn to detect in our neural systems a whole long long time ago but for machines in fact for the human optical system the estimate is that it has gone through close to at least a hundred million years of training I think it's hundred million not hundred billion yeah hundred million years of training of the if you think of it as a machine learning algorithm, it has gone through 100 million years of training and evolution. On the other hand, when we try to train machines and we try to do clustering, obviously we are dealing with much more primitive things. We don't have that level of sophistication in our AI machines. I'm just putting it in perspective because these days it's very fashionable to say that the singularity is there and the words are going to take over or AI is going to take over. AI is at least a hundred years from doing anything whatsoever intelligent. The joke is that AI doesn't need to really be intelligent even as dev machines they are taking over which is a different thing. So that is k-means clustering. One question that arises how do I know given the fact that k is not specified how would I know what is a good k? So what is the metric? Just like in regression, we have the r squared and all of those measures, mean squared error. What is the, and classifier, we have the confusion matrix. What is the metric in clustering? And there are many metrics, but one metric case that once you create the clusters what is the within cluster sum squared distance very interesting so what is a within cluster sum squared distance let us say that this points form a cluster. Then what you do is you take any arbitrary point xi and every other point xj. And you find the distance between them. Distance xi, xj. between them distance X I X G and square the distance and you find the sum of all the I J belonging to the cluster let me call this cluster C 1 or just C so this is very interesting you have to find every pairwise distance. In other words, every possible link you have to compute. And I don't know, did I miss something? This looks like a fairly complete graph. So you have to find all the pairwise distances and square them all. And when you square you square them then you divide it by, there's a normalizing metric, right, people often put a normalizing metric, how many points there are and so forth. So this is called WSS. Within cluster some squared distance. Let me write it. So for this cluster one, let me just call it C1. Within cluster sum squared distances so now what happens is suppose you have detected three cluster or three clusters in data c1 c2 c3 what you will do is for your entire clustering algorithm you compute the overall overall or total by saying WSS1 plus WSS2 plus WSS3. Or more formally, suppose the number of clusters is whatever it is, K, you would write it as a summation over K of WSSK of the Kth cluster. And obviously you add add them up and if you substitute the original value you would come to the conclusion that this is summed over k summed over i j within a cluster right and a d a d squared xi xj right any two points and then over each of these are appropriately normalized and you can get the normalizations first so this is your uh i might be a little bit off in my expression, but this is the intuition of behind the WSS. What can you do with this WSS? What you can do actually is you don't know what K is. So you build something that is famously called a Scree plot. Scree plot. You take different values of K, K is equal to one, K is equal to 1 k is equal to 2 3 4 5 and so forth and what you do is for each of them you compute the wss total you compute the wss total so what you will notice is that let's say that WSS for the first one is this, for the second one it will be like this. So then for the third one it will keep going down, WSS will keep going down. So if you connect these dots you will get a curve that looks like this is called the screen plot and what you look for is an elbow can you guys detect an elbow at what value of K do 2. And so in this data point you would say that the optimal number of clusters is equal to 2. k is equal to 2. This would be your optimal number of clusters for this data so that's how your WSS will keep on decreasing towards zero as you find more and more cluster in the asymptotic limit where you take let's say that number of clusters is the same as number of points well each point is a cluster so it is not far away from itself WSS will be zero so this keeps decreasing but you don't look for the minima you look for where it is event or more practically speaking you know you look for the point of diminishing returns at what point the returns are the the the decrease in WSS is not much afterwards and that point is around case so this is called the elbow method it is often called the elbow method of finding just just account so that's what it would be. Now, so that is all there is to k-means clustering. You can determine k-means. So now what it means is it's problematic. You need to keep running k-means clustering again and again with different values of k till you latch upon the best value. So now I'll go back to the old question. What if there are no clear clusters in the data? So here the elbow is clear when the clusters are not globular. And I'll explain what it is. They are not globular. Then screen plot does not get, does not show a clear elbow elbow what does that mean right so let me explain that is very intuitive actually so suppose you don't you know that you can keep trying different values of K what will happen to WSS is that it will just show so can you guys detect a elbow in this in this curve no very clear it's very clear so very poor cl So very poor clustering, very poor. So cause can be causes, causes, very poor clustering. That could be one cause or no Think of it as roundish, right? Nice round convex surface is globular, like a globule. So when it is not, for for example if your cluster looks like this non-globular or non-convex non-convex in such situations or non yeah in such a situation what will happen is um that k means fails. K means does poorly as an algorithm. So this is it. And the third thing is that if you get a situation that is like this, more or less a straight line may indicate may indicate may indicate you more or less homogeneous data there's data without clusters so you see that clustering is so weak that it's more or less homogeneous data. Or non-globular clusters. Very strange shaped clusters. Are we together guys? So it is not. And so this question was raised what if there are no clusters. One clear indication from data is something like this and I have seen in practical terms, I lead a fairly large in my day job, I am the VP of R&D group, in the true sense research group. So we do a lot of analysis of data and it does occasionally happen that a data scientist will come and say that you know I cannot clearly see what how many number of clusters to do using k-means clustering and what do we do and the general idea is that you ask yourself are there clusters in data or is k-means the best means to find it your next course of action usually is don't use k-means it is probably not a good algorithm go try other algorithms if all algorithms say that there are no good clusters then probably data is uniform and of course by taking samples of the data you could quickly check if there is a homogeneity in the data so it is an exercise worth doing it keeps coming up all the time so and the reason I mentioned it is that most textbook they over emphasize k-means k-means is very simple to understand and one can understand why people just talk of k-means. K-means is very simple to understand. And one can understand why people just talk of k-means. And then they talk of hierarchical clustering a little bit. And actually, most of the modern methods of clustering that one should use are almost never mentioned in textbooks. It's a very peculiar situation. What you would actually use in data sets the sort of algorithms are quite often are not discussed in the data k means works but it works only with roundish clusters and clear very well defined clusters there are situations where clusters are well defined but they also large class of situations where actually clusters are very peculiarly shaped and you need different algorithms to find the clusters. So let's go ahead. Would you mind giving some examples of some practical examples where k-means clustering is helpful and where you see We will literally do that example in our labs okay so next time right we will really deal with that so for example if you look for example galaxy data stars data generally k-means works for discovering a galaxy more or less like our galaxy a spiral but you can still shrink wrap it in a in a hull in a sort of a globular shrink wrap but there are galaxies whose shapes are very linear you know pencil thin and that don't show the kind of structure the globularity that you hope for then it doesn't work and For example, when you look at intergalactic cloud and so forth look at the very beautiful picture You know in the cover of the notes that I've given you are the pillars of creation I don't know if you guys know about that Yeah, so one of the most interesting things is I give the data and I ask people usually when I am Interviewing people who have PhDs and You have to see whether they are able to think you're not hiding them for just writing that ton of code Not being able to call you hiring them for their ability to think and reason through so sometimes I get that Okay, how would you find clusters in this data? If you look at the data, it's very interesting. It sort of has clusters, you know, the two pillars of creation, the big pillars, there are. But if you were to try traditional clustering algorithms, they actually don't have complete success they have certain degrees of success and that's an example where these things do work and by the way the history and let me take that as a point to go into the day the spinners of creation of a favorite for a historical reason if you are in astronomy you probably know that anybody who has done astronomy. One of the great milestones in astronomy was the Hubble telescope. Some of you may have heard of the Hubble telescope. In my generation, it was a very big thing. When the Hubble telescope was known, it was this giant, massive telescope that was put out in orbit in the Earth's orbit. So free from the atmosphere, it did not have any of the noise factors. And so it could look into deep space very, very clearly without disturbance. And one of the first things they wanted to see is this picture, which astrophysicists and astrophotographers love, the pillars of creation. And the picture itself was so startling, so rich. A lot of people believed that the entire investment in the Hubble telescope essentially paid for itself on the first day. In the data that it gathered on the first day. Astronomy as a subject would never be the same. With the Hubble we discovered the microwave background, we discovered many, many things. Just one telescope led to countless discoveries. And the picture that you see on the cover of it was the last picture the Hubble took. So when the Hubble was being retired, one thing that they did is they decided, let's go back and look at where it looked as the first picture the pillars of creation and it looked at that and the picture that you have on the cover is the high resolution picture one is in the invisible spectrum the other one is in the x-ray spectrum two different spectrums so you see two different sort of data coming out of it and it's very interesting uh play with this sort of data and you can see so not always are clusters wobbly our intuition thinks of clusters as globular roundish but in practice it may not be so okay so with those words let's take a quick break. It is 830. Is it 830? It is 833. Let's get a 20 minutes break. So 33, 43, 53, 53. OK, let's let's meet at five before nine, five minutes before nine minutes. Would that be as if could you please scroll up to the within cluster sum of squared error calculation? By the way, let me check the exact formula. I've given you the intuition and the exact formula. This dividing by 1 by n, I might have made a mistake. Let me check that. Within cluster sum of squared distance. So the idea is for each cluster you find this wss for each of the clusters and once you have found the wss for each of the clusters now you go and add up all the wss so what i don't understand is this distance you find with the centroid that you have chosen to the point of any points in the cluster or to any from from any point to any point pairwise point it is not from the centroid to each point it is a pairwise point so that distance must be minimum to that what you do is that gives you a loss function or something to minimize and you try to find the cluster that clustering which minimizes that loss function WSS is the loss function is a traditional loss function for testing there are other things that you can formulate but this is it let's take a quick break and let's gather right after that five minutes to 20 minutes later five minutes to nine pause the recording So, just to review K-means Clustering. K-means algorithm has a basic premise. Start with a few data points, call them the real centroids or the initial centroids, not real centroids. But then go through a cycle. The cycle includes two steps. Assign each data point to the nearest centroid and then assign, then you have new centroids, you have new clusters. Once you have the clusters, once you have assigned all the points to at least one, I mean to one of the centroids to the respective centroids recompute the centroid and once you have recomputed the centroid repeat the cycle of assigning points to centroids and recomputing the centroid and as you iterate through this process you'll converge you can wish to a stable solution where you'll notice that centroids are not moving. They are not moving much and the data points are not changing loyalties between centroids. They're not migrating from one centroid to another. When you reach that point, you know that you should stop. The other point I mentioned is that why is it so hard to find clusters in a deterministic way? Partly because how many clusters you look for in our data is not clear. It itself is a judgment call. For example, I took this example. Do you see one cluster, two clusters, five clusters? So how do you see their clusters within clusters? So it's a hard problem. We talked about WSS being the metric within clusters, some squared distances. These are the pairwise distances between any two points in a cluster. We talked about the scree plot. You make the scree plot and you look for the elbow, which is why colloquially it's called the elbow method. And then we talked about the fact that in the scree plot should give you some insight. Do you have well-defined globular clusters or not? If you find no elbow, then maybe there are no well-defined globular clusters, which means that either they are no clusters or that they are not globular or roundish in shape and in extreme case when the clusters are completely missing but then it is like a straight line you know you don't see it's just uniformly coming down WSS will come down so this is a k-means clustering it raises the quick I said how will you do the outliers here how will you approach the outliers in this for the k-means actually it doesn't do deal with outliers that is its limitation it insists that every point belongs to a cluster okay there's an outlier it will say no you belong to this cluster or if you were to take the example of somebody living far off between two cities which are really really far off let's take an example I'm told that if you are traveling to Wyoming the distances between cities can be as much as 50 60 miles 100 miles. In such a situation, actually somebody once told me that if you are coming with a truck, a big, you know, one of those 18 wheelers or 16 wheelers, you're tundering down one of the interstates, somewhere near Wyoming you will see, or somewhere around there, you will see a billboard for a restaurant and a nice recreation stop which is far away in either the state of Washington or Oregon I forget which you can imagine how barren there's one so who's you are in one of those places and very far from both the cities, cities that you're left in the city that you're going to. You would argue that a little hut or a little cottage in the woods really belongs to none of the two cities. It's a stretch to call it to belong to the cities. Key means clustering on the other hand, really insist that it belongs to one of the cities, which is its weakness. Take another clustering algorithm. This cluster. But the only motivation for me to ask that is because if somehow we can identify that, then the number of repetitions that we have to go through to come to the correct K would be drastically reduced. That that's why I was asking that it's a good point actually people do that so see the K means the basic K means has many improvements that people do one of the improvements is they will take they won't take randomly let's say two point we are doing K is equal to two they won't take randomly, let's say two point, we are doing k is equal to two. They won't take any random two points from the data. They will take a random point, and then they will find a point which is furthest from that data point, as far away from it as possible. And they will take that as a second point. And generally that converges faster, because if you want to think about it, if you go back and look at this cluster if I take a point here this actually if I take a point here this point and the furthest from this point would be this point yeah and so it's not visible sir oh my screen is not visible screen is visible but cursor is not visible my tool is not visible let me make this suppose I take this as one point the point for this from it would be this point the one that I marked red and imagine the clustering will be very, very fast, right? In one or two cycles, it will converge. You're already near the correct answer basically. So all that will happen is this will move maybe one step here and this will move one step here and you'll be done in one cycle. So that is one. The other thing people do is they look at the distance point from the center of the cholesterol the cluster after one cycle they may look at some optimizations and they may actually not count points that look like outliers and so forth so you know k-means clustering has a wide body of adaptations and improvements and there's a vast literature one of the things is you know we are assigning point by point to each centroid which is very very inefficient one thing people can do is can you assign entire regions to it so for example if you look at this point and this point you can essentially say that this entire region belongs to this point red point isn't it so fine entire regions and to do that there is a very lovely bit of specialization of kings they look at some data structures called KD tree dimensional trees-dimensional trees. They break up the feature space into a tree-like partitioning. And in a partition, there will be many, many points. And they assign an entire sub-region to one of the two centroids. Happens very fast. KD tree was one of the first acceleration of K-means clustering. Otherwise, it's a bit slow. And then people did a lot of other work. Instead of KD tree structure, they started using cover balls and so on and so forth. And then there are so many adaptations for it that people started creating metric trees, the whole class of such things in which you divide the space into regions. Okay, thank you. A lot of very interesting body of work but see today is an introductory class on clustering so we are just sticking to the big highlights. We are not going into specific, I mean there's a lot as you read it you'll be, you know, let me put it this way. There are people whose entire life is to research clustering methods. So think about that. There are many, many PhDs. And if you really want a PhD in a hurry, you can write one more clustering algorithm, show that it solves some sort of problems that other clustering algorithms cannot. And well, you have a winner okay yeah it's a very fertile field actually so the other form of clustering is hierarchical clustering it is in your book so I'll mention it let me just say hierarchical or agglomerated let me just say higher people or agglomerated you know clustering agglomerative clustering means it's a bottom-up approach it is also typically called high question it is actually computationally very brutal what it says is that when you're given points, lot of points, what you do is you create every pairwise distance. So create this distance, XI, XJ, right? And you create a whole list of these distances. Then what you do is you sort it in increasing order increasing order of pairwise distances so what happens is two points which you first find those two points in the data set which are just close us to each other are we together and now here's a here's the rule that you said in the beginning declare each point of data to be its own cluster. So suppose there are n data points. How many clusters are there? So number of clusters, number of clusters initially is equal to number of data points because each point is a cluster then what do you do you you create this list for that all the time you maintain a list of distances and in the list find the beginning once you have sorted it by distances pairwise distances find the two points which are closest to each other and what you do is you go and you merge them you say that these two points let me just call it one and two you merge them and you merge point you call a cluster now so because you create your first basic cluster let me call it C 1 2 by merging points 1 & 2 Then what you do is Once you have done that now there is a problem 1 & 2 in the list that you have the X I XJ list that you have the X I X J this list will contain things like D X 1 and something or the other and D X 2 and something or the other all of these things deleted from the list and the only thing that you do is compute the distance between D C 1 1, 1, 2, the cluster, and every point. Every point. So you treat this cluster as a point somehow, right? You say that you have merged the two points or illuminated it into a cluster. Likewise, you keep doing, and you build a data structure. Sometimes three points will be equidistant. You merge them. And then you keep doing and you build a data structure. Sometimes three points will be equidistant and merge them. And then you keep on doing this. And then what you do is you end up creating less granular clusters. And those clusters, again, you merge them and you merge them. And then you have, let's say, three points here and you merge them. And then what happens is that you may merge these two and then these may be far off and you may merge these two so you keep on merging as clusters together right you end up with a structure this is called the dendrogram These are well, this is a well celebrated visualization in machine learning, pattern recognition. In many areas, you'll see dendrograms come in. In agglomerative clustering, this is this sort of a picture. If you think back and look at New York Times, or you look at any of those data visualization places textbooks you'll always recognize somewhere or the other a picture like this is a dendrogram and program this dendrogram is obviously what it says is that we are merging points into clusters and clusters into bigger clusters into bigger clusters and so forth. So suppose you want to have k is equal to 1 let us say. If you cut here how many clusters do you see k is equal to 1. If you want two clusters you can cut here you'll get k is equal to 2. So this cluster in this cluster I mean this cluster rather this cluster and this cluster i mean this cluster rather this cluster and this cluster will be k is equal to two isn't it if you want to have k is equal to three then you know you just keep changing your cut points sorry this is k is equal to one will give you two clusters sorry two clusters this cluster and this cluster is k is equal to one okay sorry this is k is equal to two k is equal to one is here at the very top this is k and v because there are three lines cutting this horizontal line so three and likewise you can keep on increasing and what you notice is that the you can the the question of what is the best cake. What is K, you can solve it in a very visual way. Right you can look at it and see what number of clusters do you make sense? And it also answers the question that sometimes they are no perfect answers. So, especially in biological domain, sometimes you say, can you basic the data partition the data into two clusters and then another person will say in the same data and say for me it will be more useful if you cluster the data into five clusters so when you have built a dendrogram like that it is very easy you don't have to recompute it has basically built all the way to K is equal to n it has done every possible clustering that you can imagine so this algorithm is computationally intensive It is not a big deal for modern computing systems. For example, if you have a fairly powerful computer at home, this can be done very rapidly. These days it's not considered a big deal unless you have gazillions of ones. So then it does become computationally brutal. In the days when the computers were not so powerful and people really needed the dendrogram in many fields, it was considered rather intensive computation. Now it's not a big deal. So long as your data sizes are reasonable, but still more computationally intensive. So that is dendrogram. But it leaves one question unanswered. One question. One question remains unanswered. To be answered. Which is, what question is that? When you merge a cluster, let us say that I take these three points and here is a point. When I merge this cluster, let us say at some point I merge this cluster. It becomes this. What is the distance of point X I to this cluster so the distance can be many distances we can define distance to be the distance between the shortest the shortest distance between X I and the and a point in the cluster it could mean it could be defined as the largest distance between X I and this cluster it could be defined as the distance from this to the centroid distance isn't it but you see guys that we can define the distance of a point from another cluster in multiple ways and likewise the same thing can be generalized suppose I have two clusters like this and I have two clusters that are emerging I need to find the distance between clusters to do this agglomerative process so what is the distance once again the same reasoning applies do is there is the distance reasoning applies. Is the distance the shortest distance? Is it the largest distance? The largest distance could be possibly maybe, sorry, let me not use this color. Sorry, we cannot use this color. If you use this distance, maybe. Is it this distance? Or is it a distance from the the center? Okay, actually, I should have used green because in the other picture I used green. So let me say this. Is it this distance? Or is it the distance from the centroid to centroid? So each of these definitions you can argue is good or bad based on your situation. And so people in the industry, they use a word for it. These are called linkages. The definition of distance between two clusters is called linkage. It's called linkage. If you look into the chapter 10 of your book, it will talk about the different linkages. So linkage, linkage or the distance definitions could be shortest. Distance. Distance shortest pairwise distance. Largest. Distance then it could be. Also, average pairwise distance. It could be distance between the centroids. And so forth more and more and other ways and other ways generally choice of depending on the data the choice of linkage function definition of this does affect your clustering and generally you don't know which one is good you build a dendroogram you look at it and say you know for my situation this looks better so that is how you deal with the linkage functions and sometimes it is a people make it a hyper parameter of the model they look at the pick a linkage function they're looking for 10 clusters let us say and then they will find the WSS or some metric and they'll try to optimize it and see which linkage gives you better. Or they'll visually inspect it, bring in the domain experts and say, which one do you like better? Which one makes more sense to you? And they can answer that. So this is agglomerative clustering or hierarchical clustering. So k-means and agglomerative clustering are the classic I would say the clustering methods. They have been there for a very long time. They certainly are there in all the textbooks. All the textbooks have k-means clustering. Some, a fairly good proportion of them have hierarchical clustering, though not all of them. And therefore, these are the two classic methods. So before I proceed, are there any questions? Is this clear, guys? By the way, chapter 10 of your book, go review chapter 10 of your book. You'll see it there, go ahead. Can clustering expose confounding variables in the data? No, it doesn't do that oh so you just don't know how the data specifically behaves but you can separate them distinctly that is right it's a simply pattern recognition cluster recognition so it is silent on the question of are they confounding variables silent on the question of are they confounding variables. Can you go over again how to find out the k looking at the render? So what you do is you maintain a long list of all pairwise distances between the clusters. In the beginning each point is a cluster right so what you're looking at is the pairwise distance between the points and so what you do is always at each step find the shortest pairwise distance and you merge those two clusters into a bigger right and you keep on repeating this process right when you keep on repeating this process the only thing that is unclear is what is the definition of distance between two clusters that is called the linkage function? It or linkage simply so you can look at the shortest pairwise distance, you know Take a point which is in one cluster another point in the cluster, which is as close to each other as possible You can take the furthest pairwise distance You can take the average pairwise distance distance between centroids and go about thinking more ideas you can do now in this that is had a people question does that make it clear so now one thing is there is a mistake that we make look at point one two and three four make look at point one two and three four now compare a two to three and four is is three closer to two or four is closer to two for new students this is a puzzle is three closer to two or four closer to two you make a guess anybody make a guess how would you looking at this picture I think I think it's hard to tell because you see the relationship of three to four and then they were clustered already much yesterday me there shows that they're supposed to be related between two into and four but you can tell three and four are close so what happens is that when you compare the distance of two to three and four you just because three in this diagram looks to be closer to two doesn't really mean that they are closer you say that as far as the dendrogram is concerned, a two is equally separated from three and four. Why is that? You look at how many links later do they meet? Does three meet two first or does four meet two first? But actually three and four get merged into this and two gets merged into this and they both meet here at the same point. Three, four and two gets merged into this and they both meet here at the same point three four and two meet at this level so you would say the distances are the same so the so this is a fallacy to remember in a two-dimensional representation because it is a two-dimensionally uh the dendrogram is it often gives you the misconception that it often gives you the misconception that things that look on the picture close to each other are closer. But the way to look at distances is to look which meets first. So for example, two and let us say that this point is five and six. So if you compare the distance of two to three and two to five, right? So if you look at this, then you would say that three is closer to two. Why is it closer to two? Because three meets two right here itself, whereas five meets two here, way up here, right? At a higher level. So if it meets at a higher level it is further away from two so that is the way to interpret a dendrogram I mean to be energized so that's what a dendrogram is and this process is called hierarchical or the agglomerative testing. Agglomerative means you call these points together. And then you build it up. So Asif, you said that this is computationally intensive. How much data are we talking about when the computation is not practical to apply anymore? I like when the dendrogram is not practical to apply anymore. Like 100 cases, or how many? how big a data are we talking about? See, it depends on whether you're doing it on your laptop or with 8 gigs of RAM or whether you're doing it on a powerful workstation or you're doing it on a cluster of machines, like a server cluster, big data cluster or something. If you've server cluster big data clusters if you got a big data cluster or something like that with a distributed computation you can have a billion points and it will finish Monday if you are doing it on the laptop then even a million points will strain it even hundred thousand points may really stress the laptop so it depends upon the computational hardware available these days we don't consider these classic methods to be very expensive there are other methods which can get more expensive and so forth but on the scale of things you just consider it as business as normal if you have to do it you do it On the scale of things, you just consider it as business as normal. If you have to do it, you do it. There is another thing that people do. Sometimes they don't want to cluster the whole data. What they will do is they'll take a sample of the data, cluster it, and then assign all the points, the remaining points, to the nearest clusters. Are we together? So that could be an alternative that people do do we just don't if you have a billion points and you must do it on a laptop because Hamper and just to them and then suppose you have to find K is equal to six clusters then each of so you take you just cut it at that level six find the centroids and then ask the remaining points of each of six find the centroids and then ask the remaining points of each of the clusters or centroids do they belong to and assign it to them so people have fun clever ways to work around the computational aspects of it but a by and large today because hardware is so plentifully available you don't you don't worry about it anymore but just don't just don't go with a billion points of your laptop. It will just bring it to its. Even a 1M points don't do it on a lot. Take a sample. That's what I would say a quick question so you said the canines of NP hard is the hierarchical and we have our NP complete actually no clustering itself doesn't mean came in or not you came in is not NP hard but if you want see, but remember that machine learning algorithms are probabilistic algorithms. There's always an element of chance to it, of the clustering. But if you ask this question, so forget machine learning approaches, any of these approaches, forget it. You ask this question, why is there no absolute guaranteed way of clustering? That will always find the right number of clusters in data? And the answer to that is nobody has found such an algorithm. And we know that even if such an algorithm is one day found, it is at least an NP-hard algorithm. So nobody has ever found that algorithm. And nobody expects that algorithm and nobody expected that algorithm instead what we do is these are these methods you know the probabilistic or the machine learning methods to find clusters and it sort of goes to answer the question we can sort we can search why can't we cluster and the answer is clustering is an entirely different kettle of fish it is it needs machine learning to do it not traditional algorithmics. I, because I, I thought the least level you're doing. Like, they are choosing and choose to kind of what you call any to N minus 1 by 2 kind of a pair for compact. And then the next level those clusters are then compiled against each other and recursively so and so forth yeah see question will give you an answer it's a machine learning method the problem is what is it that you're optimizing at the end of the day you have a loss function and you optimize a loss function and you find a reflection point right sometimes you find good inflection points and you find an elbow. Sometimes the elbows are not visible. What do you do then? So it is not a guaranteed method that guarantees convergence. If you're sorting, convergence is guaranteed. But in clustering, there are no guarantees. None of the algorithms gives you guarantees. None of the algorithms gives you guarantees. Right? OK. VATSAL SHARANAMANIYARANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANANAN that they say you must solve it. And usually that's an odd approach because the right approach is to take the data, let the data scientists find something in it. Once they have found something on it, then to build a business around it. But when you already have a business problem and you hire data scientists to solve it, it may or may not be solvable. No, the quality of the solution may be quite variable. And then people have, and I've heard it, they say that I thought I hired a stellar data science team. They all have very good pedigrees, but they're absolutely useless. They don't seem to be producing results. And many, many companies, you hear this thing, oh, data science team is, we are unimpressed with the results that produce usually if you look at it it may be that they hired these days anybody who can draw a line through two points call some simple data scientists so if you leave so there's a lot of you know fakers or a very rudimentary data scientists when if you filter them out if you look at good quality data scientists is still there are no guarantees to success of a problem because ultimately data is king so for example if I ask you can you tell whether somebody has diabetes or not based on the color of issue and the color of his pant and the color of his shirt probably you may give you all the data you may give a lot of data you give a lot of data big data but it is useless data there is a very weak signal if at all right maybe you can go so far as to say that if you look at the size of the shirt in the pant and the shoe you can conclude that there's some relationship between diabetes or not because younger people don't have that. And so on and so forth, so you can do something like that and younger people tend to wear much more bright colors compared to middle age. Very weak signals you can do, but there are a lot of middle aged people who do have diabetes and a lot of middle aged people who do not have diabetes and a lot of middle-aged people who do not have diabetes so your project actually may not succeed right the reason is then you can have a disappointment that see I hired all these bright people and they could find nothing they could not solve the problem but another way to ask this question is was that problem even solvable because this is a classic example of close to null hypothesis none of the predictors are useful you see that right and that is the probabilistic nature or uncertain nature of machine learning another definition of machine learning is a machine learning is coming to conclusions or predicting or recognizing pattern when there is a very high degree of uncertainty information is not perfect it's very uncertain information one reason that is so useful these days in medical sciences as you know the big frontier these days in the next 10 years is going to be the medical sciences the biological and medical sciences because biological and medical data tends to be very very squishy soft data the human body is insanely complex as some of you some of you are doctors here physicians here you know that they often teach you at least they used to that the human body is the terra incognita you know it's the unknown territory there's too much that we don't know there is a certain amount that we know and research is expanding the frontiers of what we know but they are also a lot of things we don't know there is a certain amount that we know and research is expanding the frontiers of what we know but there are also a lot of things we don't know so what happens is every time you build a model or you make a prediction you're working with uncertain insufficient information there's always uncertainty when a doctor makes a with a physician makes a diagnosis you realize that you can go and get a second opinion and it may differ sometimes the reason for that is they're all making in from that diagnosis based and weighing the information they have which is always incomplete information so that is something to bear in mind and that's a frontier. There's a huge frontier for machine learning which is why machine learning is algorithms now are being considered an adjunct to physician. So what people are realizing is that machine learning is very good at coming up with let's say five possible diagnosis for illness and they will find a common diagnosis and they will also find likely. But rare diagnosis, you know, something that's possible that's likely in view of this data, but that is rare and the physician may not have thought up. So, it is a very good assist tool to physicians because now they can look at the five and they occasionally they may be surprised they may think it's the common illness but they may suddenly remember that yes actually it also seems to match these less likely diseases right but it i don't think it will replace physicians because the human body still is something to understand which you need experience to understand which you need experience, there are a lot of things not there in the data. For example, the Southeast Asian body is fairly different from the Caucasian body. We know for a fact, for example, that our circulatory system, our veins and arteries, are actually of lesser diameter than on average compared to the Caucasian, and it has a whole set of consequences and whatnot. So I'm told by somebody from where I don't know. So, that's the physicians amongst here, I suppose. Patrick Patrick, would you agree with that? And today do we have. So, are you there? Would you agree with what I said? Yes, sir. There's actually a field called precision medicine that's slowly emerging. So what it does, sir, is it tries to take the human body not as a, like in totality, so down to the genomic level. They try to look for the proteins that are more present in your body. All the medicines that would target those specific proteins in your body. So like now, right now, most medications or antibiotics are just by convention. So if I tell you you're gonna take antibiotics, you already know that it's gonna be about two to three times a day, seven to ten days. You already have an idea because that's just a convention. Precision medicine would be based on your genomic structure, will be able to use a computer algorithm to target the correct therapeutic window so that you're not overdosing or underdosing. So ideally, we're trying to get to that point, but most of the things that are happening now are just the level of understanding what's happening. Maybe the correlation will be in the next generation or might be faster, depends on how quickly it's been taken up by the scientific community yeah it's very exciting times nice okay so I'm done with the classical methods of clustering next time I'll teach you one of the much more modern methods of clustering that you don't find in textbooks, the usual textbooks, the normal machine learning textbooks, but actually is quite a bit used. And I use it actually quite a lot. And they are based on, they're called density-based clustering methods. We'll talk in particular about dbScan and DENTRI. in particular about dbScan and dentry so let's keep it for the next time because I think we already seem to have a quite a bit of conceptual overload here a lot of things to absorb so please review chapter 10 of your textbook it contains dimensionality reduction and clustering just focus on clustering the first part is I believe dimensionality reduction which you can skip over and just do clustering and that will be a very good review of what we have talked about here so do that guys tomorrow next time this Saturday we'll do some labs on clustering by the way are all of you able is it convenient for you guys to attend the labs of me I also post the recording of those labs. Saturday is fine. Saturday is fine. Okay. So remember Saturday at noon, we'll meet in my private WebEx room. And then we'll go over the labs, clustering labs. And next time, next Monday, I'll do density-based clustering. And if time permits, I'll do dimensionality reduction. And that would be the end of the workshop. And the Saturday following that, I'll take it for helping people and maybe do an example of dimensionality reduction. Okay. Close the recording. That is recording. Asif, I just wanted to make an announcement. Go ahead. Guys, please take the quiz. I think other than Patrick, I don't seem to see anyone uh have taken that so please do that uh before you uh like i think i think um