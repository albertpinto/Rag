 The topic that we are going to cover today is the activation functions. Now what they are, we know, we have studied a little bit. We know that for each neuron in the neural network, there are two sort of phases, we call it the magic box with resistors and so forth. We take the input, multiply them by the weights, and then we get what we call the zine, which is the result of the affine transformation. Then we pass that result that becomes a final input. We pass that result into a distortion function, an activation function that does some form of nonlinear transformation on that input signal. And when you do that, the result is an activated output is that that's the vocabulary or the language that people use. So while we have been doing it, and we have been using that, I've been talking about the tanh and the reloop. And in the code, we have been using these, you have seen that in practice. Today, we'll give the session to understanding activation functions in practice. Today we'll give this session to understanding activation functions in depth. It turns out that these activation functions, there is more to it than just thinking that they are a distortion function and we can use what we want. There's a long history to it. People are continuously researching better and better activation functions. And you can actually get a considerable, sometimes surprising performance gains in your model simply by using a better activation function. And the reasons are varied. Some of the reasons are that in complex network certain activation functions lead to certain pathological conditions let's say dead neurons or vanishing gradients or exploding gradients or shifts and so forth so they're quite a set of problems that you can have in a very deep complex neural networks so you have to play with these activation functions and see which one is more appropriate. And sometimes to find which one is more appropriate, you can narrow your list of the activation functions that you'll try by theory, just thinking through it, reasoning through it, and you know from experience that this is what you should use. At other times, it is not so obvious. The fact that even today new activation functions are continuously emerging just shows how important they are and how active an area of research they continue to be. So I will use a sort of a historical progression to talk about these distortion functions as I have called them in the past or the activation functions. And today let me see if I can stand and talk instead of sitting and writing. If I do so, are you guys able to? Is it, am I still visible? Yeah. Yes, sir. Perfect. You could stand more, sir. That's good. Yes, I'd like to do that. And it's an experiment today. Let's see. So with that, let us go back and review a couple of basic things that we have done. The reason I kept this today as an extra session is so that we don't cut upon the next three weeks. The next three weeks are crucial. One week is on computer vision. Another week is on natural language processing, and another week is on graphical neural networks and possibly anomalies and fraud detection and things like that. So when we talk of activation functions, let me see, recap the notation that we used. We said that there is such a thing as a, again I will call it a neuron, right, a neuron or we1, X2, and Xn coming in. And associated with each of these inputs, you have a resistor that is sort of the physical intuition I gave that manifests itself as the weight, W1, W2, W w3 and then there is a dummy input which is sort of a rate one we call it either a bias in the literature or we call it w0 and so what what this produces is a summation this sums up all the products it so it produces something that I will call the Z Z which is produced in the first half of the neuron this is equal to W 0 plus W 1 X 1 W less W and X n it is just a linear product and this product people write it in a slightly a different way quite often they will write in a more abbreviated or mathematical vector notation they will say that suppose you consider the weight vector to be this one w1 w2 three, 3, and Wn. And then if you think of likewise the x vector, do you notice that we have augmented it with a one in front of it just to take care of the bias? And, oh, sorry, this is W0. And this is one x1, x2, to xn. When you do this, then you say that the dot product of these vectors, which we can represent as this thing, w, x, you will see a notation that says W dot X in the literature. And you will also see, so this is, I suppose, inner product. These two are talking, thinking of it as an inner product. And an equivalent way in matrix notation is to think of it as a W cross X matrix, matrix multiplication. So what does transpose mean? Instead of a column vector, if you were to make W transpose would be W0, W1, Wn, like this. And so you realize that matrix multiplication is only possible if you take the transpose of w in this and the end result of all of that is exactly the same when you do so end result is z is w transpose x right and that is our basic result is equal to this W0 plus W1 X1 plus Wn Xn, right? So this is hopefully a review of what we have done so far. Now let's think of what it is doing. Your input feature space, let's say that you have three dimensions. So suppose this is your R to the N, right? This is your input feature space. And what Z is it is using a mapping from RN to R because Z is a number when you do a dot product you end up with a scalar right and so you are going from the X vector to the Z which is just a number using the weights using the this transform using the dot product this is saying the same thing in a more sort of a mathematical or sasant notation but all these different ways that i express this fact they are all equivalent so that's a summary of where we are but that that is not all that a neuron does. After Z is computed, we also have one more stage to it. The second stage that we have is we apply a distortion function. This is usually called an activation function. Function activation function of c so it is this function this distortion function that is the topic for today we will give all our time to talking about the distortion about this sort of activation function now a distortion function is a word that I use, or people don't use that. They usually call it activation function or a more common word that people use is transfer, not more common, actually less common word is transfer function so i will mention those things right now activation function activation function transfer function transfer function. What do they do? They are mappings, essentially. They do something different. They take z as input. So z is what? It's just a number. Number and the output what They again they just output a number. Are we together guys output the activation. Let me just call it a of z, right, this is the activation of g and that is also just a number. So the first therefore. Is that evident? Is that evident folks? So we conclude that the activation function is a univariate function. you can just say a the output which i will say y let me just call it the output a is just is equal to some function of z isn't it and z being a function and so it is a function of your weight and the inner product of the weight and input so this captures the relationship so far so far so good guys are we all together so far yes okay so this is a recap and so today we'll focus on activation function. Now the first thing we will learn is what should it be? If you just sit back and think what should activation functions be like? So you can think of many many answers. The non answers would be be the bad answers. So let's write this question down. What should an activation function look like? Let's ask this question. So one way to do that is by asking what shouldn't it be like? So what are bad solutions? Bad solutions. And that will give us an intuition on what a good solution. Bad solutions could be, suppose you say that activation function A of Z is equal to zero. Means, what does it mean? The output is always zero. Would such an activation function be of any use? No. Yeah, it generally wouldn't be. It represents, in other words, it stops the flow the flow forward so if this such a neuron was sitting there in a in any layer and inputs were coming in and such a AZ is equal to zero were true what what would guarantee the zero? In other words, this neuron is effectively a dead neuron, isn't it? Right? In other words, it stops the flow forward, i.e. the neuron dies. Neuron dies. So that is easy to see. you can and the same is true what what about this can we go and say az is just some constant what about this and reason through it guys you know when i talk about the activation function um i'm deliberately starting from first principles because i want you guys to develop the theory in your mind and reason through this. So tell me, is this any use? No, it's almost the same. It is the, yeah. Why? Because this term, if all that this is doing is producing a constant, to the next neuron that it is going, there will be bias term and you're just adding C to the bias term, which is as good as having zero here and adding C to the bias term which is as good as having zero here and adding c to the bias term isn't it guys so this would be about as useless not Then comes another solution. What about a linear? Let's take this question. Suppose AZ is equal to Z, always. Always. equal to Z always always for all values of Z belonging to the real number line you know so from minus infinity to plus infinity whatever input you give it always produces the output is that so if this acts as a pass-through gate isn't it you don't have an activation function you do have an active it doesn't just start it at all it just lets it go through it's just a pass to it's sort of like an open gate isn't't it? Whatever comes in goes through. So imagine that you have a gate open and whatever gets in gets out, right? So that's what a linear activation function is. And you could say, and its variant is, and its variant could be C times Z. So up to a multiplicative factor. It just scales it. Scales it. What about this now? So let's reason through that. It's quite interesting if and to see whether this could lead to some interesting possibilities. So let's take through that. It's quite interesting to see whether this could lead to some interesting possibilities. So let's take two neurons. I'll take two neurons. Let's say that this neuron is there. And I'll create a neural network like this. And let us say that you take a one-dimensional input. So let's keep our life simple and reason with this so suppose this is here this is weight one and this is the weight in layer one let me since i use a superscript for layers w1 and this is the weight in layer two and let us say that this has some bias a bias of the layer 1 and a bias of the layer 2 right or w0 W0 If you want you can the equivalent Let's see what happens this activation function. It will produce Z 1 Z of the first layer Let's say of the first layer, what would it be it would be b1 plus x times w1 does this look sorry uh let me write it in the more conventional notation w1 times x is the same thing would you agree that this would be true so go ahead So the output here, what would be the output? It would be the output of this activation, this Z1, it gets transferred and come out as Z1, isn't it? Z1, because it's your, or up to a multiplicative factor. Let's take this situation uh z1 just comes out as is so z1 becomes the input to the second second thing so what is z2 let us figure out what is z2 z2 is a bias 2 plus w weight of the second layer times z1 d1 isn't it and now let's see what this leads to. And this is equal to B2 plus W2 B1 plus W1 X, isn't it? And then the, and once again, if the activation of this is the same, if that is what comes out here, Z2 comes out, this is the response. So if you think of what comes out a little bit of algebra. Do you see that guys? Now comes an interesting thought. You could have instead a crew instead of that what if you had done this you had just created a single neuron right where the weight here was so this is the weight right and this is the bias let me put primes here to indicate that we are talking of another and this is the Z prime. So suppose you had a neuron whose B prime is equal to B2 plus B1 W2. It just so happens that you set it to be this and your weight W prime here to be for the input is equal to just W1 W2 so now think about it what is the output here z prime is b prime plus w prime x right and if you try to break it up into the't it it becomes a b2 plus b1 w2 plus w1 w2 x do you see that right these two are equivalent these two are equivalent statement so what are we saying that the yellow neural network the two-layered network in this very simplified network in which we are taking only one node per layer and just a single variable input that that situation can be summarized by saying we don't need any of the hidden layers it is just you can have an output right here a linear regression output, z prime coming out with our identity, like where your activation function is az is equal to z, right, as the activation function. See, is this little bit of algebra convincing? All we have said is that if you only do linear, like if you just do identity transformation or identity activation, you will get... there is no need. You can take two layers and they will act as a single neuron and now you can generalize three layers, four layers, five layers, whatever number of layers layers you can squash all of them into a single neuron um am i is that looking convincing enough guys yes so so from that we draw a conclusion let us draw a conclusion and by the way if you do a multiplicative factor of c and by the way if you do a multiplicative factor of c yeah so is that something to me somebody is saying something to me okay yes if can we uh can you please explain once again yeah blue one? Suppose your activation function is this. So now think about it. Suppose you have a two-layer neural network, and each layer has a single neuron. X goes in, output comes out. The output of the first layer goes into the second layer, and the output of the second layer comes out here. Work out the equation. What is Z2, the output? When you look at Z2 2 you'll realize that you can write it in terms of all these weights and biases isn't it are we together why because the input if you really look at it the output of the first layer, let me just write it here, output of the first layer. And what is this? This is the output of the second layer. Would you agree? Yes. Yes. It's simple. But the thing is, when you plug in Z1 here, you can expand it out. And when you expand it out, you notice something very interesting. There is grouping in the data. And so you can actually write an equivalent single neuron just a single neuron that will do the job so long as it's its weights and biases are like this you know they're defined like this if you define the weights and biases of the next just a single neuron like this it will do the same job as your two layered neural network is doing. Yes, sir. Is that clear now? So it looks like a equation for straight line. I can see that. So the slope will be a ways only and the y intercept is the b2 plus b1 and w. Yes, yes, exactly. Right, exactly that. You can think of it as this is your bias term, that's why it is b prime and this is your slope. Right, that is the weight part or the y prime, w right, right, you can write it as it so what do we learn from that what we learn is and by the way i did it with c is equal to one the proportionality constant equal to one i leave it as an exercise for you to convince yourself that if you bring in that multiplying uh factor nothing changes the arguments remain the same You're just adding c to the mix. What we learn is that a neural network with many hidden layers with many hidden layers but with linear activation activation is what is it what can we say about it it is it is equivalent is equivalent length to a single neuron with the activation isn't it now when you do when you write Z is equal to like we know really when you write Z is equal to some biased term plus some weight times X when you write it like this what are you doing this is this is what equation if I want to just change the wordings a little bit is this reminiscent of y hat is equal to remember in the earlier course we did beta 1x right a beta dot x from your ml 100 200 you remember this or in one dimension this is beta naught plus beta 1x do you remember this guys recall from so what is this design this is just nothing but linear regression this is the equation for linear regression isn't it a linear regression, isn't it? For linear relationship. In one dimension, it's a line. In higher dimensions, it's a hyperplane. So what you're saying, therefore, is that we can only capture, only capture linear relationships. And that is a crucial fact. Means we don't need so many layers. It is useless. We can just start with a single neuron and it will get the job done. It's practically no different from that. All this complexity is not needed. So now we came to two of these things that don't seem to serve the purpose very well. Let's try something else. What about this? Let me try another thing. What about, and so far guys, before I move ahead, this is a very important thing typically what happens is if you take a course in machine learning and any one of the universities at the Graduate School level or in the upper division level like for theater gear and engineering students there you're often asked to prove this in your homework. Typically you leave this, you go prove this. So because this idea is important. So now what about this? Imagine an activation function that goes like this. So it is undefined in this region. So it is a discontinuous function. Function. I should write it better. Discontinuous function. I should write it better. Discontinuous function. So what will happen now guys? Will this serve our purpose? Suppose this is Z, this is A of Z. It seems to violate all the manifold requirements even though it's the activation function. Very good that it would and very practically what if the value of z falls in this region? z value based on inputs and weights evaluates to, let me call this region, to the missing region. Bermuda triangle. So it would just fail, isn't it? You wouldn't be able to use it. The whole thing would stop. So we learn a lesson, it has to be defined. So from that R, which is another which from minus infinity to plus infinity. So for what it means is for every value, every value of x in real number that is minus infinity to plus infinity along the real number line we need a value we must be able to evaluate evaluate the dysfunction isn isn't it? Something like evaluate the activation of, actually, let me use Z because I'm using Z everywhere. AZ must be able to evaluate Z. So that's a lesson to learn from that. And so we learned something. And now, logically, let's proceed a little bit further. We now stipulate that the function better be defined everywhere, but we haven't really fully dealt with discontinuity. So suppose I have a function that is like this along the z-axis. This is z. And let's say that you have a function that is like this. And then this is at this point, some point doesn't matter where, it goes like this. What function defined everywhere but it has a discontinuity so it's not differentiable right what will happen is we will not be able to back propagate back propagation requires derivative so we end up with a problem zone non-differentiable point non differentiability and also it leads to be unstable behavior right there's also unstable unstable around z let me just call it z tilde unstable around z tilde means a very small perturbations very small changes of values if you're on this side of the boundary the value is this if you this side of the boundary the value just suddenly shoots up isn't it that is usually not a good idea because you want small changes to have small effects not dramatically different effects so that also doesn't seem to be so the lesson here is we we learned our next lesson. Lesson is activation functions must be continuous everywhere. So the reason is non-differentiability. Can I take gradient derivative? Gradient, I can just say gradient for back prop. Now, so activation functions must be continuous. And along with it, let's also throw in the caveat that what about differentiability? You need differentiability also, isn't it? What is differentiability? what does it mean? Must have a derivative everywhere except perhaps a couple of points. So you allow it to have missing a derivative sometimes and you'll see where except perhaps perhaps that z is equal to zero and why this is there and you can get away with this but we'll come to that right it turns out that it doesn't bother you because when you back propagate uh whichever way you look at it you you multiply the derivative, there's derivative there of the last function. You're also bringing in, well, some other things, the weights and so forth. So the only way z is zero is if the input is zero. If the input itself is zero, well, then it doesn't matter, right? You are in trouble. doesn't matter right you you are in trouble so uh so there are reasons to say that a couple of points are okay this is differential though is useful there are other qualities that you want first thing you realize is that you want non-linearity what i call distortion property what I call distortion property. Your activation function should take the input, the real number line and distort it, right? Otherwise you won't be able to capture non-linear decision boundaries. Am I clear? So we saw that, right? Otherwise it becomes linear regression. That is another lesson we learned. In other words, to capture, to capture non-linear relationships, relationships, mappings, if you want to be more formal, of Rn, feature space, to either R, regression, or Rn to, you know, what was this categorical classification. So this is literally a nonlinear relationship, isn't it, you would say why hat is a some nonlinear function of x where x vector belongs to the input feature space which is n dimensional. n-dimensional and here you would so your function should represent to be able to capture a nonlinear embedding manifold okay hypersurface you do need distortion and your network would distort it with distorting output you know activations nonlinear activations the same is true in classification you can have nonlinear decision boundaries decision boundaries. So nonlinear decision boundaries cannot be created as we just saw by a linear activation functions, right? So nonlinear decision boundaries or nonlinear You cannot do that if it's a hyper surface. With curvature, with bends and so on and so forth. so non-linearity is an important criteria that you want to have there is another criteria that you want which sort of mention you it is a good to have it's important but if you don't have it well you can get away with it so let me use another color boundedness we didn't talk about boundedness bounded if it is not bounded so in other words if the value for some input Z for some Z if AZ is very large is very very large why is that a problem because suppose this one is producing the layer L is producing this layer L is producing this activation Z it is the input to the next L plus one layer isn't it and so if this input is very large what will be the z what will be the z l plus 1b will become very large right so then you have a problem the problem is you either normalize it so you need to The problem is you either normalize it, so you need to, so it can risk some activation functions, or the activation of the next layer, some next layer activations with saturation with saturation it's just a bad idea to have such a large outputs and so but usually batch normalization salt second remember we talked about it at each level the outputs that come out, batch normalization, like pulling the values back in usually solves it. So today boundedness is important, but if you don't have it, we don't worry about it too much. Batch normalization usually takes care of it. So important, but not necessary. Important but not necessary. So these are some properties that you expect the activation function to have and the last one is actually a property that I'll just mention to you. Suppose you have an activation function that suppose that you use this different color to talk about this topic, which color should I use? Let's try green. We haven't used green. Zero centered. zero centered what are we talking about here the zero centeredness see suppose now this is different uh suppose a z is always greater than zero right Or greater than equal to zero for all z. So then what happens? Your values will keep, tend to keep getting shifted, you getting limited to the r plus, the positive values. And so you lose half the number line for the next layer. You have squashed it to only the positive values. So that is a bit of a concern because unless you normalize it, so you what you what it means is that to be centered is to have both positive and negative valued outputs. So in general, it's a good to have a good to have good to have now as we make progress and I'll take you through the various activation functions one of the things you'll realize is that this to fulfill all the criteria is sometimes not neither desirable not possible but there are some that you can't violate for example continuity is not something you can play with right it must be there the second is that differentiability must be there except for maybe a pointer or two. Non-linearity must be there. So the ones that are there in these three are not negotiable. The boundedness and the zero centeredness, well, if you have it, it gives you benefits. If you don't have it, then you have to have an adaptation mechanism, a compensation mechanism to take care of it. So that is it. So in other words, let me summarize what we learned. Activation functions in neural networks, they cannot be a constant valued or zero valued. That wouldn't be useful. They cannot be the identity transformation pass-through. They cannot be even just a linear multiplication of pass-through they simply cannot be linear otherwise they won't get the full richness of machine learning model those functions should be differentiable and they should be continuous they should be bounded so those are absolutely necessary conditions then come certain properties which if you have lead to benefits. One of them is boundedness. In other words, the values don't just explode and so forth. And the other is zero centeredness. It means you have both negative and positive values. So as we work our way through the different activation functions, keep these five properties in your mind, and we will see how well they play with these five properties. That's a great question. All right. I have a question. So is there a problem if the boundedness, if we allow our gradients to explode is it because of computational complexity or is it just um is there really something beyond the computational power that's the problem why do we have exploding and vanishing gradients uh it's slightly different from boundedness patrick see a function can be large, value can be large, but at that large value, the gradient, the derivative can still be very small, isn't it? So in other words, let's say your altitude, you're next to Mount Everest. So suppose your function is height function. You're close to Mount Everest, right? And so your value is very high, but what about the slope? As you know, the slope is finite, otherwise you won't be able to climb Mount Everest. Am I making sense? All right, I see. So there is implicitly one more property, which I call monotonicity. Sir, I have a question on zero centered. You were saying that batch normalization will solve the problem if it is not zero centered, because only in one half. If the output is positive positive it will pull it back in what does a normalization do it will spread it between typically between minus three and plus three so it will zero center it that's what that's what i say okay but if the activation function is not zero centered you have to add something to it to make it so right make make the next layer inputs to be zero centered a lot of question there as if for zero centeredness so if if in are there instances where we can allow where it's okay to allow our model for that layer particular layer to have just positive outputs it is perfectly okay and this was one of the realizations. We'll talk about it. It was actually some of the things we learned when we discussed the different activation functions. Not all of them are zero centered. So then what do we do? Fortunately, things like batch normalization comes to a rescue because it's zero centers the input before it is fed into the output before it becomes the input to the next layer. But let's say you want your output let's say it's for some reason that layer is discovering borders so let's say you want you actually want it to be all instead of penalized you just want them to to always um either reward it would it be beneficial um to just allow your your outputs to be to remain positive there are situations where you do want that so it's my case and this is one of the things you'll see in a few moments that as we go along we will encounter many activation functions there's a rich library of those in fact by torch has implemented a pretty rich library of activation functions maybe i should i have added it to the course page maybe i can give you a sense of the richness of it right now. It is worth looking at it just to see why. It's a question like why would people worry about creating so many activation functions in an industrial strength library like PyTorch. It's because when you experiment with different activation functions, you get a lot of benefits. So if you go to the course webpage, you will actually put the activation function topic pretty up so that you can immediately spot it. So as a good practice, it's better to be zero centered, but just pick up a good activation function rather than allow the output to Reality is complex these days, right? We realize that we would like a zero centered, but the, but the cheapest activation functions that get the job done are not zero centered right so uh in fact relu relu and relu is not zero centered yet it gets the job done you just but you can't have i mean you just need to have compensating mechanisms like a batch normalization so really something i'll introduce almost immediately in a few moments uh it is the topic for today, but I wanted to show you something guys. This is the activation function for this is the documentation. I didn't know the fonts are very small here. Wait, wait, before you go on. Mono tenacity is a necessary attribute right yes i was going to come to that so hold on that is the last one i'll talk about so non-linear activation function do you see how many there are and most of these you haven't heard the name of if you look at typical code on the internet that people have posted their tutorials etc they will always use either relu or sigmoid or tanh or softmax this is about it but there's a pretty rich library and there's a reason why in fact just changing and being knowing these activation functions and knowing when you should change one for the other. Sometimes, you know, it's a free increase in performance. And in a world where even a few points of performance gain, accuracy gain matters a great deal in production, in real life, isn't it nice that we can try out the different activation functions and sometimes hit the jackpot, you know, get just that little bit of extra and sometimes hit the jackpot, you know, get just that little bit of extra performance to take us over the, over the, you know, victory line sort of in some sense, right? So you can see a lot of these activation functions are there built into PyTorch for a reason. They all have a purpose and we should be more familiar with all of them. So in the textbook, in your lab textbook and these things, the richness of it is not discussed. I feel that this topic needs more depth, has more depth than we do justice to in the introductory textbooks. So today that is why we are holding this session. So with that preface, we have been through almost one hour. I would like to do one more thing. What did I just do? I did something. Okay. The last point that I was coming to is a property of activation function that we want. And that Dennis referredfer too. See we don't want an activation function that behaves like this. Why do we not do that? See for neural networks, it doesn't serve a purpose because we want the word activation inherently has as the idea built in in that because the as as Z increases, A of Z will increase, increase, increase, or at least not decrease. So at every point, if as you move along the Z-axis it increases, what call it the monotonic density. The word monotonicity comes obviously from music, in a constant pitch, sort of one direction, it doesn't change direction. I'm not a music expert, so I can't speak with expertise what really monotonicity is in music, but I suppose the word monotone in the common sense has something to do with it. Any musicians here who would like to enlighten us on the word monotonicity in music? So guys this is, I would like to take a small break of five ten minutes for you to digest this thing. Think about it. See, I'm building the concept of activation of functions from the ground up, from logic, from first principles. And it is best to understand it from first principles slowly. So let's go back to fundamentals. I'll review it again. So this is a big neuron. The yellow thing is a big neuron it has two phases to it two parts remember we talked about it as a magic box which is made up of rheostats that produces the output z right and that it sums over the so it's basically z is the inner product of the weights with respect to the input. So Z is W dot X, inner product. And that's another way of saying that you basically take all the inputs and you take them in different amounts in common sense way. So those are the two ways of formulating the same concept mathematically and in common sense. Then you take the Z and you apply an activation function to it. That activation function needs to have some properties. If you reason through what properties it should have, you would agree that it would not be a good idea for the activation function to be a constant, right? Zero and or possibly zero, because that is as good as a dead neuron, right? A constant value is a dead neuron, right? This is not a very good situation if you end up with. So we don't want that. The next thing is what if you make it linear? The simplest linear is just a transfer. on that. The next thing is what if you make it linear? The simplest linear is just a transfer, a z is equal to z. A different one would be a z is equal to c times z. So it turns out that when you do that and you have many layers in your neural network, then the entire network is equivalent to actually just one neuron. You don't need all this complexity. So that is an important observation. Just one neuron will do the job. You don't need so many of these. So that is another lesson to learn. And when you do that, when you have a single neuron, as you remember, the single neuron with a pass-through activation, linear activation, is nothing but linear regression, right? It is the equation of linear regression and linear regression, as we remember for our ML 100 and 200, it just captures a hyperplane, right? It does not capture the complexities or the non-linearities of a generalized embedded manifold, right? Or hypers hyper surface that captures the relationship between feature space and the target space it does not right cannot possibly it will always be a flat hyperplane so therefore a linear activations are not something that will serve our purpose and therefore we need non-linear activation functions so that is one lesson to learn what else do we need? It's a discontinuity of course and differentiability are important. You cannot have activation functions that are either not having any value. This is worse than that. This region, it has no value here in this undefined regions. You cannot have that, that this function is undefined in some part of the real number line. It must be defined everywhere and it must not have discontinuities because you lose differentiability, right? That's one thing. So it must truly be continuous. And then differentiability, it better have derivatives everywhere. The one special point where you can sort of get away by not having derivatives is that Z is equal to zero, but otherwise you should have derivatives. So that is it. And then non-linearity. Non-linearity is at the heart of it. You want to capture non-linear relationships, you better have non-linearity in your distortions in your activation function then only a conspiracy of many neurons will together capture the features of a non-linear you know embedded manifold relationship that is it then comes two properties which are good to have boundedness it just says that the value should not just keep on getting large because then it puts the next layer into trouble because with very high activations, you can, well, there's a set of things that can happen. If you're using a sigmoid or something like that in the next layer, it will just saturate or something. This is not a big problem these days because batch normalization fixes it, solves the problem. So we don't put too much emphasis on boundedness. But nonetheless, if an activation function comes with boundedness built in, it's a good to have. It's a good property. Zero centeredness means it should have both positive and negative values. If it only produces positive values, again, you are shifting everything to the positive part of the real number line right and the the input to the next day will have no negative values when the cure for that of course is batch normalization will cure that it will input and then also the weights because of the weights some of the weights are negative, it can put the weights there. But otherwise, without some balancing technique, compensatory technique, what will happen is all your weights will drift one way. The other thing is simple. The other thing that is not negotiable is obviously the basic inherent notion of activation function in the neural network is they are monotonically increasing. They're always increasing or remaining flat, but they are certainly never decreasing with Z. And that, mathematicians call the property of monotonicity. Monotonicity means it has only one behavior, either increasing or decreasing. In this particular case, increasing monotonically is what we want. So those are the properties that we expect neural networks to have. Now what we will do, let's take a quick break. I will very quickly review the back propagation and then we'll go through each of the activation functions. I would like to cover at least four or five and then show how they, what properties do they have, what are the plus and minus when you should and shouldn't use them and so forth. Sir, there is a function, a swish. We'll cover it today. Yeah, that over there, yeah, so it will decrease, decrease, decrease, and I think there's a much better representation of biological function, you know. We'll do swish today. Thank you, sir. Yes, yeah. All right. Sir, one question quickly. Say if it is there is linear relationship in the data, if it captures the decision boundary, then why cannot we use the neural network? I mean, you can. The only problem will be that the only time certain work is when your data truly has linear relationships that's right other situations it won't work but see a priority before you have played with the data model the data you don't know whether the relationship is linear or not isn't it right the reason you don't want to presume that it is linear and continue with that okay so uh i have a question about monotonicity so you mentioned you know uh monotonicity is either increasing or decreasing but your above above statement it says you know we expect afz to be increasing always or at least not decreasing so yeah so monotonicity is more general in the case of neural networks we we want one specific monotonicity we want monotonic increase then how does decreasing come into picture there is no decreasing Sajeevan G. How does decreasing come into picture. Dr. G R Narsimha Rao, There is no decreasing Dr. G R Narsimha Rao, Right, you don't you turn. I mean, see the typical as we go through it. And so you don't decrease the you don't want that as the increases delta Z is positive, you would like to have delta A to be positive right that's all it means it means that the slope should preferably stay positive but we'll have more to say about that as we continue see all of these quantities to what extent we we need to adhere to them and to what extent we can compromise on them or not have them that is the topic of interest in the so guys we will start by saying that certain conditions just to summarize you can't compromise on and certain you can so you must have a continuity and that is not compromising that is a given must be well defined differentiability otherwise you can't share blitty except that except at one or two points except at zero sector you must have differentiability, but the conditions that can be broken can be broken. They are boundedness. That they are just monotonic. It turns out that a little bit of violation of monotonicity is okay can be violated people used to think it cannot be violated but recently we learned actually that the new a lot of new activation functions break monotonicity but they do it in a very strategic way they sort of a careful way and then the the zero centeredness is another thing can be violated so now what I will do guys is I'll try to recap back propagation in a few simple statements and see whether we are able to do that from yesterday. So back propagation, as I said, is a topic that will take a while to sink in. it over and over again i'll keep using different topics as an opportunity to sort of review back propagation so let's take back propagation in in a very very simplified situation because that will serve for us so suppose you have a situation like this you know here is w1 here is w2 and we'll take a single you know single dimension input and imagine a chain of neurons so it's a very simplified neural network in which each layer is a single neuron and if you get this one right you get the more generalized a met multi-node per layer also so suppose this is w so this is the layer layer one layer two layer l minus one this is the layer and let me use the word l to be the last layer it is going in so what happens is w1 w2 w l minus one w l is going in and then the last layer and then what happens is that the activation of the last layer is the output is the y hat right so now you can compute the loss function out of this you realize right so you can compute the loss function the loss function. Example, mean squared error, you would write loss is equal to y hat minus y squared and 1 over n and so forth. Let me just write it here and this is the summation one over n am i making myself clear guys does this looking familiar yeah one over n this is your last msc last is like this so what do we do it's so i actually you know what this is cluttering up my image so let me uh this this picture that i want to explain let me write it somewhere else last function example of loss function would be let me write it here msc loss one over m summation y hat minus y square this is the msc loss so now what do you do if you have a loss function L, then the thing that you can do is you can compute the gradient of the loss with respect to the activation, with respect to Y hat for example here, activation of the last layer. If you can do that the idea is that this will help you compute the gradient of the loss with respect to a L minus 1 and that will help you compute the gradient of the loss with respect to the activation in the you know you can keep going one and now I'll put dot dot dot here let's say with respect to 2 and that will help you compute gradient with respect to the first. So it keeps going on. It keeps moving back Backwards, you can keep computing the gradient layer by layer right and now why is that relevant. is relevant because at each level if you ask what is see you want to make this step the Delta step the gradient descent step is W next for any particular W next and I'm deliberately dropping all the indices just to keep the notation simple because it means any one of the weights in this entire picture its value is the previous value minus the learning rate times D the loss function with respect to W so this I hope we remember as the gradient descent step the real learning isn't it now it turns out so this is what matters what we care about is this now let's look at this guy it turns out that this thing you can express in terms of the activation of that particular layer that you if you know the gradient of the loss with respect to the activation you can compute the gradient You can compute this at each layer. You can compute this Because the activation helps you decide that this or figure this out and we'll see this with very concrete example You will actually see that back propagation of the derivatives. They are not as mysterious or as complicated they're very trivial actually but you have to see it with some specific activation function to see that and so you can compute at each level accordingly so this process is the when you compute the activations at each of these levels a1 a2 coming out a3 a3 a l minus 1 coming out minus 1 and a l coming out here this is the forward pass there is a forward pass off and a backward pass forward pass is just computing the activations and backward pass is just computing the computing the gradients so let me so now you might say do you want me to show you how these things work just as a recap first of all let us recap how how can we compute if we know can we compute if we know gradient of the loss with respect to some activation then gradient of the loss with respect to W wait because that's what you'll plug into the gradient distance step is quite simply equal to this you just have to reason like this if I change the gradient a little bit what changes small change in gradient will change the activation function right of that layer because activation function is sorry not activation so the Z Z is great Z L is gradient of L weight that's right weight in a product of weight and the inputs, whatever the input is. What is the input? L minus 1. It is this, which in the one-dimensional case, it just becomes WL, A, L minus 1, right? So this is ZL. So if I change W, ZL will change and so I can write it but if you change ZL a little bit you realize that activation will change because activation is a function of ZL right it is some function of ZL so this will change the activation of the layer and and if you change the activation a little bit it will change the loss function a little bit and so and so what have you done you have shown that there is and I'll write it succinctly here the loss with respect to WL, this multiplied by activation of this with respect to ZL multiplied by the loss function of the loss gradient with respect to the activation. So now we related these two things. All you need to do is multiply the gradient with respect to activation with two multiplicative factors and you get the gradient with respect to W. Now, the other thing is that how does the back propagation itself work? How do you compute the gradient of the loss with respect to in the previous layer based on the value in the in the layer ahead of it right so let's do that that is the other bit of thing we want to do so let's again write it we want this how can we express the gradient of the loss with respect to the activation in one layer before in terms of gradient of the loss with respect to activation in the layer ahead of it so let's work through the mathematics here let's do that so once again we reason from here and move to this. You say, let's see, you know what? Let's look at that. If I change the activation in the previous layer a little bit, what happens is that, let's think through this, Al minus one is feeding into the next neuron which has a weight WL and it will therefore affect the ZL and then the output is the activation with respect to this layer will get affected right so let us see how we can work this out think of it this way I change this so let's put it here a small change of this a l minus one will change what would you agree that it will change z right because that is the input to this layer this neuron so so therefore a small change in this will change Z. So let us say that. So this will lead to a small change in the Z of the next level, a small change of the activation in the previous level will lead to a small change of the Z. Now, what will the Z change lead to? It will lead to the AL, change of AL, the activation. This small change will cause this activation to change right d l and a small change in activation will of course lead to a change in the loss itself and so we achieved what we wanted to do we related this thing in terms of this thing you see that if I have the value which is of the gradient with respect to the layer L I can find the value of the gradient with respect to the layer L minus 1 do you see this guys yes and that is it that is all back propagation is what it means is that but you know you have to start you just have to say but i must know the gradient of the loss with respect to the last activation function and the good news is usually we know that and we'll we'll see this with a few examples that is easy and so you can find the gradient with respect to the previous layer and the previous and the previous and so forth and you're looking at back propagation right these are the two just essential steps you just back propagate in the gradient and that is all it is guys that's all that propagation is then it's a recursive thing you keep on going back and back and back so now what what does that do? Notice that everywhere this A keeps coming up. The activation function is at the heart of back propagation and the shape of the activation function determines how the back propagation will work. So I'll take an example to illustrate this. We will take the example of let's say regression. Let's say that your last layer has activation a linear activation easy is equal to C or maybe you're solving a linear regression problem suppose your loss we will take is the easiest loss to imagine because we learn regression first right so guys is it okay if I just sort of sometimes I'll forget this one minus n factor because it's just a multiplicative factor right if you can reduce this it will automatically reduce it by a scale value so you might see that i keep dropping the one over so this is the loss the regression loss is the loss the regression loss sum squared loss or the MSC loss and it might have some regularization terms I'm not taking those away they're still there but for simplicity assume they don't exist it will make your realizations so now let's look at some historically how these functions evolved so the family of activation function historically one could argue that the first one that we knew about, so let me change the color here a little bit. The first activation function, as far as I know, and I might be wrong, is this. Suppose you have the x-axis here, the xy-axis, so z-axis, and this is the activation. The simplest one is, and this sort of mimics the biological system, the activation of a neuron and so forth and the shape of this is What condition use this So the shape of this is I stretched out the shape of this is I stretched out do you notice that it looks like a stretched out isn't so this and its lower isn't odd this is a Y is equal to so here of course this goes in this direction what is the minimum value that it can achieve minimum of AZ is can you look at this picture and tell what is the minimum guys a the activation y axis look at the y axis zero zero and what is the maximum that it achieves one one so and observe so this particular activation function is the famous sigmoidal people often use the word in machine learning in deep learning they use the word sigmoidal. People often use the word in machine learning, in deep learning they use the word sigmoid, sigmoid all the time, but more precisely, I suppose statisticians would call it the logistic sigmoid. Because to them, sigmoid is a whole family of S-shaped functions. But in deep learning, and this is the terminology you have to be careful with, deep learning people say that s-shaped this shaped activations are sigmoidal observe the ale at the end and they will call the logistic simply as the sigmoid they won't use the word logistics they will just call it sigmoid right and we represent sigmoid typically with a sigma you know this is the Greek symbol the Greek symbol little thing so they write it as that and the mathematical expression for this is sigmoid function so suppose sigmoid of Z is equal to 1 plus e to the minus 0. Another way that you can write it, and those of you who did ML100 with me must be very familiar with it. If you think of sigmoid as a probability, if it goes between 0 and 1, you can think of it as a probability, isn't it? So sigmoid z over 1 minus sigmoid z, that is called log odds, is equal to z. This is, there are two ways of writing it. One in terms of exponentials and one in terms of logs. So this term, the left-hand side terms, people call log odds. And you know, these days, election season is there. Most people talk in terms of odds what are the odds that this guy will win a senate seat or win the presidential election and all that and so forth so odds are numbers you'll say 100 to 1 right or 10 to 1 or so on and so forth lord odds for and against and so forth so the log log of odds is log as in natural log. In mathematics typically when you mean a machine learning, when you mean log, we typically mean natural logs. Though in computer science, traditionally, when you mean log, you mean log to the power, to the base two, but here natural log. So you can write it like that now this function has that quality that so think about it let's observe this function a little bit this function and i'll just steady the sigmoid a little bit in depth for you this function when z tends to minus infinity what happens sigmoid of minus infinity is equal to one plus e to the minus minus infinity so what is e to the minus minus infinity is one plus e to the infinity and so what does it do to the denominator it becomes infinity and so sigmoid will tend to one over infinity is and so sigmoid will tend to 1 over infinity is 0 likewise sigmoid of plus infinity is what let's figure it out minus infinity so each of the minus infinity is what 0 isn't it which is the same as one plus one over e to the infinity so this term cancels out and this becomes so you realize that this function which is why it seems to have the new saturation points these are called asymptotes lower asymptote These are called ascenters, lower ascenters. And the upper ascenters. This term, ascenters, comes from calculus. If it helps you, just think of it as saturation points. So ascenters are saturation points saturation saturation it saturates to zero and one at the two extremes. That is a rough way of thinking about it. So this is it. Now comes the interesting thing. If you go and look at it, if your activation function is the, whenever we do computations, we need two things. We need the activation function, but we also seem to need the derivative of the activation function. Do we go back and notice that all over the place, we seem to have this term sitting there. Do you see this? This term, the slope of the activation function with respect to z. So in other words, we do need the derivative of the activation function. to Z so in other words we do need the derivative of the activation function do you see that guys lurking in all this backdrop computation is the derivative of the activation function is that clear I mean I hope that is evident here in the car in this formulae that sitting there lurking in there is the derivative of the activation function everywhere make sense yes yes so what it means is it is not enough to know the activation we need to know its derivative so let's try to figure out the derivative of this function when you have the log activation this is something let's go back and do that derivation you could take the derivative here or you could take the derivative here let's take the log part because a little bit easier and you'll see why so you can write log Sigma Z over 1 minus Sigma Z is equal to Z so this can be written as log Sigma Z minus log 1 minus Sigma so those of you who remember the log a over B just as a aside basic of log is if you have forgotten a over B is log a minus log b this is a refresher back from your uh middle school days this is it and so i broke it up and now uh one more fact log of x derivative ddx of x is 1 over x are we together so from that perspective if i apply the log here to this what will what i will get is 1 over sigma x sigma z sorry sigma so this is a d sigma z dz minus one over one minus sigma z and now the derivative of one minus sigma z is the same as minus d sigma z dz right and that would be on the left hand side and on the right hand side what would be the derivative of dz with respect to dz? 1, right? And so here, this is a little bit of calculus, by the way. If you don't get it, it doesn't matter. I'm just giving it for those of you who are interested. If you just take it out and dz, dz, 1 over Sigma Z And then there's a plus sign here. That's 1 over 1 minus Sigma Z Is equal to 1 and so let's solve this term And by the way, this is a sort of thing if you remember You used to be very good at at some point in the denominator it would be sigma z to one minus sigma z and what would be the numerator one minus sigma z plus sigma z right and this whole thing multiplied by d sigma z d z and this thing is equal to one. Oh lovely things are beginning to now sing a little bit because This term cancels out and we have this result d Sigma Z DZ times 1 over Sigma Z 1 minus Sigma Z is equal to 1 Right, and therefore you just computed the derivative of the sigmoid function. Sigmoid activation function is nothing but, you can write it in terms of the sigmoid itself. Isn't it? So now going back to the, or if I just say, A, Z is sigma Z, in this particular case, you can say that going back to our notation was the derivative of D, A then by the way i abused the notation that between my partial derivative and my full derivative you'll see me that i tend to mostly use the partial as a symbol even though i should be using the the full derivative here but okay just bear with me one minus 1 minus 4 A is a potential signal, active signal and this is a profound result it's very very good result do you notice that something lovely happened you don't have to act during computation during back propagation you actually can get rid of the derivative you don't have to compute the derivative at all derivative is this it is just a multiplication it is a multiplication of the activation with 1 minus the activation itself so stop here and just ponder over it for a moment guys. We have no idea how to compute derivative isn't it? We know how to multiply. We can write some code that can multiply and now lo and behold in our computation or when we are doing the back crop it turns out that at least one of the derivative is well too easy you can just replace it with this and if you use a sigmoid sigmoid activation isn't that lovely right is that simple yes and now comes another part let's go back and inspect maybe some other parts also we don't need to worry about derivatives and doing fancy numerical computations or maybe we can get away with the cheat so let's go back and look at it oh look at this part what about this part okay these uh derivative of z with respect to the activation of the previous layer so let us go back to that and see what it means. Z of the next layer. Is nothing but activation in our simple you know one chain activation of the previous layer times the weight Of this layer, isn't it. So suppose you have this layer a From the previous layer is the input weight is WL and activation al comes out are we together and where did ZL come ZL is the activation here now what is Z with respect to derivative of Z with respect to derivative of z with respect to a. So let's go back and see what we are trying to compute. You notice that you're trying to compute this z with respect to the activation of the previous layer, right? So we go back and look at this equation now, and I'm just restating what we are trying to find, z there, with respect to a l minus one right this was the other term this term now do you notice that if this is true it is nothing but weight weight do you see that guys right and so now the whole thing is beginning to sink because we it turns out that there is another piece of the derivative that simply doesn't need to be computed we of course know the weights isn't it these this is just the weight is just weight and in the broad sense and we have many variables and many nodes for data just weights so this too is easy and now let's go back and see is this entire thing beginning to sing now and is back propagation much much easier than the than than it looked at first glance so let's go back and go back to this expression and I will scroll back and remind So let's go back and go back to this expression. And I will scroll back and remind you, let's go back to this expression, the full expression. Look at this full expression. This it turns out we can do without calculus, without doing a numerical computation of gradients, right? It just, it follows, it's just a weight. This also we got away, but what about this guy right so if somehow this also we didn't have to do some fancy numerical computation to find gradients life would be lovely isn't it guys because then it would just be a multiplication of terms finding the gradient terms finding the gradient you guys agree with that yes so now this is where and that's that's a magic with back prop actually when you do it with real activation functions and real loss functions it becomes very very simple actually so let us take a loss function that is activation of the last layer that is your y hat minus y and can i just drop the subscript of the data point and so forth just for simplicity this is your loss right i'm assuming that there is a summation in all of those things do you agree guys this is this is a summation okay maybe I should bring the subscript activation with respect to some input are we together guys so now look at this for the last layer can I compute can I compute this so what is the derivative of BD X of x squared it is 2x is it so bring that idea here what is it it is 2 times activation nothing but summed over I do you notice that Kaiser this is true that the derivative is just this just from analytical calculus do you agree guys do you see this yes and so the the last layer gradient of the loss I can write now I don't need to actually do any numerical computation this is a straightforward you know a minus a layer activation minus the actual target values isn't it it's the residuals just the sum of residuals. So this is very easy. And so what it means is let's go back now and say that at least for the last layer, the layer before the last layer, L minus one layer, this particular part, this thing thing I'll just copy it over at the bottom DL so let's look at this if I know I already know so known for no for last layer so what about the activation for the layer before that we said that this is small change in activation in the layer before that will cause a small change in Z of the last year small change of Z in the last layer will cause a small change of activation of the last year a small change of activation of the last year will lead to the small change of the loss of the last year so these were the three factors and we just computed all three right so what was this this just turned out to be two times well your summation a of the last layer I minus the actual value right so this is what this turned out to be what this turned out to be was a like for sigmoid one minus a of the last layer a 1 minus a last layer and what will this turn out to be ah this just turned out to be w if you take the derivative of the sign of this with respect to this this is just the weight w right and so now do you see that there are no derivatives actually being done you just computed the gradient without actually doing any numerical computation just look at this result do you see that guys yes right it's easy and so therefore can you compute delta l delta a l minus two delta L, delta A, L minus 2. You realize that once again, you can do it simply by just going one step back. And once again, you'll have the weight of the L minus 1 layer, the activation of the L minus 1 layer. And you already know, you just calculated the activation of the layer L minus 1, so l minus two can be found and so you can keep moving back and keep computing the gradient and once you have this activation you know that you can from this you can compute the loss function with respect to any loss function if you know this at any layer you therefore can know this at any layer you therefore can know this way we already did that loss function with respect to the weight of a layer is small change in the weight will lead to a small change in the activation a small change in the activation sorry a small change in the Z a small change in the Z will lead to a small change in the activation of the layer and small change in the activation will lead to a small change in the loss in the layer right so this was there now let's ponder over this once again do you realize guys this is what this part is nothing but a oh sorry this is just the input input of the layer what is the input to this layer it is a l minus 1 isn't it guys what is zl isn't it guys what is zl a l minus one times the weight of the last layer right and so when you take this you'll get this term when you take the activation this again becomes your a m one minus a and you just computed this activation with respect to that so what happens is that all your gradients, therefore, you compute without actually doing numerical computation in the derivative sense. And that is the magic. That's why back propagation is feasible and cheap. If you were doing a lot of numerical computations, it would be horrendous. If you were doing a lot of numerical computations, it would be horrendous. Because if you remember in American computation from college days, a derivative takes time. It's an expensive computation, but it turns out that you get the derivative without actually doing the derivative. And that's the magic. That's why it back probably. So this was true for the sigmoidal function sigmoid function. So I'll use the same kind of a term now sigmoid function let's see which properties does it have the good properties is it continuous it is is it differentiable is it differentiable is very simple whenever you hear the word differentiable ask yourself is it everywhere smooth ask yourself is it everywhere smooth yes yes right it is now is it monotonically increasing yes. Is it bounded? Yes, because saturation points are zero one. Values are bounded in the output is bounded to this zero one interval. Isn't it? All right. So it is bounded. So now things are beginning to look pretty good here. Now comes the, is it this is z does it produce negative values ever no no so this is a violation it does not have this property right now there was a one more property is it non-linear does it's nonlinearity yes sir it does have so it's dead it seems to have gotten many things right isn't it and so because it was actually the original sigmoid function the activation function when I talked about the universal approximated theorem the view the partition snippers Chivinco he proved it the original paper children is in terms of the sigmoid sigmoidal functions and in particular he took the sigmoid to prove it so it works then the sigmoid has, you know, you ask this question that, see, can I remove this deficiency of being zero centered? What can I do to remove it? And it turns out that there is a close cousin of sigmoid function. It is called the tanh. Should I use a different color? Might as as well let me go over to blue there's another function tan the tan H is interesting so this is I'm just like taking you through the historical evolution of these ideas and so guys do you notice that what is can exist does it have upper is it a bounded function? Yes, sir And what are the bounds upper and lower minus one? line one to one It is bounded. Is it continuous? Yes Yes continues is it smooth is it differentiable Yes It is differentiable. is it monotonically increasing it seems to have hit it seems to be hitting just about all of your points is it zero center yes sir it has both positive and negative values, which is a big deal, a sigmoid boson. And then, is it non-linear? Yes sir. It is non-linear too. So this is actually a very very popular activation function in the sigmoidal family, a very is the shape family stretch test stretch test so this is good and so I want to do a deal with more sigmoidals there are more so for example there is the error function sigmoidal Gaussian sigmoidal and so forth. There are more complicated things you can do, but we limit ourselves to sigmoid. Remember sigmoid and sigmoidal are different. Sigmoid is just the largest thing. And about tanh. What does it look like? Tanh is e to the z minus e to the minus z. Sorry, e to the z plus e to the minus z. See, you notice that this looks surprisingly similar to the sigmoidal sorry the ordinary sigmoid the logistic function where is my logistic function gone back prop no no no yes look at this yellow thing here do you notice one plus e to the minor z this yellow thing here do you notice one plus e to the minus e does it look somewhat similar to this guys pretty similar so you ask this question is tanh Z and sigmoid Z. This is a question worth asking, isn't it? Correct. There is actually. I'll let you confirm it. So I'm speaking from memory. So if I get it partially wrong, it's your job to correct me. Later on, go verify this and you can correct me later on. But just observing the relationship and inferring a little bit, you can convince yourself that this is true. So what tanh is, in some sense, it is twice, it's sort of a amplified version of the sigmoid the sigmoid function right and in general when you do machine learning uh i would always recommend that use tanh don't use the sigmoid if you look at the textbooks they will the two examples they give you is the sigmoid and the reloop so you think your choice is between sigmoid and reloop now and the ReLU. So you think your choice is between sigmoid and ReLU. No, a sigmoid in my view should rarely ever be used. A tanh in almost all situations is superior to sigmoid because it converges much faster. It has a bigger range of variability. It goes from minus one to one. It is zero centered. And at the end of the day it does have a quite a effect on conversions you can run it if you go to the you know the tensorflow playground guys do you want me to show you that why don't you do that to see that in real life let's go here to tensor let's go here to tensor so here's the thing guys do you see this activation function sigmoid is there tanh is there ReLU is there linear is just a az is equal to Z let it let it go so what happens when I take sigmoid and let me take a complicated thing here. I'll just leave these layers in place. And is this enough for it to convert? I'll take sigmoid and I'll run it and see how slowly it losses. You see the color change here? see how slowly it changes color actually we need extra layers here so let me do this let me add a few more times because I because so let's try it again look at the loss function see the loss is 0.499 and we are using sigmoid what are you seeing guys the learning is very very slow right if at all it's just flattened out not going doesn't seem to be going anywhere you can increase the number of notes and try to be still you'll notice that the loss is not improving dramatically now what I'll do is I'll replace it with something much cheaper. Reeloo, I won't explain Reeloo to you. I'll do that in a moment. Let me put Reeloo here and then start it. Do you notice that things are happening much faster? Look here. Are you seeing that is taking place much, much faster, and the loss is falling much faster. This is a complicated problem. And so it is still struggling. It will take more time for it to finish. Actually, let me do one thing. Let me take a little bit simpler example. Let's take this example and I'll take ReLU and see how fast it works. Done. Isn't it? On the other hand, if I do the same thing with sigmoid see how it goes you notice that it's going very very slowly it seems to hardly be making progress and finally gravitated to it are you seeing it and you need to something very interesting in the beginning it wasn't learning much and then suddenly it started learning and then it came close to the solution that's a problem with sigmoid actually and we'll talk about that now the same thing if I do with damage it's it will be faster than the sigmoid but slower than reelu you notice that tanh is much faster than sigmoid I'll run it again and we'll see that look at tanh there it is it pretty much solve the problem on the other hand really also immediately solves the problem but the same thing if you try with as sigmoid very slow and remember that it is learning slowly and suddenly it will start learning fast you see the steep decrease in loss and then it flattens out when it comes to the answer so that's a limitation of sigmoid activation function you can use this actually to play with these major activation functions so that's that now let's get back to where we are this is it now tanh now the question is if you use AZ is equal to tanh it is okay that this is the expression for it but what is the derivative what is AZ what is this this is equal to and I'll state it I'll let you verify it and at this moment stated without going through the derivation this is actually equal to 1 minus a 1 plus a which is 1 minus a square and by a of course let me be more formal 1 minus AZ 1 plus AZ and that is the same as 1 minus AZ square are we together so here we have a result it turns out that the derivative of the tanh also can be written in terms of the activation itself isn't it and so here also we sort of dodge the bullet we don't have to compute explicitly compute the derivatives it is just given analytically by this expression. So, Danich is a good property. Now, what is the problem with these two things? Sigmoidal functions. So, people for the longest time used sigmoidal because they were trying to mimic the biological systems and biological systems have a sigmoidal activation so they thought they were on the right track and they were improving it it took a long time actually to move from even sigmoid the logistic one to tanh really it took a while and then came an interesting breakthrough and people began to you know they were frustrated with some limitations of sigmoidal sigmoidal functions one of the limitations is do you notice that for large values of z large positive or negative values of z what happens if z is here what does the gradient look like what does AZ look like it is approximately zero very small so now if you remember that learning is proportional to the gradient what almost no let me yeah so you will have very creeping very slow learning isn't it not only still learning it you know in fact keep on vanishing your gradients your patients will and ultimately what will happen is the layers that are early layers you know so as you back propagate back it turns out that most of your gradients will disappear they'll become zero and so your early layers won't learn anything and that was supposed to be a pretty damning problem with your network to such an extent that for many years people just abandoned neural networks. They say it's not a problem that can be solved right and maybe neural network says was a lovely part you know walk down the wrong path. So and we'll talk about that in a little bit so then people were in search of something better and so after TANIC This was hinting and I believe it was Nair and hinting if I'm remembering right so a correctly for nephew later on can check it was Nair and hinting Sajeevan G. Did I write his name correctly. I think so. Who did I write his name correctly? I think so. Who came up with a remarkable breakthrough. They say, what about an activation function that looks like this? Now I'll again change color. Let me use some other color. What would be a good color? Let's pick a color like light green. like a light green so what if i took a relo this is a little counter intuitive which is why it took so long for us to discover it because you'll realize that it just puts a question mark on some of the properties that you want to have so reelu looks like this zero so in other words the reelu function is activation z is equal to the max of either zero is equal to the max of either 0 or C so another way to express it is it is equal to Z for Z greater than 0 is equal to 0 otherwise it's like all or none phenomenons are right and that is where the word comes from. See in electronics, when we use the word rectified, the word is rectified linear unit. So the linear part is very obvious, right? It is literally the definition. Az is equal to z in the positive half, in the positive z values. This part of the the right-hand side of the of the page or of this figure of this chart it is just linear but in this part it is zero and the word I suppose I'm just guessing here it comes from electronics because see suppose you have an AC current how does an AC current look like look like looks like this voltage over time and you want to convert it ac to dc this process is called rectification rectification you see that when you rectify ac it becomes dc dc current polarity never changes. So what will happen is it will become like this In between it's just zero And then you smooth it out you do funny things We'll put a solid knowledge or something like that and you'll smooth it out with I'm just going back to the basic electronics that that you guys may remember that the purpose of diode for example what does a diode do it does that it doesn't let current pass through in the negative direction does anybody remember these concepts yes they're coming back to me yeah yes Kate you did that so yeah that is it that's where they would rectify comes on so now let's That's where the word rectified comes from. So now let's look at the properties of rectified. It breaks, is it continuous? Yes, sir. Yes, is it differentiable? No, yes. Except at zero. Except for the caveat. Something at zero. Except for the .. Which you have to be careful. You have to argue that it doesn't harm you. And it is an acceptable exception. Is it zero-centered? Yes. No, it is not. There are no negative values yes is it monotonically increasing it is monotonic all right increasing that is there and then is it nonlinear yes now this is this throws people off because the word linear is there they think that this straight line means is this is a linear activation function remember what I told you linear activation functions can never never catch a nonlinear relationships, but it is actually nonlinear. This is, do you see this bend here? It would be linear if it was just like this, everywhere it was like this. It is not like this. There is a change of values here. It's non-linear so in other words you cannot just write it as y is equal to az is equal to b plus some weight w z you cannot write it like that or say b plus lambda slope linear relationship it is not you can never write it like this but you you'll always have to write it in this expression with two values here and so it is a non-linear so non-linearity condition is fulfilled right is it bounded oh it turns out that is it bounded no it goes to infinity isn't it yes right so it violated two conditions it was not zero center it is not bounded but there is something very nice about this thing which is why Hinton argued for it he noticed that see what is the derivative? What is a z? What is the derivative of z of this activation function? It is equal to 1 or 0 Isn't it Z greater than 0 and z less than 0 Derivative is simple. So what does it mean when you back propagate? At least this part, this part is just one or zero. So it means that it acts as a gate during back propagation, a gradient flow. People use the word in back propagation as the gradient flows backwards as the backwards backwards either it stops or let's it flow it either stops or let's it flow back without doing anything without modifying it because the gradient is exactly one multiplying anything with one remains the same thing isn't it guys so that was a nice thing except that this seems to have one problem so it obviously improves upon the situation of this vanishing gradient or this slow learning you notice that learning would be the same irrespective of what positive value you have this is the rate of learning and the same is the rate of learning with large values of C so it's a bit bit of a more democratic sort of more proportionate situation, whereas the logistics with the squashing function, you know, the saturated Jay Shah, MA, RNK Mgrgd, The, the, the, the signals. So that is something to know. So now we learned about the really the real has a problem. But once again, because it is not zero centered generally you'll have to do batch normalization together with it just to bring the values back in now I see one quick question on the ReLU yeah so literally with the way the activation function works if any of the coefficients are negative it's going to drop those out of the model, right? It won't contribute anything to the model. Yeah, if the Z that gets computed, the input to the activation is negative, basically, see, so let's think a little bit about it. What happens is that this node is peculiar. In the forward direction, if Z is zero, A is zero. So the node dies. Think about it. In the backward direction also, what happens is that if Z is, let's say that this is in particular, Z is negative you know all the weights and inputs multiply if it becomes less than zero it's terrible because not only is the activation zero what happens to the gradient da DZ is also zero isn't it for negative Z so what happens is that this node is essentially a dead node. Dead node for negative Z. So what happens is as your neural network learns, if the forward direction inputs, it turns out that the previous layer sends it a negative input that node pretty much dies which is a big problem that you need to solve so guys when you look at reelu you realize why it was counter intuitive that you would use it it has it is non-differentiable it has this problem that some nodes can die right you have the problem of dead mode on the positive side the gradient is when it is not dead the gradient is one so you don't have vanishing gradient problem in the very obvious sense right so uh that is that it turns out that re and reels are extremely cheap to compute so when people started when hinton started putting relo along with obviously suitable normalization what he found is that the neural networks used to run almost a factor of 100 faster or sometimes thousand faster or something like that orders of magnitude faster than using any of the sigmoidals using even tan edge take a very complicated neural network and it will do that it's a weakness is of course the fact that you have a non-differentiability at the origin and that you can't have dead nodes to solve that people did all sorts of tricks actually what they did is do that so they said leaky reuse the leaky relu says that let us have here the slope is 1 but here the slope is alpha very small value so if the slope in the negative Z is less than 0 slope is this and here slope is 1 then what happens da DZ therefore is equal to alpha for Z less than 0 so what does does this mean? It means that back propagation goes through. That's why you use the word leaking. It still leaks through. It's small, but it leaks through because the slope is very small. But the back propagation will continue and the node won't die. It's a leaky reloop. Then people were still uncomfortable with this. So then there are some exponential versions and so on and so forth that people dealt with. And we can talk a little bit about that. But then came some activation functions that violated monotonicity itself. So there is the switch function. Switch activation function. These are very, very recent. What it did is it did something funny. It took the linear activation. linear activation right and it multiplied it with linear activation is what az is equal to z and it multiplied this it took a product of this with uh sigmoidal or a sigmoidal activation function right so if you multiply these two so it becomes the activation function becomes X times sigmoidal what is our Z actually I should use them so now maybe I should change color I'll go to white again so it becomes one so Z times 1 minus e to the one plus e to the minus Z. Do you notice this? Right. And so the couple of variations, the moment people discover that this leads to a really good behavior, and this is fairly recent work, guys, and a lot of people are very excited about it. Now, let me remember the exact shape of it it goes something like this it goes I believe like this that's correct it goes something like this. So now you notice that let's go back to it. Is it continuous? Yes, sir. Is it differentiable? Yes, yes. It is differentiable. Is it bounded? Yes, sir. Yes, it is even bounded. Is it zero centered? No. It is zero not. Lovely. And, but is it monotonic? It is not monotonic. It is not zero centered also, right sir? You have some negative values. Do you see this? Yes. Some negative values are there. And by the way, it should have gone through zero. This line is not very accurate, but okay. Just bear with me. I'll post the actual shape of the switch and these functions on the slide. Is it nonlinear? Yes, sir. It's nonlinear. And so what happens is that we have these functions and many more. If you go back and now look at PyTororch library you will see that a lot of these activation functions are there and there are variants of the other for example tanh to make the computation faster there is a hard tanx right tanh there is a hard sigmoid and so on and so forth and these activation functions all are there for you to use and we have observed that performance in many situations like image processing etc can be dramatically improved with these because you know some of the pathologies that you have with a simple reloop don't happen with the newer things that have come so when you're learning when you read your, especially the book that you guys are using, you'll notice that it predominantly stays, if I'm right, with the review and damage and linear and softmax. Is that correct guys? Those of you who are reading through the book? It's been a while since I finished that book. Check it out. So it is, it probably mentioned somewhere that there are more activation functions, but I want to emphasize that use play around with that functions. They actually lead to a lot of benefits and like this activation function that we are talking about the one that mentions sorry guys where is that one the once again i'll go to the page so on the page guys i've given you the links to everything all of this so you see the activation functions okay hang on are you guys seeing my screen i'll increase the font is it readable now yeah so you see this activation function in python you click there go there you will notice that oh my goodness see look at the long list right what i would suggest is as time passes uh become familiar with more. Make it an objective that every month you will learn, let's say one or two, two more. C-LOOP, for example, got a lot of attention. It again leads to a lot of improvement. And you notice that ReLOOP, after ReLOOP, there are a lot of variants of that, and improvements of that. Even sigmoid has a lot of variants of that and improvements on that even sigmoid has lot of improved variants of that lock sigmoid right and so on and so forth a tanh has hard tanh right then swish a simplified version of swish is the hard swish so there is no end to these activation functions so take it as a homework how about this guys would you could you just use these functions? And plot it out in a Jupiter each of them see how they look from minus infinites to plus infinity when you plot them out see how this Activation functions. It will be very easy. All you have to do is create a lot of numbers and then Apply these activation functions are we together guys this and then um also apply all of this now softmax is something i would like to talk about separately today it's gotten late i guess we'll stop here softmax is in some it is an activation function but it's a very special activation function we use it for multi-class classification right so it has some variance to it and so forth so with that uh i guess i'll stop now Thank you.