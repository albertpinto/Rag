 So folks, the first thing to remind you is that all the videos are on the site. I checked and made sure that as of today, everything is there. In fact, the last one, the Congelational Neural Net, the standard definition version is there, the high definition version is still being processed. And hopefully by the time this class ends, that too would be ready. So just for you to know, now this is available. If you come to the course page, you would notice that they are this list of videos covering every single session that we have had Whether it's a Monday session. It's a Wednesday session. It's the Saturday quiz. It is the Python intro or it is the Sunday paper reading. So we have five events that we do every week and So we have five events that we do every week. And there are about 20 videos that have accumulated sometimes. Some of the videos are broken into two. So they are all available. Please use them. Next quiz. Most of you have not taken the activation function quiz. I would strongly suggest you do that. Activation functions are very important to deep neural networks. They are an active area of work. I would suggest that if you haven't done so, please watch the video on activation functions in deep neural networks. We talk about it from first principles. What is an activation function? Why does it look the way it looks? And so forth. And after that, do please take the quiz. Tomorrow I'm going to likely release two quizzes. And one will be on Convulational Neural Nets, the topics of this week. So please do keep taking the quiz. When I see that the number of people taking the quiz taper off, it's worrying because it means that you folks are falling behind. If you fall sufficiently behind, I may have to stop the workshop and give you time to catch up because there is no point in my rushing ahead if you guys are busy with your day job and are not able to catch up. By the way, I wanted a general feedback. Are you guys finding the course too fast-paced and hard to keep up? How many of you are finding it too fast paced and hard to keep up? How many of you are finding it too fast? Anyone? Well, there's both the theoretical stuff to catch up on, the reading, the videos, the textbooks, and then the labs to wrestle with the issues for that. So the combination of that, it's like choosing which stuff you make progress on in a limited amount of time. Ah, it means that you're sleeping all eight hours a day. I'm joking. I do like my sleep. Yes, I'm kidding. So yes, it is a bit. It is a bit too much because we are reading theory and we are doing the practice. This ML 400, the first part, the fundamentals, the foundation so far has been intense. We have learned a lot about the foundations. Now we are getting into specific architecture so it will be a little bit easier. I hope that you'll find the CNNs, the explanation that I gave on Monday to be sufficient and the labs are very simple in CNN directly. We'll do one lab today on CNN. We'll also do the solution to the river data set. I'll show the aspects to that. But guys, do do these things. I've uploaded the solution, the river solution, and the CNN lab and homework. I have deliberately not given the solutions to the California data set. There is an optional data set for which I would like to give you folks an opportunity to solve. It is the Wisconsin Breast Cancer Data Set. I'll mention a link to that in the Slack. But those of you who do have the time, the bandwidth, please try that also. In one of the extra sessions, we'll cover the solutions to both California Housing Dataset and Breast Cancer Dataset, Wisconsin Breast Cancer Dataset. We are likely to do it sometime next week. So you have a few days the weekend to try those two things. The hint is it's a very straightforward extension and application of your homework in the labs that we are covering, the guided labs. But it should give you practice and it will give you, it will sort of increase your confidence in the, in your abilities. So it's important that you do that. Next announcement is this Sunday we are going to discuss an interesting development in congulation in your life or income or in object recognition actually object detection so object detection has always been a very complicated problem when you look at an image first you have to find that there is a duck or a there's a there's a cow or a duck there or in a more prosaic way there is a duck or there's a cow or a duck there, or in a more prosaic way, there is a car and there's a person on the street, right? And first you detect that it is there. Secondly, you have to create a bounding box, a rectangle to wrap up that object in. So given a rectangle, you need its coordinates and size, and you need to identify what is it? What is that rectangle? Is it a cow, a duck, whatever, some particular object? So object detections were always complicated. It was always a challenge. There's a huge degree of relevance to object detection in so many, many ways, if for no other reason than for self-driven cars. many many ways if for no other reason than for self-driven cars. If we miss an object, as you know, it can be catastrophic. Not to be able to detect objects in time to and respond to it or not to detect objects correctly have any sort of error rate can be pretty catastrophic. So there is a whole body of research that tries to do better and better at object detection. We talked about convalescent neural nets. They are directly, one of its application is object detection. We will do object detection in quite some detail when we do the one month to object detection, I mean to computer vision. We'll do classification rigorously, we'll be doing the lung cancer data set and we'll also do the COVID data set, look at COVID x-rays and lung cancer x-rays. We are going to do a lot of object detection, we are going to do a neural style transfer, trying to paint a picture in a different style, in some artist style and so forth. So those are for much later. But for those of you who are interested, we are going to cover this fairly significant breakthrough in object detection that happened very recently. It came out of Facebook research. It is called the DETR. And what it does is something quite interesting. It takes a simple convolutional network and then, and it screws on top of it, bolts on top of it, a transformer. And it turns out that a combination of a convolutional neural network and a transformer leads to extraordinarily simple but very effective object detection architecture. So that is the topic for this Sunday. or replies. So with those words in place, remember that this week is object, it's about a convolation neural nets. Now in convolation neural nets, I'll give you guys a review, but before I do that and then launch forth into the lab, I would like to say that I will start releasing solutions to previous homeworks now. So one easy one that I gave was the river data set. It was a straightforward extension of what you had done with the toy data set that I had given to you as a solution. So I want to know how many of you did solve it, who all solved it. Maybe there's a facility to raise hands in this thing, isn't it? There's some way to get a sense of that. Let's see. Yeah, I think I solved that one. The river did as well. You did? Okay, so good. I want to discuss. The idea is that as you solve it, everybody will solve it slightly differently. And you know the learning is when we all compare our solutions. So by not doing it guys, you're losing the benefit of collective wisdom. Just putting it out there that you are actually losing it. See, I, you could say that I do know this subject a little bit more than you do at this moment, but I am always surprised that whenever I talk about it or discuss it or draw it or something, somebody will come up with a solution which I would not have thought of, but he, that person has reasoned in a very different way. Sometimes would be better sometimes it would be worse but certainly it would be novel and that's the beauty of it just to see how other people reason through this so we are getting into interesting territories guys after the fundamentals like after this we have what two more weeks of it the territory that we enter will be the territory of collaboratively working on things and sharing our ideas, sharing our experiences and learning from each other. So please do try to do the homework. If you're not doing the homework, it also means that somewhere I'm failing you. It means that you either you're not getting time or I'm not teaching you properly, which is disappointing. So if you get stuck or if you feel underconfident about a topic, please reach out to us. The tears are there. I'm there. I posted to our Slack. Remember, the Slack is a private Slack. Your employers, your office colleagues are not seeing it you can be frank it's a it's a safe zone right in the office we all bluster and try to be look smarter than than perhaps it's an engineering trait engineers always are overconfident people and so we project that image but remember this is not workplace this is a this is a workshop this is a safe zone and the whole point is that here we can be pretty open about what we know collectively we can all learn about things so please do do the solution today i'm showing you the river data set solution but the housing please do try the california housing share ideas with each other and you'll see that the architectures that each of you create and the way you solve the problem will be different. All right, so with that preface, let me launch into the river, the river data set. Actually, I was in two minds whether or not to show you the solution or give you guys more time to solve it, but then I decided to share the solution to at least one of them but housing is coming very soon by the way this uh everything this code these notebooks are posted to your website as i look for please do access it. Now, where am I? Classification. So when we talk of the river dataset, I give to you a sort of a solution of it using a lot of machine learning algorithms we tried with logistic regression and I believe does this one have support vector machines and so forth probably not yeah but it has with polynomial and so on and so forth so I'll go straight to the deep neural networks and just remind you. First, I gave a V-shaped data. And this V-shaped data, we get near perfect accuracy. You see the loss function and you see the predictions. the ground truth it's not so pointy at the edge you'll notice that neural networks tend not to make very pointy edges obviously we talked about neural network they they tend to be a little bit more smooth decision boundaries especially if you use damage and so forth so now the question is if we apply to river data set what do we need to to do? Not much. Now, once again, I put the bulk of the code into a class that I will today carefully go over step by step. And I'd like to give some time in the end to also talking about clean code or neat coding in your notebooks. It is an unfortunate practice in the data science community that almost all the code is put in Jupyter notebooks. While that is good to start with or to experiment with or to hack with, actually even for hacking, it's not very efficient. I would strongly dissuade you from putting a lot of code in your notebooks. See, notebooks came about because of a concept called literate programming and there is a movement that said that our notebook should not be instructions for the machine to do something. It would be, it should be explanations for colleagues to understand. In other words, in a notebook, you're communicating with your colleagues, you're communicating with human beings. So therefore, when you put a ton of code, the code is meant is your private conversation with the machine. It is how you train the machine to do something. And notebook is not the place. Instead, put language, put descriptions of what you are going to do or what you're doing clearly, and then back it up with code as libraries, as modules, and write that in a clean way, back it up with unit tests, ideally have test-driven development and so forth. And trust me that you do have test-driven development for data science code also, though it is very sparingly practiced. If you do that, you'll realize that code evolves in a very ordered and neat way. There is a reason why we should write code in a clean way because it is reusable. You're creating assets for the long duration. And at the end of the day, you yourself will understand your code one year later. Otherwise, if you write very messy code, then you will hate reading your own code one year later and figuring out what is it that you do. So in that spirit, you will see that I have put the classes here. I'll talk a little bit about it, but let's try to just read the code and see if it makes sense. And that is the nature of the code. You should be able to read and tell what it is doing. So when we say this network is RiverNet with a certain degree of dropout, what is dropout? Would somebody like to mention what is dropout? When you selectively zero out random nodes in the network. Excellent. We nuke out certain nodes in every batch with every iteration. We nuke out some random number of nodes, a certain proportion of nodes. And that proportion is at this moment, 1% here. 1% is unusually low when people do, and they try 10%, 20%, 30%. And so I deliberately put it at 0.1 so that you could, because I wanted you to see how the loss the overall accuracy or the net or conversely the residual loss in the on the test set varies as you increase the dropout in this particular example you can carefully see the effect of dropout on the accuracy or the loss can carefully see the effect of dropout on the accuracy of the loss. Now RiverNet, just from the name, what can you infer about it? What is it likely to be? A neural network? A data set network, yeah. What data set you're working on? Not the data set, actually. It is a for this example. To ideally handle this example, River Classifier as the name suggests, takes a neural network and trains it on this data. Now when we do that, before we train it, before we call classifier the train, you notice that I'm doing something which is, Chad advised you to do, data is asymmetric. It is at the approximate ratio of 68 to 32. So you have to give differential weight age. It is a good practice to do that. Actually in this problem, in the river data set, because the data is not very asymmetric, giving weights is not that important. In other words, it doesn't affect your accuracy a lot. But I mention it here simply to emphasize that while in this situation it may not be terribly necessary, use it for other situations. Like for example, when you get the breast cancer data set, which is highly asymmetric, this is the sort of code you will use to do that. So suppose less than 1% of the patients have cancer, then you don't want the network to give equal importance to every data point because there are so few data points that are positive test cases in patients with cancer. You want the neural network to, at the last function, to put a much more severe penalty for making mistakes on the people with cancer, isn't it? So you sort of make it asymmetric penalty. To make it asymmetric penalty, this is how you do that. And there's a bit of mathematics to that. If you remember, it's y log y hat. And so when you're doing this, what you're doing for each of the classes, yi being the class, so then you weigh it instead of y. You just multiply it by its multiplying factor before summing it up. And so your loss is sensitive to the data asymmetry. It is a good practice. I invite you to play around with this notebook and see what happens when you don't use the weight. Not much will happen, but one thing you can try. What if you give wrong weights or bad weights? What if you reverse the two? See what it does to your accuracy. Then let's read what this line does. Classify the train epoch is equal to 400. I hope by now it should be self-evident that when you train a neural network, you train it for a certain number of epochs. And epoch is one journey through the entire data set, through many steps or one step or whatever, depending upon your batch size when you do that now i have i'm returning a list of loss history what this contains loss history we'll see but if you could surmise could you guess what this class could possibly be looking at the output here what does each each of these contain? Loss history contain? Every, what was the overall loss? Exactly. So at each step, at each iteration, what was the loss? So there's a flaw here. All the steps are saying 0, 0. So I'll leave it to you guys to fix my bug. For some reason, I didn't fix the step number. So this is it. And we go and record. Then this is just printing out some. There are too many records. It prints out every 400th record. And then let's look into this. We make the classifier predict the labels and predictions. This is, it will give you the ground truth and it will give you the prediction, a list of ground truths and predictions collated together. And therefore, you can use the scikit-learns classification report to produce the report so in this particular case this run seems to have produced an accuracy of 89 percent it's variable when you uh rerun this you might get a slightly different accuracy it may increase or decrease uh the max that i saw was, I saw it go up to 89%, and then I saw it go down to about 83, 84%. So you can play around with it. Then we have a plot of the loss function. Can you guess what this is? Plot loss function just plots the loss against the iterations, as you see here. And finally, we are comparing the ground truth to the classifier prediction. Left-hand side is the ground truth. Right-hand side is the classifier prediction. If you look at it, it's not a perfect match, but it's a reasonably good match. There are areas that it gets wrong. For example, if you look at the top left-hand corner, the top left-hand corner, it certainly has gotten wrong. It's marking it as blue as water, whereas here it is actually sand. The river has bent around. In this picture, it seems to have expanded out, which is not true. So there are little errors, there are errors in this prediction classification. So that brings up an interesting question. Does it make sense to quantify the error in local regions of the feature space? And the answer usually is yes, depending upon which region. You can try, for example, to create specific subroutines, submodels for specific regions, if you wish. This problem was too simple. We are not going to do that. Now, any questions, guys, before I show you the actual source code for these two classes, RiverNet and the RiverClassifier. So today I'm going to carefully walk through the source code. So far, we have eschewed that, we have not really walked through the code behind the curtain. But from today, we'll give a significant amount of time to understanding that code and walking through it carefully. And I invite you to form small groups for code reading guys. As you make progress with this workshop, the pace will take up. So it won't be feasible for you to work as teams to be able to do those longer labs. And expertise comes when you do those longer labs. So with those words, and again and again I'm saying something, I hope it doesn't look like hectoring. The point is that is it. We have to go and put some effort into that so just just to set the ground remember that we had a base classifier which we seem to be using at many places let's go and study this base classifier is it sufficiently visible on your screens guys is it big enough yeah that's nice visible on your screens guys is it big enough okay so if it is then give me a moment I'll try to even make it full screen how does it go to full screen this one doesn't seem to does the job uh f log there we go so let's go over this very very carefully i have a whole set of imports logging can you guess guys what logging is for the name hopefully is descriptive enough it's a log logging is a best practice You should always log because the standard output, your print statements will disappear. But if you're careful, your log statements will go to a file that you can read later on. So it is important to log. Then this is just abstract class. It's a metadata. Instead of typing list and tuple, we all know what lists are, what tuples are. Tuples are pairs. Now from PyTorch, what are we importing? We are importing PyTorch itself, 14. Then from within PyTorch, there is a NN package. The NN word is suggestive of neural networks. We are importing cross entropy loss. This is, if you remember, the loss for classifier, the error function for classifier is the cross entropy loss, which is y log y hat, y being the label, log of y hat, y hat being the prediction, right? Natural log. And we include module. Module, if you remember, is the base class of a lot of things. Just about every piece of a neural network is called a module. For example, a layer, whether it's a linear layer, whether it's a convolutional layer, is a module. Likewise, even dropout, dropout it turns out is a module. Activation functions are modules. So in the parlance of PyTorch, everything that makes up a neural network is essentially modular. Most things, I wouldn't say everything, but most things are, they're subclass from module or something to them. Then we have torch.optim. This library contains a few optimizers, SGD optimizers. So this is for you to play around with different ones. Now you notice that by default I use the ADAMW. It is something that many people don't know. Most people use SGD. ADAM usually outperforms SGD. Remember I introduced you folks to ADAM in the previous work labs. So now I'm recommending use ADAMW, which is a slight improvement Adam, for more situations, for common situations. Dodge type device. See, neural networks work best on tensor processing units and GPUs, right? Second best on graphic cores and worst on general purpose CPUs. So if you have a graphic card, even if it doesn't have denser processing unit, for example, if you have any graphic card that supports CUDA, C-U-D-A standard, and unfortunately at this moment, that means any graphic card from NVIDIA, then you are much better using that. Your code will run hundreds of times faster. I believe we did a demonstration of that. If it contains tensor cores, which the latest NVDAs have, then the situation is even better. You get a significant significant boost in sense does it make to have a local workstation as opposed to just getting those resources in the cloud? So what I did is I took a reasonable amount of resource, something that will cost you about $10 or so an hour. $10, $ so an hour 10 12 dollars an hour is pretty intense i would imagine if you let it run for a day you are raking up a two thousand four two two hundred and forty dollar bill and by the time a month is out you're raking in seven eight thousand dollar bill just in that for you to get a performance so in eight thousand let, in three months, it would be 24,000. If you build a workstation that pretty much had that same amount of hardware, I mean, like basically that same amount of money, built a hardware, I actually did a comparison and worked out the calculation. You build this machine yourself and then you'll realize that your machine has unbelievably two orders of magnitude speed. The speed will be somewhere between 70 to 100 times, just closer to 100 times for even any of the standard data sets, ImageNet and so on and so forth, if you try to train a CNN. So a strong argument is to be made for having a local multi-GPU hardware. I'm saying this because some of you I know have talked to me, you are going to do your own startups in deep learning. So be aware of that. So when you start your startup, let's say in January, in the beginning, when you're not serious, use the cloud, the moment you notice that you're spending six, seven hours a day training neural nets, immediately go and get hardware, even if you release that hardware from our manufacturer, maker from a vendor, it will come out much cheaper than renting it from one of the cloud providers, any of the cloud providers. So keep that in mind. Asif? Yes. So how complex of a problem are we actually working on when we have to move out of Colab and start working locally? Because I know Colab's pretty powerful. Colab is indeed very powerful. Colab will run out of mileage by end of this, another two weeks for us. When I go deep, see the next three, four months are in depth, right? So the sheer data set, I'll ask you to train networks from scratch. Then you would at least have to move to Jupyter notebooks and GCP. And potentially if you have, some of you have very powerful workstations, use that. But definitely Jupyter notebooks in the cloud and in which you have requis requisition that on those instances you have added multiple gpus you'll find yourself doing that for at least some of the labs otherwise what will happen is every iteration of training will take you two days so then you find that it didn't train well the learning rate was bad you go and tune the parameters, then you wait another two days. You understand what I'm saying? Do you really want your machine to learn so slowly one day and things like that? So we'll see that. But at this moment, don't buy anything. One reason not to buy anything is, of course, as you know, NVIDvidia's 3090 has been announced but people have not quite received it it's in pre-order stage and there has been a little bit of a drama that's upon mentioned and then i researched um it turns out that the pre-production version uh some of the vendors they and including nvidia itself they have two kinds of capacitors. There was a cheaper kind you could bolt on and there was a more expensive kind and in the p and there was a on the board there was a place for both and the early pre-production version it seems that Nvidia bolted six of the cheaper ones and not enough of the more expensive ones. So that 3090 was giving a lot of problems. It was crashing. And then NVIDIA debugged it and they have fixed the problem and realized that what it is. They have recommended use no more than four of the cheap capacitors and a lot of the, at least 20 of the better, more expensive capacitors. So that is what the vendors will go out with, which means that be suspicious of anything that comes out early. You don't know which reference board you are getting with your card. But so in any case, you have a window of time. I don't think those cards will be generally available anytime before late October, November, because there is a huge pent up demand. I'm told that just the pre-orders this time are phenomenal. Like whosoever I go to and ask, when can I get the 3090s? They say it's a long waiting list. So we'll see. So from that perspective, don't buy it if nothing else see you don't have to get the latest and greatest but the moment those things come out and actually hit the market a lot of people they will be in a hurry to uh you know auction off their 2080 ti's the the previous generation top of the line carts and the Titans. And those things will be available dirt cheap, especially if they're used. And that would be a great opportunity. It's always a great opportunity to pick up that sort of hardware and use it, especially for learning, because you're not doing mission critical stuff. What is the worst that will happen if it crashes? So you could pick up that sort of hardware literally dirt cheap in a few months so wait a little bit before you do that any questions before we move forward all right guys so i'll move forward now let's look into that was about the device so device data loader by now you know what the data the device. So device, data loader, by now you know what the data loader is. Who's the data loader? I give you a metaphor for the data loader. What does the data loader do? Pick up the sand. Exactly, it's a guy with the shovel who takes a shovel full of sand and gives it to the processor for training. The batch of data, it is the one that loads a batch of data from whatever source and hands it over to the training. That's a data loader. Data loader can have multiple workers. It's one of the parameters you should set carefully. See, now let me start injecting real life experience, guys, because these things are rarely mentioned in books. So just as in photography, have you noticed that anytime Silicon Valley engineer gets a bonus, he invariably ends up buying, one of his first purchases is to buy a really big expensive camera digital slr right and then he would be and buy a few expensive lenses and he'll walk around taking pictures with those except that the camera would be on automatic and those of you who know photography would realize that this is a joke it's terrible yet you see it all the time right anyone can uh anyone can attest to that fact yeah yeah it's very very common and the same unfortunately is true with deep learning the way that camera example appears here, is that people will go reserve themselves very powerful instances in the cloud, multiple GPUs and this and that. And then they will even write the right code. And they will think that their code is running very, very fast. I've literally had the pleasure of sometimes walking in, making literally one letter change, one digit change. And suddenly, their thing is running three to four times, five times faster. And that thing is the number of workers in the data loader. See what happens is, suppose you have four GPUs. So when you give it a large batch of data, let's say you give it 64 data units, instances of data. Between four GPU, it will get badly processed. So each GPU will get about 16 instances of data, 64 divided by 4. Now what happens is it is ready to take that data in, except that the data loader has to feed in that data as fast. The communication between the processor and the GPU is extremely fast. It's happening at gigahertz. You know, it is connected to the PCI bus on your motherboard. On the other hand, data is being loaded from disk, isn't it? Disk IO is always orders of magnitude slower. So what can you do? You can parallelize it by having a lot of workers who all go and read the data, different batches of the data from the desk in parallel so that is the number of workers number of workers matters especially when you have a multiple gpu situation now here is a basic rule of thumb that i use um and you can so you guys can research and if there is a better rule, correct me on that. My basic rule is whatever the number of GPUs you have, use at least five times the number of worker nodes. So suppose you have four GPUs, a standard workstation configuration, or the cloud configuration where you're doing something heavy duty and you requisition for GPUs to your instance, then the number of worker nodes should be at least 20. That will make sure that your GPUs are not just sitting idle waiting for data. So that speaks to the relationship between the device and the data loader. Tertullian transform composed. Remember, once you load the data, what are the transformations that you want to make to the data? Do you want to standardize the shape, the size, and so on and so forth? That is this. Next is matplotlib, pyplot. Does this need any description, guys? I hope by now, in all the months that you have been with me, this should be looking like old friend. Isn't it it's just plotting library then from our libraries that we have created we have the save model load metal delete file file these are just functions that you are already familiar with so a neural network io i have put it into a special module of its own and feel free to just use it. But then we have some exception classes. It's always a good idea to create your own exceptions. Next is this class, loss history, in which we have some slots. Now, can you tell me what the slot business is? Has anybody encountered slots in Python so far? What in the world is this weird thing called slots am i just declaring an array of keywords is it just an array of strings or what is it it's a list it's a list looks like a list of strings actually this underscore underscore slot underscore underscore is actually a reserved thing. It's by convention. When you create this, you create the closest thing to a high performance C struct. You know, in C, you have a struct, right? How many of you know C programming? Oh, goodness, you guys are the younger generation, none of you know. No, no, there's one. Some of you do know. So you remember the struct, and you could have the parts of a struct, complex data structure. And in Java, it would be the Java bean. You have some member properties, and you have getters, and setters, and so forth. And now it turns out that in Python, actually, you don't have something like that. People think you have, but when you create a property, you know, you do self.x is equal to something, which, for example, in this class, you'll see me do. Like, for example, you see me there. I'm creating member variables implicitly in the initialization block in the constructor. It is creating member variables here, but this is not as efficient as the use of slots. What slots? So why is that so? In Python, all member variables, because it's a dynamic language and it doesn't know how many member variables there are. You can go on sticking member variables to a class, to an object, isn't it? So member variables are actually key value pairs. What really happens is internally, there is a dictionary which is referred to by underscore underscore dict, D-I-C-T underscore underscore. Are we together? dictionary. And this is the key to the dictionary and whatever object value you want to put that value will be there. Now dictionaries, inserts and removals in the dictionary, I mean, is cheap, but not as cheap as having, dealing with true array-like structure. Here you know exactly how many member variables there are. So the memory allocation that Python can do when you do this is far more efficient. So when you know that you're going to create gazillions of something, use slots. It's just a best practice. Use slots. Are we understanding the distinction between slots and just implicitly declaring member variables is that clear guys do you want me to repeat that is the slots uh supposed to have the data types same in this case they're all strings no in fact they are. It is just the name. Sequence, epoch, step, and loss are the names. Sequence is an integer, epoch is an integer, step is an integer, loss is a float. Do you notice that in the signature I've given it? just ignore line 27, if you look at this in it, if this line 27 was not there, this code would still be true. Isn't it? But it would be the kind of Python code that most of you are familiar with. If I'm 27 is removed, would you agree that the code looks extremely familiar? Isn't it self dot sequences sequence epoch etc so you would create member variables the normal way what i'm suggesting is that a lesser known but really worth following best practices uh if you know you're going to create a lot of it, or think of it as Java beans or records. If you're going to create a lot of records, declare them with slots. So I see two questions here. One, okay, one is an observation. I'm wondering by doing the slots, is it something like you're making, you're forcing Python to give you a contiguous block of memory into which it's slotting it? Absolutely, it's just doing better memory management and it's much faster accesses. That's all it is. The other thing, if you look at line 29, this code, it does not, the typical code in Python doesn't look like that. You notice that i declare types right i am saying what sequences epoch is step is losses and you may have seen this all over my code it is a best practice guys do always give the types especially in your function and method definition. It is not only a very polite thing to do for the reader of your code. It is a very nice thing to do to yourself because soon you'll forget your own code. Guys, code is transitory. It's like car. It changes hands. In the lifetime of a car it goes to what, four or five owners? In the same way, the code that you write, you will move on from that company or that team, but that code will live for 10, 15 years or 20 years. And many, many hands, many, many minds will work work on it you're doing them all a favor by giving this types other thing is then you get some of the benefits of statically typed languages like c and java if you those of you who are familiar with visual studio code and eclipse do you realize what wonderful code as you know code suggestions you get with eclipse do you realize what wonderful code as you know code suggestions you get with eclipse code hints and code completions do you guys remember that any anyone of you with experience with with eclipse and java not many who said yes wonderful yes you i know you're working with Java. Anybody else? So anyway, if you're working with either Java or C++ or C in Visual Studio, you do know that if your project is properly set up, as you type, you get a lot of code assistance, intelligent code hints. And in fact, that is the strength of those IDs. Those IDs now increasingly are I don't know if you know, but they are AI driven. Those recommendations for code completions are not not just coming from the excuse me, compiler and parser. They are actually coming from AI recommendations, the sort of recommendations that we will learn to make in this workshop in a subsequent month. You'll see how to recommend things, how to complete sentences for people, and so on and so forth. And it's doing that. Now, here is the thing. If you give hints in your function, your own coding will be very, very efficient. It will have the same feeling as writing C, any of the static language code. So I would say that the single biggest improvement in Python that has happened since 3.5 onwards is that we can do code hinting now. These are called type hints. And I would strongly suggest that at least in the method signature, please do type hinting. So with that thing, I'll move forward. And again, I'm mentioning a few things about Python. By the way, how many of you are finding this somewhat a distraction and we should move a bit faster? Any frank opinion? This is kind of critical. It's critical, all right, let's stay with that. Now, what in the world is line 35 doing? What the self there? The self tells that this function belongs to this class. It's a member variable. It's a Pythonic thing. Line 35 is providing a way of, if you ever try to print this, this is what it will do. Exactly. Excellent, Premji. So what it does is, in Java you would call it the two string method it is always a good idea that when you create a data holding class you give a string actually broadly speaking you should have a string representation in every class that you create in python but short of that it is absolutely necessary practice that when you create a data holding class and we are data people you should have this now python has two methods one is this uh repr representation underscore underscore representation the other is underscore underscore uh string str you you should always prefer to use the wrapper because this the underscore underscore str will always call this so this is the root thing and this is the advisable thing to override well all right so with that i have created a base class this is an abstract class for classifiers what you do all the classifiers do more or less the same thing, right? They, you give it data. Once it has the data, you set a learning rate, you give it a batch size. And what does it do? For every epoch, it runs a few steps in the mini batch. Then it goes to the next epoch. And it keeps doing that. And inside for each step, what does it do? It does the forward pass to compute a prediction. With that prediction, it computes the loss using a loss criteria, loss function like a cross entropy loss. And then once it has done that, right, you are ready to back, you are ready to propagate the gradients backward. Now, hopefully I can talk about back propagation, right? All the gradients move back. The opposite journey of the forward pass is backward pass. Once that is done, what happens next? You can do one step of gradient descent because all the gradients are now available. The forward pass produced all the activations. Gradients are computed in terms of those activations. So remember one of the things we learned in the extra session is great. You don't really do any derivatives. You're just multiplying things that you have pre-computed in the forward pass. And you're just plugging it into the formula for the gradients, right? And so that also becomes just matrix multiplication. And you keep propagating it back. All the gradients are in place. Once the gradients are in place, you do the delta step. Namely, W next is W minus alpha times the gradient. Gradient of the loss with respect to W. At each of these weights, you do that. Because every weight now knows the gradient. And so in one fell swoop, you can take all the weights forward. So that's the loop of learning, right? So that code remains invariant, whether you're dealing with, you know, a river dataset, the toy dataset, or you're dealing with image, or image or later on with text, or with sound, or with time series or whatever, that's your standard learning loop. It is the learning in neural networks. So therefore, all of those things, it makes sense to have a base class, guys. This is another best practice I would suggest. Don't litter your notebooks with all that boilerplate code that is so tedious to read. The first time you read it, it looks interesting. But the hundredth time you read it, it's irritating. Especially if it has been poorly written. So don't do that. Write it correctly once and reuse it. So for that, you do object-oriented programming. You create a base class and you keep reusing it. So that is what this was. So let's go over this. Azif, one quick question on just the comment notation that you have. Line 40. Line 40. Line 40. You have an R preceding the triple quotes. What is that R doing there? What in the world is are doing there so yes this is not an art that you find it most Python code or libraries that I am saying who would like to enlighten me by the way white first even a more basic question what is the triple code doing is there documentation is the triple code doing Pramjit Ghosh- Is that documentation. Pramjit Ghosh- Is the documentation is multi line basically Pramjit Ghosh- It today is for art raw text. Pramjit Ghosh- Or is for Pramjit Ghosh- Raw text. Pramjit Ghosh- Close. Yeah, it is. But what it gives you me us in particular the ability is I can insert latex into it and mathematical expressions in latex and it will be rendered correctly in the documentation you know what latex is preemjit yeah okay that's what it is math is usually written in latex and so you can embed good latex into that to explain the idea see we are in a domain in which we write mathematical expressions you know this field is imbued with mathematics and so our documentation if it is a worthwhile sooner or later we'll have mathematical expressions uh embedded or you would like to embed it and that r helps you do that okay good so uh here by the way the number of workers is Change it. I invite you to change and see is there any performance on your machines when you increase the number of workers. So I deliberately set it to two. Remember, I told you the rule. If you have one GPU, make it five per GPU. Right. So therefore, I have underutilized the capacity at this moment. So depending on your hardware, try increasing it to five or whatever it is and play around and see what it does to you. Now, batch size is the mini batch size. I hope this does not need an explanation. Now, there is another thing, 54. You notice that learning rate I have defined as 0.01. What does that mean? 001. What is that? Is it aggressively high number or a conservative number? What is it? It's reasonably small number to start with. A small number. You should always start with a small number. Right. And then now there is a bit of a theory here. Remember, I have been talking about one cycle and few cycle learning. learning right those are called learning schedulers so the learning rate scheduler in the pytorch world this is the last time perhaps you will see in the code a hardwired learning rate are we together learning rate is something that significantly affects the performance of your algorithm. You can do this, this particular code itself, this lab as you do with the image processing lab today, you can change the learning rate and you can immediately see what a dramatic effect it has on the performance and accuracy of your algorithm. That raises the, what is the best learning rate? Now, as I mentioned, the best word is nuanced. In the early stages of learning, you can be aggressive with the learning rate because, you know, at that moment you're far from the minima, isn't it? So imagine that again, lost landscape. When you're really far from the lost landscape, the last thing you want to do is get trapped in some local minima. It's all right to isn't it so imagine that again lost landscape when you're really far from the lost landscape the last thing you want to do is get trapped in some local minimum it's all right to have fairly aggressive learning rates and just run over many of the things keep on increasing your learning rate after a little while you have to put breaks hey you can't go near the minima at full speed you will overshoot it. Or you might just jump across the valley altogether. So you can't. So therefore, learning rate, optimal, depends upon which part of the learning cycle you are in and how close you are to the minima. That valley in the lost landscape. So there are techniques to adaptively increase and decrease the learning rate and make sure that the right thing happens. One shot, few shot learning and so forth happens. PyTorch has an excellent support for it. And so I will complicate the code a little bit going forward, but not today. Today is our first in-depth walk through the code. We will stay with a fixed learning rate, and I encourage you to use this, play around with this learning rate and see what happens. Default momentum. This term needs explanation. I haven't covered the topic of momentum-based optimizers. That is a special session in its own right. So in the interest of making progress on the practical side, I've taken a lot of the theoretical stuff, optional theoretical stuff as sidebars for optional sessions. So sooner or later, you'll see a video pop up and we announce a day when we'll cover it in practice interactively. But momentum just means that when you are driving too slow, you could get stuck in a local minima. But if you have sufficient momentum, imagine that you are driving your car over the lost landscape. You have sufficient momentum, you will bounce out of the local minima, isn't it? And not only that, more importantly, not only bounce out of local minimas, but what is more important is, there will not be erratic zigzags in your path towards the minima. You will have a much more smooth trajectory. Remember I took the example of moving averages and stock prices. That is exactly it. So the question is, suppose you get the data which is pointing, the next data is pointing towards a gradient quite different from the direction in which you're moving. You have momentum, right, your speed. What do you do? Do you immediately abandon and go in that direction or not? So the ratio, what you do is a compromise in between. You continue a little bit in the direction that you were already going, but you make a small deviation in the direction that is being proposed by the new batch, the gradient of the new batch. And so that is the level of that ratio, a 10% this and 90% that is controlled by the momentum. How much emphasis you're giving to momentum as opposed to that little mini batches gradient, the ratio is that. So what happens is that heavy momentum means this your your overall gradient it is not changing that quickly it's slow moving very slow moving right and so it has a smoother part towards the minimum so that's the explanation for that now I have a constructor which is taking the mini batchbatch size learning rate. Now learning rate, you have to be careful. Too small a learning rate can lead you to trouble. This is my choice. By the way, maybe there are situations, I haven't seen them, but there may be situations where even this may be too high and maybe you need to go to even slower learning rate. But I put a threshold that it cannot be smaller than 10 to the minus 5. If it is smaller than 10 to the minus 5, you throw an error. Because I suspect that if the weights are very small and other things are small, you can have underflow. So that's my limitation that I usually put. And I've historically done that. I don't know if it's modern hardware. What the situation is. If minibatch size is less than one, you throw up because it doesn't make sense. Minibatch has to be greater than one or greater. So this is called in the language, in the purists say that this is called preconditions check. Whenever you write a public function, a constructor is a public function, public method, it is always wise or safe to make sure that you have been given meaningful data before you march on. So that is that. This is a logging statement. It creates a classifier. Now what does this line do? let's ponder over this line could somebody enlighten me what this could possibly be doing so if the gpu is available it will run it on the gpu yeah thanks it made my hand hurt for a while exactly so if gs are available, in particular, you might have the most wonderful GPU. For example, Mac Pros, people are very proud of. They're lovely graphic cards. I just ordered one for my daughter, but they won't count because they don't have CUDA support at this moment. And I think open source community is working for support for AMD graphic cards, but still in the works as far as I know. So yes, this is it. If you happen to have NVIDIA card, then it will pick up. And your computations will run much faster. There is one caveat though on 968. Even though CUDA is present, if you leave your network in this mode, this is okay. This is what you should do in the constructor. But there is one more question to ask. What if I have multiple GPUs in the machine, right? Or TPUs in the machine? What do I do then? And what you have to do is you have to use something called a data parallel. You have to wrap your model into a data parallel model, which will make sure that your code has some sort of a MapReduce effect. It passes, breaks your batch in amongst the four, let's say four video cards, if you have four, and then puts the batch through then aggregates the result and then you take one step forward like that so we have to do something else if you have multiple GPUs and perhaps in the next lab I'll bring that in actually not next time we still have to do a lot of things on the fundamentals but as soon as we go into the in-depth ones, you'll see me use the parallelization aspect of it. And the overall performance gain somewhere between using a very good quality graphic card and using parallelization is just stunning. It's just absolutely stunning. If you're used to doing your neural networks on your laptops, the effects are quite, quite remarkable. That's that. So this is it. And now I'm setting up this is just this thing. What are these transforms? Transforms are something specific. Usually you do them to image processing, right? So you crop the image, you do something and so on and so forth. So you crop the image, you do something, and so on and so forth. So that is for that. But we'll ignore that because for a river dataset, there is no transform to do. Can you guess what the create data loader does? Creates the workers that transfer the data? Yes, it gets that guy with the shovel. And implicit is that if you create the data loaded, the data loaded must also know where to get data from, like what data sets. Network, it must be given a network that it will store internally. Now comes the interesting thing. By default, I've given a cross entropy loss. You don't have to stay with that. For example, when you're using this class, you can always replace this loss function with some other loss function, right? For example, if you're doing purely binary classification, you might choose to use BCE loss with logics and so forth. So there are many, many loss functions. I have been mostly using MC loss and cross entropy loss. Those are good default loss functions to start with. Their general purpose usually very effective loss functions but then there are situations where you would want to use a much more specialized loss function. You know if you have outliers then you may look at some other loss functions for regression. And again, for classification, there are different loss functions. And you can go into. So that is, again, it's a little rabbit hole in its own right. Needs a session of discussion. Again, somewhere around in the next three, four months, in the extra sessions, I will have a session in which we'll go deeper into the different loss functions that PyTorch supports. And if you feel encouraged to write your own, then of course not. One of the research papers we'll cover, which is a specific loss function that was quite effective for image classification that Facebook came out with. But anyway, we'll do that. It's a little world in its own right. Asif, loss function, can we play around with other loss functions or just use this and optimize like yes of course so you know just because in the constructor i set a loss function doesn't mean you can't overwrite it okay the moment you instantiate the object the very next thing you may do is bolt on a different loss function into it okay optimizer once again uh what is the optimizer? Now, there can be many optimizers. See, all of these are pluggables that we have designed this class. By the way, this is a helper class I wrote for you guys. Feel free to use your own, but it should give you some flavor of a bit structured code can be written. What is the default optimizer I'm using? I'm using the Adam W optimizer. No reason you can't try different optimizers. You can plug in SGD, regular, this and that. There are many many optimizers. You can plug it and you can also plug in different learning rates and so forth. So that is so much for the constructor, I hope. This is it. Now, one more thing is you don't have to give it a network. Because suppose you're loading a classifier from the disk. Like you're trying to create, you have saved a classifier object to the desk and you're trying to restore it. Or you have persisted, more practically speaking, you have persisted a neural network to the desk and you're loading it back then you might want to have a situation in which you will give the network afterwards but then it throws a warning saying that you are in that situation that's what we said device the rest of it is very straightforward with target labels you just tell what the labels are zeros and ones so this is the main training guys. Let's think a little bit about this loop. And by the way, let's see. Once again, a preconditions check, because you expect the user to tell you how many epochs to run this code. If you don't tell the number of epochs, by default, it will run it for 10 epochs epochs cannot be negative i hope that is pretty common sense you throw an error if you find the epochs to be negative now what am i doing at line 145 remember that loss history uh record structure data structure that we created at the top class with slots do you remember that that we made it high performance so that we could create a lot of it so i am creating what am i creating at line 145 a list called history where i would store instances of the loss history object are we clear guys? Is this line easy to understand? Yes. So then we make progress. Yes, I just found where I screwed up. Oh no, sequence is sequence, step is I. I didn't scroll, this is fine. Okay, so sequence is zero. You start with a zero sequence. Now, what is your main loop for every epoch? You run through all the mini batches, right? So this outer loop is for epochs. The inner loop is for, what is this inner loop for guys for each step exactly so when you go to the trainer when you go to the data loader it will give you one shovel full of data you need to process one shovel full of data right so that's your mini badge i'm using the word shovel full but the word is of course mini badge so the first thing i do is i take the data which has come from there when the data loader gives it to me to this code it is all operations happening in the cpu memory right and we don't want to do the computations on the CPU. If a GPU is there, if you have the luxury of a GPU, we want to instead do the computations on the GPU. So therefore, the first thing we do is we just push it over to the GPU. Do you see me do that? This line is pushing the data over. Now what is the data here is made up of the label and the Input in the output, the input in the label. In other words, capital X and y little y in the notations that we have been using in ML 100, 200 So does this line 153 make sense, guys. Anything special that we want to say about it this is funny so then then it is important that for each mini badge you get rid of whatever the gradients the gradients are being stored from the previous mini badge you don't want those the gradients to come and contaminate your results. You reset. This is basic code initialization. You initialize and you get ready for node. Now, let's look at line 160. What is it doing? Self.network. You're calling the network. Now, this is a very, very interesting syntax. Network was an object, isn't it? If you go back and look at the network, where is the network? Let's go back to the constructor. It turns out that you're going to send, give it an object call, where are we? You're going to give it an object called the network which is an instance of module isn't it so how can you call the object how can you treat the object itself as a function that's what it that's what seems to be happening out here isn't it we're calling the network itself as though it is a function We are calling the network itself as though it is a function. Do you see this interesting syntax? Now if you're coming from C Java background, this will rather throw you off. How many of you were a little bit startled by seeing this code? Or am I the only one who was startled when the first time I encountered it. I thought you're creating an object of network. No, this statement doesn't do that. Network is already given in the constructor, but you're calling the network which is an object, but you're treating it. Right. And how can we do that? And this is one of the lovely things about Python. It's basically functions in Python are first class objects. And they can be called upon. So if you create a function object, then you can just call it. And a network or a module, to the extent that a network inherits from modules call it and a network or a module to the extent that a network inherits from modules it's a it is a function now that function in the module class if you go back and look what is it designed to call when you call the module itself as a function it goes and calls the forward method now let's go back to our neural net and see, do we have a forward, oh boy, we don't have the neural net here, but do we remember that for every situation we had a neural net, right? So let me go back to this thing and go here. Let's take any particular neural net. We created quite a few of the neural nets. Let me go to this CIFAR. Today we are going to study this. Might as well bring this. So look at this. We have a CIFAR net. Are we calling the forward here? So the minimal requirement for you to create a neural net is you must give a definition of forward the forward function right this is the prediction function that network should be able to make a prediction given an input are we together because the network is a function effectively you give it an input it must produce an output so you're wiring all the uh yeah all the layers but at this moment just pay just pay attention to the function uh just line 35 you need that because what that does is it forward it just takes the input and produces the output are we together and the word forward comes from the fact that it is the forward pass remember in the forward pass of learning you make a prediction in the backward pass you back propagate the gradients and then you make the step of learning isn't it after the forward in the backward pass you make the step of learning, isn't it? After the forward and the backward pass, you take the step of gradient descent. So this is just the forward pass. You're making the prediction. Together? But if you go back and look at this code that we are dealing with, it is not calling forward. Where are we? where are we yes we look at this line we are not calling forward are we now why is that who would like to give an explanation of why that is true why are we not calling for dot shouldn't it make sense that we should say self dot network dot forward inputs. Yeah. Vaidhyanathan Ramamurthy, But actually, that would be an anti pattern that would be a bad practice. that do exactly that, they will say self.network.forward, but you should not do that. Because when you call this way, the way it was meant to be called, the PyTorch will do certain important bookkeeping work, administrative work, before it goes and calls the forward. And so if you just manually yourself force a forward call, you're actually bypassing all of that and you should not. So it is actually a wrong practice to just call a network.forward. That should be left for the PyTorch to do under the covers. So don't ever call that. Whenever you see that kind of a code avoid that. Is the forward a keyword reserved for this purpose? Meaning what if in the network class you have init and then you have forward then let's say I had backward. So will it matter what name I give to that function? the reason is and this is object oriented programming uh the way this thing works is where was i yeah object oriented programming says that this method forward it comes from this module so you're using uh word, technical word is you're overriding a method which is there in the base class. In fact, you're implementing an abstract method in the base class. So from that perspective, so long as you subclass a module, the word forward is reserved. It must be called forward. the word forward is reserved it must be called forward now to your other point don't ever have a method in your class the neural net called backward it will confuse people because backwards is reserved for pie torch itself remember back propagation it does on its own the that is why you call it auto differentiating framework or autograd framework. You only call the forward. It takes care of computing the gradients and doing all of it. But what you do is it will implement the backward. You don't do it, but you, and here's the crazy thing, you know that it is there so now look at the syntax on the network by calling the network itself you have implicitly called the forward step so guys are we together so far this is the heart of learning in neural networks and i'll repeat it go ahead frankly as if the missing link which i'm kind of uh struggling to understand is when you do this thing called self dot network open parenthesis and inputs right yes you're creating you essentially on the network that the network object you're instantiating it with that inputs as the inputs no you're not it is not the constructor that's what I'm trying to tell the construct it is not the constructor okay that's the part i'm missing then okay network is already there but paradoxically or rather once you get used to it very sensibly that network object is a function and of course you can call a function so imagine a cat a class called cat and it has a meow function. And every time you call the meow function, it will print out literally the string meow on the standard output, let's say. Right? So if you have a function, now what I'm saying is in Python, the class itself can be a function. Right? And the way you give that ability is, there's a bit of a technical detail in Python. There is a underscore underscore call and so on and so forth. If you have that, it's a then it's a callable. Basically, it's a function object. Okay. If you're only used to C C++ Java, it throws you off a little bit. That's why I spent so much time talking about it. Okay. I have to play with it to kind of get it. as java it throws you off a little bit that's why i spent so much time talking about it okay i have to play with it to kind of get it yeah remember do not do not call dot forward that is an anti-practice anti-best practice right it's an anti-pattern always use it like this. Despite what the code examples on the internet persuade you, stay away from that. Now that we have the forward part, guys, please mute yourself. When you do the forward- In this case, is the forward then, sorry, the network is inherited or do you have a implementation of that object? Yeah. This is a base class so when we use it you will see how we create the network so look at this example here we are creating the network a simple network okay you define a class for simple network here is another class that i defined for another network right so two three networks for you to play with. And so that is that. So this far, guys, it's very important that we understand up to this, because if you understand this bit of code, this learning loop, you pretty much have learned the heart of PyTorch. Are we together? You'll be very confident doing other things in python if you get this main loop down now what in the world is line so what is the network doing it's calling the forward on the input so when you call the forward on the input the forward pass produces what it produces the predictions the What is the predictions? You have the predictions, but unfortunately your predictions will not agree with the labels, is it? There will be errors in there. So you need a loss function to quantify the amount of or the severity of the errors, isn't it? So we use the loss function. Now, if you recall the built-in loss function by default loss function that I used was cross entropy loss. That is a good loss function for binary. I mean, not for binary, I'm sorry. For generally for classification, right? If you're doing classification by default go with cross entropy loss. If specifically you have binary binary you may prefer using bce loss with logics but or you could just stay with cross entropy laws in the beginning so that is that so this is it now you pass it to the last function so what will it produce it will produce your loss right the the severity of the errors in your prediction so using the severity using that loss what can you do now you have a back propagation of the gradients loss that backward will do a back propagation of the gradients isn't it guys remember this was part of your quiz that is this is this reminiscent of things we have done and talked about so yeah so this is it and now rafiq to your question do you notice that even though i'm calling uh this loss that backward etc etc it is implicitly done by PyTorch. So you don't implement a backward function in your network, right? You don't try to compute the gradients and things like that because the gradients and those things, there's a lot of, you have to worry about the loss function itself. If you plug in a different loss function, then it will have a different gradient. Loss functions depend on two things. The gradient function, I mean sorry, the gradient, so the back propagation depends on two things. What loss function you chose and what activation function you chose. Both of them affect, right? So where do we specify the activation function that you do in your neural net right so if you look at this code could you identify any line here which could be the activation function from what we have fc uh 3940 yes 3940 and 36 37. yeah that's we are using the activations there right so you can therefore surmise that we have effectively three like four explicit layers of activation and one fifth implicit layer of activation. Right, so it seems to be very close to the Lean-at-5 in some sense, this design. So that is the backward step. Then you take the step, now that the gradients are propagated, this is a step of, this is your delta step of gradient descent, isn't it? W next is W minus alpha, gradient of the loss. I hope by now we have been over this in theory enough. This is just a counter sequence, just a counter, keeping track of how many steps overall we have taken. Then the loss will be, what will the loss be it will be a number isn't it it's a real valued number but in everything during training is a tensor it's a pie torch tensor and potentially it is sitting on the gpu so we need to fetch it back as a number because we want to store it how much the loss value really is and once you have that you notice that i'm using the constructor of the loss history i'm giving it the sequence the epoch the step the loss and appending to a list a history list uh yes one question so the step number 162 and 163, why have we split it? I mean... The reason is, it's sort of the way PyTorch API is designed. Some APIs don't ask you to do that. They sort of hide it under the covers. For example, in Keras, it's not on the surface. It's not very obvious. At least the version of Keras that I remember working with more than a year ago. Anybody who has been using Keras recently can confirm or deny whether these steps are very explicitly visible by default. But PyTorch community made a decision to be accessible both for learners as well as for researchers so they took a they're neither very low level nor very high level Keras is very high level the rest of tensorflow is very low level so what happens is if you can get away doing cookie cutter stuff with Keras then it feels very easy you know you don't have to write any of this code you can do this thing in keras in actually fewer lines of code the trouble is you can only do so many things with keras then when you go into anything more sophisticated or you try to create your custom architectures then you're totally on your own then you have to create your custom architectures, then you're totally on your own. Then you have to go down to the nitty gritties of the underlying TensorFlow code API. And that is very complex, or perceived to be very complex till you become familiar with it. So even with the TensorFlow tool. So the way PyTorch took the position is, they leave all these separated these steps out so you can choose to call it or not call it are we together so for example you may take backwards at that moment what can you do you can actually print out the weights and the gradients so you can see what the previous weight is what the gradient is and therefore you can see what is going to happen to the weights given the learning rate isn't it it becomes much easier to put a break point here at line 162 after the backward and inspect what is the state of things before the before the back propagation i mean before the gradient descent actually takes place. You have made the prediction, you have computed the gradients, right? Your weights are ready, your activations are ready, your gradients are ready. It might be worth inspecting it just to get a sense of what you think will happen and then let it happen. So it makes debugging easier. Tremendously easy, tremendously easy. And when you're when you're creating custom architectures, features like this and PyTorch are a godsend. There is a reason why most of the research community has moved over to PyTorch. See, TensorFlow is a great framework. TensorFlow 1.0, when it came out, it literally wiped the slate clean. All of the frameworks pretty much were in the limbo after that. But then it had significant demerits. It was very low level, hard to get used to, and so forth. Then they tried to fix it with TensorFlow 2.0, which was based on Keras. When you're in the Keras world, all is fine. But the moment you get out of keres 1 you get into trouble pretty fast it gets more complicated and the other aspect is which is a little bit of a an unpleasant surprise tensor flow to tensor flow 2 is actually significantly slower than tensor flow 1 right and of course much slower than pi touch those guys have really gone and screwed themselves so i'm sure they're trying to dig themselves out of that hole, but at this moment, that's the situation. All right, so now we create, so does that answer your question, Prajeet? Oh, yes, it does, thank you. Okay, guys, so you notice that we have dwelled almost for an hour on this little bit of code. It was important. The rest of it is bookkeeping. I'm keeping track of the losses and printing out the average loss per run, you know, for every few steps, 100 steps or something like that, and so forth. And then I'm storing the history of this training, because I'm going to use the history of this training where? Oh, lovely. I'm going to do a plot loss now let's go back and see where where do I call the plot loss ah look at this now just to recapitulate what is what are the steps we are doing we are taking the classifier we are training the classifier now this is a step one of you ask can I change the the loss function yes you can you create the classifier you can just go ahead and change the loss function right here and put some other loss function if you wish now just to recap what is the thing you do you make the classifier make predictions and we'll see how and then which is straightforward and then you use the scikit-learns classification report which you folks should be quite familiar with by now then we call classifier that plot loss now this is is classifier is our class. And this plot loss is something that we wrote. So let's go back to that and see what did we write in the base class. sequence id you know every step that you take every iteration is a sequence id has a sequence id maybe i should have called it iteration then the y is the amount of loss that that emerged in that step right and so when you plot this plot x y this is a standard pi of the matplotlib plot and this function returns a plot so that in your jupyter notebook you all you need to write is this now guys i'll give you a choice do you think that this code belongs to where where am i this code belongs to the jupyter notebook Would you rather see it there? No. No. No. It doesn't serve, it's just syntactic clutter. There is nothing profound about this code. It's just mechanics of drawing a plot. Right? And the meaning is already conveyed by the name of it, plot loss. Right? And so forth. So these things, you should not do it, right? I'm trying to tell you some things that are pretty much a disease in our community of data science. People write terrible code, you know, just almost disgusting every time you look at somebody's notebook, right? Especially if you care about the aesthetics, you practically have to hold a paper bag next to you. care about the aesthetics you practically have to hold a paper bag next to you i'm kidding but that's that anyway the next is plot predictions this is a simple plot what it does is it plots uh in in this particular case it plots the ground truth and the predictions if you look at it this is standard code what do i do i take the predictions i take the ground truth and the predictions. If you look at it, this is standard code. What do I do? I take the predictions, I take the ground truth and then make one plot for ground truth, one plot for prediction. What is it producing here? It's producing this graph, isn't it? Now, why is it that I put it in the base class? Because I can use it everywhere, isn't it? Everything, whether I'm doing image processing or whatever data set or California housing data set or breast cancer data set or word processing and whatever it is, we could do, we can use all of these rather useful functions. That is it. Likewise, what does the save do? Save just saves it to the file. And once again, this calls other functions, delete file and save model. You can go into the network, into the classes and NNIO and see the way I've done it. It's just a little bit of best practice there if you want to know from a coding perspective if i go to an io where's the nio gone uh commons and i in an io here we are so in the nnio you say save model so um amongst other things these days it's considered a best practice and not to use this is so much but directly to use this class where am i uh path file path uh apart to part lip somewhere here i must have included the part lip my eyes are not able to see where i included it but okay oh as we learn common utils okay it is implicitly there because i'm putting it in another place so or is or is it? Okay, somewhere there's a part lib. So this comes from all of that, part, parent directory, make directory. It's supposed to be the new way of doing things, a little bit. So that's what it is. It's a cleaner way to make sure, load a model and make sure it is there. Nothing sophisticated. Just do it, does the obvious in a little bit more careful way, that's about it. So going back to this base class, we're almost done. Load is the opposite of save. It does that, you give it a network and it will go populate the network with parameter values. You give it an empty network with randomized parameter values and it will read that dictionary of parameters that has been saved to the disk and it will load it and populate your network with it. That's all it is. The predictor- One clarification. So when we are saving, we are actually saving the trained network. The parameters of the train network. If you look a little bit, a little subtlety here,, you don't save the network itself because it is an object and not worth storing. You save the state of the network. So the state of the network is this thing. You see the state dictionary. Given the network, you save its state as a dictionary of key values which is the parameters yes exactly it is all the parameters of the network because that alone is what so long as you know that you know you remember the architecture of your network you can always create that like in the load you see that given a network you just populate it with that thing then comes predict which is straightforward what you do is you take a network in this particular by the way i put images because the first version i wrote for images in your lab one but it shouldn't be images i should call it batch a batch of data. So given a data, it will make a prediction on that. And then given the prediction, the way it will make prediction is, it will produce energies. It will say, cat has this probability and that has that probability and so forth. Normalized and making it look like probabilities by dividing it and normalizing it. But the thing is, when you are making predictions, you don't want to know what the probability of each class is most of the time. You want to know which of the classes has the highest probability. Is it a cow or a duck? Is duck higher probability or cow higher probability and that is your prediction isn't it guys so that you can find out which of which element has the highest probability which class with dodge max this is a standard code you pretty given the associated energies you can just get the indices and the index, the rest of it is very easy. Suppose it says the second guy in your list of labels. All you have to do is dip into your list of labels and find out what the second guy is. And it happens to be cow. You get the basic idea, right? That's it. So this is a convenience way where I return the actual string, cow, duck, whatever. Because PyTorch is a little bit low level it returns just the indices and so forth then evaluation of the model as if sorry could you please repeat that the the one with the yeah see suppose you have three classes huh cow is a cow duck zebra right so you suppose you give it give this neural network all of those labels the way you would give it is you would sort of one hot encoded you would just give it values you would say that the first label is cow the the next is a duck, next is zebra, right? And that's an array index, 0, 1, 2. When PyTorch will predict, it will predict this. It will predict for you some probability. Let's say probability of cow or some other energies of a cow. It will come up with some value. Let us say it says 30 10 2. all you need to know is which is the maximum value and what is the index of that so the first one is the biggest one it is at index zero in the array now what was index 0 in the array? Cow, duck, horse. Cow, right? Patrick, you're getting that? Yes. So it is a convenience method I wrote in 28. Convert that index back into the object, back into the class. It is far more readable to get back your prediction as cow rather than to get zero or one or two or indices oh okay i get it now that's all it is it's just see these things make your life easy uh it fills the gap remember i said that pie torch is neither high level nor low level so how do you make it high level for yourself recover thecover the same fluency. You create a base class which has all of these conveniences, which bridges that gap, and then you use it. Isn't it? Why do you have an underscore before predicted indices? Underscore, comma? Ah, the first part is actually the energy that it produced, 30, let's say. Or actually, it depends upon which function you use. We have used softmax, so it won't be just 30, it would be normalized. So it will be like a 95% cow and 3% duck and 2% zebra and so forth. But the bottom bottom line is it will tell you the value also but do you care for the value you don't care so when you don't care for something in the return value of pytorch and in sorry not in python and in java in python and in scala, the convention is you can say whatever, underscore stands for whatever. I'm ignoring it. So you acknowledge that something is returned in the first, what is returned is a tuple, right? But you're not bothering to save it in a local variable at all. You're saying throw away, throw away the first part of the tuple and just give me the second part of the tuple. Got it. So, Asif, so this is like unpacking and you keep what you want and discard what you don't want from after unpacking. Absolutely. That's all it is. From the tu temple. You keep the, you keep the only the right hand part, not the left. And outputs a comma one means what, what is comma one? If you remember, uh, how many do you want? Top one, top two, whatever you want. Oh, I see. Maximum. Max. So that is that. Now. Two, right? Come again. Rather than using, bringing in tuple, we can use indexes too, right? Yes, yes. You could have done., you could have done. So you could have done some return value and then you could have written RV in square brackets one, because the indexing starts from zero. You're welcome to do that. You're welcome to do that, right? What I have done is almost by habit, I do it. Then I convert it to string. So this is, by the way, this is not learning PyTorch, it's learning my way of doing things. What you're going through actually, it just happens to be my library that I put on top of PyTorch. If you find it useful, use it. And then evaluate model. Remember that this classifier is sitting upon the training data as well as test data. Isn't it? Because it's sitting upon the test data also, I can evaluate the model's performance on the test data. And how do I do that? Very easy. And how do I do that? Very easy. I just need to make sure actually one of the things I should do here, which I didn't do I'll do that. See when you say torch no grad, you're basically saying we are in the inference mode, but there is an extra precaution I could have taken, which was that I could have said self.eval, like forcing the model into pure evaluation mode. It would have the same effect. Maybe I'll add that line just to be extra sure. It's redundant. If you're using one, you don't need to use the other. But Torch No Grad says what? Torch by Torch is an auto-differentiating framework. If you give it data, it will immediately try to capture the gradients. You know, it gets ready. It holds the intermediate values because it expects you to compute the gradient, the backwards. If you don't need it, then you can tell it that don't bother saving intermediate values. All you need is the final prediction, the output from the last layer. use. All you need is the final prediction, the output from the last layer. To do that is to say no grad. And another way is that you can say .eval, put it in the eval mode. .train puts a model in training mode and .eval puts it in the evaluation mode. So then what do I do for images and their labels right so they can be an image of a cat and the label would be the string cat right and for dog and horse and so forth so what you do you you use self dot predict and once again guys if you go and look at predict I did not use the dot forward do you see me use the dot forward no i'm using the network itself as a function it is always very important to do so so you get the prediction after that what do you do you use you remember that when i return the prediction i return it as classes and so forth you use use that and then you append it to this list. You know, one is a list of predictions and one is a list of labels, the correct answer. So you have Y and Y hat. Why would you care about these two lists of Y and Y hat? Can one of you guess? You calculate the error. Exactly. You can, you will have a classification report, if and only if you have the y and y hat. Otherwise, how would you produce a confusion matrix? Anyway, guys, that is a summary of the base class. I went over it very slowly. It is nine o'clock. Let's take a 10 minutes break, guys. Today, is it okay if we overrun by half an hour? Actually, I have to wake up early tomorrow. Yeah, it's okay. And listen to the recording. After that, yes, all the recordings do come to the website. So guys, I'm going to pause the recording. the website so guys i'm going to pause the recording all right guys so now we are going to go to the river data set and see the code that we wrote for it we have two things we have the river net the neural network and the river classifier river network if you remember it takes the input space is two right two-dimensional takes the input space is two, right? Two dimensional. And the output is of course a single dimension. It's a classification problem. Now in a neural network, when you create it in PyTorch, you divide it into two sections. You make it as a init is where you build your layers, layer by layer. And forward is where you pa you take the input and you pass it through the many layers are we together you pass it through the many layers so let's see that we are in the input here we are going to build the layer we know what the input dimension is two and what is the output dimension why is the output dimension two isn't y hat supposed to be a cat or dog just a value why is it two dimension so i'll give you a little bit of a preview. It has to do with the softmax. What basically says that you put a neuron there, one neuron for each of the classes. So each neuron will, it's like a prediction, but each neuron will predict how likely it is that the right answer is that particular class. So because we have a cow and duck, let's say a scenario of two classes, in the river data set, the values are 0 and 1. Therefore, you need one neuron to tell how likely is it that it is 0, how likely it is that it is 1. Think of cows and ducks. How likely is it that it's a cow? How likely is it that it is zero how likely it is that it is one think of cows and how likely is it that it's a cow how likely is it that it's a duck and when it says that it need not say that as probabilities it can it will all independently come up as a number it could be three or six or something right negative number or something and then you can normalize it that's what the soft arc max does normalizes it and converts it into a probability but that's the basic idea because the output dimension for a classifier is the number of classes in the category in that categorical variable in this case for the river should the probabilities calculated by the two classes add up to one yes they will add up to one and that brings us to the topic of softmax we'll do i'll try to cover it this week itself sometime soft with max is at the heart of the classifier i Asif, what is line 35? What does super super mean? Oh, it is calling the base class. It has to do with object-oriented programming and inheritance. If you're not familiar with it, just think of it as a magic mantra you always have to write. Whenever you extend, when you create a class that inherits from module, then you have to write the sentence where the first word will be the name of your class, the second will be self, and you're calling init all over again. But you're calling init on the base class. Means if there is some work that your base class is doing, you say, first, go do the work that the base class is doing, and then we'll do the work that this particular that I want to do right this is so input dimension output dimension now obviously notice that I have put a dropout fraction by default value it is zero that is another lovely thing with the the new language like the new you know the python 3.8. It has been there in Python for quite some time, but I like it. That you can give default values to variables. We couldn't do that, at least in Java, for the longest time, or C, and so forth. Now you can give, you can just specify that if a value dropout is not given, what should it be? So that is that. Question. In output dimension, you said two, right? Yes. Isn't it like a river, not a river? Yeah, so it is in the river or not. It is river or sand. Therefore, there are two classes. The output are two classes, zero or one. Therefore, the output dimension, as I said, is the number of classes in your categorical variable. Got it. Right? Because one neuron championing for one class. Yes, sir. Okay, so now you have a fully connected layer. FC, it's by tradition, you know, in Python, you could have called it whatever you want. But remember that joke in Python, you could say import numpy as PD and import pandas as NP or something like that. You can give most confusing names and confuse everybody. But in the data science community, there is a very strong convention. Well, let me not use the word convention redundantly. Otherwise, I would say convention to follow convention. There's a strong tradition to follow certain conventions such that our code remains readable across different audiences, different people. And so in that tradition in PyTorch, our code remains readable across different audiences, different people. And so in that tradition in PyTorch, the fully connected layers, you always call FC. FC is your dense layer or linear layer or fully connected layer. And then you start ordering them from 0, 1, 2. So anybody who reads this code knows that later on when you stitch them together in the forward, the FC0 will be an input layer, FC1 will be a, not input layer, the first hidden layer getting data from input, then the FC1, FC0 will pass data to FC1, FC1 will pass data to FC2, you know, after the suitable transformations and activations. Now I've deliberately commented out this thing, batch normalization. I talked about batch normalization on Monday, I believe. Once you have reviewed the topic, I leave it as an assignment for you to just go uncomment this and see what effect does batch normalization have on your code. See, guys, this field you will learn through experimentation. It's a scientific discipline. Especially with neural networks, we don't know all the answers. The theory goes only so far. It's a very experimental science. And you have to learn to do experiments so that you have a feel for the situation, feel for these little things, these little knobs. What happens if you turn this knob? So it's a little bit like being in the cockpit of an airplane with a whole dashboard with lots of knobs and you don't know what each of them does. But dangerous, but you have to ultimately play with those knobs and see what it does. Suppose you didn't have an instruction manual at all. And you're sitting there in the cockpit trying to fly well, actually, the whole thought is so preposterous. Let's not bring that up. But anyway, something like that. Better analog is mixing a song, sir. Yeah, mixing a song, yes. You have to play with it and see what it is. I used to be very interested when I was young in learning to fly. I still dream of it. Of course, once you have children, you become extra careful. You need to be there for them. But I suppose late in life and all my responsibilities are over, I'll again take up flying. But with one of the nightmares I used to have is what if you're sitting in the cockpit and you absolutely forget what each of the thing does? Then you'd be doing things like this, fiddling around. Anyway, so that's the forward layer, the linear layers. As you can see, for River, we are just using a feed forward network. Feed forward network is the simplest networks. They're everywhere. Whatever architecture you create, sooner or later you'll add a few feed forward networks in it somewhere, right? So it is the granddaddy of all things, learn it properly. The demerit is that they're very dense. There are lots of parameters, like exponentially large number of parameters. For example, input is 2 times 512, thousand parameters here. Here, 512 times 256. I don't know, whatever it is, a million or whatever, half a million parameters. And then 256 times two, 500 here. So those parameters, they all add up and suddenly you're looking at a giant parameter space, hypothesis space, in which you're trying to do your gradient descent. So that's a demerit of it. You need some way to discipline it. And that's what, one of the ways we learned to discipline it was using congulationals last time on monday so anyway this is batch normalization now till now i've been talking about relu and you guys have become very used to reload uh uh i have a question on this um so why do we have to provide output dimension as to FC2? What's the relation there? The reason for that is you're trying to build a network. Whenever you build a neural network, it has to know, see what is really happening in FC2 is, the number of nodes it is creating is the output dimension. So, suppose the output dimension here is 2. So, the FC2 layer actually has only two nodes. But you are saying that data into the two nodes will come from 256 nodes of the previous layer. That's what you're really saying. I think a better name in the linear could have been a previous comma current. Think of it like that. In the current layer, how many nodes are there? That's the second argument. And the first argument is how many nodes were there in the previous layer which is going to send signals to this layer okay that's the way to think about it now guys one more question before we move on so you said s a self dot fc0 fc1 fc2 where were these variables fc1 fc2 defined? Literally here, Python is a dynamically typed language. Whenever you refer to a variable, this is what I was saying that there is an __dict facility in Python objects. The moment the interpreter sees that you're referring to something as though it's a member variable, immediately it creates a member variable named FC1. And in the dictionary hash table or dictionary, it will actually go and insert key as FC1 and value being the right-hand side and then .linear object. Okay. So essentially in defining this class called river set, a river net, you're making the determination that it will have three layers and you declared layer one as FC0, layer two as FC1 and layer three as FC2. Absolutely. You got it. Thank you. So, excuse me guys. So this is that. I'm not tired, but today was a very hot day and on hot days I yawn a lot. I apologize for that. So the other thing that I do is, remember I've been saying ReLU and TanH and Sigmoid and so forth. So just for change, instead of ReLU, I use CLU. CLU, by the way, generally outperforms ReLU. You don't see CLU mentioned quite often, but if you remember, when I gave the talk on activation function, I covered CLU a little bit. And then I said swish, and then I said there's mesh. So one of the things you should always do is play around with different activation functions and see how it helps you. In this simple data set, it may not help you much. But become familiar with it. Don't develop tunnel vision in which the only thing you use is ReLU. ReLU is classic. I would go so far as to argue that in most situations, you can find a better activation function. Asif, in PyTorch, your activation function should apply to all the layers? You can't do it one layer at a time? No, no, you can actually. And this is another thing, because I am using the same activation function everywhere, so I created one instance of CLue. But you're most welcome to create a different activation functions see some people what they do is they'll put their softmax right here i didn't do that you notice that i put my softmax here at line 64. it is my style of doing it but some people they treat softmax as a layer in itself and all of these activations as layers. So what they would do is they would say self.outputactivation is softmax. You see that, right? So in the hidden layers, you're using ReLU or CLU. And then in the output layer, you choose to use softmax. Now, I do it differently, but I do the same thing. I'm using the CLU internally, but in the final layer, the final output that I return, before I return it, I softmax it. Look at line 64. Oh, okay, I see it now. And you're most welcome, if you want to go really crazy, you can choose an activation function per layer. Create a different activation function for each layer. It's absolutely a choice. Now, I don't know how effective that would be, but it's certainly your choice. And again, in this field, experimentation leads to very interesting results. Like for example, one of the papers that you're going to discuss this weekend, as I said, somebody bolted a transformer on top of a CNN and got excellent, I mean, not for somebody, Facebook guys and Facebook research, and they got excellent in a very simple, powerful image object detection or image segmentation algorithm out of it, right, or architecture out of it. So you don't know. Sometimes you may say, well, in hindsight, isn't that obvious? But no, it isn't. It becomes obvious when somebody tries it and finds that it works. But you also have to remember that for every time that somebody tries an experiment that works and publishes a paper, there are a thousand things people have tried that did not work. have tried that did not work. It's the nature of research. The only way to find out is by experimentation and you have to experiment. So, Asif, I have a question. How robust are these models which are published? You know, the paper you're talking about, would it, I've heard about model degradation. So model degradation is entirely different. It has not to do with the architecture. So, Model degradation is entirely different. It has not to do with the architecture. See model degradation is the fact that a model is trained on some data. That data becomes stale. So your model's performance degrades because the data has changed in the real world. So you're making a prediction on the sort of data you have not seen, right? So imagine that there's a river right the river temperature remains let's say between 32 degrees centigrade uh cold water to about 40 degrees centigrade and you have all sorts of models on how much how much fish you'll find um in different seasons and different whatever right let's say that you have a prediction model like that. And then comes, let's say, these nuisance things that we all are going through, the California fires, this tragedy. So let's say that all the rivers here in California, some of the rivers, they are feeling the impact of that the water temperature has changed so will your model still make reasonable predictions for the for fish concentrations in that river not at the subsidy yeah so why has your model degraded because it is you're not only seeing new data it is only seeing new data but it is seeing the kind of data that it wasn't trained on it is outside the input parameter space it was trained in one region of the input parameter space input sorry not the not input parameter input space right all the parameters were trained or the model was trained effectively on one region of the input space right but isn't wouldn't that be the same case for image segmentation absolutely obviously absolutely so let's take an example you have an image recognition system which recognizes modern today's cars all the cars today are some deformation some homeomorphism of a box right box upon a box with four wheels, roughly speaking. You take a long rectangle, you put another rectangle on top of it, a shorter one, and then you add four wheels, you approximately have the modern car. And then you give some homeomorphism to it. Tomorrow, let us say that you train your model, your network and all that. Tomorrow the entire shape of cars change gradually. What happens? Suppose most cars are three wheel now and they look like the beak of a bird. So that was my question. All these papers which are published with different architectures, question. All these papers which are published with different architectures, how robust would those architectures be? Will they only work for obviously the trained data or would it work across different data sets of a similar quality? Distinguish between architectures and training with data. An architecture is an agnostic object, is data agnostic. To train it, you need to give data and train it. Once you train it with that data, then you can say it's making good prediction or not good prediction. But so long as you have trained it and you're getting good prediction with a certain data set and assuming that you have not over fit then so long as real life instances come for prediction inference time and that are within the feature space on which it was trained you will get excellent predictions but what happens is that gradually you start getting data which is outside the bounds of the feature space on which it was trained. Right? I mean, to get like this example of river and fishes. Now suddenly the water is warm and your model is not able to make good predictions on how many fishes you'll find in the river because it wasn't trained on that. So then you have a situation, the model degradation refers to that. So it is not the architecture that degrades, it is the specific model that got trained. In other words, there's model parameters. You just need to retrain the model. And this, by the way, is a recurring problem. For example, if you look at an e-tailer, online retailer, the purchasing patterns of users keeps changing. So your recommendation engine needs to be updated all the time. Seasons change, many things happen. And so your recommendation engine needs updation. Makes sense, right? Otherwise, you have model degradation, your recommendations become weaker over time. Got it. So I'm guessing the parameters would have to change over time in case. Yeah, you have to keep retraining the model. The fundamental rule in the industry is you have to retain the model. Otherwise this, you Otherwise, this cloud providers, half of them would go out of business. What AI will they offer? If you need to train your model only once. So what is the passing criteria for a paper to be published on a new architecture? The many, see, you may have just a good idea. There are two kinds of papers. Papers that make a theoretical breakthrough. You'll see them filled with theorems and proofs. They arguably advance the field with some new breakthrough. And large majority of paper, what I call benchmarking paper, they will come up with a new architecture and show that, see, look at this standard benchmark, ImageNet, this, that. On some benchmark, some standard data set, they have better results than the winner. You got it. And the moment you say you have better results than the winner, you have a paper. You have an architecture worth considering. So this is incrementally your evolutionary development. You move the needle forward. So the vast majority of papers are in that category. And every once in a while, you have a fundamental paper. Like for example, attention is all you need paper. December, 2017, It changed the world. Right? But after that many architectures came based on transformers, but there's that and so forth. But again, was an interesting idea because it took half the transformer, the GPT took the other half, but took the encoder part, GPT took the decoder part. But after that, all these variations came and each variation had something interesting to contribute, but they were a mix of new ideas as well as benchmark driven that, you know, they would work well for some situation and they could prove it. So, for example, the latest paper, there's one paper here. Somebody came out with a paper, I believe a Misha. He came out with a new activation function called Mesh. And so when you look at the activation function, the idea is quite simple. If switch is x times sigmoid of x, mesh is x times tan of h, tan h of x. Now, well, it's still a good contribution to the field, but you wouldn't call it a profound breakthrough, would you? But in practical terms, even though in the idea, in the theoretical world is not a profound breakthrough, but in practical terms, it's a very useful thing, because he showed that well you know what, it beats a state of the art. It does better than swish. And so let us all use mesh. And the next guy will come and he'll show that his little function activation function beats all of these and so it keeps going on so those are benchmark driven papers got it sasif back onto this one second you'll get a flavor of this see every week we do papers you realize that by the time this workshop is over we'll have done 20 research papers so you'll get a sense of all that go ahead now back onto this uh the forward uh function there is a new um what is the capital f i guess it's coming from the base class but what is it doing it's not from the bill it's pi torch they have a package called functional and it is a convention to refer it by just by the letter capital F. Okay. That is it. And it contains all of these, the dropouts and softmax and so forth and relu. So all of these things are there. Siv. Yes. Is there the, you know, aava doc kind of a documentation of by doc by doc yes by talking about that extensive documentation guys uh since you brought that up let me take you there by the way guys when you review these videos uh feel free to i mean help me out feel free to help me out. Feel free to leave some comment if you liked it. Do you see the docs here in the PyTorch website? Yes. This is it. Ignore the Java doc part of it. Here it is. This is very much like Java doc. Java doc was a broke tradition, tradition i mean created a very interesting concept that code should be self-documented that can become a website and now i think just about every other language has followed suit so you were asking about functional uh preemjit for example no what i was looking for example if you click on that torch.nn then what what are the functions available for you can go and create literally here do you see this so for example what are the last functions available oh lovely see how many last functions available one loss what is l1 loss? Do you guys remember? Mean absolute error? Lasso. No, not lasso. That is regularization. L1 regularization. L1 loss is, remember, instead of y minus y hat, instead of squaring it, what if you just added up the magnitude of the residuals? Absolute value of y minus y hat. it's a loss function that you use in situations thanks thanks yeah that is a cross entropy loss but then there are other losses right do you see how many losses are there poisa if you have a counting variable then you use Poisson loss. BC loss is specific to binary with large x loss. So it's basically a sigmoid. Margin loss and go on and so forth. So see each of these are a little rabbit hole and there is an enormous amount of theory behind each of these loss functions. So you can go and play around with it. See guys guys, this is how it's a, any one library of these things takes months before you become a guru of it. But the only way to do that is not to go read the documentation, but to read the documentation on a need-to-read basis. Whenever you need to, go browse, see what is here, and you'll gain something from it. All right, guys, so we go here so the forward makes sense this is where we stitch things together so what we are saying take the input pass it to the fc0 layer first hidden layer whatever comes out will be the z the z of it then you activate apply the activation function on the Z, Z of first layer, you'll get the output. Take that output, once again, apply dropout, means go kick out some of the nodes. Then I've commented out the batch normalization. I want you to experiment by uncommenting one or the other or both and see what happens. Then you see that X keeps getting fed in. or both and see what happens. Then you see that X kept getting fed in. If you look, the X of the previous line becomes the input of the next line, output of the previous line. So step by step, the X keeps, input produces activation, produces activation, produces activation, and finally till the last layer, whereas the last soft layer activation produces the output and you return the output. So that is your neural network. So we have to call the batch normalization every time, after every layer. It won't apply throughout. Yeah, yeah. Because you can choose to. You can choose to batch normalize after one layer but not after the other layer. But I mean it won't be possible to just give one batch normalization throughout and it will just have batch normalization after every layer. Applied everywhere automatically. No, that is the thing, like I told you, right? PyTorch is not very high level, but not very low level. It takes a medium place. And part of the medium place is it says, says why don't you spell it out how do you want it because in keras it was just one line yes but the trouble with keras is keras is great so long as you're in the kindergarten of keras but the moment you can't do something with the keras api now have you ever tried to write something in raw tensorflow no yeah so if you remember No. Yeah. So if you remember, you, I remember, took my class with TensorFlow in those days when I used to teach it with TensorFlow. Right? Yeah. TensorFlow in Keras. Yeah, TensorFlow in Keras. So I exposed you to both Keras world, and then we did some exercises in TensorFlow. Yeah. But you're right, I mean, it wasn't as flexible. Yes, it wasn't. You know why I transitioned over to PyTorch. I didn't even know that this kind of flexibility mattered, but now I can see the- Yes. See, what happens is that, if you remember, we understood the theory, but after a little while, the entire batch was lost in figuring out the TensorFlow details. So when you teach deep learning, you don't want it to become a big class in just TensorFlow and struggling with TensorFlow. You want people to learn the subject, you know, Monday theory, Wednesday lab. One day of the week should be enough to learn the library. day lab you know one day of the week should be enough to learn the library it was really slow as well because yeah it won't work on like the gpu code was different right that's right so uh here's the thing guys so what do i? I create a river classifier. Oh boy, see how absolutely simple it is. Everything is done in the base class. I don't need to do anything. So all I need is that you give it the network and you give it X train, X test, Y train, Y test. Do these things look very familiar guys? If you have been doing data science with Python, scikit-learn, trains print will produce these four pieces i give those as argument that you can choose a batch size i deliberately oh goodness i've chosen a rather big batch size by default um they can make it smaller play around with it actually i must be experimenting play around with these batch sizes and see where it takes you right this is a little on the high side. You should make it smaller when you play around with it. A learning rate is also a little bit on the aggressive side, if you notice, compared to what it should be. You can be even more aggressive. You can make it 0.1. Remember guys, do not start with big learning rates. If in doubt, start with 0.04 and run it for a whole lot of epochs. It's perfectly okay to run it for a few thousand epochs. Asif, is there a heuristic for learning rate related to... Yeah, so the heuristic is not just heuristics. We are in the fortunate situation that we'll have a whole theory of how not to have to specify learning rate upfront, but let the machine learning not only learn the parameters but learn the learning rate itself during training but for like the interview questions like why do we have learning rates you know at within this range like is that large learning rates means that you might hop across the minima right global minima all together but like what what is large and what is small like how do we distinguish here's the thing a 10 to the power minus 3 is a good place to start right 10 to the minus 3 10 to the minus 4 are good places to start anything more than that begins to get aggressive. 10 to the minus 2, 10 to the minus 1 are aggressive. Okay. And that is just because that's how the gradient . Yeah, yeah. And that is why, see, remember in the base class, we talked a little bit a while about that. Where is the base class discipline? Okay. I said that I've set the base learning rate. If you don't specify said that i've set the base norm learning rate if you don't specify it yeah it's 0.001 yes so that is pretty much a convention people use that okay so there is no like relation to the data size or path size i need to like that no no no so guys i want to point out one thing Do you notice that I'm doing no hard work in writing this class? Which means that instead of river data set, now suppose you have the breast cancer data set. Do you realize that your code will be very short? Do you agree? Yes. Be very short, very boilerplate. And that's what you should do. Create good libraries so that in practical situations, you're writing boilerplate code that leverages the power that you have created under the cover. So now let's do this. Now when you do the work, you can- Asif, one more thing. The code above, you have network and then you specify module is that like a convention no i'm saying that it is the data the data type of network is module you don't have to specify remember i do type hinting right so i gave a type hint when it is not clear give a type hint i see so that is a way to give a hint yeah that's right put a colon and then give a hint anything after colon and before a comma is the hint and is not strictly necessary as if you don't specify a random state anymore here just for fun oh the random state is in the data split isn't it data when i split the data then i give the random state 42 see here in the main code the driver code do you see me give the random state as 42 the meaning of life oh i see okay all right guys so let's look through the code of actually using it see once you move systematically through it right it becomes easy so now let's look at this code what does line 87 do could you guess sir uh min max color can we use the standards min max color is zero to one is not that good right yeah so i gave it here just so that i would encourage you to play around with it. In fact, if you go to the river, where is river careful? So the same code, whatever is in main here, I have written it here. Where is the river one? And see what I have used here. Standard scale, isn't it? Sukpal, you're seeing that, right? So another hyper parameter of your model, which scalar to use, there are no correct answers. And the reason I use one here and the other there is just to show you the variety or the flexibility you have in choosing the scalar you could use standard scalar you can use min max scalar you can use robust scalar and so your choice is good yeah because i heard that in general it's okay to avoid min max is there any specific condition where is you know min max produces really good results because standard is really good one any specific I can give you a reason suppose there is a massive skew in the data data got it is right good standards the standard scale scalar would get screwed as simple as that try it so see here's the thing and this is it see underneath all this you have to remember that there's a whole seeding cloud drawn of deep mathematics right and you can dip into the deep mathematics and learn a lot of things there. Or as a more engineering with Matt attitude, if you don't have time to pick up on all that deep math, you can just experiment and figure out which works. Yeah, like, early biostat is always like for on good for Gaussian distribution, right? So when we try to bring every data set into Gaussian distribution, and then it would be really good. Yeah, see, Sukpal, if you remember, whenever we saw skew, we use the box-cox transform or something to normalize the data. One of the things that we found often common was we would take the log. So for example, if you look at biological systems, when you look at something, quite often you take the geometric mean. Have you seen that in the medical biological community? We often take the geometric mean. We don't take the average in reporting something. the average in reporting something right and why do we take the geometric mean it turns out that people sometimes forget why we take the geometric mean so i actually read a paper in one of the medical journals in which one researcher pointed out that you know what geometric means are not always a good idea and he gave a few good examples to show that it is too. But if you are a person who's familiar with the Box-Cox transform, what would be your reaction? As alumni of support vectors, I hope your reaction would be of course, because there's no guarantee that one particular transform Box- so why why geometric mean geometric mean implicit to geometric mean is you take the log and then you take the average so suppose you take numbers a b c d e the regular average would be a plus b plus c plus d divided by four isn't it and what would be the geometric take the log of it and take one-fourth of the log right you see that like what is the like if you take the log it will become log a plus log b plus log c plus log d isn't it all of it divided by four and once you get the answer then you can go about taking the entire log yeah that's why we usually pick up more more time but mean is not a good a good idea it's statement people you must have read it some somebody arguing that standard skill is always better but if standard was always better, the creators of the library wouldn't create other scalars. The reason you have such variety is that, remember I told you the no free lunch theorem? It sort of applies here. There are no perfect answers. Data is king, and every situation is different. Yes, sir. There is a lovely, lovely paper that was written by, I think, either a Berkeley or an Oxford professor. He asked this question, are normal distributions normal? And he came to a pretty interesting finding. People always use the so-called. There's a theorem actually in statistics, eluding my name, the very famous theorem. What's wrong with me? I'm a bit tired. Anyway, because of which you use, when the effects are additive, right? Bell curve emerges on its own, center mean theorems. There's a class of theorems, group of theorems called the center limit theorems or center mean theorems that all say that whenever there are additive effects, you'll have bell curves. So he looked around the world and he said, are there really additive effects or multiplicative effects? And he found that quite often when you think it is bell curve, it is actually a log-normal distribution with a standard deviation of one, which looks uncannily like a bell curve right and there are a whole class of distributions that look like bell curve look at the t student distribution and so on and so forth right they all look like bell curves so resemble a bell curve they are not they have different skew kurtosis and so on and so forth. But they are. So the world does not have as many normal distributions as people thought. It turns out that log normals are sitting everywhere. Anyway, that goes to the deep end of mathematics. In very practical terms, you try your different scalars. Do not believe these things. I mean, just be skeptical of what you read on the internet. Our great president keeps saying that everything is fake news. The country doesn't agree with him. So here I would say that not exactly fake news, but be skeptical of what you read. Okay, so. So there we go, try it out, you know, play with it and reason, is it really true? Why would library makers create so many libraries? So many functions, if only one was the best. Yeah, so with the simple knowledge, like, if I can somehow bring my data distribution in Gaussian, my accuracy and everything will gonna be increased, right? That's a normal thing I believe see let us put it this way generally if I can transform the data yeah people tend to argue that quite often the world is a Gaussian mixture feature space is a Gaussian mixture, feature space is a Gaussian mixture. As such our job is simply to tell which of the bell curves is which class. It is not always true. Sometimes it is true, but it is not always true. If that were true, then in your case getting to a normal distribution case, getting to a normal distribution will have benefits if you are looking at a Gaussian mixture. But if you are not, then that situation won't help you. And that's a mathematically more rigorous answer. So imagine that you have a data, rectangular data, and the upper diagonal is blue and the lower diagonal is red. It's somewhat like a grape, a grape scent, Raja Ayyanar?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro somehow doing a transform to Gaussian does not buy you much. It only gives you a hint sometimes if your data has big skew that maybe you need to do it and maybe you'll benefit. Because if some transformation of it brings it to Gaussian, then your model will have better performance. So in other words, we have discovered a more natural representation. But it's not always true not always true yes thank you sir so now the next thing is the rest of it i hope so far so good it should look very obvious to you guys this much code here we split the data now you notice that i split the data 50 50 right because i felt that the data size was enough that you didn't need so much data to train, right? Let's keep a large part of the data to validate, right? And this is common practice in neural networks. Sometimes you'll notice that data comes in ginormous amounts. Be careful, because the more data you give to training, the longer the epochs will run. So sometimes it is worth playing with it. At what point is your accuracy saturating? Maybe giving less data is enough. In such a simple problem, you don't need to give too much data. It will zero into the thing pretty quickly and in fact that's it then you can keep half the data for validation otherwise what happens you end up in the other extreme you might have a good model but because your testing data was so small that you you are report you're under reporting the performance so small question here, sir. Because I was also thinking the river data set is kind of very simple. And in the first layer, you took 512 nodes. Isn't that too much for a river data set? It's an overkill. We live in the world of plenty. And so these days, it's fashionable, usually, if I may use that word in a humorous sense, it's fashionable to use no less than 5.12 to build your first layer. But for a river dataset that's too much, right? We can go with this. I can tell you what the minimum is with which you'll get away. If you attended that class, that Sunday class on the Ola's manifold and manifold topology and... Yeah, I was there, sir. We bought Klein bottle of beer. Yes, yes, Klein bottle of beer, isn't it? So, yes, you can argue from that and find the answer, but I won't tell you. So there we go, guys. So we can run it. Loss function is pretty stable. It learns. And the point that I want to tell you guys is if you go back and compare the accuracy, it's 86, you do different runs, it'll go from 84 to 90. 84 to 89 or 90 in different runs. Compare that to other algorithms. What did we use before that? What have I used here? Classification report. Oh, by the way, I should also draw the Arusi curve. Do you notice what is this algorithm that I used here? Confusion matrix. What did I do? I use feature extraction. Simple. Remember that exercise in feature extraction? And build a very, very simple logistic regression model. And that simple logistic regression model gives you, look at these values, right? Very good accuracy for the majority class. I mean, the minority class is one. My majority class, I mean, the minority class is one. It gives you 92% F1 score for the minority class, which you care about. Recall is very good. These things are very good. And I hope you would agree that in spite of training this giant neural network, end of it, it still beats it hands down. Isn't it you burned so many so many so much iterations of the cpu and still couldn't achieve that performance the reason i'm mentioning it guys is again the no freelance theorem remember the neural networks are not the panacea they have a use but you can't forget other algorithms in machine learning right if you could then you could all ask me for refunds from your ml 100 and 200. it would be pointless to have done that right so then we don't need to learn anything else then we don't need to learn anything else all we need to do is start with neural networks which is what a lot of people these days are doing they'll pick up or tell you know the three hours six hours of by touch and say we are machine learning gurus well that is that now let's go to image classification in the last half are not very different at all now remember, remember, so I'm going back to the CIFAR code, and I've slightly modified it for your fun. Do you remember CIFAR? Like you're predicting in one of the 10 classes? Let's go back. First, let's look at the code so that it reminds us of it. so that it reminds us of it. Datasets, CIFAR. So, oh no, this is the actual code. I have to go to the notebook. Notebooks, datasets, CIFAR. Remember, there were 10 classes, right? It is a this or a that or whatever it is. Ship, cat, plane, frog. Those were your classes, right? And some other 10 classes. And so I ran a neural net. So this is a code we'll go and look. Once again, do you see this code is beginning to look familiar to you? You create a classifier to which you feed a neural net isn't it and with the neural net you go and train your algorithm and you see what comes out of it so far so good guys okay uh this is it you evaluate the performance and you realize that there are three errors. You have 81% accuracy. You save it. You load it. Do we remember this code, guys? We did it in the first lab. Even before we learned what neural networks properly are, we got a taste of that. So let's look at this code and see if it makes sense. Does this code now look very obvious? Can you guess pretty much what the code would be if you go to the code class? Let's go to the code class now. There are neural networks and there's a classifier. Let's look at the classifier. Classifier, you go and create a data loader to load the data. Create network is just a constructor. You create a network in the classifier. Train data. So this is the creation of training data. Don't have to wait. Then some sample images that you can get from the test data and get predictions. So let's look at the code. You create a classifier with a network. You specify the labels. That's nice. You train. I'm training only for one epoch. It's a lot of data. One epoch is enough. Prediction. You predict. So on and so forth. And then producing the rest of it. Let's look at the network now, which is interesting, because the rest of it is easy. So this is the code. But what I did is I broke it up with comments. So this is the code, but what I did is I broke it up with comments. So this is a code in which I'm using a pooling layer. Remember the max pooling? What does the max pooling do? It picks the strongest signal in a grid of 2 by 2. Do we remember that? From last time, the max pooling picks up the strongest vote within a 2x2 grid. If you create a max pool with a kernel size 2. And then you switch and then you switch and you keep picking up. So I create a max pool layer that I'll use again and again. Now, this picture data in CIFAR is 32 pixels by 32 pixels. Right. So the input dimensions are what? Height 32, width 32 and what else? Straight side of one. Channel. The channels. The channels, channels yes the color rgb red blue green right so data is three dimensional isn't it it's a solid cube it is height width and channels three dimensions so you take the three dimensional data 32 pixel by 32. now when we say do you notice that in this thing i have not specified the size. I have just specified how many input channels are there. Then I say the output channel. What does output channel mean? In a convolational neural net sense, what does it mean? In the way that we understood it from the last Monday session, let me remind you, it means how many filters you used in the convolutional layer, how many kinds of filters you use. So perhaps one filter is picking up the diagonal, another is picking up a vertical edge, another is picking up a horizontal edge, right? Each of these filters are picking up different signals, different features in the image. So how many filters do we have? 16 filters, right? What is the size of each filter? It is five by five. So the word, remember, filter and kernel are interchangeable. So in the PyTorch library, they use the word kernel. So the kernel is five, which means it's a five by five size filter stride is equal to one we remember talk about stride as you move it over the image do you move it one step at a time or do you jump across i just wanted to point out that you have the flexibility default value is one so writing this was redundant but i put it there so you know that you can play with it now in the class i told you this formula so now if you put your input data 32 by 32 by 3 through this system what will come out the channels is easy you'll have 16 channels come out but what will be the size of the image let's reason through it which is why i put comments here will be the size of the image let's reason through it which is why i put comments here the output size is given by so this was the expression we talked about if you work it out you have to this is the formula for what size it will be so when you pick it up it will come out to be 32 picture 32 was the size it was a rectangular m square image. So 32 minus 5, because padding is 0, p is padding, we assume that we don't, we are not using any padding. Not to use any padding is to use valid padding, the name of it is. So 32 minus 5 plus 1, right, is right, is 28. Actually, why in the world did I put four here? Please ignore this. That is a typo. Is 28. Then when you use max pooling on 28, remember, what is it doing? It is coming up with one answer for a two by two grid so it is squashing to the you know two rows and two columns together into one value so the size of your output will reduce by half right if you just plug in so stride of a max pool is two if you plug it in back into this expression you'll realize that your output size will become half so i put all these comments so you can see how it is happening when you go back and review the video on convolutions you'll see i've explained how these things move forward then you feed it i have a question on the out channels the number of uh yes that you're using i i conceptually get the idea but wouldn't every filter have the same output like based on the logic which we remember output channel is not about output size of the output as we just found is 28 by 28. Each of the filter will produce the same output, 28 by 28. But how many filters do I have? It's purely your choice. Pick a number. I pick 16. People pick powers of two or sometimes not. They may take 20 or whatever. No, but I don't understand why we use multiple filters in that case. Because it will be similar output right no no no it is not that's the point right remember one filter is like a stencil yeah no i so conceptually i understand what the filters do but if we have filters of similar size similar stride uh working on the same pixel, then wouldn't the output of that stencil be the same? No, no, no, no. See, remember that we talked about this fact that filters are initialized with random weights in the beginning. So suppose they have only two filters, right? So when you train this neural network and you say, let the filter learn its weights, and let's say that your filter is five by five, so 25 parameters, you know what they will do? They will say that we cannot, if we discover the same thing, let's say horizontal edge, it is an inefficient solution, isn't it? You will have a high loss. Your loss function will return a high value because it's a redundant. So the way the back propagation will work is because you have initialized those filters with random values, they will both gradually drift so that they both detect completely orthogonal features. They will be forced to detect valuable information that one filter will be forced to find information that the other filter is not learning. You understand that? And so what you have is, if you really think about it, 16 filters are saying, we will discover 16 independent features in this image which are as different from each other as possible one will pick up color gradient you know gradients one one will pick up edges you get the idea and you're asking it to pick 16 different features got it got it and can we buy any can we we consider these filters as nodes or that's a completely different? Oh, no, you don't think of them as nodes. I mean, internally they are nodes, but you just don't think of them as nodes. You think of them logically. See, these are mathematical operations, right? You just think of them as kernels or filters. So there is no feed forward, no gradient descent. Well, there is gradient descent, but there is no activation here. You don't think of it like that. Let's leave it simply. These are mathematical. Convulation is a mathematical operation on the input, isn't it? But then how will the back propagation work if there is no it works think of the one rough way is think of the entire filter one filter as a node with like if it is a 5 by 5 filter with 25 uh 25 weights and a bias term 26 terms a biased term 26 terms without activation without activation right so so here is how the flow goes activation is there so think about it this way you have input image right input image it goes through a filter filter scans all over the image and produces an output. That output is just the convulation of the filter with the image. Once you have the output, what is the next thing you do? Let's say right after convulation you can max pull it. Max pulling is in some sense saturating the signal. That is the best intuition. Up till now it's all a fine transformation it's just a fine you can think of it that way and now comes the interesting part of it then you activate it then you apply relo or whatever you want to oh we do yeah you do yeah and we will see that in a moment so hold that thought in your mind right in fact why don't i show you to see what are we doing you're convulating the input look at this line oh okay you're re-luing it you're pulling it oh first you're redoing it and then then pulling it right and you could reverse the order if you want and so forth right so that is what one layer is if you and again you know say last lecture was a bit compact. We'll do all of these things in great detail when we do a month of image processing. The fundamentals is a little bit compacted because it's sort of a high level tour of the entire landscape. But the bottom line is watch that video again. You'll realize that when we think of one layer of convulation we mean three operations together convulation operation pooling operation activation operation all three puts put together makes a layer are we together are we together yeah just as in a feed forward network you do the affine transformation and you activate both of those things put together makes a layer right the same is true here just the transformation is separate the transformation is separate There it's a straight Z is B plus W dot X and here there's a convulation operation. If I understand it's a two part, correct? So the first part is what the feature extraction and it's afterwards the output is fitted into the regular deep network, correct? Yes and we will come to that in a moment so let's go back to our architecture so let's look at it i have a question on these you know that i'm not going to believe that right so can you explain me like how we came to see why can't we have 32 or is there any added us told that you know we need to yeah let's think through it see the input first layer the input layer when you get the signal it is 32 by 32 right picture if c4 pictures are that size it's think of it as god given if you're dealing with c4 then it is as 32 bit 32 by 32 right and three rgb channels by 32, right? And three RGB channels, no alpha channels. Now think this way. If I get 32 as height, what will be the output height? So output height is given by this. And it's a formula you can derive yourself. You realize that, forget the padding and the stride. Imagine that padding is 0 and stride is 1. Now think about it. If you have an image of 32 and you take a five by five filter, you realize that you can't go all over the image, right? You can go from one edge to the other, but at the end, you will hit the right, the right hand side, and so you wouldn't be able to make 32 horizontal movements, slides. How many movements you'll make? 32 minus 5, isn't it? So imagine that you have a grid of 3 by 3. Data comes as 3 by 3, and your filter is two by two yes how many steps horizontally can you go yes either you can be in the left corner or you can be in the right corner that's it right so you you your output will be there for two by two because now you can do the same thing vertically now let's think how that two came from you took the input size of three minus the size of the filter but then you already you need to add one because you started out with one position to begin with so So you from the starting position, you took one position to the right. So you add one more. So three minus two plus one is two. So that is the way you have this formula, n minus f plus one. That's it. And so that's how you know what your output size will be. And that is it. That's why I mentioned this formula. Guys, if you read my note, if you go back and watch the video again, you'll see this there, this formula. We covered it last time. Do we remember covering it or am I dreaming that I covered it? Yes, sir. I did cover it. Good. So this is it. And that's why in the comments i've taken pains to mention it this is it i have a question can i ask go ahead in the output channel right so it's like if i put a value of 360 so it's like for every single degree i have one feature right that was i thinking like because for image recognition we don't we either we are just looking at the edges right edge detection only no no no no no no that's not true that is not true edge is only one thing but you have gradients so density gradients itself is a feature yes sir so we split the into our busy and then in our busy it's just edge no just imagine monochrome just forget just take one one channel the red channel the intensity of the there may be no edge you may just have a color gradient a red gradient going from dark red to just black. Black, yeah, but yeah, so I was thinking it's a my confusion. It looks like a one feature, but like you put it's like 16 different angles of one line. If we I was thinking if you take three that it will going to detect only three edges for the rectangle and then like we are putting 360 then it's like a whole circle. Vaidhyanathan Ramamurthy, Oh yeah see every degree circle is like one feature. a good thought you came up with. That is the way people used to do it in computer vision before deep learning. So if you go back and look, so when I did computer vision in the second half of 1990s at Urbana, this is what we used to think about. People put, they literally had theories for edge detection and so forth. And this, you know, extraction, what to look for an image and so forth. Those theories are still true and they are very useful. But with deep learning, what we are saying is, let the deep learning learn the best features. It may come to the conclusion that edges are completely useless and it may not even make a filter for the edge so what filters it comes out with you don't know but you can visualize it remember i told you yes last time there is this continent website you can go and see and in when we do the image processing so we'll have patients we will do a lot of visualization of what the intermediate layers are thinking you know what features they are extracting as a lot of visualization of what the intermediate layers are thinking. You know what features they are extracting as a process of representation learning, piece by piece, layer by layer, what are the higher levels of abstraction, feature abstractions they're coming up with. That will become apparent to you when we do the second half of the, when we get into deeper aspect of this course alright but at this moment let's learn to walk before we run but at this moment be watchful I know where you're coming from so see when we do I think you're coming you're bringing it up from a radiology background. Isn't it? You do look for edges and things like that in the edge. Edge detection is a major thing in radiology, used to be. And this is broadly in the class of computer vision and so forth, the theory of it. But today, actually counter-intuitively, these filters, they come up with all sorts of odd features and we'll discover what those art features are in due time okay so that's that so the rest of it is very straightforward and to your point I think it was Praveen who pointed out that in the end remember I said that you stick in a lot of dense layers these are the dense layers right so what is the input size of the dense layers first of all there is 16 where 16 coming from the output channel but then I have to say what is the output of the second layer and so again I use the same formula 10 was the input was 10 like first of all the input was 14 14 minus 5 plus 1 is 10 and then you max pool it it becomes 5. so therefore the output is 5 times 5 5 height 5 width and 16 channels which is why when i create the first fully connected layer i give it 16 times 5 times 5 16 channels time 5 height plus 5 width after that the second layer how many nodes I put in that layer completely my choice we put 128 nodes you can you can play around with these numbers guys and play with this see what you need and then you do that later on now let's think about it you take the output of the first conv layer which includes three things convulation activation pulling do you see all three things here then you take the you feed that into the second layer you get the output of the second layer now you are ready to feed it into the feed forward network but before you do that you have to feed it into the feed forward network. But before you do that, you have to flatten it. So how do you flatten it? What does the minus one signify? Minus one says, I don't know how many images are there in the batch, whatever number of images are, deal with it, right? So minus one signifies that whatever the number of rows there is, so be it. And the number of columns, which is the real number of features, is 16 times 5 times 5. And that flattened input vector. So now this becomes a true X vector in the dense neural network sense. Passive? Yes. The syntax here really has to be multiplication or can you just put a single number oh you can put a single number the reason i wrote it this way is so that it is obvious why i put it there like if i had put 400 there it looks like a magic number right how did you come out with 400. so this is for yours negative one again it's like anything minus one minus one stands for whatever number of whatever the batch size is see remember the row is the number the batch size oh the batch size yeah that's right whatever it is you don't know and you can feed it different batch sizes and so be it right that is it better to convert it to dynamic code come again if we convert it to dynamic code. Come again. If we convert it to dynamic code, isn't that better rather than putting hard numbers here from very beginning? As in what I did not put in any hard numbers. So notice that nowhere I have given you an example here of 32 by 32 input image, right? But it is not necessary. You can do the computation with any size image. This entire network and this architecture will be correct for any size image you're feeding. Just for illustration, I took the fact that in CIFAR, you use 32 by 32. So the architecture is invariant to the size of the picture you put in. It is also invariant to the batch size. of the picture you put in it is also invariant to the batch size so it is just illustrative that i use 32 pixels uh sir i forgot the use of pooling could you please explain pooling is a word so suppose you know you take a 2x2 filter and you it's a window not don't use the word filter, window, and you put it on your output, right, on something. And then whatever four numbers are there, right, in the cells, you pick the biggest of them. I see, yeah. It is just a way to, I mean, see, nobody knows why pooling really works, max pooling really works, but my intuition is it saturates the signal. It just picks up the strongest signal. A representative signal, sort of. A representative signal. It should say pool max, sir. That's a good name, pool max. Yes, that's what it is, max pooling, yeah. So the other way around, yeah. That's it, but essentially it does the same thing. The rest of the code, guys, is absolutely familiar to you. And so now if we go back to CIFAR Jupyter code, so look at this. I'm taking a network, giving it to the classifier. It is producing that. Remember, now do these things make sense? Three channels input, 16 channels output. Kernel size five by five striped disk. Max pooling of two by two. Then the second layer was 16 and 16. Well, this is not for our one, because I took two examples. This is for the simple net. What I want you guys to do is change it to the other net. The other, I have put two networks in the code, use the other one. If you go here, deliberately you'll notice that I have put two networks. One is the simple CIFAR net and the other one is another CNN example net, which I've given all sorts of explanations to. So try your code in the notebook, try both of these out, play with the number of epochs, right? And so here it is. Evaluate its performance. By now, all of this must be looking very familiar to you. So it seems to get three predictions wrong of 81% accuracy. This is how you save the model, load the model at inference time. And then I gave you a lot of homework. And then I said, well, yeah, here, see, I'm now using the other example, just for fun. And it goes and gets trained or did i even train it i don't think i trained it right uh it's predicting everything is good i think i don't think there was a error somewhere let me go and see if we trained it let's run it and see if it got trained Let's see if it got trained. Oh, so you can see that it's not learning much, which means that in the network I created, there is something wrong. I'll let you guys go figure out what is wrong and how you can fix it. Actually, this is good. Tell me what I did wrong. It's a very simple network. You should be able to spot somewhere, something is off. What is off? what is off, figure it out. Did I forget to do the laws? Did I forget to do something? Learn from the gradient, what is it that I missed? Right? You can figure it out. So well, that is it today, guys. It's 1030. And we went through this example of CNNnn are the cnn's now making sense yes more so so the model is another cnn net or it's buried in the c410.py code yeah yeah it is buried in there so remember that the notebooks are to exercise the code and you write your code back in the in the in the in those scripting files this classes so go there and guys let me let's see who can find the bug fastest what was wrong with the another in this class here i use the another cnn example net see see guys if you can find what's off with that and let's see who can get to that fastest all the code as i said is uploaded to your website where are where is the code handouts and lab solutions do you see week four river solution and cnn lab homework that's what we did today see week four river solution and cnn lab homework that's what we did today and for now in the fundamentals we are done with image processing we have another entire month of image processing but that is in a subsequent part of this full workshop next week we'll go to natural language processing Next week, we'll go to natural language processing. All right, guys, it's time for my India meetings. So I need to sign off. Thank you.