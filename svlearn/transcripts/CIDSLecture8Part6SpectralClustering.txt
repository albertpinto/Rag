 I can do spectral clustering. The trouble with spectral clustering is you need to know something called eigenvalues, an eigenvector, an eigenvalue decomposition. A lot of German sounding names. So eigenvalue decomposition. Decomposition. What in the world is this? So first, let me start with eigenvalue decomposition. This is something we talk about in the... Actually, this is a good segue to dimensionality reduction and principal component analysis anyway. But today, there I'll talk about it in great detail. But I'll give you a basic intuition. See, when somebody gives you a number, this is very, very high level intuition. We will do the matrix decomposition very carefully when we do principal component analysis later. But suppose somebody gives you a number, big number, and you can decompose it into its prime factors. You feel you understood what it is built of, isn't it? Are we together? In the same way, when you get a matrix and you can decompose it into parts, and there are many matrix decomposition methods, but eigenvalue decomposition is quite interesting, and I'll tell you what it refers to. You feel that in one way you have gotten an insight, a visual insight into what that matrix is. So what happens is, remember we talked about covariance, right? So suppose you have two features, X1, X2. So you can create the variance of X1. So you can create sigma X1, of x1. So you can create sigma x1, x1, sigma x1, x2, sigma. This is the covariance. Sigma x1, x2 is the covariance. x2, x1, and x2, x2, right? Sigma squared, of course. Well, covariance. Think of it as covariance. Matrix. Do you remember that? We did this, right? Now, when you look at this matrix, when you look at this matrix, well, of course, these two values will be exactly the same. It's a symmetric, positive, it's a symmetric matrix, not necessarily positive, symmetric matrix. Now, there's something very interesting about it. And I'll just give it as a fact of life at this moment, because this is the end of the day and you may not be in the mood to retain a lot. So suppose I have data. then this covariance and let's say the data has even been normalized so if you normalize the data what does it become between zero and one so then it will become one one and this will be some value between zero and one the correlation it becomes doesn't one and this will be some value between zero and one the correlation it becomes it doesn't matter but i'll give you the basic intuition it turns out that whenever data is like this you can think of the direction of maximum variation of the data the first axis right literally it's begging if you can wrap it up in an envelope ellipsoidal envelope wrap the data the data in two dimensions in an ellipse we can wrap the data in an ellipse isn't it and when you wrap the data in an ellipse, what happens? You do have, ellipse has what? Major axis and minor axis. Do you remember basic coordinate geometry? Right? This and this. This would be the major axis, minor axis. And if you have forgotten that, Sonal will remind you of it. She's doing it. Sonal, is that right? You have done coordinate geometry? Yes. So go to her. She'll teach you all of this coordinate geometry and refresh it for you. So we have major and minor axis it's a given fact now here comes something beautiful you know that this data inside that ellipse has a certain correlation covariance if you take the covariance matrix and take it as a fact when you take the covariance matrix matrix and you do an eigenvalue decomposition guess what it finds it will find it will be made up of a matrix so let's say that you take a matrix covariance matrix like this m it will become v d v transpose so the transpose of a matrix means just switch the off diagonal in this particular case and guess what this uh so first of all let's look at the diagonals the diagonal would represent something interesting if you imagine that this was a circle interesting. If you imagine that this was a circle, this tells you by what factor this diagonal was stretched in this direction. And lambda 2 tells you the amount that it was stretched or rather shrunk in the other direction to create this ellipse because every ellipse you can think as a deformation of a circle isn't it so it tells you how much you had to pull or shrink along the axis those are your these are your lambda 1 lambda 2. it's a diagonal matrix this is oh what i'm doing touch off touch on okay it's not catching on to uh that and what about this v it turns out that v is made up of two column vectors u1 u2 remember we represent vectors as column vectors x1 x2 kind of thing so it is a column vector u1 and u2 right each of them two by two and guess what this is the beautiful thing u1 will be a vector in this direction and u2 will be a vector in this direction. Means if you wrap up the data in an ellipse, basically the eigenvalue decomposition of the covariance matrix tells you what the eigenvectors are, what the principal axes are. And it even tells you how much to stretch. That's the eigenvalues. So that is the intuition of eigenvalues and eigenvector are we together right that is the geometry of it and there is something beautiful you can actually do this question you know the relationship to circle imagine that you got data random noise in a circle like zero or gaussian noise or any form of noise says that the correlation is zero right and what you do is you take this data this is data and you apply to this data x you apply this covariance matrix m you pick your covariance matrix and guess what will happen this will transform this into literally isn't it beautiful and magical? It will transform it into an ellipse whose covariance will be exactly given by the matrix and whose directions, axis will be given by the eigenvectors. Are you understanding it? I don't know if you find it. I find this quite amazing, actually, quite beautiful, this geometric way of looking at what it does, what eigenvalue, eigenvectors do, especially to data. In fact, I'm almost halfway there to principal component analysis. This is a big intuition. Now, how and we'll worry about it. And how do you compute eigenvalues? In practice, of course, you just use NumPy. In one line, it gives you the eigenvalue decomposition. Would it be okay to summarize it as what we got with the eigenvalue decomposition, which is all the essence of the the sense of it exactly exactly what is this matrix really doing it tells you that in some way and that's the beauty of these decompositions there are many decompositions eigenvalue decomposition singular value decomposition so what eigenvalue does to square and like the symmetric kind of nice well-behaved matrix, the generalization is SVD, more stable, right? It works with rectangular, more generalized, similar to that. We'll talk about that later. In fact, the recommender systems, initial generation of recommender systems, in fact, the one that, for example, went into the likes of the Netflix, etc. Guess what? This is the idea they were all based on. Not eigenvalue, but singular value decomposition, because they were rectangular matrix with the same intuition. And we'll talk about that much later. Will we get time for recommender systems now? I don't think so. I'm almost tempted, guys. See, we are reaching the end of this workshop, we have a lot of things to do. Out of the topics that we may not be able to finish. Are you guys interested in a follow up mini workshop. We'll cover recommender systems and automated machine learning and explainable AI and things like that okay think about that i'll propose that and see if you guys are interested we can continue on for the next two to three weeks and do that i always joke that holidays can be depressing it's better to study no i'm kidding uh for me yes no actually i shouldn't say that because I have a lot of fun during the holidays. My children are coming and we are going to have a blast. You start from eigenvalue to leave the spectral anatomy. Right, I'm getting there. I'm not there yet. So anyway, so the basic idea is that eigenvalue gives you in some sense a good intuition into the matrix into the essence of a matrix now it turns out that eigenvalue has so much varied all this a matrix decomposition and linear algebra is all over machine learning right so now let's go to some other place let's take a graph of this remember we learned about k nearest neighbors so we are going to do i'll give you one version of the spectral net and clustering there are a few more version of it what you do is suppose you have points that you want to cluster let's go to this have points that you want to cluster. Let's go to this. We consider each point as a node in a graph. So what does a graph look like? You know, things like this. Something like this, right? Graph. Are we together? Do we know? Graphs are made up of nodes and edges. Graph G is a collection of nodes and edges are we together right and so when you have a graph like this um we that is all we need to know about a graph graphs looks like this they're made up of nodes and nodes are connected by edges there's a a variety of it. They can be directed graphs and weighted graphs. There can be weights to it and so forth. But I'll keep it very simple. All edge weights are one. Let's take it. Now, there are two approaches you can use. One is epsilon neighborhood. Number one. number one epsilon neighborhood and k nearest neighbor do these words ring familiar to you did we study today we did right we know both so using either of these two methods find so suppose i take epsilon neighborhood so i will take this neighborhood and all the points that are inside that epsilon oops sorry uh if my hand is causing it to move uh within the epsilon neighborhood of this point right so what you do is you connect this node to every point in its epsilon neighborhood, right? So you put all points are nodes and you connect it to all the neighborhood points. Are we together? Or if you're doing KNN, the same approach, find the K nearest neighbors and connect this point to those points, make a graph like that and continue doing this, not do it for this, this, you will end up with a graph isn't it once you end up with a graph something i don't know this this this this something like that then what you do is now comes the interesting part so get a graph this is the part, build a graph. Now that we have built a graph, now comes a part that is interesting and a little bit graph theoretic. Suppose the graph has five nodes. So let me take a simple graph. One, two, three, four, five. Let me connect some of them. One, two, three, four, five. This, this, this. I'm just giving an example. Right. And maybe this graph. Let's look at this, create a matrix in which you have 1, 2, 3, 4, 5, 1, 2, 3, 4, five. Is one connected to one? Of course. So we know that there'll be diagonals around here. This will be, this is easy peasy, right? Now, is one connected to two? Yes. Is one connected to three directly with the edge? No. Is one connected to four? No. Is one connected to five? Yes. Is 1 connected to 4? No. Is 1 connected to 5? Yes. Is 2 connected to 1? Of course. Is 2 connected to 3? No. Is 2 connected to 4? No. Is three connected to four? Yes. And is three connected to five? No. And is four connected to one? No. To two? No. To three? Yes. And to five? Yes. And is five connected to one? five connected to two yes five connected to three now five connected to four yes what you just built is called a adder sensei matrix you just created an adjacency matrix just created an adjacency matrix. Are we together? Now you create one more matrix. You create another matrix and you ask from one, what is the degree of a node, it says, is how many things it's connected to. Are we together so once again we'll do that exercise three four five one two three four five so now one tell me what is the degree of one two what is the degree of two two what's the degree of three one what is the degree of 2? 2. What's the degree of 3? 1. What is the degree of 4? 2. Degree of 5? 3. Now, we can, 2's degree is? Yeah. So, after that, it begins to get similar. Two, right? Sorry, I take this back. Sorry, this is silly. Why am I even looking at... I just need to look at the degree diagonal, right? So one's degree is two. Two's degree is two. Three's degree is one. Four's degree is two. And five's degree is three. And degree is two and five's degree is three and obviously the rest of these are zero this is called the what is it called degree matrix matrix the degree matrix right and now and what you do is you create the matrix M, and by the way, I might get it wrong. It is D minus A, and who knows, maybe it is A minus D. I have to remember, you know, I don't have it at the top of my head, but yeah, you realize that I can create a matrix D minus A now? Isn't it? Both of them are 5 by 5 matrices. I can compute that. This matrix is a very interesting matrix. It turns out that this matrix is called the Laplacian matrix. So if you really think about it, D minus A is, you know, everything will be, of course, adjacent to itself. By reading that out, right, you are basically getting the rest of the information in the matrix, right? Now what happens is that this matrix has a lot of good properties. Graph theorists love it, right? theorists love it right one of its properties one of its properties is that you can do eigenvalue decomposition of that and then what it tells you is i'll give now i'll do some hand waving arguments because it gets a little bit technical from here in terms of some things when you do the eigenvalue decomposition you look at uh you'll get some eigenvectors and so on and so forth but uh look at the eigenvalues in particular lambda one lambda two lambda one lambda two lambda three what it tells you is the the strength of the lambda one tells you if lambda one itself is huge it probably means you have just one cluster fully connected cluster right so what is a fully connected graph all points onto each other that's it right that's that the asymptotic limit so a small lambda very small close to zero lambda one means there are actually two clusters right there are actually two clusters right and then likewise, lambda 2 is a measure that, can I then break it up into yet another set of clusters? Go on partitioning. So that is the basic idea. And then now there is a mechanics of how do you detect those clusters and put those clusters together. They come from your eigenvectors and so forth. I won't go into that, but this is the basic idea. In this, what happens is that, see, you may ask, why don't we just use adjacency matrix and why do we need the degree? It has to do with the fact that adjacency matrix, right? Suppose you have, I'll take this typical moon shape, half moon shape when you look at the neighbor right it can switch over do you notice that it can switch over and consider a point in the other one and it turns out that when you look at the laplacian and do the eigenvalue properly in the reduced space so So what you do is you project data. So, okay, let me finish the full argument. Actually, let me tell you the whole argument. So every point in this, every point in the original space, x vector, can be represented in the point in the dimensionality reduced. Now I'll explain the word dimensionality reduced now i'll explain the word dimensionality reduced i was going to explain this after i taught dimensionality reduction but okay if you don't get everything now remember i'll repeat it dimensionality reduced uh space now what is dimensionality reduced space what happens you get your lambda 1 lambda lambda two, lambda three. If you sort them, you'll notice that their sizes go like this. So keep the big lambdas, lambda one, lambda two, lambda three and ignore lambda four, lambda five, lambda six. Ignore them. Right. And so when you look at the eigenvectors your eigenvectors were let us say six dimensional right any one of the eigenvectors were six dimensional one two three four five six values what you do is you keep only the top three values you zero the other one these this is okay this is okay this is okay right so your vector now becomes only three-dimensional isn't it so now every point you project it into the reduced dimensional space right once you have reduced every point so it's a mapping from this to a lower dimensional space but remember the eigenvectors are not coming directly by looking at the data space and doing eigenvalue decomposition we will do that in principal component analysis here we are looking at a graph and graphs laplacian's eigenvalue decomposition that's why this is considered an advanced topic and kept out of the books usually, the simple textbooks. So once you do that in that lower dimensional space, let's say two dimensional space, what will happen is this will your data will be much better separated out. Much better separated out. And now you can apply your K-means. Simple K-means and get done with it, or density or whatever you want. Are we together? So the whole point of this is, leveraging graph theory, which says that there's this magical thing, not magical, it's a useful thing, Laplacian matrix. But the thing is, how do you make it into a graph? Connect each point to either its nearest neighbors or all points in the epsilon neighborhood of the data space right you get a graph now that you have a graph it was an intellectual journey first create a graph somehow created a graph create an adjacency matrix degree matrix then create the laplacian then do the eigenvalue decomposition then keep only a few axes throw away the rest so you have lower dimensional space in the lower dimensional space go ahead and do clustering and it works better that is spectral clustering how many of you got it you got it oh Nice. Nice that you those of you who are on the zoom. Did you get it? Like, did you get the basic idea of what we are doing? Yeah, we'll repeat it. Don't wait. Once we do dimensionality reduction, all of this will make a lot more easier sense, and we'll revisit eigenvalue decomposition again. Did I miss the part where you transformed from the problem statement to the graph? Okay, let me do that again. So suppose you have data. Let me show the whole intellectual journey in one single suit. Suppose you have data like this and then you have another which you what you honestly know is another cluster which is another cluster which is and you wish there was a way that you could separate these out one doesn't look globular see one of the limitations with a db scan and by a spectral thing work because db scan takes epsilon right so if you have clusters on different scales let's say one cluster is like this very tightly knit cluster you know the epsilon that you need to discover it is very small but the other cluster is like this it is still a cluster but bigger distances. You realize you have a problem of epsilon, isn't it? There, that's where spectral works. So from this, the journey is you take, connect each point. So first, number one. Okay, let me write all steps separately. Why am I not able to erase it? Okay. Might as well write it. Yeah, I'll write all of these. Number one, consider each point as a node. Are we together? either you have a choice epsilon never heard directly reachable points or k nearest neighbor or pick one doesn't matter pick one method right so now what do you have connect each so you now have a therefore you have a graph g would you agree premjit either i connect each point to neighboring point using the epsilon neighborhood or k nearest neighbor. But now there are edges in the, initially you started out with just n nodes, one for each point, but no edges. Now you have edges. And now you therefore you have a graph, graph of, sorry, a graph graph of uh sorry a graph of uh sorry let me just disable touch you have a graph of nodes and vertices would you agree uh notes and edges sorry vertices yeah you have that now the number three step build adjacency matrix yeah of course yeah and in this example i took undirected graph let's keep it simple for now and you know there there is a possibility of weight for example if it is weighted then the adjacency matrix will not just contain once it will contain the number the weight right four build the degree graph oh sorry degree matrix what am i saying degree matrix d number five the degree of a point of a node is the number of edges on that node so let's go back and review what was the degree so for point one how many edges do you see two so it's degrees two the number of edges connected to a point is the degree of the point and there's more in graph theory you also talk about inward degree you know incident degree versus degree out degree and so forth we won't go there let's take this very simple unconnect uh simple graph or not let's get the intuition right before we complicate build the degree graph next build compute the laplacian and my gosh i hope i get this laplacian right. I'm writing it as D minus A. Yeah, D minus A. I think I got it. In the beginning, I wrote it wrong. No, you got it correct. Oh, I got it correct. Okay. D minus A. It is D minus A. Okay, good. I wrote it correct. So good. My memory isn't that bad. So you do the D minus A. Number you do the d minus a number six you see what a long intellectual journey this is right do a do a eigenvalue decomposition why can i do an eigenvalue decomposition because it's a square matrix i can do that composite composition sort of a symmetric square matrix, eigenvalue decomposition. Number six, number seven, look at the eigenvalues, zero out the small ones, throw away some dimensions. So, for example, you started with six dimensional space, lambda one, lambda two. So suppose lambda one was 100, lambda two was equal to, let's say, 80, lambda 3 was equal to, let's say, 70, and lambda 4 was equal to 3, lambda 5 was equal to 0.1, and lambda 6 was equal to 0.0042. Do you see a pattern here? You can say throw this away. So three dimensions you threw away right you say that i'm going to keep my space so keep your eigenvalue eigenvector so eighth step is keep keep only only useful or big useful axes or dimensions dimensions in the eigenvector can I stop you over there this piece that you threw away lambda 4, 5 and 6 are those the outliers or things that are outside no no no no so basically what it means is whenever see whenever you create data so if you go back to the covalence matrix see because this is a whole topic i'll discuss in detail next time we don't have enough time here's the thing hold that thought in your mind let me give you the intuition nothing to do without that it has to do with the fact that those fee the there are certain things that don't matter and it has just discovered things that don't matter it has found yeah yeah so what happens is you have rotate you have transformed to a new space which is still you started with six dimensional. Now you are in another six-dimensional space. Right. But it turns out that the way the data is distributed in that new space, three of the dimensions are just noise. OK. There's not much information capture. Outliers with you alone. Outliers all with you. Yeah. Everything you have carried out. Yeah, package is still there. No, no, no. Yeah. So one way to think about it is, see, imagine that in this room, there is a diagonal plate. Right. And it's just there. From here to the ceiling there, it's like a diagonal steel plate. At the center, there is a fire burning right right so imagine so for those of you who are there imagine a diagonal plate so this diagonal plate exists in three dimensions right and now suppose there's a fire at the center of it a lamp at the center of it and you look at the light intensity everywhere around it right imagine that light this is opaque or maybe take it heat air is a non-conductor so the warmth of this is only going through your metal thing because metal conducts heat at any given point you can tell the temperature is well inversely proportional to the distance or some power law of the distance from the center right so the only thing that matters is what distance from the source of the heat right so what really you should look for if you want to find the temperature on the sheet is a point on the sheet where is it on the sheet and how far is it from the center so you have reduced a three dimensional problem to a two-dimensional one right in fact to a one-dimensional one all that matters is how far are you from the lamp right right so that is prince that is your dimensionality reduction you you have a lamp let's say that okay so we don't even need the sheet okay suppose you have a lamp in this room hanging here, and you're saying, what is the intensity of light everywhere in the room? Now, this is three-dimensional. You can have data. You can go and take a sensor and measure the light intensity, luminosity everywhere, right? You will get X, Y, Z, and, you know, some temperature, some luminosity or whatever it is, some measure. What this dimensionality reduction's goal is to tell you that what this data is saying, that there is a new principle component, there's new axis, and that axis is actually just distance from the lamp. And the other axes that are that are there they're actually irrelevant they don't matter but because data has noise they will have small values lambdas will have small values do you get the intuition and that is the heart of dimensionality reduction that is the heart of it yeah so that is what we are doing. You take this, what happens with this Laplacian and the dimensionality reduction is, see, there are certain connections that are crucial. Certain things are genuinely meaningfully connected and certain things are not. Like we talked this example, the moon example, where did my moon go? Yeah yeah that certain connections are not good this bridge is not as relevant but certain connections are very relevant right so it helps you clean up the data remove the noise in some sense of the data and so that is what it does when you do this and number nine does it also remove anything that is correlated automatically it doesn't do that it does not remove correlations etc so keep only the useful dimensions map each point data point x to a data point in the other dimensional space let me call this dimension space other dimensional space let me call this dimension space xi to chi i right this vector map it to lower dimensional space and then number 10, go cluster in the space. That's it. You can use k-means or whatever. If you read actually the paper on spectral, it's rather intimidating. It will take you through a very complicated journey. And there are technical issues here, some of them, but this is the basic intuition. Consider each point. You got through the journey? These are the steps to do spectral clustering. That is right. That's how you get to it. And in a way, it boils the problem down to meaningful dimensions where using graph theory and from there you can do the clustering. Well, you asked for spectral clustering. So don't say it's complicated. Who asked for it? Anil. Anil asked for it, yes. Anil is going to go to an output next week. Right. Right, Anil. All right, guys. I'll just review what we said. So we ended up covering a new topic. So you take all the points as nodes. You create edges by using either the epsilon neighborhood or the k-nearest neighbor. So you have a graph, build the adjacency matrix, build a degree matrix, build the Laplacian. Once you have a Laplacian, so your goal is first from the data points, a graph. From the graph, the Laplacian of the graph. Laplacian says a lot of things about the graph. So a lot of properties it has. You do an eigenvalue decomposition, it tells you even nicer things. It gives you a nice lower dimensional space by throwing away some of these useless things. It gives you a lower dimensional representation of the data. Once you take the data projected to lower dimensions, go cluster. Right? You can go cluster it. And that is spectral clustering flow so far so good right well actually there's more to it uh this is like similar like what you do spectral analysis kind of that that element analysis that you do right you get the basic parts of it and then bring it all together yeah is that the notion of the spectral part of sort of like that so guys i mixed up one thing i just realized usually what you do is you do spectral clustering your spectral to bring it down to lower dimensions okay and then you just go cluster that is it okay so long journey guys so all right it's uh five do you want to one more topic or go home i think that's good what's it it's time to take an analysis we have tuesday evening right yes we have tuesday evening all right guys so uh do you realize that we have covered a lot of territory in the eight sessions doesn't it i hope you are catching up and reading the books and beyond the book we have been covering territory so do read do understand do do catch up otherwise this will all fade away we are it doesn't look like it but we are moving rather fast