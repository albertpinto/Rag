 All right guys, today our topic is autoencoders. What are autoencoders? We studied autoencoders as built of two things, the encoder part and the decoder part. The encoder takes the input and creates a latent representation or a hidden representation. So input vector goes in, a hidden representation vector comes out. In this picture, the letter H is used, and it's very common to use either the letter H or the letter Z for latent representation or hidden representation. That gets decoded and becomes the x prime vector. I used x tilde in my notation. So suppose you have only one hidden layer, then what happens? The activation function, the result of the activation is essentially... So it's okay, hang on. Let me just first put it like this. Given an input, you create a hidden layer, isn't it? Now the dimensionality of the hidden layer, it need not be less than the dimensionality of the hidden layer, it need not be less than the dimensionality of the input layer. I've been continuously making a conical structure in which this is smaller than the dimensionality d of the hidden layer is smaller than the dimensionality of the input. This is a classic autoencoder, but then there are variants of the autoencoder that do what is called overrepresentation. Where D is actually bigger than N. You create a hidden representation that into a much higher dimensional space, but then you have a sparsity problem. You need to be able to manage the sparsity somehow. So those are slightly advanced topics since we are moving, you know, we covered the encoders in just one session foundationally. So I keep to this simpler one, the lesser one. Suppose you don't have an activation function at all, and you have a simple hidden layer layer like from input you go to edge so what happens if you remove activations from your neurons you say the identity activation function it becomes w dot x plus bias term right in that case it is just a principal component analysis in fact it is equivalent to a pA, right? So you can say, and we talked about it, it's a review. A PCA is in, remember we said that it is a linear form of dimensionality reduction, and its generalization is an autoencoder, because what does the activation function do? It deforms the output, right? It deforms the affine transformations that things like a PCA would do. PCA would do a linear transformation, then activation function would go and deform that affine transform. And so that's one way to think about your autoencoders. And then the decoder is a mirror function, is the opposite of the encoder. It goes from the latent representation to an output representation. So if you look at the opposite of the encoder it goes from the latent representation to an output representation so if you look at the journey the entire journey of the autoencoder it's a journey of first applying f being the encoder function g being the decoder function so given an input first you apply the f function then you apply the g function uh is that simple guys at the end of it your neural network is what? It's an approximator. It's a function. It's a differentiable continuous function on your input. So f is one function for the encoder. G applies on the output of f, which is h, and produces x prime. What is our loss? And so that is why it's called auto encoder and people also call it a self-supervised learning or this debate has been there quite a while is it supervised learning is it unsupervised learning and different people will have different interpretations and in my view those worlds words supervised and unsupervised are somewhat beginning to get uncomfortable in the modern world where you have a whole continuum of algorithms, very clearly predictive algorithms and unsupervised learning pattern recognition algorithms. I would say that roughly speaking, you can think of autoencoders as not supervised learning. Is it completely unsupervised learning? Well, sort of, except that there is a target variable, X prime, and you do a loss function between the input and the reconstructed input. That's why it's often called the self-supervised learning, because you use the training data itself. Training data is the label remember if this was y i in this equation the output the only difference is x i uh if it if i replace with y i and i replace x i prime with y i hat that would be your standard regression loss function isn't it guys the only difference is the label is the input itself right and of course this is your standard output but it is conventional in the auto encoder world to call it not y i had but to call it xi prime and so your loss function is the sum squared error mean squared error actually if it is a mean squared i forgot to put a one over n guys and please add it there is an error here uh again it just shows i can be sloppy this is put a one over n in your notes here how do i do that let me do that here itself and i'll upload it uh where is my observations lambda sum okay operations. Lambda sum. Okay. Hopefully it's better. Yeah. One over n. Yeah. So one over n. This is your mean squared error. So that is something to remember guys take the quiz on autoencoders it is important that you review your knowledge do that so there are a few things to know about the autoencoder i would we will walk through it autoencoders have variety if you look at how many kinds of autoencoders are there, I would, I mean, I myself have looked at at least two dozen, but I'm not the guru of autoencoders. I'm just a generalist, deep learning person. I'm sure that there are gurus who, I mean, whose entire PhD thesis is on, you know, new kinds of autoencoders. So they might come back and say, oh, only two dozen. We know 100 or something like that. So I don't know. But they're lovely websites devoted only to autoencoders. So it is, again, a world in itself. If you're in a hurry to write a research paper, this is a great topic. You can easily, well well easily is relative with with some amount of effort but not extraordinary effort you can think of new variations of autoencoders and write a paper right uh hint hint those are a few who want to do summer internship with me this could be a very very good topic pick a specific domain problem domain and for that problem domain take one of the autoencoders let's think of variants of it what changes you would make what special things you would do and you will get an autoencoder you would get a new autoencoder that works better for that particular problem set than anything else so that is that there are many variants to it now one class of variants which is important is the variational autoencoder. I talked about it because they are generative models. I did not get time to cover variational autoencoders in the regular three hour session because we ran out of time. But what I would like to do is take one of the Sunday sessions, make it a longer session and go into the variational autoencoder deeply. So remember that these Sunday sessions do happen. When they do happen, if at all, you can listen to the recording. If you miss it, that would be great. Some of the material, extra material, I cover there. Though for for the course if you know autoencoders you have it's not that far of a stretch from that deviation. Another thing that I would like to cover is the Wasserstein autoencoders, which is the state of the art, remember that in the Monday lecture I covered the Wasserstein GAN. Monday lecture I covered the Wasserstein GAN, but it just occurred to me that I did not cover the foundational concept of Wasserstein distance and Wasserstein autoencoders. I would love to cover all this. See, ultimately what happens is it's very difficult to pack everything into a sort of an eight-week course. So you have to pick and choose as part of the curriculum design not to make a course too heavy and i think at this moment we are at the limit of how much we can absorb in this limited amount of time but anyway there's some basic summary of the auto encoders the auto encoders the encoder and decoders are laterally symmetric opposites would agree? If you look at it visually, they're just opposites. Another way to put it is, whatever you're, roughly speaking, it is in some sense the inverse or the transpose of it, right? It's the transpose of the function in many ways, the weight function. Not really, because it's gone through all these sigmoids and the activation functions and that, but very rough high-level thinking, you can think of it like that. In our basic autoencoder, we made it purely of fully connected layers because it's the only neural architecture you know at this moment, are fully connected layers you connect. It turns out that that's not the only way to do it. One of the neural architectures you learn is convolutional neural networks. That is the last lecture, this is our eighth week. But what I have done is because it exists, I have added it to the lab notebook so you can play with it. I'll just give you a one minute understanding of convolutional networks. Convolutional networks are amazing when it comes to image processing. It is the domain in which it is used most. It was inspired by the way cats, when people researched how does cats' eye and brain interact what areas of the brain are lit up when a cat sees something right and different parts of the image of what the cat sees leads to the lighting or being processed in different areas of the brain by studying that the convolutional networks apparently were inspired they had a great idea convolutions we will study that though i must also say that for computer vision now, there is a big shift to visual transformers. So visual transformers is next week, and I'll put convolutional networks and, if time permitting, recurrent neural networks, LSTMs, etc. to the last week, simply because I wanted to make sure that we cover transformers in depth before we go anywhere. They're far more important these days. So there are an odd number of layers. Why would there be an odd number of layers? Because in a pinch, if you see that whichever way you design your encoder and your decoder, there is this extra layer edge so if this has n number of nodes and this has n number of layers sorry n number of layers here and number of layers here then it will be 2n plus the hidden layer 2n plus 1 2n plus 1 is an odd number so you have an odd number number of layers in the standard autoencoder. At this moment, we made it purely out of that. The activation functions for the internals is relu. Actually, if you look at the code, I have sort of played around, used tanh, relu. This is, by now, you are experts at activation function. This statement may or may not be true in the latest code because I keep playing around with it. You can check. So it would be either tanh or relu. But one of the fun things you can do is play with different activation functions. Try leaky relu, try clu, try, I don't know, glu, try tanh, et cetera. But sigmoid won't give you anything. It will just slow you down. TANH is the logistic function, won't give you much. But in the sigmoid class, TANH would do you good. We are using the standard MSA loss function here. So in the exercises, we will use this as a loss function. And then this lab, we use the famous MNIST data set. MNIST data set, maybe a bit of history is important. The MNIST stands for Modified National Institute of Science and Technology. So National Institutes of what is it standards. Okay, now i'm forgetting what Nist stands for what is it national institute for standards of science and technology. Yeah, we can quickly look up guys and tell me enlighten me i've forgotten uh unbelievable institute of standards and technology standards and technology yes and m stands for the modi so the first there was the data set that was produced by um nest nist and then that data set was followed by um was modified or cleaned up a little bit. And so it is the modified NIST dataset. It is a data set of handwriting, right? And handwriting is very, very important. Why is it important today? People, or at least it was customary that when people would send a postal mail, which some of you remember, you put something into an envelope and send it over. I mean, I'm sure all of you receive postal mails in the US still, and the young people have all moved to email. So when you receive a postal mail, you have an envelope and envelopes, when you get it commercially, then things are printed on it address. But when you send a letter to your friend which you may or may not have done in your lifetime it was traditional that people would hand write the address and put a stamp there and send it okay so we so a lot of mail in the us used to go uh with handwritten addresses on the envelope now processing handwritten is very very uh sort of laborious and for someone to understand what is the address. In the old time, a letter used to take a long time to be processed and reach its destination. Lots of people along the way had to reinterpret or understand what the address is. Today, it is an amazing thing that letters, the moment you put a letter into the mailbox, and assuming that you have handwritten, believe it or not there's no human being trying to figure out what address it is it goes to a central location there there are their ai machines optical machines that look at that they do handwriting recognitions and they figure out what address you're putting and at the bottom of every envelope that you receive for a hundred you will see a little bit the bottom of every envelope that you receive for a handwritten, you will see a little bit of a vertical lines there. That is a postal code, computer postal code of the address in computer language. And then the routers, automatically the sorters and routers along the way, redirect your mail from place to place. And no human intervention happens after that, till it reaches the postman who delivers mail on your street the postman in the morning gets a box of mails belonging to your neighborhood and then when the postman is coming by your particular house or your street that's the only time another human being touches your envelope so that efficiency has led to many many good things uh there is there is a concept of dead letters dead letters are letters that got completely lost or misunderstood or went to the wrong place usually if males go to the wrong place they are rerouted back to the right place and so forth it used to be quite common for people to receive, to lose emails or to not emails, lose mails or postal mails and receive mails of some other people. Today it practically doesn't happen and so the concept and also the concept of dead letter is more or less different. Very very small proportion of letters actually get lost. Not only that, letters reach their destinations quite rapidly. It's surprising how quickly a letter posted from California, let's say, reaches New York. It is the basic transit time. Other than that, it is very fast. It wasn't like that a long time ago when it was a human driven process. So there is a lot that is going on here in the history of deep neural networks. So people for the longest time have tried to create handwriting recognition systems, right? Classifiers that would tell whether this is an A or a B or it's a 0 or a 9 or a 8 or something. MNIST is simpler. It is only about digits at this moment. The alphabet also, but we limit ourselves to digits, 0 to 9, and we recognize those digits, right? MNIST used to be the benchmark to test algorithms against. There was a time, Dennis, could you please make sure that the AC is down to 69? I'm feeling a bit warm here. You know how to turn it down? Thank you. So it used to be that we would, that algorithms would have accuracies in the 60s, 70s, or something like that. In fact, one of the big changes that came was support vector machines, which made a radical improvement in the accuracy of that. But today, the gold standard is deep neural networks. In fact, the accuracy is so good with deep neural networks and recognizing digits. You did that in the digit classifier, I hope. That was one of your labs. It is uncannily accurate. So that today, we talk about the success rate in 99.98 or something like that person. And the few cases which are hard to, where there's a classification error, no, I mean, they are literally not possible to legitimately call those scribblings as any digit whatsoever. We have an accuracy rate with machines that far exceeds that of human beings. Literally, they are beating human beings in handwriting recognition. One would have imagined handwriting recognition is a very, very human thing to do. Machines will never beat us at it. And yet with deep neural networks, machines do far better. Machines are getting far better than us for most things. You can create specialized machines for each task and it outdoes us. Where we are the machines of course is in generalized intelligence. We are pretty good at many things, whereas a machine will be very good at one thing. We'll learn to, a deep neural network model will learn to do one thing very well. So it's still a pursuit perhaps to be followed in the next hundred years to create generalized intelligence. So that's that. We'll be using the MNIST dataset in case. The number of hidden layers can be... So what happens is I've created these classes for you, base autoencoder and so forth. So you have to... And I'll walk you through the code in a little bit. So you will see how to create it. It's not hard. You have the base autoencoder and you'll give it, I've made it open so that you can give it, you can sort of design your own autoencoder. Part of your homework is to design your autoencoder. Decide in the first layer, I want so many elements, in the second I want, how many layers I have. And I made it as simple as giving it an array of numbers that's all so there are certain classes that you'll find i will import those classes i will just show you how it runs so you train an auto encoder so what does an auto encoder look like let's let's say that you create the most vanilla form of autoencoder, which I call the base autoencoder. Now the input dimension is 28 times 28. Why 28 square? Why do you think I picked this unusual number 28? Could one of you guess it? Anyone? I think Jin Li and Manish might know. Yeah. What was the size of the input data again? Yes. MNIST images are 28 by 28 pixels. Right. And that's pretty much the reason why we chose the input as the pixels so that we can identify each one of the input dimensions by the 28 by 28 matrix that's right and if you linearize it then it becomes a single vector which is 28 by 28 28 times 28 28 and so that is what it is. If you look at the first layer, which takes an input, input features is 784, which is 28 square, and it produces an output of 64. So there is only one hidden layer, 64, and the second sort of hidden layer then produces the latent vector, the latent dimensions I kept it 16 16 dimensional by the way this is something for you to play with what happens if you try to reduce it down to two dimensions ask yourself are two dimensions enough to capture the information of a digit who knows play with it 16 is what i took now this is the kind of things you should play with. What is the dimensionality of the latent space that you need to get a good autoencoder? Because that tells you how many dimensions you need to capture the richness of the input, most of the information in the input. These are things that you should play with. So play with this. So 16 and what is the decoder is the opposite it goes from 16 to 64 and from 64 to 784 which is the so output feature dimension is the same as the input feature estimation does this make sense guys encoder decoder do they do they look sensible right and the output out features for 16 that's always in the multiples of 16 right or i just it's a convention that people tend to take powers of two right uh in for the layers now for no rhyme or reason, actually, recently, there has been a very interesting interpretation coming from theoretical physics. What is the result? It shows that for neural networks, there is actually a good ratio between the number of layers and the number of nodes in each layer. Right. That leads to pretty efficient, fully connected neural networks i will uh i saw that paper recently i can uh there's a book also now there's a lovely book that has come out in amazon it's a very recent book i will post it to you guys it's a but be prepared you can understand it only if you understand quantum field theory and relativity. Okay, because it literally uses arguments from those. I say it lovely book, why? Because well, I happen to be from that those fields. But leaving that aside, you know, there is a way to read mathematical books without reading the math. In the first part, you just say, you read it, you see, they'll make some arguments and they'll say, and therefore we conclude that this works. So then you only need to read, therefore we conclude that this works, right? And sort of glaze over everything in between. That's what I do. Whenever I take a math book, in the beginning, I just glaze through and pick up on some gems along the way. And then the second part I come and try to make more out of it and make more out of it typically mathematical books are best read. Vipul Khosla, Ph.D.: In multiple passes, at least that's the way I do it i'll point out to that, but anyway. Vipul Khosla, Ph.D.: This is a huge digression The point is that it used to be everybody's favorite how many layers how many notes in a layer they would do people would tinker around right most of deep learning is tinkering around guys there is a very simple analogy and the analogy is this see telescopes were discovered by the dutch craftsmen by the Dutch craftsmen. The glass, magnifying glasses were already discovered. Arguably, the proto-telescopes were already there. And then Galileo put it all together into a telescope and to look at the planets. But the optical theory was not well developed. We did not have a full and clear understanding of why a telescope should be the way it is with this compound lenses and so forth. We had some understanding, but not all understanding. Some understanding was needed to make a good telescope, but there was a lot of tinkering around. Much of science progresses in a very strange way. Quite often, tinkering around in experimentation produces some really good breakthroughs. And then the theoreticians come follow and really understand why this works. They develop the subject, they put a theoretical foundation to the results. And on other occasions is the opposite. For example, black holes were predicted by Chandrasekhar. What? In the 1920s, when he was traveling on a ship, they were only recently discovered and proven to exist experimentally. Gravitational waves were known to be there. Actually, when I was in graduate school doing my doctoral work in theoretical physics, it was well established that gravitational waves would be there. I remember that our research group was trying to build a gravitational wave detector. It took close to 30 years to successfully make a gravitational wave detector and actually detect gravitational waves. So sometimes theory is ahead of the experiments and experiments catch up. When you look at deep learning, it is very evident that it is the world of tinkering. All the explanations that I've given you, they're true, but there is a lot that is not known. We just know the outlines of the theory. Like, so don't ever have the illusion that we fully understand why deep neural networks work. Right? There's a bit of a magic around here. And it has a lot of people marvel and be frustrated. I remember there was a professor in computer vision i was i was who wrote an article actually he said that see computer vision had such beautiful theories like elegant theories of this and that and all sorts of models and then gave deep neural networks all right convolutions and this and that and it basically blew the state of the art based on those theories and you looked at the deep neural network and and we And it basically blew the state of the art based on those theories. And you looked at the Deep Neural Network and we'll do that in the next, in the eighth lecture, the convolutions. And you look at it and all it is is the convulation function, mathematically very simple. And if you understand Deep Neural Networks, all these layers, layers, layers. So there'll be like so many filters and so on and so forth, that you are sandwiching together. And lo and behold, it does an excellent job at computer vision tasks, a class of computer vision tasks. And it beats the state of the art of models based on actual computer vision theory. The fellow wrote in his the professor wrote very interestingly with a reaction, which captures a feeling that people have with deep learning. They're both fascinated by the quality of the results because for practical purposes, those results are amazing. They really move the state of the art forward. And yet they're disgusted with the lack of theory, because there's almost no theory in deep learning. What is it? It's just some neurons firing, some activations, some little bit of theory. I mean, there is some theory. I wouldn't say it's not there. But at this moment, the subject is by no means fully understood. So we are in the world of tinkering, guys. You can, and in a very real real sense you can take whatever you have learned so far just tinker with the with just create a little architecture with feed forwards and make some small changes put some ideas and just tinker around and there is a fairly good likelihood that you'll come to a breakthrough and come up with a particular variant of the neural architecture that does extremely well for some tasks. I know it from experience. People have been doing it in the labs. But if you ask this question, well, could we have derived this neural architecture from first principles? Is there a theory behind why this particular kind of a neural architecture works better than other kind? Or do we even know if this is the best kind of neural architecture? People will just shake their head and say, no, we are totally in the world of just tinkering around. So it is very much like building telescopes before optical theory came into optics. The physics of optics was really understood. So that is a flavor of this world. And so get into the habit of tinkering. If you notice, almost all the homeworks that I give you in this, it is all about tinkering, change the network, change the shape, do this, do that small things, but develop a habit of tinkering and seeing how it affects the result. Because what you need to be an expert in this field is, you know, the more you tinker, the more you get a gut sense, an intuitive sense that you can't explain why, but you will be able to find the optimal way of putting things together much more quickly than if you don't tinker around. Tinker has developed a sense for it, right? Now, there is a whole field called automated machine learning that tries to do it using Bayesian learning, and I was going to tell you, but it is also not fully sufficient. There is a whole lot of tinkering that goes on still. A lot of the things are still tinkering around. So get into the habit of tinkering. Why did I take 64 as the hidden layer? I don't know. Why did I take 16 as the hidden dimension? That I know because for MNIST has been so well studied. I know that if you take two, it won't be enough. Four won't be enough. By eight, it's getting better. 16, it really gets better. And try it out. Change the latency. Right now, when you download the lab and today, you will do the lab. Change it to two and see what happens, right? Or four or eight and see what happens. Now I ran it for five epochs. Why? The epochs run a bit slowly. It takes some time. So run it for five epochs. You look at the loss function. I'm plotting the loss function. By the way, I've simplified the code to an extent that i hope these elements are very easy to read when you say auto encoder or train epochs is equal to five what am i trying to do guys here yeah training the network for five epochs right then i'm plotting the loss function again iterations if i just train for five epochs how come i have nine thousand close to nine thousand steps or iterations what's the explanation for this could somebody give me an explanation for this you're saying why nine thousand yeah why is it look at the x-axis why does it go all the way to 8 9000 because it has not stabilized in terms of the loss and it continues to reduce uh yes but i just said epochs to five five and nine eight nine thousand are very different numbers how did it go through so many iterations? Yeah, remember, guys, that epoch. For each epoch, we run through the entire data set. Exactly. So what do we do? We make mini batches of data. So one step is learning from one mini batch of data. That is one iteration. Because we have a lot of data, within an epoch there are many, many iterations. Are we getting it guys? That's why the x-axis goes to this. And as you can see, as Manish pointed out, yes, the loss does continually decrease. And you can see it here, the loss is decreasing slowly i would say relatively slowly so now the question is can you make the loss decrease below these numbers that is a challenge and last time you remember that you guys did a very good job of fitting the neural network to the curve the hard curve for regression so try here also see if you can bring the loss lower regression. So try here also, see if you can bring the loss lower. Now let's visualize the result. We give it a 5. Now for each of them, this is the output, this is the x prime, and this is the x, the input. So when you look at the input and the output would you say that if i give it five and the five prime looks relatively like five three looks there is some loss can you guys see the loss when you compare the digits at the top and the digits corresponding digit in the second row can you guys see that the output is not as sharp as the input would you agree guys yeah it seems slightly blurred yeah it's but the interesting thing is the input was 784 dimension you reduced it to latent space of 16 dimensions so what it means is 16 dimensions is enough to capture quite a bit of the information isn't it right yes go ahead so i understand that the conceptual part behind autoencoders is it's taking something that's represented in a high dimension and representing the same thing in a lower dimension and then converting it back to the same dimension but what is actually causing these individual neurons to activate and act in different ways in which uh as like it's different than any other kind of neural network which is just fully connected no no it is a fully connected neural network in every sense it is just a pinch you just created a bottleneck in the middle but you you mentioned that there are also autoencoders where your D is greater than N. So how is that any different than- Oh, okay. So why do we use that? Overrepresented or overcomplete autoencoders. Why don't we keep it as a separate topic? Those are the sparse autoencoders, why don't we keep it as a separate topic? Those are the sparse autoencoders. Okay, okay. I'll come to those, I'll come to those. At this moment, put it in your back pocket that one day I'll answer that for you, right? Okay, but for now, I just need to know that this is going from, we can see 784, 28 by 28 to now 16 by 16 almost. Yeah, and so whatever the reconstruction is, it is reconstructed from just a 16 dimensional vector. 16 dimensional vector has been decoded back into a 784 dimensional image. And it looks pretty good. I wouldn't say very good, but it looks well, a reasonable approximation to the input, right? It means that the 16th dimension was able to capture a pretty good amount of information. So what you should do is go and play with different numbers, guys. You know, 16, what happens if I make it four? What happens if I make it eight? What happens if I make it 32? And what you will notice is that the encoders do well for certain digits and not so well for other digits. So for example, do you notice that zeros come through pretty well? Wherever you see zero, zeros come through reasonably well everywhere. Zeros look like zeros, they don't get destroyed. You will also notice that some other digits come through very well. Seven and one come through fairly well. One is particularly good. It comes through very well. Can you identify the digits? You can see that three and five gets mixed up in, say, the first image, second row, I mean, I think seventh column. Yeah, this one. This five and three. Yeah. I mean, I think seventh column. Yeah, this one. This five and three. Yeah, they're getting mixed up and things are not looking so nice. And which other like four is a bit of a problem. Do you notice that this four here, the last row in the second set of images here, there's a four here and look at the four here is beginning to get a bit mangled right uh eight and three are also getting mixed up yes eight and three let's see to get mixed up so and this eight here is looking almost it's hard to tell whether it's a nine or a one or a eight do you see where my mouse is it's a little hard to tell whether it's a nine or a one or a eight. Do you see where my mouse is? It's a little hard to tell that. So obviously it is, first of all, it's impressive that it can do that. Secondly, we realize that, well, let's tinker around and do a better job. So which is your work, guys? First, reduce the latent space dimensionality and see the effect. How small can this dimensionality be before the reconstructed image degrades unacceptably? So in other words, how much can you reduce before it becomes gibberish? So for example, can you reduce it down to just one dimension and try to reconstruct it back from one dimension? Try it out. See if anything can be reconstructed from that. Do it to two dimensions. If you reduce it down to two dimensions, you'll have one lovely side benefit, two dimensions, you can plot it out. The hidden representation can be actually plotted out. Isn't it? Now, what about if you add more hidden layers in between? If you make the network richer, then maybe you can use less dimensions, but you can do much richer transformation, much better representation extraction, so that you get much better reconstruction. So play with that. How does the number of epochs have a difference? So I ran it for one epoch, no, five epochs. The notebook that Kyle has uploaded deliberately, it has been run with one epoch, right? I invite you all to, in the break, download that notebook, play with it, and see how it looks for, how different it looks between one epoch and five, and see how it looks for how different it looks between one epoch and five. Or just do it on your own. Remember five epochs will force you to take a coffee break if your laptop is not powerful enough. It takes three, four minutes to run or five minutes to run. It takes almost a minute or more per epoch. Now, we learned about the denoising autoencoder. Does anybody remember what a denoising autoencoder was? So you give it a noisy picture and you force it to clean that picture. Because if you could train an auto encoder in such a way that you take an original image, add a lot of noise to it, push it through the auto encoder, but the loss function you make not against the noisy image, but against the original image. The auto encoder will have to learn to filter out the noise, denoise it. And you can immediately see the value of it for cleaning up images like grainy images so the first autoencoder does a data compression isn't it reduces it to lower dimension but now the second denoising autoencoder removes noise i ran it for five epochs you can see that pretty soon it has stabilized maybe it was unnecessary to run for five epochs let's look at it yeah the loss function yeah loss function is still going down but very slowly okay you can run it for more epochs and see what happens let's visualize the result oh in this case the results are not terribly i did i even run it epox train directory yeah so in this case noise ratio clearly i have not run it because i have not added enough noise let me add no noise then i'll run it again and it will take quite some time and when you do at this moment notice that these images noisy images i did not add enough noise. But OK, even without adding. I can share my screen to show the noise. Why don't you share your screen? I'll walk you back. And in the meanwhile, I'll make this thing run here on my machine. Yeah, we are looking at Kyle's screen. Yes. So. Yeah, look at this. Could you please again? Yeah. Yes. So you see three panels, guys, three rows. The first is the original image. The second is the same image, but we have added a lot of noise to it. I hope you can see the noise there. White, white you know sparkly dust of white chalk on it and the third is the output the auto encoder has been trained to clean it out and kyle i believe you have run it for only one epoch uh no i yes i've run it for one yeah but one is not enough if you run but let's do it in the break run it for five it will be far more impressive when you see the results for five epochs but uh do it in the break you fire it off in the break let me just explain it so guys when you look at this you can clearly would you see that if you come if row two is the input after the auto encoder has been trained and you take it to the field to use would you be impressed if row 2 was the input and row 3 was the output anybody sees the value of this kind of a thing right it is getting there by the way that it has been trained on only one epoch it's far more impressive when you see the neural network to be fully properly trained. And I hope we can do that. It's running on my machine. And after the break, we can show you. So that calls for a break. Kyle will help you download all of this on your machine and set it up. I'll give you guys 20 minutes to set it up properly on your machine. Please help everyone to make sure everybody has this properly set up and they can run it okay instructions okay so under the section hands-on labs and solutions we have this lab 6 zip file click on it and it will automatically start downloading is this updated with the with that added term for the msc function the one over n uh no it doesn't okay So you can download it and store it somewhere where you can open with open with the Jupyter notebooks right so and once you do that so you should unzip your file and it should look something like this without these two zip files should look like this where you have auto encoder and sv learn okay so inside auto encoder uh inside the auto encoder folder you'll find this auto encoders dot uh ipy notebook okay so you'll have to run this notebook and it is it has dependencies on svlearn library okay so do let me know if you face any troubles and as usual you will have to change your in the notebook there's one change you'll have to do is you have to set the path right so currently it shows the path that i've said go ahead and change it to the path in your system okay Thank you. Kyle, what do I do after I download it and unzip the file? Open this autoencoders notebook from Jupiter notebooks. Open this notebook. yeah yeah so it's inside the folder auto encoder auto encoders Thank you. Okay. Is anyone having trouble downloading the lab file? No. Okay, thank you that's still trying to download so after downloading what do we do to unzip the folder done and after unzipping it open your jupyter notebooks yeah you opened it now go to uh the folder lab 6 folder okay and click on auto encoders folder go inside notebook yeah i opened that okay and without running the notebook you should see these output images do you see them yes i see them okay good so now you just start running from from the top and yeah before running this is the step you have to do from the top and yeah before running this is the step you have to do in sys.path.append change this to your lab6 folder okay I don't have the high pipe plot module installed. You don't have it installed. Okay. Just install it. Just pip pip install yeah okay put an exclamation in the notebook it says okay install i by plot okay Thank you. is that the only thing i need to change the the path Is that the only thing I need to change the path? Yes. Okay got it. Thanks. And you may want to do this as well. Oh, okay. um carl you want to put lab 6 in lab 1 folder or where i just keep it you can keep it anywhere where you have access from your jupiter notebook home page Kyle, what is that exclamation mark in front of P for? That is for like using your command prompt. Oh, so every time I issue a command prompt, I will need to put the okay. Yeah, thanks. That's something new for me. Thank you. you Thank you. Thank you. Kyle were you able to run it for five epochs yes this is the output yeah Yes, this is the output. Yeah. So guys, if you compare the second and the third row, would you feel that it's a significant improvement as a denoising device? It is. It's not as great with the standard autoencoder. Are you using the convolutional one or the regular one? Variation of the... What about the regular one? Because we want... so i'll probably try mine should i stop staring uh no let me see whether mine even i. I didn't run it actually. So yeah, continue to share yours. Convolution autoencoders. I am... Can I share my screen to show you something really good? Definitely. Sure. Okay. screen to show you something really good definitely sure okay so i'm in the denoising autoencoder section and i'm seeing that these images are marked as noisy but i don't see the same kind of noise on uh your uh like it's it's the same with your by default i added only 30 percent of Gaussian noise, which is too little noise. You have to look pretty subtly to see the noise. What I would do is go up a little bit. I'll tell you how to add noise. Go to the constructor, go up a little more constructor. Yeah, more up. Yes, so here, stop that. Denoising, after the hidden layers, put a comma. And let's say noise underscore ratio is equal to one. Try running this. Okay. This should hopefully produce some really good. I'm training it with five epochs. So it takes maybe like a minute or two. Yes, and then, what's that? Can you show that again? Oh yeah, so could you, Manish, scroll up and show to everyone what change you made so everyone can see that. Where you said ratio is equal to five, scroll up a little bit more. Oh, yeah. So guys, this element, in the denoising autoencoder, add one more parameter called noise ratio. I'll go up again, sorry. Yeah. sorry yeah so guys i hope you all are trying this lab kyle is everyone able to load this into the jupiter notebook i think jin amrit and dhwani are able to manish what is your status i am uploading the, I was getting some errors in the odd and quarter file. So I'm kind of doing this again, trying to spot where the error is. Okay. So I think parameter noise underscore ratio that increase it from 30%, 0.3 to one. Yeah. From 30% to 100. yeah see the gaussian is a very tiny signal in itself and i was taking one third of the signal yeah now scroll down why is this not showing uh the noisy part something is amiss so i mean I'm seeing that in the below images you have. Yeah. But is this is this this is different. No something yeah maybe it's the same thing this is this is a convolutional autoencoder. Some fishy I'll have to see why the code is behaving strangely maybe there's a little bug you know I might be repeating just the same first and second row as is without replacing noisy image. I'm going to scroll down, scroll down. Oh, sorry, sorry. Original image, noisy image. I think there's a minus screw up. I'll see why it is, but it will work for the convolutionals. Okay, and that extra parameter that you added to that function what does it actually do why just set it equal to one yeah it increases the amount of noise see what i do is to every to the image i added a bit of gaussian noise right i didn't so gaussian noise is already tiny i took 0.3 of it which too little noise. Then when you make it all the way to one, you can even make it two, then you add a lot more noise, but don't do it for the denoising autoencoder. I think there may be a slight bug in the code. Do it denoising convolutional autoencoder. Sure. Because the convolutional I know is working quite well. I'll just see, or take it as a homework to figure out what the little trick is. Go down, convolutional autoencoder, below that, below this is a denoising convolutional encoder. Yeah, so here you add the same noise ratio is equal to one. Good, now run this, or yeah. And set the number of epochs to five, it'll be more fun. Oh, right, right. Yeah, stop. Can I stop it from running or? I suppose it'll have to start at the top to include your, yeah, say five. Now try to rerun it. Okay, let me- Not in train, not in train, in the construct oh yeah in train epoxies and train yeah run it again see what happens Kyle is there is something module that needs to install IP plot yeah I yeah yes yeah just do um yeah Amrit can you show Manish the install? Whenever you see a library missing, just do a pip install. Manish, you see the line? Yeah. Yeah. Start with the exclamation. It will take you to the underlying. yeah so by now guys you must be realizing that it is a good idea to have a powerful machine. Because things are beginning to get slow on your laptops. I would like to take perhaps this weekend to go over variational autoencoders and possibly voices time or distance maybe the next week I'll do versus time distance and the next week's lab will be on GANs and we'll do transformers. Asif, you also mentioned you wanted to cover W and B. Oh, weights and biases and dashboards. Yes, that should be. Oh, goodness. There's so many topics. Hard to squeeze in. We should do. So one of them would be the boards, the dashboards. That is a lab thing. So let's keep one Sunday for that. And for regularization, I think skip connection and residuals also. Skip connection and residuals, yes. So we need to have a theory session on skip connection and residuals yeah explaining what skip connections are and residuals are by the way guys what's your general feedback on the textbook the this inside deep learning are you guys liking it Inside deep learning. Are you guys liking it? It's great. It's a great textbook. Yeah. I can understand. I stop after chapter two. Oh, Jen, if you're having that, set up time with me or with someone. We'll help you walk through it. Like if something, if the going gets tough, we can unblock you. And if you're in the Bay Area, remember, you can always come and meet me here at support vectors, you can schedule a time. Okay. Where are you located? San Francisco. Oh, it's not that far. If you're willing to take the hike up to Fremont. far if you're willing to take the hike up to fremont we could say oh i go to fremont quite often uh oh you do yes yeah so uh okay uh what is the schedule on a weekend for you guys no you'll have to schedule a time oh i had to schedule a time with you okay yeah okay i can i can see it uh you can see that compared to the noisy one the cleanup is pretty good isn't it yeah yeah very very minor i can see this nine over here has been kind of divergent to a five over here no see you have to you never see the original panel the purpose of a denoising is what you will get is the second row. Data will come as the second row. So you're comparing the second row to the third row. Yeah, yeah. But originally, it was this, right? The point is, originally, you don't know. It's like you have to take a look. Oh, right, right. Because you start off with inputting this itself. Exactly. So once the model is trained, you have a AI, basically you have a denoising filter now, right? So you took an iPhone picture in real candlelight, right? Where things look very, very grainy. Now you take that grainy picture and you pass it through your autoencoder and it will come out clean. Yeah, so I can see how this would be very beneficial for as you mentioned like fill in the blanks our encoders are very good for that that too i should have created an example for the masking one fill in the blanks also yeah okay thank you all uh i can stop sharing my screen i guess so guys i'll show you the code now i'll walk which will be a straightforward walkthrough but today is all about doing guys so the point is we do things yeah see look at my screen guys do you see the distinction between the very noisy ones and the this one and this is only at how many epochs? Five epochs. Did I actually do five epochs? I don't know. We'll see. Now there is this wonderful topic which I didn't run, variational autoencoders. Keep it in your back pocket. I'll explain it to you. And I haven't run, variational autoencoders. Keep it in your back pocket, I'll explain it to you. And I haven't even taught you what is convolutional. What in the world is this convolutional neural net? So I'll give you, again, we are ahead of that because we're going to do convolutional in the eighth week. There are different neural architecture, again, inspired by the cat's brain, the convolutional filters, lots of filters are applied, filters and pooling, right, filter layers are added, which are very different from the traditional feed forward network, fully connected network. And just staying ahead of that, I put it here. Now, let me walk through the code. After that, guys, the rest of the lab, I want you folks, I know you are busy, you won't get time. I want you to stay here and finish the lab. In other words, do it. I'll just walk through the code so that it all looks easy. output, like where to keep images, this makes sense. I created a class called base auto encoders, right? It's a very simple auto encoder. You give it the input dimension, you give it the latent dimension. And if you want, optionally, you can give it, like when it says none, this is Python way of saying, it is optional, so yeah, default. So you can give it hidden layers, no hidden layers, And this is a constructor, which by definition, of course, produces nothing. Now, there's some basic tests. You can't give input dimension less than 2. It doesn't make sense. Latent dimension, at this moment, it is like a lower dimension representation. So we say that latent dimension cannot be smaller than input dimension right it cannot be negative if not hidden if there are no hidden layers but if there are hidden layers then do what how do you determine the size of the hidden layers you do you do this if hidden layers are specified do the hidden layers. That's all. There's a little bit of a magic here. Now, what do I do? You build all of these things. Now comes the forward pass, the encoder and the decoder. So let's go and build the encoder. You will see that the encoder building is dead simple. What do you do? You create an empty module array, modules array. First you append a linear layer made up of what input dimension right and hidden layers some hidden layers right then what do you do you add you also append some relo to all of this right if hidden layer if there if there are hidden layers, then for each of the hidden layers, you do this business. You add a linear layer and a ReLU. So I'm using ReLU all over the place in the base one. Then module append, that's it. Basically, you're stacking them together. And now this thing has PyTorch has a simpler syntax and then sequential, which is what I'm using, a star modules, right? Vector of modules. You're passing it. That is it. Build the decoder. If you look at the decoder, it's nothing but the hidden layers. You know, this is a Pythonic way of saying, just read it backwards, the hidden layers. Would you agree that it's pretty much the same thing but written backwards? And so you have all of these modules. You start with the latent and you go back, keep on expanding. And finally you expand it out to the input dimension. The output is the input dimension. So output layer, same size as the input layer oh for some reason at the end i put a tan h there i am thinking why oh yeah i wanted to produce results between minus one and one for some reason yeah because images the the the m list images they are grayscaled. They have values from 0 to 256, and that has been scaled down to 0 to 1. So we will scale it back to 0 to 1. This function is a static function. It's shaped to image. You have a vector. You need to make it into a 28 by 28. How do you do that? Very simple. We take it. First of all, this tanh goes from minus 1 to 1. So if simple we take it first of all this tanh goes from minus one to one so if you take uh you sort of you add one to it it will go from zero to two and you divide it by half it will go from zero to one which is exactly what you want because the input was from zero to one this is basic you know manipulation of color scales of the gray scales batch size whatever the batch size was channel was one we know there is a monochromatic image so by now you must be realizing that we are playing with images so you have to have some very elementary familiarity with image manipulation guys this is not neural networks at all it is is just manipulating images in Python. I would suggest you become familiar with the vocabulary of images. What does a channel mean? Channels are the RGB channel, red, blue, green channel. And the alpha channel potentially the transparency channel. What does image size? How do you take an image, a 2D image, a 3D image rather with RGB values thrown in, and vectorize it or make it into a single vector, represented as one long vector, and things like that. So this kind of syntax, guys, I would consider this as a homework and become familiar with it. I put ample comments here to explain what I'm doing at each stage. The view is a PyTorch function that renders it into the shape that you wanted. So you can take any vector and reshape it and say it must have this many batch size, these many channels, image size, image size. That is the height and the width. This is the height and the width. You have to do that. And then original input also, when you get an image, what do you do with the image? You need to linearize it. In other words, flatten it out, two-dimensional or three-dimensional image into one nine vector. Then you need to do some compression transformations. When you train the data, you need to do some transformations. First is that you need to convert it, a vector into a tensor. Then you need to normalize it. Normalize does what? It makes the, well, you all know what normalize it, but it will make, normalize it to a center of 0.5. Why? Because the grayscale values are from 0 to 1. So you want that the medium tone color should be 0.5. library called TorchVision. Again, we are jumping the gun, but when you do the deep learning, the computer vision class, we will go over this very, very carefully. All of these functions, the TorchVision or the computer, the image manipulation is the first lecture or the first week in which you learn all these image manipulation techniques. But there you learn at great detail. Here, I've simplified the code to such an extent that if you try a little bit, you will understand it and use the book as a guide. You will immediately understand what we are doing and look into the Torch Vision website also. It will give you documentation for this. You load the data and then you run the for the epoch for the data. You do this, you know, you build input by flattening it if you need to add noise you will add noise you create an output you create a loss function you optimize everything after that should be a no-brainer the only additional thing is after every hundred steps we save some images we save that we save sample input output so that we can compare it remember in the jupiter notebooks we compare it that is it plot loss is quite straightforward you plot loss linearize that function how does it do you give it a tensor what will it do it will linearize it to a single vector right arbitrary long vector that's. Then the rest of it is straightforward. Now, convolutional neural network, you can sort of glance through it, but it won't make sense because we haven't done convolutions. We will do convolutions in the eighth week. Now, this is the sixth week. We'll do it in the eighth week. And there's nothing magical happening. We're just building two simple convolutional layers rather than linear layers that's all the autoencoder structure remains the same now this variational autoencoder i think we should remove it from here because now it is in a separate class of its own vae uh here a variational autoencoders. I will teach the theory of it maybe this Sunday for those of you who are interested. Its main quality is that it is a generative autoencoder. Once you train it, you can create arbitrary many samples that look like the input data, that look like the input that look like the data that you trained it on so if you train it on shoes now you can create all sorts of shoes right you trained it on people now you can create lots of people and so on and so forth right so you can imagine that what we did with gan is the next step we will do this after some time not Not now. So skip the variational part. So guys, what I would suggest is enough of theory. We have, it's already 842. I would like you to take the next hour completely to running the labs on your own. And also don't look at the variation, but look at the Vanilla autoencoder. Try to understand this code. Right. At least the base encoder you should understand properly. Base autoencoder, which is from line 34 to line... It is a bit of code, but it should be very simple code, similar to what you're used to, 200. So it's about 150, 160 lines of code. I give it to you as are used to 200 so it's about 150 160 lines of code i give it to you as a homework to study this code the previous batches are founded fairly easy to understand after a while but they have also said that it took them a couple of hours to study it properly but run the jupiter notebooks guys and do the labs, the homework that I mentioned in the class, treat it as a classwork and please do it. Right, please do it. So this is it. Kyle, could you please help people get through this. Guys, post your and everyone, post your updates. Show what happens. What happens if you run, so one of you try it out. What happens if you reduce the dimensionality to just two? Are you able to reconstruct anything? What you will notice is that it will be... It does do a good job. With two, right? Not all digits. It will do a good job with two right not all digits it will do a good job for some digits and badly for others okay right yeah find out which does it it does a good job for which deserts digits it does not do a good job those are the things to play around with guys play around with, guys. Play with more layers, less layers. Play with more epochs, less epochs. But I hope the autoencoder looks simple, at least in utilizing the autoencoders. Are you all becoming a bit more familiar with it? In reality, what happens, guys, is that you don't write and practice autoencoders from scratch. There are libraries practice autoencoders from scratch. There are libraries of autoencoders. People have published a lot of autoencoders. And you just take one and tinker with it a little bit. The way you're going to play with my autoencoder, the one that we are releasing here at Support Vectors, in the same way, you're going to tinker around with other auto encoders that you find on the internet and use it for your real world problem to solve your company's whatever use case is there the main problem that you're solving because it takes a bit of effort to design the auto encoders correctly and apply to your problem domain. So learn to tinker with it. Today, your whole lab is to tinker with it. And at the end of it, autoencoders is this, this picture. It's as simple as that. It's one of the, the reason I started with autoencoders is because it's the simple neural architecture that is different from a random feedforward network. It is a feedforward network but of a special kind. Any questions guys before we leave you guys to do the labs?