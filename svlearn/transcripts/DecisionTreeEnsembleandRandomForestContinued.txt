 We are in the process of going live. Let's give it a minute. All right, folks. So last time we talked about trees or decision trees. Decision trees were the first algorithm that were, I would say, in the class of nonlinear algorithms. Before that, all the things that we learned so far in ML100 and the previous lectures would draw a linear decision boundary. We would draw either a straight line to fit the data, or we would draw a straight line to bifurcate feature space into two halves, or maybe three halves or four halves, partition it into multiple halves using logistic regression, linear discriminant analysis, quadratic discriminant analysis and so forth. I mean, and this is only true for those of you who attended the previous workshop, but the bottom line is if you didn't, the bottom line is we would take the feature space and we would partition it into components or pieces or sub-regions using linear decision boundaries or linear hyperplanes as the decision boundary. Decision tree, on the other hand, was the first algorithm we encountered where you could make all sorts of complicated decision boundary. Now I'm going to review what we did the last time. What we did is we started out with a region. We took the example of San Francisco Bay Area and we said if you look at the Bay Area, the region of the Bay Area is at the tip like you have the Golden Gate Bridge, you have the north side of the Bay Area is at the tip, like you have the Golden Gate Bridge, you have the north side of the bay, you have the big bay in between, and you have the financial district in pink, you have the commercial, these are the buildings, then you have the Golden Gate Park, the San Francisco's park, the Golden Gate Park, then you have the residential areas. It's a very rough picture of what the San Francisco. Are you sharing something? Oh am I not? I apologize. One second. But I was. Better now? Oh yeah, now we can see. Yeah, that goes, yeah. Well, I hope this reminds you of the example that we took. We had the Golden Gate Bridge, this bridge here. This is the north side. We all are familiar with the Bay Area. We have the commercial district, right, the financial district and the commercial district, which spans out at the tip of the peninsula and along the bay side of it. On this side we have the Golden Gate Park, then we have the residential area. Of course, this is a very crude approximation to what really exists, but we'll use this as our as our setting to illustrate how we can use a tree to do our classification process what we want to do is we want to divide any point in this space in this big rectangular space and be able to tell whether it is a commercial area whether it it's a park area, trees area, or whether it is a residential area. So our classes are commercial, trees, and residential. So this forms our, this is our categorical target variable. Now how did we do that? We took this entire rectangle, big rectangle, in which we wrapped the peninsula, and we said, you know, the most logical thing is to first bifurcate or partition the space along this axis, along the, pick a point along here, X1 axis, and split it. And when you do that, you have a very clean separation on the left hand side you have the commercial district on the right hand side there's a mix-up well you say oh well all right we can improve upon that how can we improve upon that we can do another split the second split and now we can say that if you do the second split on one side will be the parks the greenery and on the other side would be the residential area. You say, well, that's an improvement, but we need to do further. Then you make a third split. The third split is here. You make this horizontal split and it separates out the commercial from the residential. And likewise here in this region, you can do one more split and you can separate out the commercial from the trees and the park area. And then you can say that in a sense you are done. Now, there are mistakes, for example, in this region. You see that we have some mistakes here and mistakes are possible. There will always be some mistakes. So you're not shooting for perfection, but you're shooting for a classifier that with reasonably low accuracy, sorry, reasonably low error rate can tell you exactly what a point in the feature space looks like or what a point looks like. Are we together? Is it a financial district? Is it a resident? Is it trees? Is it this thing? So this was the problem with that. Now, how did we go about doing it? This whole process of bifurcation, just to review, what we did is we started with the whole region, R nought, this whole area of land. And we said that the first split we'll make is along the X1 axis. We divide it into this and this so this is it uh the the commercial area and the rest of it when we do that we realize that the commercial area is a good split r1 is a good split we will probably not need to split it again but r2 needs improvement so the next split we make is one more horizontal split to just put the greenery in between. You notice that we put the greenery in between. And we say, well, that's an improvement. And then we realize that, well, this region, R22 now that we created, still has a mix up of commercial and residential. So we do a horizontal split to take care of that. And subsequently, we do a horizontal split of the middle region also and so this is what we end up with if we try to represent it as a tree r not split into r1 and r2 r2 split into r21 and r22 and r21 went through its split and r22 went through its split so this entire process if you notice we can represent it as a tree and this tree is called the decision tree of course the sum total of all the leaf level regions is r naught likewise at any level if you add the regions up they will all add up to the original region that's the way we want to do it so so just to review the decision tree algorithm how do do we do it? The steps were you start with the whole feature space as R naught. Then you find the best cut point on one of the axes. Now how do you find the best cut point? You'll have to scan each of the axes to find the best cut point. So it is computationally intensive. For each of the sub regionsions next, search for the best subregion to split and the best axis in the region. So suppose you have three regions. You'll have to go and search for a best cut point in each of the regions. How do you find the best cut point in any of the regions? You'll have to go to that region and scan the two axes here and find is it best to split against x1 or is it better to split against x2 whichever turns out to be better you would go ahead and do the split and you would recurse that downwards but when do you stop how do you decide that we have had enough well when the number of points in a region is below a threshold let's say 10 or 100 you decide not to split anymore right now the question is how do you decide the best split point what is the criteria now it depends upon whether you're doing regression or classification just to about your recollection regression is the uh is the process of predicting a number how much ice cream would you sell on the beach for example or in this example to illustrate or motivate using class the decision tree for a regression I took the example of an open field in a cold region. So let me remind you the example that we took last time. We took the example that you have a big piece of land, a rectangular piece of land. On one side are these cold tundra forests. Those of you who know about the Arctic, you have this deep dense forest out there, the tundra and the tiger forest. So imagine that there's something like that there, right? It's deeply forested. Then on this side, what you had is you have a house, and the rest of it is open field. And to motivate regression, to predict a number, we said, let's see what the temperature would be on a cold night at any point in this box in this region so to motivate it from a physical perspective an open field will radiate most of its heat so it's likely to be the coldest right a tiger forest or a tundra forest is likely to be dense it will the trees will act as a cover and so it will not be so cold. And finally, if there is a house, let us imagine that there's a fireplace in the house and there's cooking going on in the house. So the house is likely to be the warmest. So what would you like to do? How would you like to build a decision tree you may start out by first splitting along at this big blue line it will bifurcate out the forest and now you can say in the beginning because this house is a tiny region of the field you can say that the field is cold with including the house and the forest is warm well that is a good split and it works for the forest because uniformly the temperature would be the same in this region. On the other hand, in the field, we got something wrong. Most of the field is fine, but we need to box out this region because the house is warm and around the house is warm. So what would you do? You can do a split, horizontal split to just cut out this open field of this and one more split here to cut out the house now you have boxed out the house region and you have a tree now this tree is a perfectly good tree you could perhaps optimize on it by erasing for example you could erase this. I'll just put a marker here. No, let me take a red marker. Hang on. Please give me a moment. This tool doesn't quite work very well with Zoom. It tends to. Let me see if I can. the way guys give me a moment let me see if I can bring all the tools to the side instead if there is a feature to do that no it won't allow me to do that all right be as it may let's use this so suppose we use oh goodness let me try if I can move the controls instead to the side. Hi, meeting controls. Okay, we'll do like that. You guys can all see my screens, isn't it? Yes, no problem. Control, Alt, Shift. I should remember this to get it back. So what you can do, if you think about it, this particular part of the segment, if you remove it, you realize that you will get a simpler tree. Isn't it? I'll just draw the tree here. You would get a region split like this. Big split between the green and the forest and the open field, and you will get a split like this do you see that guys so you will end up with non rectangular region this is your field this is your house and let me just say open field and this is the trees open and house and this will be your best decision tree that you can build and that would be quite useful but let's stay with that unpruned tree and ask ourselves at each given moment how do you find the best sorry guys for some reason I'm hearing my own echo back are you guys also hearing my echo back or are you guys on mute okay I heard one fragment yes I think somebody was listening to YouTube at the same time is dying so you'll have to mute your YouTube guys if you are. You have to be on one or the other. All right. So now the question is how do you split? To split, you need a mathematical measure. Now here are the rules of the game. So we are playing a game here. The rules of regression is in any region region what is the value here for this thing you have training data so you have temperature values for all of these data instances you take the average temperature right of this region r2 you take the average temperature and that will be the prediction for temperature in the forest likewise you take the average temperature in each of these regions and you take that to be the prediction. So if that is your prediction, you can quantize your total errors by taking the value of the actual value, subtract the average from it, which is your prediction, and the gap is the residual. So far so good guys. Any doubts about this? By now this should look pretty obvious. Our errors are the gap between prediction and reality. You predict the temperature to be let us say 30 degrees. It turns out that the forest was actually 18 degrees and 18 degree Fahrenheit. So you do have a gap. you got it wrong by 12 degrees and the total error is the sum of square of these mistakes or the errors so where would you like to split you would like to find a split point that splits the region into sub regions says that this e is minimized isn't it you want to find the point at which if you split you would have the maximum reduction in errors because if you take a region big region initially it will be the average of the entire region if you split it you will end up with two different averages, this and this, and the sum squared errors will be better because this is warmer in general, this is colder. And so the cold will have a cold average, the warm will have a warm average. And so you will be closer to the true value of numbers here and the numbers here. So this will help you determine the best split point are we together I said so this is what we talked about last time so we use that to compute the errors for regression so regression is straightforward you use the sum squared errors which you are familiar with to find the point of maximal gain maximum bang for the buck where should i split so that the errors get reduced maximally any questions before we continue folks and we are reviewing last time now let's move on to classification suppose it's a I convert this into a classification problem. I say any given point x, let us say point x in the feature space, this point, you have to tell does x belong to that point is an open forest, is a house, or is a well open land or it is a forest which of these three open field which of these three does it belong to right so how would you go about doing it you you have some training data the algorithm sees some trees has been given some answers the algorithm sees some trees, has been given some answers, what could it do? It could just take the training data and say, where should I split? Show that I get as pure a split as possible, right? Or one way to think about it is that if you look at this entire region and you take the majority, let us say that the majority area, the data points happen to be white, right? The open field. And let me color the house with a different color. Let's say that the house is this color. So what would be the majority here? You would look around and say most of your data points, let us say, is the open field. So if I were to ask you any point before you split the regions, what is it? Your answer would be the majority. So you would declare every point to be an open field. Well, if you declare it to be the open field, you'll get it wrong here and you would get it wrong here. So you will have a lot of mistakes, isn't it? So what you do is you look at the proportion of mistakes here. So you would have the proportion of points that are open field, open, which would be the proportion of correct answers. You look at the proportion of points that were actually trees, that would be the proportion of correct answers. You look at the proportion of points that were actually trees, that would be the proportion of errors and house, that also would be errors. So what you would end up with is a proportion of each of the classes that will help you realize how many correct answers are there. So for example, the proportion of open points are correct, proportion of errors, and what can you do with proportion? You can say, hey, let us play with proportions we want to you want to find a split per point with the lowest proportion of errors generally proportions are not used they're crude measures they're not well behaved so what you use are slightly sophisticated mathematical tools those tools are called impurity indices. Those are Gini and entropy. Now, Gini and entropy are both interesting functions. I think last time in the review, some of you said that you were not very clear about the basis and the information is the same guys i'm getting uh is that a question somewhere were you asking a question uh no asif okay sorry so we have this impurity index and the feedback i noticed in the polls was that we didn't quite understand it fully so what I'm going to do today before we continue on I'm going to give some time to explain it all over again and then we'll come back and review this these notes is that fine guys all right So let us go and look at, before we do that, let's do it very simply. Let us consider a function x. Right? And let us consider a function g of x. And x belongs to the interval 0 to 1. So in other words, it's some sort of a proportion. Proportion of any point, any type, any class, let's say open field or trees or house points, would be as a proportion of the total, it will be a fraction between 0 and 1, right? So it will belong to this interval. Now, let's consider the mathematical function X 1 minus X are we together and I'll also consider another mathematical function let me call it entropy is minus X natural log of X. Are we together? So we are going to forget about all decision trees and all of that. We are going to study the mathematical properties of this. These two functions. What do they look like? These, these sort of functions actually are very, very useful in machine learning. You'll not only see it here, they are fairly ubiquitous in the landscape of machine learning everywhere. Let's study it. Give me a moment, guys. I'm having a sip of soda here. sip of soda here so let's take the unit interval from zero to one and here i'll go very slowly guys if you don't understand please do stop me let me draw and this is the unit so this is x-axis and let me draw g with, well let's use different colors for this. I will use g, blue for g, and I should use, let's say I use this color for entropy. Let's make it a little bit thicker. Okay, much better. Sorry. Are we together? We use this. So let's see how this behaves you realize that when X is 0 G will be G will be 0 isn't it if you substitute 0 here what will you get 0 0 times 1 is 0 on the other hand when X is equal to 1 what do you get you know once again you know because this term will go to zero and so when now if you look at this function it's very interesting uh those of you who know calculus can uh just directly do it you would know that g prime that is d g d x and you don't have to know calculus by the way is equal to what is it it is equal to 1 minus x right minus x right and if you set it to 0 so if you want to find the extrema you have to set the derivative to 0 dg zero is equal to dg dx is equal to one minus two x implies x is equal to a half isn't it and if you take the second derivative of this d square g dx square you will easily realize that it's minus two implying that this is a maxima. When the second derivative is negative, you know that it's a maxima, right? So this is a bit of calculus. If you don't know calculus, no harm done. Just try to put in a lot of points between zero and one, and you can convince yourself that whatever happens, this thing has to be symmetric. so this thing will have to go did I make it symmetric almost there let me try again let me like this and try to be a better artist no oh boy almost so let me try to be oh so just pretend that it is symmetric. And I got it right. This is how gx would look. So in other words, it will achieve a maxima at half. So this is interesting. We will see what it is trying to say in a moment. Let's look at this entropy function. So g, the word people often use for that is Ginny, but you don't have to pay attention to the word. Let's look at this entropy function. This entropy function also behaves like this. It sort of behaves like, it is just a more, this it sort of behaves like it is just a more well this is terrible this is anything but an entropy function okay let me just be a little bit smarter there so it just rises up a little bit quicker and well it doesn't look quite symmetric, but you guys are better entropy. Entropy, and this is g of x. Are we together? And this is, so far so good guys. Is it easy to understand? Anyone who's having difficulty understanding this, this is a great moment to stop me. We are good here so far, right? Asif, can you explain it again? All right, I'll explain it again. See, look, I'm just drawing ordinary single variable functions. Look at this function x one minus x, right. R. Vijay Mohanaraman, Ph.D.: What does it look like, what is the shape of that function? It looks like the blue line here. R. Vijay Mohanaraman, Ph.D.: Right. Does it look like the blue line to you? Anil? Anil? Yes. It would look like the blue line. And now look at the, this function, it looks like this. Now, why it goes up a little bit over, it will take you a while to see. See, see that if x is zero what will the entropy function be zero time where log of zero is minus infinity but we as a convention believe that this is zero but now why is this zero it's a little more complicated it is called the laupital rule means when when whenever you have a peculiar situation like this what you can do is you can take the derivative of this in the limit you can do limit epsilon tends to zero of like well okay x tends to zero x ln x log of x and if you're trying to do this, you can take the derivative of these terms and you can convince yourself that this will all go to zero. I'll leave that as exercise for you, but take your interest. When x is equal to 1, this term will be 1, but log of 1 is 0. So it will come down here to 0. So it's basic, you know, the sort of pre-calculus stuff. So it's basic, you know, the sort of pre-calculus stuff. And if you again take the derivative of this, so let us do that. I'll do that here. d is equal to minus x, right? Minus, what is it? What is the log of x? Do you guys remember one over x minus x right so here you so sorry first term you would have is minus log of x and the second term would be minus x over minus one isn't it and so you would you would essentially come to the conclusion that suppose you have this oh by the way this is not i made a mistake typically in um you take the natural log for entropy, but in machine learning, you actually take it to the base 2. My mistake, log to the base 2. So log lg to the base 2. If this is equal to 0, you're saying lg to the base 2, log to the base 2, x is equal to 1. So what is x? So what is what is x? x is equal to minus so this is minus one so x is equal to a half so it again achieves a at half it also achieves a maximum right this statement is log X is equal to minus 1 if you look at this so is it any is it any better guys I'll be following it a bit again anybody who didn't get it so guys remember when we do decision tree today is the theory day this ensemble and so forth the theory is something you learn only once in your life it is worth understanding it in practice you won't be using the theory. You remember that when we write code and do the machine learning practical exercise, you can actually do it without understanding the theory. But after some time, you hit a roadblock because you won't be able to think through your data unless you know the theory. So theory is good to understand, but practicality is somewhat a different set. There's some intersection, but it's a different set. So any questions guys? Anybody who didn't understand, please be frank and I'll repeat again, or tell me which part you want me to explain and I'll explain again. repeat again or tell me which part you want me to explain and I'll explain again. So, Asif, like both genie and entropy, like they are, we used to know the impurity index. So, what is the differences between them? Can you please explain? Yes, and in fact, I'm coming to that one now. Thank you for asking that question. So now, we did this exercise in the abstract. Let us bring this exercise to the ground. What is X? Now, let's relate it to this bit of mathematics. How do we do that? Let us say that in the definition of Gini and I'll again use G, Gini G index, impurity index. Our G impurity index is defined as that. Let for each class, so suppose you have a region, right? In the region, there are certain points. Then there are certain house points, you know, where the house is there. And then there are certain green points. These points are all green. And obviously I'll just exaggerate the quantity of white points. So you see, let us take some number. Let us say that the whites are the open field points are the open field points are let me take 60 not 60 50 let me take them as 50 the tree points forest points are 40 the house points are 10. are we together there are 10 yellow points 40 green points and 50 open points and they all add up to a grand 50 over 100. The proportion of forest points are 40 over 100. And the proportion of house points are 10 over 100. Would you agree, guys? Right. And now from this, let us derive for each of these quantities of sorry what am I doing here Let me get rid of this point. Well, the eraser doesn't seem to be working. I'll leave this line here. I apologize. So. Oh. All right. We'll go back to the points so now suppose I compute the genie the G of open points I mean say just take the proportion of open points remember in genie the formula was G X was X 1 minus X but there are three of these X's here let's do it for open open points times 1 minus proportion of open proportion of house points times 1 minus proportion of house points and proportion of forest points times 1 minus proportion of house points and proportion of forest points times one minus proportion of forest points so because all good with numbers we can compute that this is a half times a half isn't it this is a fourth this would be uh house points are oneth times 9 isn't it so that would be 9 over 100 and this would be 1 10th times 9 10th isn't it so this is equal to a half this is equal to 4 10 this is equal to 1 10. so 1 10 1 minus 1 10 and that is equal to 9. So 1 10th, 1 minus 1 10th and that is equal to 9 over 100. And what would this be, the forest? It would be 4 10th times 1 minus 4 10th, 4 10th and that would be 6 times 4 is 24 over 100, right? So I might as well make it 25 over 100 just for uniformity. so i might as well make it 25 over 100 just for uniformity are we together now does this make sense guys so now what we can do is the genie impurity indexes index of this entire region before we split it this entire region before we split it is the summation of all of this sum over each of the classes each of the i p i 1 minus p i right so means we add up these things if you sum it it becomes equal to 25 plus 9 34 plus 24 is 58 over 100. does this make sense guys are we together guys right so we say that this is 0.58 right or 58 by 100 are we together any value is this? Does anybody need clarification with this? Bhuvana, I'll come to your question. What's the difference between the two? I remember that. So far so good. Are you guys getting it? Anybody has a question? Now, I could have computed this for the entropy also, but let us not for the time being do it. It will be a repeat, just here doing it with log. Now the question is, where can we split? Let us say that we split this region. Let me redraw this region because we have come down here and we'll do the computation again. So this is a region. These are your white points. These are your yellow points belonging to the house. And then these are your forest points maybe I'll make the white also a bit more pronounced let me take this alright guys so the All right, guys. So the question is, where should we split? I'm deliberately going to take us. So intuition tells you that we should split. Let us say at this point. What just happened? That wasn't very illustrative. Let me take yellow. Well, not useful. Okay, let me try again. More color saturation. Red. more color saturation red that also didn't tell well highlighter doesn't seem to be the right thing to use i will use a color let's use blue again and use a thick version of blue so would you agree that this intuitively makes sense as the first good split would you agree guys yes and let us see why our intuition is telling us to do this now if you do this we end up with two regions region R2 and region R1. Now you notice that R1 is not a pure region, but R2 is a pure region, isn't it? So now we'll have to do the same analysis that we did across the two regions. So we do it for R1 and R2. R r2 what is the proportion of open points in r2 zero what is the proportion of houses in r2 zero proportion of a forest in r2 well all 40 points are here there are 40 points and all of them are forest points, isn't it? So you would say it is one, right? So now see what will happen. If I do zero, one minus zero, zero times one minus zero, the same thing, x, one minus x, or let me write the formula in terms of proportions, p, one minus p. And here it will become 1 1 minus 0 and when i do the same summation for r2 what is the summation coming out to be this is 0 this is 0 and what about last one will be 1 minus 1 i think oh sorry 1 minus 1 i uh thanks avijit thanks for correcting one minus one i uh thanks abiji thanks for correcting and what would this be also and so what is the what is the impurity on index on this side so genie of our zero the initial now now let's do the same thing proportion of forest here is and this is proportion of houses is equal to how much would that be guys 10 over 60. excellent and so once again let's do the p 1 minus p part when we do that 0 times 1 minus 0 5 6 times 1 minus 5 6 and this would be 1 6 times 1 minus 1 6. so this is equal to 0 oh sorry this is equal to the second one this is equal to 0 would you agree this is equal to one minus five six is one sixth so five over 36 this would also be equal to one six times five six is five over 36 right and so if you sum it for region r1 what do you get get? You get a 10. A 10 over 36. Are we together? And so now let's ask this question, what's the split worth doing? Let us ask this question. What was the initial Gini before split that is Sigma R 0 is equal to 0.58 you remember guys we computed that here right yeah we computed it right here this result and now after the split r 1 plus r 2 what is it we are adding a 10 over 36 plus zero isn't it guys if you notice this is 10 over 36 this is 0 and so what does that come out to be yeah 10 over 36 this is approximately equal to would it be fair to say that this is approximately 0.3? Or less actually 0.28 or something like that. If one of you were to pull out your calculator, you would see that it's something like that. So now, so the total Gini has it decreased or increased? So the total Gini has it decreased or increased? You're comparing 0.58 with 0.28, which is smaller. The after the split is smaller, isn't it? Decreased. It has decreased. So this decrease between the Gini before and the Gini after is called information gain. So you say information gain is defined as, is by definition, this is a symbol of definition, is measure in impurity before minus impurity after. I use the word impurities because you have two regions now, R1 and R2 that it's split into. So the difference between the two therefore is here is equal to here it is 0.58 minus 0.28 and that is equal to 0.3 approximately I don't know what these values are you can plug it into the calculator what 10 over 36 comes out to be. 0.3, this is greater than zero. Do you agree? So what you do is you compute this 0.3 and now, well, okay, I'll just mention it that it's also happens to be greater than zero. Now, you ask yourself, look at this picture and ask yourself, if you had done the split anywhere else would you have gotten a better split so for example if i had done this split i won't go through the full calculation but ask yourself if i had done this split instead a horizontal split between the above and the below a horizontal split between the above and the below would that have been better no what you can literally see that there would be much less pure regions that get created and so you would say that the information gain would not have been as much are we together and that is why, and I'll remove this red region from here. Oops, sorry. Red region from here. So this also, well, let me get back to blue. So this is it. So do we understand the concept of information gain now, guys? Is it any better? Let me know. Otherwise, I'll explain it all over again we will never come back to the theory of decision trees again so it is important that once in our lifetime we understand it any questions any any doubts anybody would like me to repeat? Anil, are we clear now? Yeah. So guys, what exercise I repeated for... Ginny? Yeah, go ahead. I'm sorry. I just have one question. So this impurity gain over here, like the differentiation was very easy since we didn't have as many properties or qualities. Let's say, for example, an insurance company wants to, or either a bank wants to give out a loan and they have certain criteria. So there would be N number of criteria which they would want to have. so in that scenarios with the information gain will be as clear it can always be computed is the answer oh here we have two axes right yeah this is one x1 and this is2. So what we're doing is we're scanning for the best cut point along X1, the best cut point along X2. And theoretically, we are finding the information gain for every possible cut point. As you can imagine, there are infinitely many cut points on X1, infinitely many cut points on X2. infinitely many cut points on x1, infinitely many cut points on x2. Okay. Right? Mm-hmm. Theoretically speaking, we are finding, we are doing all of those cuts and finding where the information gain is maximum, which cut point gives us the maximum information gain. Okay. Though in practice, the mathematics kicks in and you can be way smarter. You don't have to do infinitely many computations but we're going to the technical details okay so now generalize from this this is to depend so someone bank has insurance has a 50 features mm-hmm yes you'll have to scan all 50 axes right and find the best cut point along each of those and look at the best information gain that happens by cutting at any point along any of the axis what it will be. So you're really looking at a massive computation, which is why decision trees used to be in its days when it came out, considered a brutal computation. People would find all sorts of clever ways to make it faster, better, more efficient. The algorithm of decision trees, therefore, is pretty sweet actually. The internal implementations. See one part is the theory, another is the numerical implementation. So there people have found clever tricks to make it faster over the years. And so 50 dimensions is no big deal. These days. Yeah. So I'll say I have a question. So yeah, basically, we intuitively we could see so we drew the line there between the green and the white open field and forest right. Now the computer, how would it do basically it will start from zero till the end, how will it know from here to start and right to end and what should be the increment see it starts from zero zero right it goes along the first axis and finds the best cut point then it goes along the second axis and finds the best cut point so so the the in a particular axis when it is scanning where it has to end it will basically know from the range of that variable not only the range of them it's actually cleverer than that see what happens is that at some point See what happens is that at some point then you notice that impurity decreases That's increasing Right, you observe, you know all of those points you can easily find the minima of Where should I split to get a minima of the impurities? So computer does that basically it automatically stops. Yeah. Okay. So there are heuristics like that. See, you have to be in highly nonlinear situations. You still do have to scan it multiple places. But there are ways to quickly do this these days. So that gets into the internal details of a numerical implementation. Yeah, exactly. That's what my question. Okay. A lot of depth. See all of these algorithms when decision tree came out it was slow and sluggish and then people improved it and so forth over the years so the cart we call this cart classification and regression trees there was cart 1 0 there was card 2 or 3 or etc etc 4 then 4.5. And then those guys decided to just, I think if I remember right, they tried not to, decided not to open source it, but take it commercial for some time. Some of those guys, they made it card five or so forth. Then open source caught up and so forth. So obviously there is a lot of deep numerical work that goes in perfecting these algorithms. And a lot of these are done at the level of C and Fortran. Because there it runs much, much faster. CC++ Fortran is where you implement it. There is a Java implementation also. Weka has a Java implementation of the card. I believe they do card 5 or card 4.5, I don't remember which one, one of the two. If you're curious about that, all of this code is open sourced, remember? You can go and look into the code, but be prepared that, you know, it represents some 20-30 years of accumulated optimizations. Isn't it? So the code will be, it will take you a while to catch on to the code. In broad strokes you'll get the code immediately, but to understand the subtle optimizations it will take you quite some time. Any one of you here have a background in numerical computations? Or Fortran? Am I the only Fortran guru here? What is the latest Fortran asset? i remember i heard like i used to do fortran 4. yeah i worked with f90 i don't know what the latest is but in my generation we were writing fortran yeah f90 is i think still that see i still write sometimes actually what am i saying only recently i wrote fortran code so I think I use the F90 you use that open source thing right that Linux comes built it they're built with no I use Fortran long back for uh that modeling and simulation for control systems yes yeah it's a system for I think I used yeah possibly it's a 95, very simple method. It is very simple. And the performances are absolutely, uh, nothing can come close to it. It was very, it was meant for mathematical operations. I'm not sure if, if anything, it evolved into other stuff, but no, no, it just, even today, you'd be surprised that most of the scientific computing is done in Fortran. When you are doing it and you think you are doing it in Python, if you observe very carefully, what is happening is when you install the Python, Skicket libraries and so forth, it is actually installing Fortran and Fortran libraries. So it is actually utilizing all those highly optimized Fortran libraries in the background. Fortran and C libraries in the background, all that BLAST, LINPACK, LAPACK, ATLAS and all of those are heavily being used. Because they have 40, 50 years of evolution behind them. Alright guys, so this is uh do we feel we understand the concept of information gain better now yeah all right guys so with that let's go back and review and then we'll move to ensembles. Remember today's topic is ensembles but we always give an hour to reviewing the previous week's services. Now entropy is just another measure of disorder or impurity. Entropy, literally the word is disorder. Entropy has been well understood and studied by physicists, for example, Gibbs and so forth, the thermodynamics and statistical mechanics, or well studied more than a century ago. It made its way into computer science with the work of Shannon and information theory and so forth, where he brought it into, well, the whole encoding aspect of it. And entropy is all about machine learning. It's always a measure of disorder. The underlying belief is that if you look at the data properly, you can observe it in a way that you see the least disorder, things like that. So obviously there is some disorder in the data inherent data but if you look at it for example if you look at it let's get back to our example here if you split on the big thick blue line you realize that you have created more order now green points on one side white and yellow points on the other side, right? If you think of them as gas molecules the green gas molecules and a white gas molecules that they are well compartmentalized Isn't it? Whereas if they are all mixed up then that's a state of disorder That's sort of that's a basic thinking there So now let's go back and review everything. Now it should be very easy hopefully to do. So we go, go ahead. I see one final mathematical question there. Why is it base two? Is it because you're always splitting the regions by two? You said that, why is it always? You said that it is actually not natural log, it is log against 2. Oh, you could actually, here's the thing. Log can take the natural log actually. Right? It would just lead to different split points and so forth. So the thing is, now to Bhuvana's question, which is better, the genie or entropy? Which of these should we use as a measure measure of disorder the answer to that actually is no one knows do you like that answer no there is actually a deeper thing I keep speaking about. It's called the no-free-lunch theorem. What it says that in the absence, no one algorithm or approach is ever better than the other on average. Data rules. So data will occasionally prefer and give better results for Gini, and sometimes it'll get better results for entropy. Gini used to be very, very common long ago. Entropy came later. A lot of people swear by entropy. They say in general they find better results with entropy in their domain of knowledge. So it may so happen that for the problem that you're solving, let's say that you're working with specific, you're making classifications with respect to some chemicals, and so on and so forth there or in your situation uh one may be more effective than the other case but case basis you'll come to know only when you do it however the gap is not too much However, the gap is not too much. If you just replace Jenny with entropy or vice versa, the small gain in model performance, it will be small gains. You're not talking about big, like these are small potatoes. These are not big deals. You won't go something like from 30% accuracy to 90% accuracy. I can't imagine a situation where just flipping the impurity measure would give you such a drastic improvement. You might go from 82 to 84% or 86% improvement. And you know a model that has gone from 82 to 86 just free without doing anything, without any effort just changing from genie to entropy which in code is just one word change well why not get that four or five percent extra performance isn't it so that's what it comes down to which is why you should always experiment with both now let's go back up and finish our review. So I said you review the region into parts, right? We got the concept of entropy. This is what we talked about last time. After that, now that we know this stuff, we talked about the fact that when you build decision trees, we can do pruning of the tree. Why do we do pruning of the tree? Because sometimes when you build decision trees we can do pruning of the tree why do we do pruning of the tree because sometimes when you go down and build a tree your tree becomes inordinately complex and pruning of the tree is regularization or simplifying the model and when you simplify i won't go into too much details but there are techniques so that you can get non-rectangular boundaries you know for example here if you imagine that this line was there and now you erase this blue line so you got a non-rectangular boundaries okay so pruning of the tree another question is that how is it that you can prune the tree and get better performance and that question is worth answering you won't get better performance on your training data training data would prefer an unpruned tree and give better results where will a prune tree give you better results can you guess guys Dr. Raghuram G. On the inference or? Dr. Raghuram G. Yeah, on validation data. So if you take the data and you split it into your training and validation data, what will happen is training data, it will overfit. But on the validation data, you check that if I remove, if I prune the tree this way, does my performance improve? And you will notice that by pruning the tree, your validation test results, you know, your results on the validation data will improve. And that's how you know you're improving. And you keep on pruning till you realize that it doesn't improve anymore, it becomes worse. And you stop, I'll put you together. And remember that it's a see it's a buy it's the same thing as the bias way in straight off you need to find out a model a model that is too simple is biased a model that's too complex it has variance errors so when you start at the root you have high bias errors. You have one big region. Then as you recursively keep on splitting one by one, you go and remove bias errors, but you're making a very complex model and you start developing a lot of variance errors. So you need to backtrack now. You need to return, Remember the, let me just give you the idea of the bias way instead of this is the bias errors. It decreases bias errors, right? And then the variance errors on test data would be, let me use another color. Let's say that this is the variance errors complexity errors right remember we talked about it and so the total error which I should depict I suppose with the red line is this so what happens when you make the model very complex in the learning process you end up somewhere here right here the comp if this is the complexity complexity you have ended up with a very complex model so what do you need to do you need to backtrack you need to get back to this point this is your prune tree mm-hmm you need to prune and step back in complexity till you come but you don't want to prune over prune the tree because then you'll start gaining into so you need to come down this path to this point but you don't want to go back up this you want to stop here am i making sense guys about pruning of the tree yes that is the purpose of pruning the tree that's all it is that is what it means to prune the tree and so the review of trees finishes i would like to now move on to ensembles of trees finishes. I would like to now move on to ensembles. Ensembles I talked about is the wisdom of crowds. There is actually a theorem in mathematics that was discovered a long time ago. It says that if somebody's chance of guessing an answer is just a little bit more than 50%, right? So in other words, the person is a weak learner or just about slightly gets it more right than wrong then in a systematic way you can harness the wisdom of crowds you can take a lot of these weak learners and excuse me you can create an ensemble a crowd these. Then you can aggregate their wisdom. How do you aggregate it? If you are doing regression, so suppose you want to ask people, like I did, like I told you, I used to do that in class when we used to physically meet in the before COVID world. I would give you guys a jar of stones and ask you to guess how many stones are there invariably almost all everybody would be wrong maybe one or two people will be close to the right answer but the average of those answers was uncannily close to the right answer the reason for that is all of us have a bias we have certain information in our decision and certain bias, right? So we may guess either optimistically or pessimistically, but then as you add up everybody's answers and average it, the average of the errors cancel out. The average of the information remains the information. And so this, the right answer begins to stand out that is the wisdom of crowds now why weak learners i talked about this question this thing that weak learners are better than strong learners it turns out that strong learners have a higher tendency to get correlated with each other and tangled with each other in the decision making so in other words they begin to think alike entangled with each other in the decision making. So in other words, they begin to think alike. When two learners are thinking alike, they are redundant. You can just use one rather than two. So for example, if you need to get an understanding of whether there's global warming or not, the last thing you want to do is get two members from the same think tank, right? Because all the members in the think tank have been looking at the same data and coming to the same conclusions. So either both of them will strongly favor the statement that human beings are causing global warming, or both of them will strongly favor the statement that human beings are causing global warming or both of them will strongly favor the statement that human beings are simply not the cause of global warming, it's natural cycle of the earth. You see my point, right? So if you really want the wisdom or you want the truth to emerge, you want to take intelligent people who are making, but not necessarily pundits, you want to take intelligent people who are making, but not necessarily pundits, you want to take just intelligent people who are capable of thinking on their own. Enough. Just more than 50% right or a sense in them. And then you take a collection of them and you'll be surprised at how uncannily right they are generally. So the basic ideas are you need weak learners, you need a lot of them. And then comes the other aspect of it. See, what happens is if all of them are reading the same book, they'll reach the same conclusion. So the way you ensure that the learners don't get correlated or entangled in their learning is by giving them different windows into the data so one metaphor that you can use is blind men and the elephant so you have a big elephant of a data but to each blind person you blindfold them. And so each person gets to see and feel only a part or rather feel only a part of the elephant. So once one feels the trunk, one feels the ears, one feels the tummy, one feels the legs and so forth. But if all of them are willing to agree that they are all correct, the picture of the elephant will emerge, isn't it? Or more prosaically, see, whenever you take a picture, a photograph, a photograph is a two-dimensional representation of a three-dimensional thing. You take enough photographs of a thing, you will be able to reconstruct it in its full three-dimensionality, so long as all the photographers are not taking photograph from exactly the same vantage point. If they are all taking pictures from the same vantage point, you would really not know what is out there on the other side, right? Or on any side except the front from which they're taking the picture. So for them to get collectively be able to reproduce the building in all its three dimensionality, you need to take an ensemble of those photographs from different vantage points, seeing different realities, subset of reality. See when you look at a building you don't see the whole building, you see a subset of the building, you know, the part facing you. And that is very similar to what you want to do. You want to give each learner only a subset of the features. So suppose there are 10 features there, you want to pick a small number of features, two or three, and give it to them. So the way, if you remember, I motivated it was with the story of or the metaphor of a teacher taking a bunch of school children to the to a special zoo or a meadow in that meadow there are only two animals cows and ducks right now the teacher being highly scholastic she begins to explain in intricate detail how what are ducks and what are cows so she talks about the fact that cows have, cows are big compared to ducks, that they have a swishy tail and big horns, right, and they have four legs and hooves and so forth. On the other hand, when she talks about the ducks, she emphasizes that they're small, feathery, webbed-feet-ed, beaked, right, winged creatures. So you may speak a lot about the attributes of a duck and the attributes of a cow. Imagine that these children are all pre-kindergarten children, preschool children. You have taken them and you're explaining so much. What will happen is different kids will latch on to different attributes. One may remember, for example, that if it is big, it must be a cow. If it is a duck, it must be small. And she may also remember that if she sees a tail, it is probably a cow, right? Things like that. If there is no tail, then it's probably a duck. Some other child may latch on to the fact that webbed feet is it. Look at the feet. Are they webbed or are they hoofed? If you see hoof, it's a cow. If you see webbed feet, it's a duck, right? Or she may notice that there are beaks. Is there a beak? If there is no beak, it's not a duck. So different kids in that crowd of collection or school of kids, a class of kids, would latch on to different pieces. Being little children, they're not going to understand all the anatomical differences that the teacher pointed out. And so now the teacher shows an animal and asks, what is it? Let us say that the animal is at a distance. Then to the kid who is looking for size, that cow may look rather small and may confuse it to be a duck. Right? But let's say that the duck is flying around the other child may happen to notice that it's a winged creature it flies occasionally and so it may come to the conclusion that it's a duck so all of these children they will come to their own conclusion between cow and duck using what they have learned right they're looking at different subsets of the reality but collectively if you take the majority vote, you ask those children, what is it cow or duck? Let's take the majority. Whatever it stands for, the majority stands for, you will notice that time and time again that will be uncannily close to the right answer. And very infrequently they will be wrong. Whereas each child would individually be often wrong. And that is the point of ensembles. The other point, so that is your point C here, the point is not only should you let them look at different aspects of the reality, you should also let them see or learn from different samples of the data. Because the data may have, any data sample may have some peculiarities. So just by accident, it may so happen that overall the majority of the animals are white in color or something like that, or brown in color, right? And so you might begin to draw conclusions like brown things are cows and ducks and so forth, and ducks are not brown and so forth, but that may not be really true. So if you give subsets of the data you sort of protect against against an overall bias of the data right so that's the point of giving different subsets of the data to different learners to learn from so all of these things put together those are our main four points a you need big learners you need many of them you need different learners looking at different aspects of the subsets of the features. And D, you give them different samples of the data. At the end of it, you want to make sure that the learners are not correlated. If they are correlated, then the bully feature becomes important. So suppose there is one feature that always misguides. You see the problem with decision trees has always been that if there is one, if you make your first split wrong, you get completely screwed thereafter. It's a fatal decision. The rest of the tree takes you down the wrong path. But then the root bifurcation that you do, you based it by scanning all of these axes. Let us say that one of the features is dominant. And for whatever reason, it misleads. Because either the data is wrong or biased, or whatever reason it misleads because either the data is wrong or biased or whatever reason and that feature always misleads so what will happen your decision tree will always come out wrong but if you take you create lots of decision trees like let's let's take the example of decision trees those learners can be decision trees but they can be anything else suppose you take a lot of decision trees but to each tree you only give a few of the features so what will happen is the majority of the trees will not get that that dominant feature that is spring of the analysis so some trees will make a mistake because they got that feature majority of them won't. So I'll illustrate it with a number and an example. Suppose there are 100 features. It is a good idea never to give more than square root of the total features. Here the total features are 100. What is the square root of 100? 10. So to each tree you give no more than ten features so let us say that there is a bully feature and you create a few hundred trees by the law of large numbers it will happen that only ten percent of the trees one tenth of the trees will get that bully feature so one tenth of the trees will get the answer wrong quite often. But then that's the beauty of it, because you're ensembling before you're aggregating the results of the ensemble. 90% of the trees will come back and give you the right answer. And so the bully feature gets muted. It gets suppressed. It gets neutralized. Do you see the point, guys? So that is it. Now, how do you see the point guys so that is it now how do you take the aggregate it's very simple if you are doing classification as we talk you take the majority if you are doing regression you take the average of what they are saying so you ask the children like for example I would ask you guys how many stones are there in the jar whatever number you say we take the average you want to know what's the temperature outside you ask all the children what do you think is the temperature outside and then you take the average value and that's your prediction of temperature outside are we together right so that summarizes what we have talked so far guys please stop me here if you have not understood so far are we good so far guys good uh did you follow any part of it so far this must all be looking a bit strange yes as if yeah getting it you're getting it very good anybody else has a question please ask me otherwise let's take uh it's 8 30 now should we take a 15 minutes break and regroup at 8 45 let's get some water and so forth and let's get together at 8 45. asif I just wanted to ask you when you said you know we need a mechanism to keep the bullies who can dominate neutralized is that is that the way we like when you gave only a part of the features to each decision tree is that achieved by that exactly in fact the rule is square root if you you have p features, you never give more than square root of p. In fact, that is one of the mathematical theorems. It's a theorem about data projections. You just need square root of p at most features to get. There are words like isometric embedding and so on and so forth. I wouldn't go into the mathematics, but take it as a fact that it works okay thank you behind all of this you know i've given you the intuition the gut feel or the picture each of these are deep wells of knowledge mathematical knowledge so you can keep drilling deeper and deeper into it knowledge. So you can keep drilling deeper and deeper into it. It's a big rabbit hole. Okay. Go ahead. Just want to make sure just my understanding. This Gini index is giving like the proportionality, right? Of when it derived from there. It's derived from proportionality. Both the Gini are derived from proportionality and roc curves are also derived from proportionality right that you know true false is different it's about model metrics okay it is about the precision and accuracy or rather a false positive versus true positive or rather a false positive versus true positive. So in other words, as an aside here, ROC curve is, suppose you take a certain cutoff in the data, like, you know, see, suppose you have a probability that it's a cow. Suppose you can say cutoff is half. If it is 50% probable market as cow, or you could have taken 1 4th even if it is 1 4th probable that's a cow it's a cow or maybe you need a lot more assurance 3 4 so you can you know the probability X where you put the cutoff determines the rate of false positive and true positive okay right so if you're not careful see a bad model ready that is making random and true positive. Okay. Right. So if you're not careful, see a bad model, right, that is making random guess will have equal rate of true and false positive. Yes. Because it's true less. Right. But a good model will achieve very low. Right. For a very low false positive rate, it would have already achieved a very high true positive rate, which makes sense, right? You want a good model that is, that has a high true positive rate for a very low false positive rate. So this is the ROC curve. Right. So if like after partitioning, if Gini index gets better, the ROC will get better, right? Oh yes, of course. That's how they're connected, right? Yeah, in that sense, yes, they're interconnected. Yes, totally. So yeah, that's a good point. Suppose, so this is your square box, right? This is a pretty bad baseline. You can actually do worse than than baseline but let's say that in your first split you're getting an roc like this then the second one will give you an rsc like a better roc a third one will give you rs like this and a fourth one will give you a roc like this and hopefully a very good model will eventually give you a roc like this so let me mark it zero uh so let me call it the the the first split one two after the two okay impurities keep getting better and better right yeah you need to keep betting better and better yeah okay that's intuition fire keeps improving thanks a lot yeah so this is your roc curve so this is a good observation actually yeah yeah i just want to make sure thanks a lot rc curve so this is a good observation actually yeah yeah thanks a lot this is processed all right guys uh if we are clear, it is already 8.36. So let's make it 8.50. Let's meet in 15 minutes. And I'll pause the recording for now. And we will restart the recording. If you guys want to talk without worrying about being recorded, this would be it. Control, shift, halt. Pause. worrying about being recorded this would be it control shift hold I'll catch up with you guys Thank you. Thank you. Thank you. Thank you. all right Thank you. Thank you. Thank you. you Thank you. Thank you. Thank you. Thank you. Thank you. Takk for ating med. Thank you. Hello? Hello? Are you guys here? Yes, I see. Now I think they're waiting for us. Thank you. So guys, just one question. Have you guys formed any groups for this project? Looks like most of the guys they want to do it go alone. Sorry, Abhijeet? I'm saying most of the guys looks like they want to go alone. Okay. That's why I was responding to you. Okay. Okay. So maybe, so that's okay. I'm in the same boat. I just thought to ask it anyway. Yeah. So I think we are kind of maybe two, because when we used to meet at support vector, it was easier to go to each other and say, Hey, you meet first five guys and say here's my group yeah you cannot you cannot replace that online no way sorry they got built out of Fremont. Just curious. Is this Mardul? You live in Fremont? Yeah. Okay. What's your background Mardul? I am, my background is in telecommunication, hardcore network engineering side. Any experience with R and Python? No, no. I mean, this is you didn't do any R and Python machine learning before? No, no. I, I am not from any coding background. I'm in Fremont and I have already partnered with Patrick. He's a doctor. Okay, very good. I have a healthcare background. Okay. We both. And if you, I don't mind if you want to join us. Sure, sure, sure. Yeah, I want to make sure that this discussion is happening so we both work together at the same office and uh around maybe five six o'clock we uh we are discussing our you know data sets people are having trouble you know making their teams so i think you should step in and help them out. So me and Patrick already decided. You guys have decided. And Mardul is in Fremont, and he couldn't find any partner yet. Who couldn't? I was talking to him. I was asking if he had formed any teams or not this time around. Seems like there are some moments i can so uh yeah but you didn't do any coding before in r or python right none yeah yeah python uh okay a little bit on python yeah yeah so then yeah because patrick is very uh efficient and uh uh i don't go you know uh i i just like to run fast so no that's okay yeah yeah so i will ping you uh or patrick on slack so we can discuss it yeah sure let the class continue now okay all right let me ask shanka are you going to do the projects do you have the time i may not have the time yeah i might try but i don't want to get okay anil how about you are you planning to do the project uh yes Are you planning to do the project? Yes, I'm planning to do the project. Nice. So have you picked your mate? Yes. Who is doing with you? Shiva. Me and Anil. Okay, you guys are formed and you want to stay as a team of two. Okay. Pradeep, how about you you have you formed your team? Yes sir, Prashant is there. Okay so you and Prashant have paired up. Dennis you have paired up already with Glenis and Prachee and Kate, Kate you also part of the same team or you're a different team? I'm right now on the team of Harapriya and Balaji and we were going to add a couple other people. I don't know where Harapriya is tonight. Okay, maybe she's not here. Harini, how about you? Do you have a team yes I'm with Balaji asset Balaji and Jay oh my team too oh yes okay you guys are a team then middle do you have your team are you planning to do the project? Yes, yes. I think that's, I was asking, I'll see if Sukpal and Patrick, I can join them if they are okay with it. Okay, that's it. Avijit, how about you? Are planning to, do you plan to do the project? um you know i will be a little bit erratic because of the work i have okay i i will i mean someone can tolerate me you know to be my partner be confident uh we are all busy people time is what you make out of it i'm sure that we are all needlessly sleeping seven hours a day. We should sleep three hours a day. I don't mind that a model can join us. But, you know, we will gonna just, I'm not going to me and Patrick, we don't, you know, spend time like how we import what is the meaning of this and that quickly, because we already spent two, you know, two like how we import what is the meaning of this and that quickly because we already spent two you know two semester with you so i will just go jump into uh magasia news so one of your first task is to go find your mate you can form a team of at most four or five and we have a project so those of you who joined a bit late let me give you a primer on the projects guys we are doing the labs obviously as you notice that the monday sessions are very theoretical and the wednesday sessions are very lab oriented and practical but while we are picking up the labs it is also important that we do a practical project a capstone project labs it is also important that we do a practical project a capstone project i've given you guys the problems for the project now here's the thing if you do the project and you do it well complete the project and you complete it well the bonus that you get is you get my recommendation you get me as a reference for your either your job or your graduate school. And frankly, more than that, you also get talking points for your interviews. Generally, if you have done a project, data science interviews become a piece of cake. The most of the interviews start with you having to describe what work you have done. They'll ask you to describe some good work you did. And it has to be non-trivial work. It has to be more than just a lab. Your project becomes a basis of conversation. You talk about your project and as you talk about your project, you sort of gain, you gain their trust, they begin to realize that you have something to say. More than that, as you do your project and you put it in GitHub, or you put it in Kaggle and put it in the usual thing, and I would highly encourage you to make an article out of it for Medium, and I'll of course help you write that article, help you with the language and the voyage and so forth. It gives you legitimacy in the field. Generally you want to be in a situation in which jobs chase you. You don't chase jobs. You know that you are, one of the easy ways to know that you are good in a field is when you don't have to chase jobs. They all the time keep chasing you. So it is important to reach that level. That's how you want to be. You don't want to be scared, especially in this economy. You don't want to be a person who's scared whether I'll get a job or not, or whether I lose my current job and so on and so forth. You want to be the person who's always and so on and so forth. You want to be the person who is always wondering whether, you know, just finishing a project and pondering what would be the next project you would do because you have too many choices. And you get there, you know, there's a word, there's an old saying, success leads to success. Success breeds success, I believe is the word. You need to succeed. And I'm giving you a chance to succeed at a project. We'll give you all the assistance, all the help. So go form your team, work hard and succeed at a project. You'll have my full assistance and then showcase it and then take that as your basis for getting job interviews. That's it? Yes, please go ahead. Did you share the details of the project on the slack or I talked about it in the very first lecture and I will talk about it let's keep it as a conversation point for Saturday I'll review all of that on Saturday that early noon yes Saturday send you you miss that part is it yeah I was not on the Saturday call you and also I think you missed the first day is it no I was there on the first day you talked about the project okay I missed the one hour of the second day on the first day but maybe I so remember all those recordings are there on YouTube one of you asked where are the recordings or maybe I'll take a minute to show you the recordings, guys. Let me go and show you. In fact, if you look at my screen, what you're seeing is actually the live broadcast happening on the Support Vectors channel of YouTube. So how would you go to this channel? It's very easy. Just go to youtube.com slash support vectors. Very easy to remember. Channel support vectors. When you go to this channel and you notice that the moment something is live it shows we are live now. Then if you go here, so these are the latest uploads. For example, this is the quantum talk and so forth from yesterday. Was it yesterday night? Yes, it was yesterday night. We seem to be having a lot of talks. Now you see the weekly tech talks. These are the Sunday talks. The quiz sessions are put here, but I decided not to move forward with that too much. But the first one is here then there are two workshops we have been through some of you have taken my workshop introduction to data science and then at the very bottom is the workshop recordings practical methods in data science do you see all of these videos here guys the very first session the river data set session two so first session second maybe I should order it what one of the things i should do is put it in the right order but if you go to this recording you will see that one by one it will play through all of these recordings all of these are here so i will just cancel it out for now. So how many sessions we have had? We have had two weeks, which means that we have had four sessions. And we have had this Python, then two, actually every week we have three sessions. Actually how many sessions do we do? If I'm right, we do five sessions a week Monday Wednesday then we do a Python session on Sunday then we do like you know QA session on Saturday and we do the seminar the weekly seminar so we're having a total of five seconds every week they are all showing up on different channels for example this latest Python session is here I don't know if you guys noticed it or anybody saw it. Oh, there are three views. So only three people have gone back and viewed it. The river data set, I noticed that 15 of you have gone back and reviewed it. The decision tree, last week's, last Monday's topic, 33 of you have gone and reviewed it. That's very great. The practical methods, the first and the second lecture, again, 30 and 41 views. These are all views I'm hoping are coming from you guys. So yes, you have been reviewing these videos. So do go here and watch these videos, guys. Are we together? And remember the channel is youtube.com support vectors. The name of our workshop series is practical methods ands in Data Science with GCP, Python, and R. So go down. Maybe I'll pull it to the top. But I deliberately kept it to the bottom because it's not of much significance to the general public. Also, these videos are here temporarily. I'll take it away, I'll make them private and only registered students will have access to it. In other words, all of you who are here will have permanent and forever access to it. So you can forever keep going and reviewing it as you see the need for it. Just like for introduction to data science, those of you who registered for the workshop, they don't just see three of them, you should be seeing all 18 of them in your library. Three of them I've made completely public, just to give people a flavor of what the introductory workshop was. And Magesh, for you, did you get a sense of it now if you go do you see these lectures at the end yeah now I got it so I will go through them it took is good to them and one of the things I'll do after this class is I'll order them in the right order because one week one or like a date or something because i think 15th and 17th you had or i don't know which day which so something like that all the orders actually for me to order it i can order it right now before we start uh playlists um Let me fix the order right now. Yeah, there we go. So I need to just move it up. Now it's in the right order. First session, second session, decision tree was the third session. This was the fourth session. So that covers the two weeks. this is Python informal session 2 and also put the Python informal session 1 and now they are all in the right order so just watch yes I can informal session 2 so there is a Python informal session 1 also yes I just realized that I haven't uploaded it. See what happens with all of these videos is I try to make at least some effort to clean out the gaps in time. You know, the break for example. It's a 15 minutes break, it's worth removing it. So I do those things and then I post it. And of course, if you look at this video, for example, if you look at this video from the beginning, you'll see that I would have, yeah, this is the raw recording. Make sure that everything has come through clearly. And sometimes you'll notice that I'll add, for example here, I'll add a little bit of a preface to it. Do you notice this support vectors banner? So this is, just think of it as a show off for editing movie editing yeah movie editing a little bit of editing these are bragging rights you know this is how you create brand awareness as if i don't think so we recorded python one series one no no we did actually we did okay have it it's just that unfortunately i also stay very busy so i need to which is one reason see it's a it's a chicken and egg dilemma you know when you have a one-man operation like this last year we had a pretty good revenue and this year i was going to start out by hiring an executive assistant, a secondary. But then came COVID and all our lives upside down. COVID destroyed everything. It destroyed the revenue stream, of course. And in a way, it has a silver lining. One good thing is that I'm taking the virtual classes much more seriously than I used to before. But the point of the matter, the unfortunate thing is the revenue stream is much lower. So I can't get the executive assistant. I really need to do these things. See, after I finish the class, somebody should just help me upload all of this, right? And I don't have that. There are people who are helping quite a bit. For example, Prachi is helping a lot but already her hands are full we need somebody who does the editing editing takes a lot of time each video like suppose I give you guys a talk of three hours it takes me typically one one and a half hours to edit and clean it out before posting it so it's the nature of this and help in that someday where the assistant would be good. So there we are. I can do it, Asha. Prachi, you already are doing a lot. But still, like. No, I can do it. Yes, I can help you. I mean, I can definitely get you started. It's a good skill to have. Yeah. But I hear that you're already busy on a lot of fronts. That's okay. It's a lot of friends but that's okay it's something to learn about so and even dennis said so we both can do it and like okay i can do that i can also help us and dennis can do it which means that they're meeting i think in the world nice guys if you all help of course then we can keep that as soon as the lectures are over we clean and post it yeah yeah this is abhijit going back to the project right do you think tackling those projects by by oneself is a good idea or it's a good idea to have i mean from a workload standpoint the good idea is one partner you know have you so the best advice i can give you and this is by the way my personal advice is i follow the reverse Fibonacci series the numbers eight one two three five eight so the basic rule is if you are when you really are a novice go join a team of five or eight right you watch how they do it and you contribute in wherever you can. You were doing that, if you remember, in the bootcamp. Yeah, I could not complete, yeah. Then after that, a step down. If you are in a team of eight, go to a team of five. If you are in a team of five, go to a team of three. If you are in a team of three, go to a team of two right and try to do the next thing in with a smaller and smaller team and then go solo if you go solo right away then there are two demerits multiple demerits to it first is that you will get stuck and you will struggle right number two every day we have pluses you know days when we feel at times or hours when we feel motivated and then hours when we feel a slump right so for example uh abhijit you and i both come from the same region of india northeast so right after a nice dose of gulab jamuns we are we have a we feel quite happy we are ready to conquer the world but right after after our heavy meal, we slumped. We don't feel like doing anything. So you'll have your ups and downs. So then the point of a team is it sort of integrates or average over is when you are having a slump, somebody else is motivated and making progress. And also the again, the same wisdom of crowd comes in we just talked about it a lot of you working together have different ideas you have different strengths and weaknesses and so a collection of peak learners leads to a very powerful team and so i would initially suggest do not be shy go join you know uh one advice somebody gave me in life is that in the early stages of your career, you should only work in teams where you have a guarantee that you are the dumbest of the lot. Never work in teams where you're the smartest guy because then you won't learn anything. I don't know. This is a career advice we always give to the young. And then gradually as you mature, you know, if you can be, if you can survive, you may be the least knowledgeable person in a high power team, but the high power team will teach you a lot. That sort of mentality. So don't be, you should never feel that, you know, how will I look lousy or something. Join the, join a high power team. I don't know where is that high power team, how to find the high power team i don't know where is that high power team how to find the high power team here yeah here i would say if you want to join the high power team uh who just find out who are who are the people making most progress right at this moment almost everybody is more or less at the same level some people are at a slightly slightly better level than others but there is no exceptionally high-powered people you all are capable you all can do it so jump into it and make progress and so anyway that's my advice on going solo versus being in a team start out in a team and then in your next project in the next project at the end of the day you'll become a lone wolf you know you'll run your own thing as if i'm also looking to join some team and i'm not sure which team yes so remember that you want to join any team is good which has a a mixture of talents. So don't join a team. Let's say that if you're completely new and you know, you have been doing some analysis, isn't it? Yes. Yeah. Then you pick anybody in that who would like to join Bhuvana, by the way. Abhishek, do you have a team member? No, I don't have any team member yet. Bhuvana, would you like to join forces with governor oh yeah I am fine you guys do it any let's increase the team size by one or two more anybody else would like to join this team yeah I can join us if I'm Arita Arita yes so there you go Arita Arita Abhishek three people is it fourth person Abhijit you you want to join? Alright. That makes four of you. I hope I am not forcing myself on them. Nothing like that. See, remember guys, I'll be helping you and making sure that you don't fail. I'll also be giving you guys starter code. On one of them, the archive project, I'll give you the starter code. I'll give you the starter code i'll give you a notebook that gets you started when will i give you i don't know because i'm very busy as you can imagine we are already holding five sessions a week but i'll find time to give you guys that all right so bhuvana aritra myself and uh pradeep you have a team member Pradeep, do you have a team member? Pradeep is sleeping, sir. Pradeep is sleeping. Sonal? I am not. Pradeep, do you have a team member? Yes, sir. We have two members, three of us. How about you, Kamya? Kamya, do you have a team member? I do not. Pick a team member quickly. do not i do not pick a team member quickly who would you like to pick um who would like to join forces with camille to give you a background on kamya she uh she is in college she is going to purdue and has exposure to to machine learning techniques she could be quite tacit anything i don't have a team member okay yeah there you go and anybody else would like to join this team sonal how about you okay but i'm not too experienced with all this. None of us are experienced. We can all join forces. Okay, yeah. I'd like to work with Sonal and who else was it? Bhavish. Bhavish, yeah. Okay. All right, great. Is there anybody who is still without a team? Balaji, you have a team. I think everybody else here has a team, isn't it? Okay. Akana, who did you join forces with? Are you a team of one or you have some people? No, I'm not. I'm not because of, as if I'm repeating the course, right? I'm done all these labs almost oh no it's not the labs it's the project the project the covet project and the archive project okay okay uh no i don't have a team yet no i don't have a team i was i was going to ask anil if you wanted to join because i know anil from before and anyone else who wants to join is fine. Anil, you have a partner yet? Yeah, so I'm with Shiva. Okay, so Shiva, would you like to take Professor Kala? Yeah. Okay, so the three. Three of you guys are a team now. So, all right. Anybody else who's left behind? Mridul, you have a team? You do, you were just talking about yeah yeah Prachi how about you you you Glenis and Dennis are a team alright guys is there anybody left behind Sanjay how about you who do you have I said can you hear me yes Sanjay go ahead yeah so meabh, we are doing together. You are doing? Yeah, we're chatting with Sukhpal. He's also doing with somebody else. We're there too, and we are trying to see if we can make it four together. But I'm not yet sure. Me and Saurabh, we are together for sure. OK. I have a user showing here as Aro? Aro Khanna, sir. Who is that? Aro Khanna. I want to know. That's a good one. It's true. Well, you don't know basically these days, right? In politics also there's a lot of machine learning goes on. Yes. Aditya, who did you join? I haven't joined anyone yet. Would you like to join? I don't know. I don't know. Yes. Who did you join? I haven't joined anyone yet. Would you like to join Sonal and Kamya and Shiva? Wait, do you mean Bhavesh's team? Yes. Okay. So pick a member, pick a team. And Jaya, do you want to do the project? Would you have time? So pick a member pick a team and Jaya do you want to do the project? Would you have time? I'm doing that inside thing now. Oh great. Yeah, so, but I'll be there for two more weeks then I will see you I can join something. Srini how about you? Do you have a team member? Or are you are you in? Are you intending to do the project? Actually, I have decided to go with my previous team. I think we have solved the pairing problem now, hopefully, right? All right, let's move back to our staff. I should ask a question for you. If people are doing in pairs, if Anil wants to do it with whoever, that's fine with me. Is it OK if I can do solo? I can ask Sandeep for help? That would be a great team, mom and son. Yeah. So, yeah. He's not following this class per se, he's not. Okay. But. He can help you with the project, Kale, that could work for you. Yeah. And so the only limit I'm saying is no more than five. But solo is definitely good. And especially with Sandeep, it will be great experience for Sandeep also. Yeah. But COVID data, by the way, you know, the project that Sandeep is doing? It directly relates to that. Okay. Okay. I've given him the project to work on drafts for kovat okay yeah it's very pertinent I have an air conditioner in my room which doesn't have a thermostat so either gets to it is warm or it's too cold okay what are your expectations with the COVID project? You're going to give us some kind of outline. Yeah, so here are the two projects. Let me summarize the two projects. One project is there is this archive data. So archive is a, you know, archive, right? All the research papers, preprints go to archive in every field especially in the mathematical disciplines like physics and so forth and computer science uh let me let me show that let me give a review of what i said what are projects maybe i'll do a write-up and put it here yeah that's what i was hoping for uh what your expectations are with regard to what you expect to see in the project kind of a rubric and this is how I'm going to do you see this archive? This is where all the papers are published. So for example, if I were to search for myself, you'll see some papers are published here. If you were to look at anybody, let's say yesterday, Dilip Krishnaswamy was our speaker. By the way, you missed the talk on quantum computing. Yes, I did. But then I followed the video today. Yeah. Okay. I don't know why only three of his papers and only two of mine shot. He has produced over 100 papers. Oh, yeah. Now they're they're listed okay something is weird here so um this is it so these papers get all papers these days get submitted to archive so now there is in kegel a data set you can take it from kegel or you can take it from any graph website. Let me just look at RxI. This is like data set, archive data sets. Okay. This is archive and date view by data sets type can apply to archive data datasets for 2400 papers. There is a bigger one for, okay, I'll paste the data again. It's in gigs. So you have a lot of, maybe let's start with this, just to give you an idea of what it is. Suppose you take a data set like this. It's a data set who the authors are, what the summary, you know, the abstract of the paper is, what the title is, the year of publication is, but more importantly, it will also tell you the references in the paper somewhere. It should be there. Okay, so author, title, and so forth. Not in this particular one, in another one. It will also tell you the references, right? What are the references, what other authors they are referring to, and so forth. So anyway, you take this. Now with this archive data set, You can create multiple kinds of projects. You can make a graph, right, looking at the graph relationship between authors. So one author cites another author, or you can create a citation network, or you can create a citation network of look for structure in the network. That would be one thing. The second is you can take the abstract of all of these papers and then try to find, do natural language processing in it. And can you just from the abstract classify it into different topics? Just if I were to give you a few sentences from the abstract, would you be able to tell which area, is it computer science, machine learning of computer science, or is it theoretical physics, or is it whatever it is, the area of the paper. So you can do all sorts of different projects out of it. Pick an idea, propose that this is what we want to do, and go ahead and do it. Pick an idea, propose that this is what we want to do and go ahead and do it. That's it. Any simple thing, but do it such that there is good machine learning in it and you'll pick up some area. Make a graph, make a statistical analysis, start with basic data, exploratory data analysis, see how far it takes and then do something something make a graph do something so we have to do one of these uh like linking and topic or both of this the more the merrier okay yeah see the more you do right the more practice you get the whole point is that this field this subject of data science there is a theory part of it which is important to understand and there's a practice part of it practices like tennis you know no amount of watching the great champions of tennis on TV you have to get onto the field and do it so that's the doing part of it let's go do it alright guys so we are running a little bit behind I would I was going to well it's 9 30 already i don't know how much i should teach now but let me give you the big ideas i will finish the the something called badging and boosting today i'm going to start the recording now guys one second So we'll talk about the concepts of bagging and boosting. These are two important topics right so we'll talk about this except that today we'll limit ourselves to bag this word bagging is actually a combination or the What's the word? Portmanteau? Of two words, bootstrapping. Compound. Say that again. Compound. Compound, yes. Bootstrapping and aggregation. It's an often interview question that people ask, what is bagging? See, usually people don't know, and sometimes even interviewers don't know, they just talk about the mechanics of it, but it is actually the compound of these two words, bootstrapping, B-A-G. Do you see that? Bag. So what is bagging and what is bootstrapping? One way to, so let's talk about bootstrapping. What is bootstrapping? Bootstrapping is, and I'll give you a very rough idea. I won't go into the formal definition at this moment. So suppose you have points, data points. Let me mark these data points with a certain color. What color would suit us? Let's go with this color this time. So suppose you have data points like this. And this is a sum total of your data points. And you want to generate training data out of this and give it to one of your learners, right? So I told you that don't give the same data to each of the learners. What you should instead do is pick a different subset for each, pick a different subset of data. So for example, to one, you can give this data, to another, you can give this data, to another, you can give this data to another you can give this data to another you can give this data to another you can give this data do you see that right and you realize that different people different sets different samples, let me call it i, j, k, l, m. So sample i and sample any two samples given i, j, sample i intersection sample j is not necessarily is not necessary is not necessary final in other words samples may have overlapping elements Are we together? And you obviously take random samples. So this process is bootstrapping. Are we together? This is bootstrapping. And I'm giving you just the whole flavor of the whole thing at this moment. Let's go with that. So far, so good. And nature of bootstrapping is some people will say, you take a sample and then you replace it back into the bag. And then it's like, you know, if you have peanuts in a jar, you put your hand in the jar, you shuffle and you take some peanuts out, look at it, and then you don't eat it up, you put it back so that the next time you again put your hand in the jar and you take another bunch of peanuts out and some maybe may have belonged to the first time you took the peanuts out. So that is bootstrapping, roughly speaking. Then there is aggregation. Aggregation is much simpler to understand. Aggregation is much simpler to understand. take the average prediction from each learner and for classification and for classification by the way guys is my handwriting reasonably legible are you able to read it or it's clear yeah okay and for classification take the majority. So for example, if you have vote of the learners. So for example, if you if the learners are little children that you have taken to the me. You look at an animal and you ask each child, is it a cow or a duck? And now look for the majority answer. Right? And let us hope that you took an odd number of children, so you always get a clear majority. In the same way, if you are taking regression, so suppose you're asking them, Okay, if you're taking regression, so suppose you're asking them, for example, for the temperature, whatever their prediction of temperature is, one child will say it's 72 degrees, another will say no, it's 60 degrees, another will say it's 80 degrees, and you're asking them to predict the weather as it would be, let's say, one hour later. So they will all make their predictions, take the average of the predictions, and the average is the correct answer, is the answer that you use. You say that it is the answer of the ensemble or of the crowd. The crowd has answered and the crowd says the temperature is likely to be 80 degrees. So do we understand aggregation also now? So do we understand aggregation also now? So these two put together makes bootstrapping. Sorry, not bagging. Bagging is bootstrapping plus aggregation. Did we get that? Now, that is bagging. Boosting I'll keep for the next time. But let me mention a very popular algorithm that uses bagging. It is called, can somebody tell me what it is called? A random forest. Random forest. And its close cousin forest. Random forest. Random forest. Random forest. And its close cousin, which is slight improvement, but somewhat lesser known. There are many variants of it. For example, there is extremely randomized trees, et cetera. But let's go with random forest. So what are random forest? Random forest is, the criteria is the learners are the trees. Trees. Right? B, take lots of them. Take or create lots of learners. My basic rule is I start out at greater than 100 and I go up to typically based on that situation 400 400 or sometimes a thousand. Lots and lots of learners. We live in the world of computational plenty. Go at it. The second thing is, see, to each learner, tree learner, give a bootstrap sample of the training data. Don't give it the whole data. Give it a bootstrap sample of the training data. By now, can I assume we understand the word bootstrapped sample? Just to remind you, this is it. This red thing here is the bootstrapped sample. Randomly go pick a small sample of the data. The next thing is not only that, let the number number of let the feature space be Rd, i.e. R not D, let me make it P. There. The P features. P can be five or ten or whatever, a hundred features. P features. Let the features space be then. Then give to each learner only a subset. only subset let me call it s which is much smaller than obviously then equal to P of the features so you can give it you know all the features to the of the features but ideally less than P features that you see the random voice algorithm doesn't put a restriction but math says right and i won't go into the math there's a whole random projection theorem but i'll just leave it as a more intuitive word. Just say that the wisdom of math says, math says s should be less than actually the square root of p. So I'll just leave it s of the features. Matt says, yeah, ask s of course, the maybe so p of the features. But actually you should not take. So for example, if the 100 features, what is the square root of 100? 10. So take only 10 of the features. That's it. And then you build a model and then the last thing is aggregate the results. Train each build their decision tree models models right then for models. Right. Then for inference, let each tree learner make its own independent projection a prediction Make its own Prediction Prediction y hat right so each of them will make their own independent prediction, right? So let me, let's say that the learner is L and you're making a prediction on the data set I. They will all make their own prediction, right? And the last step is aggregate the predictions to get an overall prediction. To get an overall prediction. This is a sequence of things. What have we done in random forest? Let's look through the steps. We are saying you take the, so you need to pick some learner in random forest. You use trees, which explains the word forest. What do you call a whole bunch of trees growing next to each other? You call it a forest, right? Now I'll explain the word random in a moment. So now you create a lot of trees, obviously, for it to look like a legitimate forest, you better have a lot of trees. More or more practically, you need a lot of learners. So you need an ensemble. So this is your ensemble piece. Once you have the ensemble, now what happens to each ensemble, you need to give it data, each tree you need to give it data to learn. Give it a bootstrap sample, don't give the entire training data to the tree. Once you have done that now, what you do is you even from that sample, you don't give all the features, you take a random projection of the features. So the word random comes from that, a random subspace. Random is a random subspace of the data. So the random in random forest is not that you take a random sample that would be bootstrapping. It is the random projection that you do or the random subspace that you pick up in of the features of the feature space. That's what you do and so do you notice that these words they sound rather formal when they are written in textbooks but they actually have a very simple interpretation. Now all of these learners there's no reason they need to wait on each other. So they can all learn in parallel. In fact, they better learn in parallel. The more unaware each is of the other, the better. You don't want them to get correlated. And finally, you draw an inference. Let each tree, so suppose you want to make a prediction on a data. You show it an animal. Let each of the tree tell, is it a cow or a duck and then you take the majority or if you ask the tree to tell what temperature is going to be tomorrow at noon let each of the trees make an inference and then you take the average prediction so that is the aggregation part so you notice guys that we had to put a lot of ducks in a row right we took the idea of ensembles, we took the idea of random projection, we took the idea of bootstrapping, and we took the idea of aggregating. So we had to put four ducks in a row to get this algorithm, random forest. It is a beginning of what our world is going to be going forward. If you have done ML100 and so far, you have been dealing with simple algorithms based on one idea. Random forest is your introduction to a complex algorithm where you have to put many good ducks in a row to get a good algorithm. Many ideas come together or strands of thinking come together to form the big picture, the big idea, the full algorithm. It has many good components to it. Random forest, the limitation of random forest is, limitation of it is, it is a black box model. If you ask a random forest, why are you making those predictions? It will just shrug its shoulder and say, well, I like to make that. That's what I learned. Now, what exactly did you learn? It won't be able to tell. It's a black box model, right? All you get is the x vector goes in as input and y hat xi goes in and y hat comes out as a prediction. Are we together? You can't open this box and ask what is there. Open this box and ask what is there? How is it making the prediction? Explain it to me. It is not a model that has a parameter. That's why you say it's a non-parametric model. It's too hard. You cannot say that there is beta naught, beta 1, beta 2, and so on and so forth. And so these are the knobs I can use to explain to the user why it made, why the machine made that prediction. It's quite a loss to be able to lose interpretability, so it degrades into, so interpretability is a big sacrifice you make. Sorry, I pulled that word. Interpretability sorry i know that word interpretability sacrifice wait a bit but not completely there is one silver lining one silver lining so we better take a silver colored pencil i suppose for silver lining is the silver color it looks like it silver lining i'm joking guys silver lining so there we go we have a silver lining silver lining is one of the one thing that you have in linear models or straightforward models is interpretability you can compare the size of the coefficients and get some sense of how it relatively important each feature is that is preserved feature importance feature importance Feature importance, the relative importance of features. Rough global average level is preserved. Now I used a lot of words. Let me explain it. Rough global average level is preserved. But forget the rough global average word for a moment. It will tell you that suppose you're talking about diabetes and let's say weight is a factor, age is a factor, diabetes and let's say weight is a factor age is a factor right and well weight or size of the person and maybe the two put together make a BMI but let's skip that in your age and your and and a few other factors genetics and so on and so forth and you're trying to determine given all the factors is the person likely to have to be at risk of diabetes or not at risk and when you build a model let's say that you successfully build a model in which Xi goes in and you predict at risk, true, false, then this model, this random forest that you have, you can ask it that in making the decisions, how important were each of these factors, each of these effective contributory factors, weight, age, size. So it might come back and say that, hey, you know what, let's say weight matters most. Weight matters most and then is greater than age. Let's just say, I i'm just putting i don't know a doctor may disagree so size genetic i don't know what the right order globally would be something like that but there is a problem with this random points gives you that the trouble with this is that it gives you sort of an average adult feature impact that on the whole this is what it is it doesn't capture the nuances of prediction so for example you know when in different regions of the feature space for example this region of the young people uh young people which have a background sound if you can please mute yourself okay so suppose you're looking at the young people here then for the young as you know unless you have type 1 diabetes so let me just make it make it type 2 adult onset type 2 diabetes unless so children tend not to have type 2 diabetes right even if they are relatively overweight they tend not to have type 2 diabetes unless there's a genetic predisposition to it right so you might say that for young genetic factor is here genetics is overwhelming is more important but even but as a general population you may you may agree that for people who are older over 40 greater than 45 now suddenly what begins to matter a weight weight and size of course together making up the bmi matters and let me throw in exercise here exercise matters exercise matters things like that they begin to matter much more. I don't know, medically I'm quite likely to be off, but I'm just using this to illustrate a point that in different regions of the feature space for different age groups or different weight groups. So, for example, if you take all people over 45 in the same BMI, now all of a sudden genetics will begin to matter a lot or the level of exercise will begin to matter a lot. So feature importance is actually a very complex and nuanced topic and it is almost very poorly understood by most people doing data science. They don't understand the sheer complexity and nuances of feature importance. So anyway, I'll stop today with the statement that whenever you talk about feature importance, and even when you read these books by all these publishers, they actually can be quite misleading. They will tell you that in random forest, etc., you have feature importance coming in. Remember that those are very rough measures, global measures. tell you that in random forest etc you have feature importance coming in remember that those are very rough measures global measures feature importance is a deep topic and whenever you look at feature importance so for example a given user suppose a user is a person is 25 right the person does like at that particular moment what do you think is the risk of diabetes? What is more likely to cause it? Right? There are multiple things, you know, that the young people are active, they're on their toes always. So you'd probably advise them to be careful about their diet. On the other hand, as people get older and older, their exercise level goes down, but their food habits remain more or less stable. So now you want to emphasize, well, they're not likely to suddenly start overeating and assuming their food habits are reasonably modest. Now you want to start emphasizing exercise right because people become sedentary as they as youth fades away so things like that what you advise or what factors matter more it's like asking that how much does y change is something as that you know if y is a function of x how much y changes for each of the individual predictors in a given give for a given value of the x vector right so it's a local feature because you're essentially comparing roughly, the partial derivatives in the local region. You're looking at the gradients locally, gradient of y with respect to each of these features locally. So this is getting a little messy, forget about that. But just remember that while people extol the virtues of feature importance in random forests and these models, take it with a grain of salt. Feature importance is more nuanced. There's a few. I noticed some of you are here after having done my boot camp last November, December. So you would remember that we devoted an entire day to the nuances and nuances of feature importance. And it also goes to the heart of something we are trying to do these days recover interpretability of black box models so but anyway this last part by the way is out of scope so if you i just gave you a teaser for things to come later on if you didn't get the whole thing of it don't worry at all the only thing you should take back is the feature importance is don't worry at all the only thing you should take back is the feature importance is is there in a very rough way in random forest random forest is a complicated model it is built out of some four you need to line up at least four ducks in a row you need to use the concept of learners ensemble of learners those learners should be weak learners you make them weak by giving them a random projection of the data random data projected to a subspace of the feature space. You give each bootstrap sample. Then finally you aggregate the results. All these things are efforts to make sure that the trees are or the learners are decorrelated. Right? That's what you do. Now one remarkable fact about random forest and dirty little secret is I said that number of features should be less than the square root of p. Now here's the dirty little secret. Quite often if you create a forest in which each tree is given only randomly one of the features. Those are called stump trees, stumps, a forest of stumps, no branches. It performs remarkably well. Quite often it performs as well as a more complex tree. Are we together? So that is it. So that is random forest. What it addresses is this tendency in decision trees to fit data hard, to go and overfit to data. Decision trees always have the problem of overfitting. People did all sorts of clever pruning algorithms. And then came random forest and then random forest created quite a tremendous debate in the community. People asked that machine learning used to be not just about making predictions but about interpretability or providing some inference about what happened, why is it happening. You lost that, you know, you lost interpretability. Was the sacrifice okay? Today we of course say that it is on a case-by-case basis. Sometimes where only performance matters, you want the best model that you can build. So for example, if you want a self-driven car, you just want it to perform well and keep you safe and keep the other people on the road safe, right? Prevent accidents. You probably are not too concerned on what the interpretability of its decisions are. You just wanted to make the right decision, right? But on the other hand, in many of the situations, for example, if you're trying to give a job based on algorithms and how people performed on a test, you want to make sure that the models are highly interpretable. You're not willing to take a black box that says, like an article that, all right, this guy is better than that guy. So take this guy, don't hire that guy. Because then interpretability of paramount guy, don't hire that guy. Because then interpretability of paramount importance because there may be biases. If the data is biased, your machine learning algorithm will be biased. And a black box model, you will not know easily that there is a bias in your data. In fact, companies have been sued for doing this. In US, you can get sued because we have the protected classes. You cannot, you need to guarantee that you don't discriminate against the protected classes, race, gender, age, disability, and so forth. And so in the history of this is, without taking names, there was a company, a big big company is a big company that was sued because it could be demonstrated that it was discriminating against the protected classes they claim they're not deliberately doing it they're feeding all the data they have into the algorithm and the algorithms are blind and impartial and so they are making predictions therefore it is right the judge did not quite agree. He says, no, it doesn't matter. The fact of the matter is there's evidence that your process is creating discrimination, is doing discriminatory hiring. Therefore, the circumstances of how you produce that doesn't matter. You're responsible for the wrong decisions. They lost the case and they lost, they had to pay a lot of huge penalty. So obviously in these situations, now you ask why it happened, it had to do with the fact that the data had inherent bias, their algorithmic training had bias therefore. All of those things creep in. So there are situations where you can use black box models, there are situations where you cannot. And nowadays we are entering a world in which we are saying, well, we cannot but help use this very complicated black box models, deep neural networks and so forth. But we want interpretability nonetheless. And so we are trying trying to create a whole new wave of body of research and tooling and stuff to restore some semblance of interpretability and explainability on top of black box models. So that's a way promising areas. Those of you who did the boot camp with me will remember some of it. And in the next boot camp, which is in deep learning, it will become a topic of paramount importance. It's also a very hot topic today. So with those words, I will end now. Any questions, guys, on random forest before we close the session today? Yes, sir. So in random forest, every tree is decision tree, right? It's a decision tree, yes. Asif, is there a recommended sample size to do random forests? Generally, it will do the right thing internally. It will look at your training data set size and it'll pick a sample size. The basic rules of sample size is you know you try to take a reasonably big sample but you don't try but you make sure that you don't take the whole set itself right and you know suppose you have 10 000 points and uh what's this square hundred so long as you're taking more than 100 points or so in a sample, it's good. It's a good sample. How does random forest deal with high correlation between features? It is actually very good. Let's say that the features x1 and x2 are highly correlated, right? The beautiful thing is one tree will get only x1, another tree will get x2. So you don't have the problems that you have, the pathologies that you have in linear models. But doesn't it overcount the effect? So what happens is that the beautiful thing happens is that when you ask for the importance, it will ascribe equal importance to X1 and X2 because some trees will be using X1, some will be using X2. Okay. That's how it is. So the problem of multicollinearity, therefore, is moot when it comes to random forest. That was one of the big reasons people, you know, when random forest came in, they were a runaway success. Every bit... I think we lost you. he was using it you take a little cable and use trees boosting you hear of all this cat boost xg boost and so on and so forth and we'll talk about boosting the next time next monday remember i'm not sure if everyone heard a little bit of gap of what asif was saying or it was just me okay No, it was for everyone. Yeah, audio was dropped I think. We lost you in between Asif. My goodness. Okay. So I'll say that again. What I was saying is that a lot of the mod, if you go to Kegel and you look at the winning entries for tabular data competitions, wherever you get a tabular data, not image or sound or text, but the simple tables of data, you will often find that the winners are in the leaderboard. You'll often find entries of teams that are using one of the ensemble methods. They're using random forest or gradient boosting, you know, this OCAD boost, XGBoost, whatnot. They'll be using things like that. Very, very often you'll see them. You'll see at least some entries in the leaderboard coming from this. So this is a very popular algorithm. Obviously, like with every algorithm, people do get carried away, but there are people who solve every single problem using a random forest. Right? So that's that. Actually, it reminds me of something very interesting. My good friend, Dilip, who presented in quantum computing yesterday, he and I used to have a joke. So he would say that we have to solve, write code. So he would say, wait a minute, he would open the code editor, write int ij, and then it would say, now tell me what the problem is. Because it would always need the variables ijs, like that. So nice after all these years when I think of random forests like that. A lot of people swear by it. They'll say, okay, let me first write, instantiate a random forest, and then you tell me what the data is and what should I fit to it is very effective and now we are entering the space of these powerful methods very very practical and Very usable and you use it a lot become good at it guys And you'll it will solve a lot of your problems will do after this will'll do boosting and its implementations XGBoost, CatBoost, AdaBoost and all that and then after that we'll go to something yet another very powerful idea called kernel machines or kernel methods. We'll do support vector machines and so on and so forth. So in this workshop of course we are moving fast. We are moving through rather powerful algorithms. Theory is done of this. Next time, remember, Wednesdays are for practice. We will do lab work in random forest. Lab work is entirely different kettle of fish. You'll realize that lab is very easy. You just instantiate the random forest and fit it to data right but we'll learn to see uh its nuances it's important to know some of the nuances involved and we'll develop practice in that any questions guys before i end just uh one question is weak learner right right like in this case you are showing them only weight or age, not everything, some subset of features, right? Yes. And these are, they are doing prediction based on decision tree algorithm? In random forest, yes. But in general, you don't have to, you can create a bagging of anything you want, any other learners you want. You can create a bagging of anything you want, any other learners you want. You can make a bagging of, you know, you could use logistic regression. So let's take this example. You want to do a linear regression. Remember, in linear regression, you have the fundamental problem that if there is multicollinearity between the features, you don't, right? You will start seeing explosion of variance. You start seeing pathologies. Your models are very unstable and they exhibit variance inflations. Very sort of a slight change of the data will cause them to build entirely different models. One easy cure, just take an ensemble of linear regresses you'll be and give them a random projection of features you will realize that lo and behold that problem is solved so but all of these learners are using same algorithm right to learn yes that is so but now i'll teach now the next thing i'll, actually you sort of anticipated where I was going. You will ask this question, why can I not mix a few decision trees, a few linear regressors, logics, yeah, things like that, different algorithms. Why can I not do that? If I'm doing regressors, I can mix 20 different algorithms and instances of them. And likewise for classifiers, I can take a whole bag of classifiers, different kinds of classifiers and do it. So yes, you can do that. When you do that and you create an ensemble of heterogeneous classifiers, you actually get very, very powerful prediction machines. In fact, that is one of my own dirty little secrets, hardly a secret, it's an open secret, because I keep insisting that everybody should do that. Never solve a problem with one algorithm. Use every good collection of algorithms and lots of instances of learners from each of the algorithms. Do the same process, instead of trees, replace it with those with the heterogeneous bag of learners from of different algorithms and when you do that it is surprising how much efficiency you get because the weakness of one algorithm is made up by the strength of the other algorithm and so forth and you get very good thing so bagging is one we can do boosting and the third thing is stacking right stacking is something that i actually i find are very very useful i'll stack predictions from different machines right different learners and then put a learner that learns from the predictions of the different learners. That is stacking. We'll gradually come to that. I don't know whether we'll do stacking in the practical methods, but we'll certainly do it in the boot camps. I see. So here in random forest and say ensemble, ensemble means ensemble of weak learners using decision tree. Okay, thanks. In random voices, because the word forest is- Forest is, huh? Yeah, forest of trees. All right guys, so this is it. I will stop the recording. I have one question. Yeah, go ahead. So when you said heterogeneous classifiers or heterogeneous algorithms, they're like the example you gave right so you know the hundred features and then you have to say. You know 1010 learners or hundred learners like that getting 10 so they are so you distribute those algorithms like equally among you know the learners so let's say there are ten learners and you're using five so two of the learners I mean generally what happens is in those areas my experiences and this is the experience see this is where you know each just like each artist has his own style each data site will have his own rules of time and their own experience my experience is I don't try to be very clever there I just take a bag of learners and run with it, different heterogeneous learners and run with it. I'll throw the same number of learners but I'll make sure they all are doing their own random projections and bootstrapping and so forth. Generally I try not to be too clever because in my experience being too clever doesn't help. That's sort of micro optimization. Alright guys, any other question? Thank you. Have a good night. Alright, good night guys. I'm stopping the recording and I'm stopping. Okay. Thank you. Have a good night. Alright, good night guys. I'm stopping the recording and i'm stopping