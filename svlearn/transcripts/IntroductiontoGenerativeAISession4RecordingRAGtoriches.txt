 All right. Good evening, everyone. For those of you who are on the West Coast, and I suppose it's late evening for those of you who are on the East Coast. And for people in other time zones, I have a little hard time to catch up where we are. This is our final session in this Generative AI introductory series and we have an interesting topic. We hear of all the stories of rags to riches. Now, while it is sort of stories and some of them are real life stories. Today we are going to look at it in a different way. There is a technique that has evolved in generative AI which underpins many many things that people are doing today, especially when we use large language models. And as we use these techniques, or as we use these large language models, it turns out that when you hybridize it or you bring in AI search or semantic search with it or information retrieval, then it makes for a very powerful combination that particular technique goes by an acronym RAC and retrieval augmented generation so when I wrote here tongue-in-cheek rat to riches it isn't just there's actually quite a bit of truth to that these days many many startups are doing very successful things using this technique with their large language models. In fact, I would go so far as to say that in the many startups that I've engaged with, that I'm looking at, and who are either aware of or who are consulting with me and so on and so forth, everyone is using RAG in some form or the other, for some purpose or the other. So it's a very powerful technique. And so today we are going to talk about this technique. And that will be the final topic we'll cover. This has to do with large language models. When you think of large language models as generative generative models. One easy way that you could do it, or for the purposes of today's session, think generation of text. If you recall, on Tuesday we focused on the generation of images, which then extrapolates to generation of music, generation of videos, and so on and so forth. But today we come back to text. And we look at one of the fundamental problems that you have with a large language model. It is the problem of hallucination. These large language models, they tend to make up things as they go along. And there's a large amount of research that is going on on how do you make these large language models not hallucinate, not just make up things. You ask it a question, like I mentioned before, I used to ask a question, who's the author of The Tale of Two Cities? And it would give an answer, Charles Dickens. I used to ask the question, who is the author of A Snail of Two Cities? And then long ago, it used to come up in random names. Now it has smartened up. Now it says there is no such book. So when it comes up with an answer like that, that is hallucination or you give it a picture. One very common thing is you give it a picture and you see here is a dog. Well, that hardly looks like a dog, but imagine it is. And you ask the dog is running and you ask it to describe the scene and it will say it is running and playing with a lot of children and when you look at the picture the children are nowhere to be seen right so that's an example of hallucination many ways there are that people are trying. Nobody has a successful recipe, but perhaps the most powerful approach, and that has stood the test of time, by test of time, well, we are on a very, very accelerated timeframe. One year is already ancient history in this world, one or two years. So rag has been around for some time and it is extensively used today and it helps mitigate the problems of hallucination or there is another problem that llms have let me just write it lm problems lms have a training date LLMs have a training date. Let us say an LLM was trained till July 2021. I believe that is what Chad Gipity today says, that I only know things till July 2021. So you scrape the web, you get all the data from the internet, and you feed it into this machine and you say learn and this machine goes grinds and grinds and grinds and grinds and comes back and says I have learned all this now but whatever it has learned has a date and so when you ask it for facts beyond that date it is not able to answer that question and you must have noticed if you ask it for something very recent it will say I'm sorry I don't know the answer because this is after the date I was trained on so the training date or the staleness problem that is there staleness of the modeling. These are two serious problems. Now the way to look at this is what does a LLM learn and where does it store it? So an LLM is a giant box right or one physical way of remembering it is that it has and obviously I'm not making it right, but it has gazillions of neurons, activations, weights, biases, which we'll call parameters. At the end of all of that, suppose you give it a sequence of words, you California. Describe it. This is a text. This is just tokens. As far as this machine is concerned, and this final token, it all goes into this giant machine at inference time when you give it a question and what you notice is that there is at this tail end there is a dictionary english dictionary let's say that english dictionary has 30 000 words this is the most common dictionary people take a words dictionary a word's dictionary in, let's say, English I'll take Hyper-32 Then what happens is that this machine is it just me that has a freeze yes uh no no i i think asif is frozen sometime when you when you run a lot of different programs on a machine, sometimes that causes the Zoom to freeze. I've learned not to run Jupyter Notebook. No worries, no worries. Okay, no worries. I'm sure he'll rejoin ASAP. Technology, got to love it. you Thank you. Well, does anyone want to share the most amusing responses they've gotten when they've been playing with large language models? Besides that New York Times article about the reporter where the large language model claimed that she was in love with him and that he should leave his wife. Got very stalkerish. It was very shocking and creepy. Way beyond the claiming a prime number is not a prime number. Okay, I sent Asif also a Slack message to say we're awaiting his return. He may be rebooting the computer. It's very, very likely because the drawing software he uses that will also freeze. I blame Microsoft for that. So I have a question. So I know this company is in Fremont, right? Do you contact courses in person there? Yes, indeed. In fact we have an ongoing Saturday boot camp for large language models taking the applications all the way to production right now that's running through into December on Saturdays. Okay thank you. Yeah the our supportvectors.com website should be Yeah, the our support vectors.com website should be a listing classes and he he may be, he was saying he's contemplating on giving a course for that. Do you think anything coming in the pipeline? Mostly about security aspect. I believe that's one workshop that definitely is planning, is in the works because it's a very relevant. Yeah, there is the security and then there was some other ones that are planned on that and that that would be like a probably a weekend in depth workshop. Just one or two days at most. I'm interested in that probably i'll chat with you or ping you okay i'm taking note of that okay oh we still have people popping in. Okay, for those who just joined, Asif is likely rebooting his computer because it froze up and he will be with us as soon as possible. Checking my Slack messages. I know personally, I have Slack on both my laptop and my phone in case my laptop freezes, I can always send a quick Slack message to let people know that my laptop is letting me down. But ASIF has to wrestle with multiple machines and specific ones that will use that writing software that is good for lecturing until it freezes up and needs rebooting. So yeah, security workshop. Yeah, let me see in my notes. What are the workshops he's got planned? I'll do a search for workshop. workshops. Okay, workshop, workshop. Yeah, in the past we've even had, let's see, data visualization, data wrangling, one day session on recent AI developments for managers. Yeah, these are some others he was considering. Let's see. I know there's one for flow models specifically and data graphs, that structure has been very popular. Let's see. What else? Machine learning. Startups. Let's take a peek at the website. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . coming workshops. And then there's also specific machine learning on the Google Compute Platform, because not all of us have the individual hardware to deal with the demands of neural networks and these larger models, of course. So yeah, so I am, okay, I'm letting someone else. I'm going to give Asif a call and check how he's doing. So. Thank you. Thank you. 1. Okay, Asif's logging back in. I'm going to make sure he has the Slack link, the Zoom link. Just a quick question. So are you going to share the recording links for us? Yes, yes. Well, we'll email that and certificates and all that after the course concludes. Guaranteed. Now let me give us if the link so we can log back in. E-mail Takk for watching! Thank you. Okay. Okay, Asif is right there. Yay. Hello folks, I apologize. I had no, I never realized that I had been kicked out of Zoom. I suppose this is the bane of remote interaction. Sometimes you don't know. So the last thing I was saying, I suppose I must have covered a lot of territory where I was unaware that the Zoom had gone off. So suppose we have an example, So suppose we have an example, and I apologize if it looks repetitive. So you have a question prompt. What is the capital of California? Describe it. It will go into this machine with gazillions of parameters. And this parameters, what it will do is it will essentially put a certain weightage on the words of English. Let's say you have a vocabulary of 30,000 words for English, which is, by the way, the most common vocabulary selected these days. Out of those 30,000 words, to each word, it will associate a certain probability. That's the softmax function. In the softmax, you you can so it is like this text went in into this parameters out came certain probabilities now what happens is you don't pick the most probable word which looks counterintuitive why would you not pick the most probable word as the beginning of an answer right but let us say that you do that. You pick a word, the probable word, then using that word and what was given as a prompt, now the next word comes and the next token comes and the next token comes. And what you see the machine generate is a sentence that capital of California is Sacramento. Now, in reality, you don't take the most probable word. The reason you don't take the most probable word is because from experience we have learned that it leads to very repetitive answers. It is not very creative, it's very repetitive and in general is bland, you don't like it. So sorry for the interruption. You're not able to see the screen. Oh, yes, indeed. Thank you for pointing that out. Is it any better? Are we able to see the screen? Yes, we see your nice nighttime tent. Okay, there we go. So, oh my goodness, I'll sort of quickly repeat that. I said, when you put in a sentence to a large language model, you ask it a question. Imagine you're sitting in front of a BARD or Cloud or ChatGPT, or if you have downloaded the latest model, open source model, Zephyr or Mistrel or, you know, Lama 2 or whatever it is, and you're asking this question, what is the capital of California? Describe it. What happens is that text goes into this big, big machine with this gazillions of parameters, its rates and biases and neurons and activations. And what comes out is a probability map. What it does is for every word of the language, it produces a certain probability. Now, the intuitive thing would have been that you take the most probable word, then you feed it back into the machine and say, okay, first word was the, now what? And then it will come out and say, well, the next word should be capital. And the next word and the next word and the next word. And so word by word or token by token, it will start generating this and the full stop, and then it will keep on going. Now, there is something that happens here that is relevant. This is standard large language model theory. The point that I want us to pay attention to is two facts. First of all, you never pick the most probable word just like that. What you do is you sample from it. Otherwise, it gets too boring, too redundant. And when you sample from this, and imagine that you throw sort of a dice, and then you see which word is picked up, and you give more likelihood to the words that are high probability, there are ways, there are mathematical techniques of doing that, then not always is the most probable word picked. Sometimes the less probable words are also picked. And so that explains why when you interact with OpenAI API, you notice all these parameters, temperature, total probability K, like how many top probability items should you randomly or weighted randomly pick from. that is that but now i will point out an interesting fact to you let us say that this machine because it's a next word prediction next token prediction what it does is it is doing in all these things that it is doing at inference time it is doing a greedy optimization it is saying at this moment what is the best token i can produce all things considered and it produces that and the next token and the next token but the sum total of all the tokens which is the full text may not be the best text that it could have written and it has no way to first think through the whole text, may not be the best text that it could have written. And it has no way to first think through the whole text, what it is going to produce, in one shot, and then produce it. One may argue that we as human beings perhaps first form a conception of what we are going to speak first form a conception of what we are going to speak and then speak it. But LLMs don't work like that. To the best of our knowledge, they are next word prediction machines. And yet they do an incredible job. Even when you put all those words together, the text that comes out looks very good. It looks very plausible, but not always actually now here is a simple probability argument let us say that 99.9 of the time it produces the right word the best word that that would be that would be there in the globally optimal answer right what would have been the best text written in answer to this question it emits that word There is still a 0.1% probability that it makes a mistake or it does not pick that word. Now what happens is when you ask this question, what is the probability that by the time it's done producing a thousand tokens, everything is right. Now I'm oversimplifying the situation because the way these LLM probabilities work is a little bit more nuanced than this. But in a very, very simplistic way, if you think about it, if the probability is 99.9%, you take the thousandth power of it because you have to be right a thousand times. You're producing thousand tokens or thousand words, you will realize that 99.9 to the power of thousand is practically zero. Means you would have decline or a very high likelihood that the answer is not the globally optimal answer. And that is the main, in a way, the limitation of generative AI today. That is the state of the art. We are in a situation that it seems we have made a lot of progress, but on the other hand, this particular problem is so fundamental that many people have, many very serious people, in fact the founders of the deep learning theory itself of neural networks uh in one of the interviews i was listening to him and what i think it was lakone yakun um who said that it it is time to rethink the whole transform and the whole deep neural networks from scratch all over again right so sometimes people feel that we need a better mechanism than this next word prediction engine when it comes to generative AI so what is the problem how does it in simple terms to you and me oh by the way he used a very evocative phrase he caused it the diffusion of errors that you make one small step misstep it it then leads to another misstep to another misstep it's a diffusion process it's a markup process and so at the end of it you end up with far from where you should be right and while researchers are busy looking for a fundamental way to solve a deep-rooted problem what does it mean to you and me you and i are people simple people who like to use llms and do something practical we are asking questions can i use this llm in my enterprise can i can i move the needle in the whatever mission my company has, healthcare and so on, space science, whatever it is, can I make a difference? Can I use it for patient diagnostics? Can I use it for doing a better reading of an x-ray and so forth? And so when we ask questions like that, what we have to ask for is, can we make incremental progress, incremental things we can do to solve the problem of long text generation and the possibility of hallucination or imperfect answers for long form text that you ask it to generate. Now why would you want to generate a long-form text? One of the, actually here there's a summer internship boot camp that was happening at Support Vectors and one of the projects that we had, just to illustrate this, I've given them is suppose you have the background of a patient, patient history it's called in medical records. And then the patient comes, when the patient comes to a clinic, there's a consult. In the consult, the patient says, you know, my left shoulder is aching and I'm having difficulty sleeping and so on and so forth, and describes the problems. The physician listens to it. And from that, does a differential diagnosis comes up with some assessment of what it could be those assessments are internationally coded as icd codes international code for diagnosis and they become those numbers but and then usually those icd codes may have consequent procedures that are done for example if you say, I'm having real difficulty breathing, the doctor may put a tiny lit endoscope down your nose and down your throat to see if all is well with your larynx and so forth, your throat and so forth. So that's a procedure, it leads to a procedure code. So that is a plan of action. So in other words, the consult effectively and obviously I'm not a physician, but from what I have learned myself in guiding this AI project has four parts. So subjective, subjective objective assessment so this is almost the this is what the physician is doing this is your DDX differential diagnosis and what's your plan and the plan may be for example to do some procedures or to send the patient home with a prescription or whatever it is and to manage the illness so this is working but what you get what we did is we gave the uh llm this conversation here is a patient talking to a physician and obviously this was done in a simulated environment like our own interns became patients and some of our interns excuse me some of our interns were also happened to be actually some of the people researchers here at support vectors are physicians and surgeons so they took on the role of the physician and so we had simulated consoles and from that we gathered data or we converted that conversation that was happening we use generative AI of course we we use voice to text conversion and we use whisper and things like that. First of all, what we did is we made it into text transcript, then we gave it a generative AI prompt and we said produce this, produce so. And this is how we learned actually what happened is that we realized that it would do the good good news was quite often it would come up with pretty good diagnosis in fact we looked up the situations and the physicians assured us that actually um the situations and the physicians assured us that actually the primary diagnosis typically three four diagnosis it comes up with and usually the primary may have occurred but the second and third diagnosis are also vital because they help you think they help you just check something and maybe the physician will think ah i didn't think of that now let me ask the patient to raise his hand and see if the fingers are moving properly or not or something like that you can you can do further investigation because it raises the possibility of other illnesses the physician may not have thought so we reached the level of usefulness in that in this in this project but what we also noticed so we felt pretty pretty elated, we thought, oh goodness, this can be a very good tool because there are a lot of medical errors of formation. Physicians are busy, the people follow a pattern, most people have the same flu and fever and so on and so forth. So sometimes they miss, because they are so busy, they miss certain things. And having an AI like what we were producing to guide it, to just say, hey, by the way, just ask this, right? Part of the plan is assessment and a trigger is just check for this, right? So it can improve the consultation process in the consult room so that finite amount of time that a physician gives you, 15 to 30 minutes, can be better optimized. So that was a good point. But there was a problem with this. The problem that we noticed was that it was very hard to keep the machine from hallucinating. It was continually coming up with other illnesses that the patient didn't have. And it said that the patient had it in this history. For example, it would read a transcript and it would, in one case, I clearly remember, it says, the patient is a 63-year-old Afro-American male with a history of diabetes. Nowhere in the conversation, it was ever implied that the patient the patient is afro-american and in fact wasn't what the patient had said is that i'm a young person having this problem or something somewhere uh maybe maybe didn't say young but they said i have these problems um so the machine was hallucinating it didn't have the background to fix this. It made things up to align with facts. So in other words, what it did is, in effect, it produced a history that statistically somehow looked probable. And when you look at that hallucination, you can see a reason behind the madness. You can say that yes that could be a statistically probable background of a person who is having this concept but the thing is you don't go with what is probable you're dealing with a real patient who has a real history and you don't want the machine to make up its own history about this patient. you you Thank you. I apologize for this interruption. So what this machine was doing is making things up. How do you prevent it from making things up? See, it's a mathematical engine. In a way, if I were the machine, I would be pretty pleased with myself that I made up a very plausible history about this person. It just happens to be wrong. And that is the topic of today. How do you prevent machines from doing it? Now, this doesn't quite prevent it, but it goes a long way in helping machines solve the problem of hallucination. So what you do is something quite interesting. You take, for example, let's take this particular example. If you think about it, what was missing? What was missing in this picture? And let me put it in a different color. What was missing is providing the history of the patient. That when you sent it to LLM, you should have said, this is the history. This is the consult happening. This is your voice to text being generated here. And the LLM has to take the history and the consult together to move forward. Are we together guys? And when you put it like that, it is dead obvious that we should have done that because in a consult, the person says I have fever and I have some symptoms. Now those symptoms in a healthy person can have or can lead to one differential diagnosis in a person who has been suffering from or who's had a remission, cancer remission. That could be a completely different and alarming situation. And so context matters. We should have provided the context to the large language model. And so we did that. And immediately we saw a vast improvement a vast improvement a sea change in the in the reports that the machine started producing right we wouldn't say that it completely solves it like this is a good for us it was a good litmus test to see how well this technique that i'm going to explain to you folks now, how well it works. And it worked dramatically. And everywhere I hear people are seeing those effects. It's a dramatic thing. As you know, there are now plugins to chat GPT. You go to BARD and BARD, one of the things BARD does is you ask it a question. And for all you know, it is running a Google search under the covers to see if what it is going to say is right or not. It still hallucinates. For example, today morning, I believe it was a very important paper that came out in 2022. I asked it to know the authors. It gave three authors right and cooked up two new authors, which didn't exist in the paper. So it happens sometimes and it's pretty unusual, but I look back and I noticed that part of it was my fault. I didn't give the title quite correctly. So these things are not solved, but they are greatly improved now to such an extent that you can de facto use an LLM in a practical environment to solve a business problem, but with the appropriate warnings and caveats. So what is this technique we are seeing? When an LLM produces an answer, let's say that it says the capital of Sacramento is the capital of California, Sacramento, somewhere you can imagine that in its parameters is encoded. You can't call it memory because LLMs don't memorize, but the weights are encoded in such a way that it manages to look as though it knew the answer. Even though it's a next word prediction machine, the weights are so arranged internally that when it produces a text, it effectively, it feels effectively that it knew, it remembered or knew the answer. So that's why you can call it that sort of answer uh in the original paper people often call it parametric knowledge then the effectively new part has a name it has a parametric knowledge now rag comes from a different idea a different idea. Rack says, let's use yet another powerful technique in AI called sentence embedding and use search. Sentence embedding leads to something called AI search or semantic search and search could be traditional search like as in using open search and so on and so forth like a keyword search or it could be semantic search and so on and so forth, a keyword search, or it could be semantic search. And traditionally these things are backed by a vector database. You save the semantic search into a vector database. So now, what it says is something actually dead obvious and simple like the case of mine and that is what we did by the way what we did is we kept patient histories in a database so this is dead simple and we also made it searchable. We also did something else. We put a lot of medical textbooks and articles. We put it all into a vector DB. And now what we did is, and this is the essence of RAC, what it says is when a user asks a question, let's say that the user comes and it says, what is the capital of California? So imagine one thing happens. You take it, you put it through a search engine. Now, search engine will take this text and it will come out with search results. And then what you do is you do something interesting. You take the search results and by the way, you don't just use a search engine these days, or at least in the implementation that we have, we use a hybrid search. On the one hand, we use the keyword search. On the other hand, we use the AI search, the vector search, and then we put it through a cross-encoder. So this is, by the way, I apologize for the technical details, but for anybody in the audience is interested but we cross encode it and then re-rank all of those and get the best answers so let's just prefix it with the word best search results we take that and we generate a prompt so let's go back to this picture. You see that there's the history. Where would the history come from? It can easily be pulled from the search and so on and so forth of the patient. Then, or straight from the database, the writing from the database. I think the history we just pulled straight from the writing of the database in the upper digital case. But what we also did after a little while is, here we put relevant facts. And those relevant facts, what we were doing is, we were pulling it out of this vector database of the articles and textbooks and obviously we have put away a specific diagnostics related knowledge into that vector database we fed that into it here and now what happens you get this long prompt in which you have the history long prompt in which you have the history, you have based on this, you also have certain relevant knowledge from the articles and textbooks. So let me just say knowledge base. From the knowledge base you search and then you pull out certain facts and then you give it a prompt then you give it the prompt and you say that these are facts base your response like i i mean because this facts are also going in in fact it's going in before the prompt I mean, because these facts are also going in. In fact, it's going in before the prompt. The LLM is forced to consider all of those things. And then you feed that, you feed this into an LLM, the entire thing into an LLM. And then you say, now you tell me the answer. So I hope you see, folks, that that will significantly guide the LLM. Let us say that you're asking at the capital of California. And right away, it pulled up an article on the state of California from Wikipedia. And Wikipedia is one of the most common things, perhaps the most common things indexed by AI and genius insiders these days. You create a vector database of all the articles. says sacramento is the capital i mean the california is a state this big this is this is this history politically this is a joke and this is the capital and this is the economy and blah blah blah right now you realize that when you go to what is the capital of california ai search is actually cleverer it will go to specific chunks of the text and pull out where which is closest to answering that question somewhere maybe a text that says the capital of california's capital happens to be this or that you realize that by the time you reach there at that moment the large language model is not getting the answer from its parametric knowledge you know from the knowledge or the learning that it has embedded in its weights and biases it is literally staring at the answer from the search results you see that right and it becomes as simple as that on the other hand if it is a question for which the search has no answer because the knowledge base that you have and usually these knowledge bases are very unique to your company very often companies will not i mean most often enterprises will not share their documents with any public service or open ai or with anybody they would say no no no these are our secret documents but if you have a llm inside, but the LLM has been trained on the public data, which is usually the case, when you go and get Lama 2, when you go and get Zephyr or Mestrel and so forth, open source LLMs, those things have been taught and trained on all the knowledge of the world. So when you ask it a question, one of two things can happen. Either your vector search has already produced facts, very relevant facts, which contain the answer, in which case, what will the LLM do? It will consider all of those facts and then produce a coherent language answer, right? Because those facts may be written in a particular way that you have to parse read and figure out what the answer is but llm will make it into a out of all that bag of search results it will create a very direct precise answer to your question so that is one value that the llm got on the other hand it may so happen that your search your specific corpus of documents which you have indexed may not contain the search result will not contain the answer in which case the llm will now rely on its parametric memory right parametric memory to do what it usually does. It will, in other words, it will default to its usual behavior. It will go on doing its next world predictions in such a way that basically the only input to it is the actual user question. So this technique is called retrieval augmented generation. Retrieval is another synonym for search search results you search get the search results retrieval results you feed it in as part of the prompt and sometimes your search results may be irrelevant or may not be helpful sometimes they are in any case the LLM will consider that when it is producing an answer case the llm will consider that when it is producing an answer and it's producing a text so that is right retrieval augmented generation so i would like to end sort of with that it really is that when i said rat to riches It really is that when I said RAG to riches, I was obviously being chrysotious, but there is a quite a grain of truth to it. A lot of companies that are well on their way to riches are using RAG in their generated AI. In fact, I don't know of very few companies whose use cases don't somehow or the other involve RAG in their data pipeline somewhere, in their AI pipeline somewhere, or AI systems somewhere for some purpose. Almost everything is using it. So in other words, RAG is a very powerful thing. And just to conclude, those of you who are playing out with things like land chain or etc etc you'll see some basic examples of rag implemented obviously those of you who are interested in an in-depth hands-on programming based lab oriented course on generative ai do register for the course that i'm offering now but i'm told that a lot of you are leadership audience, you're not likely to go and get hands down. So I hope that this topic was interesting. And with that, I'll conclude the series on generative AI. I'll just open it to questions. So we have covered a lot of territory, obviously at the level of an introduction we understood what generative ai can do all the various things it can make a revolution in protein proteinomics it can make a generative art generative music generative text stories and some of the stories that rights have evocated um i believe somebody uh was it or somebody said, Okay, what would be a conversation between India's iconic figure Hanuman and I believe some warrior Alexander or somebody it is. And Charjee Piddi came out with actually a most remarkable dialogue between the two between two different perspectives on on the world or life so that is generative ai today if you play with daily three or if you play with stable diffusion you make evocative pictures you can make videos and so forth so we are into very exciting times many things are happening it has limitations the biggest limitation with this is that the thing about hallucination hallucination by the way is not just in text one of the biggest problems with generative art is sometimes it generates faces with two nose right or four eyes and things like that and hand used to be particularly hard for generative ai to get right it's only very recently that only in the last few months i would say that we are seeing that this generative ai is not making mistake with with just the fingers and in fact that used to be the clue if you want to tell if an image is real or fake, quite often you look for those weaknesses that you know exist in generative AI, those hallucinatory mistakes. And so it gives you a telltale sign that this is generated. So we have come a far way. It's getting harder and harder to distinguish. Generative AI is becoming bigger and bigger. Nonetheless, theoretically, there is still the problem that next-word prediction engines are greedy algorithms. Greedies always miss the global, or not always, sorry, I apologize. Greedies have quite a risk of missing the global optima. that taking locally optimal steps leads to a globally optimal solution. Sometimes it does, sometimes it doesn't. And then there is the issue of the diffusion of errors if you produce a lot of text. So RAG, the way I explained to you, is the earliest version, and it has gone through many iterations. There is something called active RAG, FLARE, and so forth, and there are many new things. And almost every week there is one more improvement that makes the LLMs or the generative AI hallucinate less. Both on the textual as well as the visual domain. There are many, many things that are coming up. But a lot of those things are best learned by doing hands-on labs writing code playing with it and seeing what it all means so and obviously that is technical in depth so we won't go into that but i hope you enjoyed this workshop this series and please stay in touch look out for more such series coming from support vectors and be part of our community of learning we are a learning community centered around our ai labs so we do give a lot of these sessions have some more questions no it was wonderful thank you you know for sharing all wonderful knowledge thanks again It was wonderful. Thank you for sharing all wonderful knowledge. Thanks again. Okay. Also, Asif, one person expressed an interest in a security workshop. I believe you had some in mind for like a one or two day one with enough demand. That is true. Just take down the name. The AI. Okay. Yes. Yes. Certainly. We should create a list and see how long the interest list is. Okay. All right. All right, folks. Any questions? Any raised hands? Yep. This is Gopi. I put my hand up up i'm not sure if you see it but uh yeah um do you have any um like a one pager view um of the kind of continued education you know to get hands-on you know uh like a I'm looking like a simplified table of contents or something like that you go from here to here to here something like that to uh give some ideas to us on what what we could do next you know I'm kind of thinking uh some of the I'm kind of thinking some of the things I'm interested in is, you know, some of these use cases that we hear, like how law firms are using, you know, chat GPT to extract useful information out of tons and tons of material for pertinent to cases and things like that so as a small law firm if they wanted to go and implement something in-house yes what would they do and you know i mean obviously i'm not looking at it from what is the costliest or the least costliest way of doing it but just technically what would they do? How would they go about it? Okay, Gopi, because I am advising actually not one, but three law firms who are asking pretty much similar questions, specific use cases. I would be happy to talk to you because it's very specific. Just schedule some time with me. I'll be happy to give some guidance on that okay i use the law firm use case as an example but i've got nothing to do with law oh okay yeah so i'm i'm just asking in terms of you pick any use case for any industry uh you know how do you go about navigating through that process right in japan i'll give you a broad answer yeah for textual domain and this this sort of thing that you ask start with doing good prompt engineering like give right instructions like what we what you you notice that what i did for this medical problem what my team did here during internship did for this medical problem, what my team did here during internship, is that they ultimately created a very rich prompt with proper instructions. And so, and by the way, it's very common at the end of all your instructions to say, also add the sentence, if you do not know, say you don't know. That's how all the prompts end so you're hallucinating or making yes and it doesn't quite work it still helps you but perhaps a little bit less so then prompt engineering is what you should start with it's surprising how many a large number of easy problems or straightforward problems are solved just through prompting The second level that you do is you take in-house data and by the way you can do this prompt engineering either against an in-house open source LLM or using these services. Any day, I mean today the open source LLMs have become caught up really well and the advantages and disadvantages. If you can afford the hardware then you pay a fixed cost putting a hardware for inference and running your own llm makes a lot of sense because you're not paying a recurring cost on the other hand if you don't have the hardware budget or don't want to pay a friend then use these services the trouble with these services is your data leaves your your perimeter and goes to a third party and comes back with answers so that is a security concern the second step that you can do which you can only do in i mean you can do it with this commercial ones but with in-house makes sense you take a open source large language model which has been pre-trained and then you fine-tune it on your data. So when prompt engineering is not sufficient, then you start doing it. And by prompt engineering, I'm also including RAG, part of it. And then what you do is you start fine-tuning the LLM on your specific training data. Now, training data to create label data is arduous and expensive but experience has taught us that these LLMs learn from surprisingly small data sets. To catch on to what you mean in your specific case that fine-tuning usually takes a maybe a few hundred or thousand uh some data samples and then it fine-tunes to your domain your company information so fine-tuning is the pretty much by then 99.9 of the people have solved their problems most business cases are solved at fighting finally if none of that works and if you have grand ambitions, that is when you say I'm going to build or train a large language model just from scratch. And that is a big budget thing because if you talk about pre-training or training your own custom LLM, now you're talking about requisitioning hundreds of GPUs or sometimes thousands and training them for weeks on absolutely web-scale data, like ginormous amounts of data. And there are a few companies that are taking that route and when they do it there's a big press release that they have created their own large language model and so on and so forth. But for most practical cases in the enterprise, your go-to method should be first prompt. And if that fails or that doesn't give you satisfactory results, fine tune or ideally, if you have the resources, do both. Do fine tuning and do prompting. And that is your path to a successful application stack on top of LLMs or generated AI. All right, thank you. I have another question that was brought up by Gregory Murski, and he would like to know, "'Hello, could you please send me, oh, nevermind. Let's see, the one, while working on a Python program, and this is from the second lecture, while working on a Python program this past week using the wave function collapse algorithm, I learned that the WCF algorithm could be implemented in the generator of a generative adversarial network, GAN, to produce output to be tested against the discriminator in the GAN. What other common generative algorithms are used in GAN generators to create data to be shown to the discriminator? So there are four broad classes. I mean, there are many, many generative algorithms. In the more theoretical sense, any model whose task is to infer the entire probability distribution of the data itself in its totality inherently becomes a generative model. And so you can create those through all sorts of means. For example, things like Markov chain, Monte Carlo have been around for a very long time, years, years and years and years. But broadly these days, if you look from a practical perspective and what we tend to refer to as generative AI, people mean four or five specific ways in particular. One is the GANs. And as you pointed out, GANs are great for generating data. Perhaps the most commonly used method for generative data. The second are autoencoders, variational autoencoders, and their successors. These generative autoencoders are there. They are also very, very commonly used in these purposes. Then there is a community, or there's an alternate path, which is the normalized flow-based methods. Now, in this thing, in the last lecture, I talked about normalized flow, but I didn't cover it to the full extent, because obviously I just touched upon the topic. Normalized flows also very successful and have led to their own set of models, practical models that generate. Then they are energy based. Well, in a way you can say normalizing energy are somewhat related, but they're energy-based methods. And then finally, you have just the generative, as in the transformer-based methods. And then there are more, there are others, but if you take these four, five big ones, you'll see that most people these days take one of these tracks based on the use case. Thank you. Okay, there's one other inquiry about from PKP about how do we schedule consult time on use cases with Asif. That's a question for you, Kate so reach out to kate amon okay and i think when we send a follow-up email to all the participants um we can include information on that and some upcoming some possible workshops for enough uh based on enough interest like security and other very relevant and important topics give all this information along with the recordings that everyone has been requesting that's right and three of the recordings are already on the YouTube the last three sessions and today's will also go. Now I must say guys that there is a boot camp that is currently in progress that is running house full and so we had to actually send some people home because we reached capacity. This boot camp will again take place. It is a large language model boot camp. A lot of the things that we talked about these people are doing. The people who are here are people with two kinds of audience. One is they are architects, et cetera. And they want to start a big AI project at their company. They want to really learn it well to lead and lead teams, large teams with it. So in the boot camp, we are making them do practically a starter it's a project that will involve creating quite a substantial amount of code and a full enterprise class at least the outlines of our enterprise class architecture for ai so you learn all aspects of it ml ops and the data engineering and the and how to fine to fine tune model how to do best hyper parameter tuning and so on and so forth and the other audience is literally people who are successful entrepreneurs or people wanting to be entrepreneurs who come and say hey i'm an engineer i want to do my own startup and can i in effect is this boot camp the accelerator and it is they whatever they do they take with them and one of them one team is actually literally the boot camp is only we are seven or eight weeks in out of the 12 and one team is already getting its pitch deck ready to pitch to the venture capitalists, VCs for funding. So that boot camp will start in February. That is one. The generative AI boot camp, this thing, like a more hands-on version of this course, I'm yet to fix the date. It will either be in November or it will possibly be in November or it will be in Jan but if it interests you I would strongly suggest I register for it now. Then we have the next year we'll start with the whole sequence starting from the basics of comprehensive introduction to data science which is just imagining that you have no background, no technical background in the field, but a lot of desire to enter the field, then you should take the comprehensive introduction to data science, which also will start in the February timeframe. So these are the three big initiatives that are scheduled. Besides that, I do on demand, obviously, when there's a sufficient audience, we do specialized topics, for example, biotion optimization, or time series with AI, or anomaly detection using deep neural networks for, you know, fraud detection and anomalies and so on and so forth. So those are things I do on a need-to-do basis and on request. But broadly, these are the three main things we do. Okay, there's one last question that says, from Chris, about what do you think about the new woodpecker approach to hallucinations? It is good. I just read. Thank you for bringing it up. I haven't read the whole paper. I just managed to read the abstract and the introduction, and I think it is significant. It looks very promising. If you have read the whole paper, for me, it's scheduled as a weekend reading. If you have read the paper, please enlighten me. What are your thoughts? But it is very interesting. I am looking forward to reading it line by line this weekend. ELANA GORDONRUZOVICHY- Yeah, Chris, you can feel free to unmute, and we'd love to hear your feedback. So I have I mean, well, I have questions. Is there any suggestion? How you know, how do we keep up with these things? Now? What are the sources that you actually keep up to date yourself? Actually, I have, I'll tell you how it works for me, I suppose every person is different. See, textbooks in this space are all very fairly obsolete in the sense that by the time a textbook comes up, AI has already moved far ahead. But still, make sure that you have the theoretical foundations, they matter. But once you have that, read, like I read a lot of papers. I tend to, well, this is a bit idiosyncratic, but I go for a hike early in the morning. From my house, I go to the bay. And then at the bay, I sit next to the water, turn back, look for the sunrise. And I usually carry one or two papers in my backpack. I sit there quietly and read it, highlight things, and then come back. I suppose walking really clears the mind and helps absorb more things. So a very basic room, read at least one or two papers every day. Pick your time of the day that works for you. The field is moving very fast. In the beginning when you read papers it's a little hard but gradually you know the pieces begin to fall in place and it only gets slowly but imperceptibly you'll realize that you get the hang of papers very quickly and you can read a full paper not just glance through it but actually read it study it and in and study it and in one straight reading, you can get everything that you want out of it. And you'll end up obviously highlighting the paper in lots of places. That is very helpful. Now, there are YouTube channels that do a very good job. One of them is AI Coffee Break on YouTube. It is good. the other is so there are many actually but um the other one a little thing yannick yannick is another person who comes up with not that frequently but he makes videos um fairly good ones then uh keep keep an eye on andrew inc's website i forget the magazine that he comes out with but he has a magazine from his uh you know the ai company that he has started again right um deep learning or something deep deep courses for deep learning courses they come up with their users so the many many actually the sources of information is such that it is more appropriate to ask that news is coming at you. See, today, close to between between six to 800 research papers are published in this field every single day. Or submitted. I don't know if they are probably submitted. It's just crazy. Just crazy. Right. But you can't read all of them them keep an eye and form a community like at support vectors what we do and that is the last part i was going to say what really works is support vectors is a learning community people who are part of this and they have been even if they are alumni from five six years ago seven years ago they they stay connected and we have unlike let's say twitter or so forth where there is a lot of chatter and noise so we have very community helping slack channels and so on and so forth and people often post if they hear in their company a water cooler conversation because people talk they hear that some paper has been published or something is public obviously they don't reveal company secrets but they tell us about anything the moment a breakthrough is significant let's say that within the company they are taking some research paper very seriously and it just came hot off the press it's just been submitted to archive and they are taking it very seriously. Well, they will point us to it. They say that, hey, this paper is getting a lot of attention. Would you guys see if it is interesting for you also? So because of that, in the learning community, actually, I get some of the things like I said, I'd like to read two papers a day that's and quite often in spite of that i miss things and when i miss things this is how uh the community sort of helps me catch up or tells me something that i missed so use these means see learning is such there's a reason why researchers live in communities university communities or you know in companies and so on and so forth their teams because with with the community you have less blind spots you catch up much much sooner so have social learning thank you cool thank you very much all right folks if there are no other questions, thank you all for attending this series. We have taken the list of participants. We'll send out certificates, e-certificates for completing this Generated AI course, Introduction to Generated AI course, to each one of you. I presume, Kate it am i right in assuming that we have everybody's email address well actually i need people if they have a mysterious login today like some people i only see a phone number so i won't be able to confirm that their name and email address are final um yeah if your login is very brief, like if I just see a phone, well, there's just one person I see as a phone number, please in the chat before we end the meeting and the chat goes away, confirm your email address. I'm taking other notes from the chat and making sure I have them in my notes so that when the meeting ends, the chat will go away. But I've got that. Okay, cool. I've got other people confirming their emails and hopefully these match with what we have. And if you've received emails about the course, then we should have that so you should be good but if you haven't if yeah just make sure it's not too different okay great i'll end with one final thing support practice is a learning community and the learning part of it not the consulting part the learning part is run as a de facto non-profit. In other words, most of the training we either give free or we price it fairly nominally to break even the cost and so forth because we are in the process of building an AI learning community. So if you like this, we request you to share it in whatever your favorite social media is. Talk about it. Or tell it to your friends, your colleagues, so that they are informed. Next time we offer something, either free or paid, if they are interested, they can enroll for it. So we'll really appreciate you if you can get the word out about the support Vectors Learning Community. And we should include that in the email, follow-up email we send to everyone good idea yes let's do that thank you guys okay i will end the recording then thank you so much