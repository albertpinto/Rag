 So, last time, where are we? Do we need to review? It's a new topic, but last time we covered the loss functions, right? And we came upon the cross entropy loss. The session before that, we did the logistic regression, a very popular technique to do binary classification. Now, when you have a multi-class, you know, you're not trying to classify in just cows and ducks, but let's say cows, horses, and ducks. So you have three classes now or more classes now. So logistic becomes a little bit problematic. You have to use techniques like one versus rest, you know, you'd build multiple models or uh and you then work out the probabilities find which one is most strongly uh you know you have much more confidence that this is a cow not so confident that it's a horse cow versus not a cow horse versus not a horse duck you know things like that so you you get into those exercises where you have to build a lot of models internally. So the class of methods we learned today, they are very, very useful. And they reason very similar to logistic regression, but take a different approach. They build a different hypothesis. We will learn about them. And this topic is called discriminant. And we'll start with the linear discriminant, linear and quadratic discriminant analysis. So it actually predates logistic regression by many years. Now this discriminant, the words are, so linear obviously means somewhere we will be talking of a linear decision boundary. Quadratic means somewhere maybe the decision boundary will be a quadratic curve. Discriminant comes from discrimination, namely to be able to discriminate one class from another or distinguish one class for other. It has nothing to do obviously with the social meaning of discrimination which has negative connotation, it is purely in the scientific sense, so now we will learn what exactly is this approach. R. Vijay Mohanaraman, Ph.D.: It is at heart, a very simple approach and so we'll cover it in a short time. R. Vijay Mohanaraman, Ph.D.: let's go back to our cows and ducks suppose I were to. to our cows and ducks suppose i were to look at the question so you mentioned about linear and quadratic right does it stop at quadratic or there are higher powers generally at least it's not common to do that no right see you can always do polynomial expansion of the basis functions of the features itself. But you don't do it like discriminant. You'll see why. You'll see why it is enough to have two of these. You can, of course, remember all these techniques you can mix and match. Right. Nothing stops you from first as a preproprocessing step going and building up a feature set in which you have included polynomial features right so so all right so i will deliberately because cows and ducks it turns out their weights are not negative so i will say this is their weight and this is their size, the volume, right? If you want to take it. So I'll call it X2 and I'll call this X1, right? So let us say that we are looking at it. Would you agree that the ducks, what would be a good color for ducks? Ducks are mostly white, so I'll just leave it as white. Their weights would be somewhere around here. They're small in size, small in weight. Would you agree? And there may be some outliers. Broadly, their weight distribution would be somewhere, something like this. And obviously I've normalized the, imagine that the scales are we have scaled the data would you agree with this guys they would be well separated yeah that's right they're very well separated if we look at cows on the other hand they will be cows are cows. They are big. So they'll be here. Data points will be scattered here. And they will have some outliers and so forth. So when data presents itself in this aspect, and it can present itself in this aspect, would you agree? So there's a very natural thing that happens. Look at the projection of this data set. So there's a very natural thing that happens. Look at the projection of this dataset. If I were to plot the histogram or the density plot, smooth it out histogram, I would call it, these are called marginal probability distributions, fancy word, but just saying that we are making a histogram a smoothed out histogram uh at the bottom right uh see it will actually be histograms are upside but i'm inverting it just so that it doesn't clutter up on the data are we together guys right would it be reasonable that it would be like this and once here for we'll see this and here we will see this right and would you agree agree first is straightforward, isn't it? Yes. So now, this does a duck look? Well, something like this and a cow. Well, this is a terrible duck, but we'll go with that. And a big cow. Big swishy tail. Well, I don't know how much of a cow it looks, but imagine that it's a cow right so we can do a separation let us say using common senses that hey intuition tells us that we can just take the perpendicular bisector of the center of these two regions. So what we can do in our mind is we can go up here, this line, this line, this coordinate is one center. Then this peak here where the maximum probability is there to here and this, and then we end up with this point right so one one easy suggestion that comes to our mind very naturally in this situation says join the join the peaks distribution peaks right these are hills do you see that if you look at this you'll have contour plots here. We have been seeing those contour plots by now. Equal density plots like this. So what you do is you connect these two centers and then find the perpendicular bisector, and this is your decision boundary. A very intuitive and simple idea. Would you agree with this? Yes. Yes, it is right. So now comes while this idea is very simple, it actually leads to a whole bunch of other goodies, actually. One of the goodies that it means is, see, suppose I create this is your x1, x2 axis. Let me write this down again. This is your x1 axis and this is your x2 axis. But now what I can do, amongst other things, I can say, hey, you know what? Suppose I rotated my coordinate axis what color should i use for my rotated coordinate axis let me try i'll try red right so imagine a coordinate system which is literally like this which is literally like this. Let me call this a new coordinate system. What should I call it? Let me just call it x tilde 1 and x tilde 2. You realize that x tilde 1, x tilde two is some rotated version of x1 x2 isn't it that you just you're just rotating x y x1 x2 axis to this right kind of like an inverse function right that's right so we have rotated that and now comes a beautiful thing. You realize that for practical purposes, for example, for classification, we don't need to. So if you look at this point, any one point, let's say look at this point, right? It has some value here, x1 for this point P and x2 for the point P tilde. These are the coordinates with respect to this is x for the point P2 and this much is x1. You agree with that, right? These are the coordinates with respect to the new coordinate system. Yes. that right these are the coordinates with respect to the new coordinate system yes right and now observe if i were to classify or do anything does does x2 even matter all you're looking at is on which side of the decision boundary you are. The further you are, the more likely it is to be either a cow or a duck. Isn't it? So X2, if you think about it, it doesn't really matter. Right? So one of the things you conclude from this is, when you are lucky enough for this sort of situation to work, you get two benefits. You get two things come out of this two uh consequences let me just call it consequences of this reasoning one is you get a classifier and the second that you get is you get dimensionality reduction so let me just say the two consequences a classifier classifier word itself is enough and demand now i'll introduce something called dimensionality reduction what in the world is that dimension am i writing it correct dimensionality reduction dimensionality reduction right now what is dimensionality reduction we are saying that x2 in the new coordinate system x2 doesn't matter so data instead i could write the data as So data, instead, I could write the data as, let's say, let me go back to the initial data, whose coordinates it x1 tilde right x1 tilde and uh implicitly x2 tilde we we just cross it out we don't care right using some rotation or some transformation we go to a new coordinate system and one of the we realize that the only thing one axis matters right the other axis is irrelevant other dimensionality other feature is relevant so now if you think about it geometrically it is obvious what is happening to data this is a feature in the original space features these two isn't it and what we are saying is using some transformation if we can find another feature set such that we can ignore some of the some of the new features so you can call these derived features transformed transformed or derived features x1 tilde x2 tilde right derived features and our goal is to find it in such a way to find this in such a way that some of the derived features do not matter much they may matter a little bit do not matter so far so good guys isn't it right so when you do that now you can analyze your data in terms of x1 till day and y the target value which is cow or duck. You now are building a model in only one dimension. Are we together? Now, let's review the reasoning. What I'm saying is, let us say data presents to you like this. Ducks, this is a duck. These are cows. The ducks occupy one region of the feature space and the cows occupy another region of the feature space in in such a situation then what happens is you just look at it and you see sort of a bell curve distributions right along each of the marginal distribution looks like a bell curve and this is a gaussian like a multi-dimensional bell curves are called normal distributions normal or gaussian normal distribution what does it look like in real life if you ever hike and you see a nice round hill, right, or elongated hill, ellipsoidal hill, that is effectively your Gaussian distribution, approximately, that's what it looks like, right? So all we are saying is, in such a situation, common sense says that a simple classifier that we can build is find the Center of the peak of our hill. R. Vijay Mohanaraman, Ph.D.: Right and the peak of the other hill and basically wherever it is whatever the coordinates are and what for gossians what do we call it bell curves, what do we call it, that would be the new vector right. R. Vijay Mohanaraman, Ph.D.: New vector made up of this is mu 1. This is mu 2 for the duck. Right? Mu 2 for the duck. This point would be mu 1 for the cow. Remember the center of the bell curve? And this point would be mu 2 for the cow. So guys, I hope this all makes a lot of sense now geometrically right so all you do is you find and when of course you put it the mu vector the mu coordinates of the cow is equal to mu 1 of well mu 1 cow let me just put the cow on top cow is mu1 of the cow mu2 of the cow right the coordinates of that and here likewise this point is duck of duck is mu1 mu2 associated with the duck right would you agree with this so far yeah i've not done anything uh i hope what you're seeing is i'm making a set of very obvious geometrical statements but what is obvious in geometry is actually has profound implications when you think about it as data and features and so forth. One of them is out of this common sense says the best way to separate is to take a perpendicular bisector of the of the line connecting mu1 and mu2 right these two points the mu of the duck and the mu of not mu1 mu2 mu of the duck and the mu of the cow right mu is what the mean the center right center of that bell hill right now so you get a classifier and also you realize that if you do that whatever the the the line joining the center says that becomes your principal axis that becomes the most important axis the question that you have to ask is do but if i if i rotate to that axis then other axes that remain like x2 tilde how much do they matter quite often you'll find that they don't matter much they matter much less than the first one. And so there is a certain amount of mathematics here, which guarantees that this will be true. There is a sort of a, well, okay, I won't get into it. We can look at SVD and eigenvalue decompositions and SVDs, et cetera. We'll look at that later. But bottom line is- The math class. The math class, exactly. But there is enough reason to just intuitively see that this happens. So you can actually reduce the size of the problem. So one way, sort of intuitive way that I can give you is, let us say that you're looking at the price of a house. Somebody gives you the length and the width of a house, right? And what, I don't know, well, that may not be the best example, but the length and the width, let us say, is most houses are proportional. So what will happen? So suppose there are houses that belong to, let's take two class of houses, the houses that belong, that are starter homes and houses that are Mac mansions. So would you agree that they would be distributed pretty much like the way the cows and ducks have been distributed? And so the real measure that's, if you look at the axis, this axis that connects these two, you will realize that it has some amount of X1 and some amount of X2. Basically it is saying that in the price of a house, both length and width matter to different degrees, right? And that is what it will say to you. And you can then along that axis separate out the two classes, the starter homes from the much more expensive homes, I suppose. So that is the basic idea. But now comes the caveat. See, first of all, do you notice that we have made assumptions of a Gaussian distribution of the data? This goes to the heart of something we have in machine learning. See, before you do an analysis of the data, you don't know what the data. This goes to the heart of something we have in machine learning. See, before you do an analysis of the data, you don't know what the data looks like. You should try your best with data visualization, etc. It will give you a strong hint of what it is, right? Especially if you see Gaussian distribution, and they are far more common than you think. Often data is a mixture of gaussians. So this sort of reasoning that I showed you, I mean, you can generalize it a little bit more, but I've showed it in the simple situation of one gaussian for cows and one for ducks. It does hold, and when it does hold, this discriminant analysis works very well. And we saw that in the lab on Saturday, remember one of the problems I solved using discriminant analysis. Now, what is the, there's a difference though between a linear discriminant analysis and quadratic discriminant analysis. Linear. Yeah, As say, Sonal had a question. Can you go back a little bit? Sure. So, Sonal, decision boundaries are always linear? No, here I made it linear. And in fact, I'm coming exactly to that question. Okay. Hold that question in your mind. So, linear discriminant analysis. Analysis. So, what happens with linear discriminant analysis is, you assume, so now I will just take this again well I suppose I should reproduce the picture but I won't reproduce it in its full glory did I use whites for the ducks I believe so right so here we go so here are the ducks remember the axis of weight and size and the the curves, I use some other color. Maybe I use this color. The curves are here. And assume that there is some distribution. What assumption we make is, see a bell curve, a normal distribution is given by multiple things. A normal distribution, actually let me write it in yellow because I'm writing the concepts in yellow here. So a normal distribution is written by, whenever you see this n written in a very strange form, calligraphic form in mathematics, this is the symbol for normal. So the question is, what is abnormal? Well, it isn't meant in that. There's a reason why we call it normal distribution. It sort of is ubiquitous. Very, very often you do see it. But it is basically the Gaussian distribution, Gaussian or normal. In your mind, think of a bell hill, whose contours will be either circular, but they need not be circular, they can be elliptical also. In other words, the variance in one direction, so if you look at this, do you notice that in one direction, it is more spread out than the other direction? That is perfectly okay, right? So this is it. This is the x given what? Mu and what else does a bell curve need? Sigma. Sigma. Because it's a multi-dimensional vector we call it and it becomes the covariance actually okay right the covariance matrix but i'll just write it as this right and intuitively what we are saying is see mu's are determined by the data What we are seeing is, see, mu's are determined by the data. This will determine your mu for cow, mu for duck, right? And this is your mu for cow. I'm mixing up my notations all the time. I think last time I put ducks, the superscript, and now I'm making it a subscript. But anyway, be as it is. This is it. But there is still be as it is this is it but there is still one thing that is uh undecided if you make a hypothesis of a bell curve a bell gaussian distribution you have to decide what do you consider the variance of these two all right the sigma one so there will be, well, let me just write one dimensional notation. Sigma duck, sigma cow. There is a spread to this, right? So at this moment, just for simplicity, I assume that these are nice round bell hills. We'll go to the ellipse later, complicates things a little bit, but the reasoning remains the same. In other words, are these, but the reasoning remains the same in other words are these are the widths the same do you make the hypothesis that they both share the same width sorry the same sigma the same if not i guess you could standardize, normalize. No, no, no. It's a question. It's your choice. You can just decide that as a simplified version, I believe that the data will have different mus. But to the first approximation, the variances are the same. You may do that. Or when you do that, and you can sort of do this experiment, by the way. When you have you ever you know on the beach have you ever created two sand hills you know by pouring sand from there and both the sand hills are about the same height you know the same same amount of sand you have poured at both the places right do you notice that where they meet, how does it look like? Like a straight line? Except that it's curved in the z direction. In the vertical direction, there's a curving. But if you project it down onto the ground, it will look to you. If the two sand hills are the same, the line where they meet will look to you like this right and it will be in fact the perpendicular bisector of the two sand hills do this thought experiment in your mind imagine that you are pouring sand from two big bottles into two places nearby and so those two hills will meet somewhere in between. What do you see there at the region that they meet? A line forms, right? It is going down and it rises up again. Would you agree? Yes. Yes. And if you ignore the fact that the heights keep changing, but if you just look at that thing, it is a simple straight cross section it is a straight cross section here so that is an interesting thing when you make the assumption that the mus are the same imagine pouring sand like that then you get a linear And there's something very interesting guys. I often think of data as sort of in the feature space if it helps you. For me it helps. Often I imagine it to be like, you know, some kids have come and poured sand all over the place. And so the surface, the probability surface that you're looking at in the feature space is roughly the result of some kids playing a pouring sand all over the place, right, from the cones. And so it's more formal term would be it's a Gaussian mixture of some sort and things like that. But let's not use that. I mean, I tend to use very simple reasoning, I suppose that helps me think through that. So if you think through that, it will be this, linear decision boundary. Now imagine a different situation. Imagine that what you're pouring is not the same sand, but you're pouring something with different slipperiness, right? Which is a little slipperier, more slippery than sand. What could be such a thing? It would be, think of some marbles, you know, the children play with marbles, except that perhaps not as smooth as well-used marbles that have some degree of friction. So if you pour that into the ground, you would get something like this. So I'll just draw the contour plot. You will get, well, actually, let me remove it. I take that back. This would be unfair for the oh sorry what am i doing give me a moment yeah so what i'll do is uh let's take this ah what's happening it's gotten stuck somehow this ah what's happening has gotten stuck somehow all right it's not letting me do that so ducks if you think so now think of the weights of a duck ducks weight will be what i'm just guesstimating about 10 pounds plus minus, or maybe 15 pounds plus minus 10 pounds. Does that look reasonable for a duck guys? Or maybe 10 pounds plus minus five pounds. That's a lot, five pounds. Yeah, yeah. So in other words, the majority of the ducks will be around, whatever we take, let's say 10, and this will be 5 and this will be 15 and there will be some outlier ducts beyond that also but you would realize that the pretty much the variance of this will be sigma of the duct will be much lower than of the duck will be much lower than oh boy what is uh this thing has a bug here oh okay maybe i'll try another color no it's gotten stuck here guys so i don't know so you can just close it and open it again the whole the whole of this one note yeah all right okay we will do without it let's see if we can do without a change of colors so um would you agree that the weight of a cow would be far more distributed right it would be because you you you may see a variation like this, right? For whatever reason, let's say that you have many species of cows, some are very short cows, some are very big cows, and you look at the weight of them, and they are much more spread out. Are you seeing the point that I'm'm making the sigma of the cow genuinely is much greater than the sigma of the duck in this situation sigma is what the variant the well sigma is the standard deviation but so i should use technically i should use sigma squared, but okay. The variance in the weight of the cows is much more than the variance of the weight of the ducks. I'm making a hypothesis here. Let's say that is true. A biologist may or may not agree with standardized data, who knows if it's still true, but it is there. So now what happens is we cannot compare we cannot really honestly say that sigma curve approximately is sigma duck which is the assumption we made for a linear decision boundary here what did we decide the two sand hills are pretty much equal are we together they're just placed at a different different place. But here, it's not the question of being equal. I mean, their total volume is the same, let us say. But because we use different material, here, one place we use sand, which is rather sticky, so you can get a nice, fairly compact distribution. Another place you use some sort of marbles or something like that, which is much harder to create a hill out of. Have you ever tried to create a heap of marble on a flat surface? It's much harder. It just spreads out. So take some rough marble so that you still manage to create some hill out of it. You would agree that it will be far more spread out, isn't it? So it will have much more. The sigma of that will be much more when you have that situation just in your mind's eye i try to do what will happen to the decision boundary where will the decision boundary get constrained you realize that it will not be a straight line anymore You realize that it will not be a straight line anymore. Yeah, right. It would be now, let me let us do our imagination, which way to lean. I was going to think, Okay, let's let's try to do this. So if you try to look at the if you if you take that and you try okay, let me give you the first answer. Now, I don't know which way I would, okay, which way it would lean, but whichever way you look at it, it will be, I think it's a curve. I hope I don't make a mistake here. But be on the guard, I might be wrong. I think you'll get a decision boundary. Yellow. Where am I? This is hopeless hopeless let me restart it guys yes and and why is it okay so Okay. So, goodness. It is behaving somewhat strangely with zoom. All right, I will make make it like this and my pencil thing has entirely disappeared now for whatever reason control z to use g force okay now all right guys i will use the same marker whatever color it is to mark the decision boundary but the decision boundary would be like this i think or or maybe the other way around. I forget which one. So excuse me. I'm not able to reason my head clearly. It will be one or the other. So you can do this experiment and see it at home. You will see that what is splitting the two up, you won't see. You'll see a curve here. You'll see a bend. I think the bend will be this is more likely to be it if I remember it. I do these things guys, you know, well, okay, there is a mathematics, you can grind through the equation, but I actually like to do things like this on the table and find out. But anyway, what happens is that the place where these things meet, you will see an arc here. Go ahead. So as if the question uh was about to ask us what is the intuition for the bend facing any of the direction because intuitively right now both of those are contours right that is they inherently have a curvature towards the center each okay yeah so why would the bend be either of the directions see what happens is that i'll give you an answer to that so look at the asymptotic limit this guy is very peaked right very peaked means sigma is small very small huh very small very very peaked hill this guy is so so yeah actually now i got good that you asked this question because it helped me reason through that so this one is so as an asymptotic limit it is so thing that it is practically like this flat so now if this is practically flat and you can assume it to be flat and if this is the only thing there all the contour lines are essentially your decision boundaries effectively do you see that right and at some point the this thing will keep coming in but this will have the nature of essentially flat surfaces and these curves will dominate because it's so peaked here. Okay. That is it. So, and you know, see, there is a bit of mathematics. You grind through it in five minutes, five, five steps, you'll get the answer. But do it. I can't tell you how the, how you can just simply get it by doing this experiment on a table and see what's going on. So this is it. When you do that, you get curved. And it turns out that this is a quadratic curve, which comes out by grinding the equation, quadratic curve. So the decision boundary is quadratic. So if you assume that the sigmas are the same, you get linear decision boundary. If you assume that even the sigmas are different, each has its very different, then you get quadratic decision boundary. Now, what is the hypothesis that you should make? See, my general thing is that, first of all, is it even Gaussian, the data? Is it distributed as Gaussian in two regions of the feature space. Remember when we talked of grapes and what was it? Blueberries and cherries. Was it Gaussian? Now let's go back and look at one of our previous examples that we took. Where are we? No, it wasn't. It wasn't. Where is the picture? I would get to the picture. Where have my blueberries disappeared? It will be on the top, sir. Top of even, oh yeah, here. So let me increase the size of this and this. So you notice that this is not a Gaussian distribution, right? So you can ask this question, if you did a discriminant analysis, would it work, right? You'll be surprised, and even if you just use basic linear discriminant analysis, would it work? It actually works surprisingly well. It has to do with the nature of the gradient descent process. It will somehow try to fit a Gaussian here, somewhere here. You know that actually, if you look at the blues, their distribution, actually I shouldn't destroy this image, but OK, will be peaked like this. The blues decrease. And the reds are peaked the other way around like this. Are we together? At the bottom, if you look at the x-axis, you look at the probability distribution And so you can say, well, you know, the distribution along, even the marginal distributions don't look like bell curves. Yet, to the first approximation, if you say, well, it's a bell curve, I assume it is, it tends to work surprisingly well. And that's one of the interesting things about machine learning algorithms. Because of the nature of optimization, often it tries to fit the hypothesis to the data, even when it is actually not the greatest fit. And quite often it does work. So from that perspective, my suggestion to you always would be, start with linear, see if it works, linear discriminant analysis. And if it doesn't, then go to quadratic discriminant analysis. But now comes something beautiful here. Where am I? Yeah, we are here. Now I will do something different, which I really hope that the maybe I need to kill this and start all over again. One more. Yeah. OK okay these colors have come back all right thank goodness let's see that let's see how long it's very buggy it's very buggy isn't it um let's find a better software for this so suppose in oh in this data space let me stick to one and And guys, if I lose my colors, please forgive me. Suppose you have three things. I will now make the ducks, well, forgive me for using whatever color I want here. One, and there is another creature here. In the feature space like this. Yeah, go ahead. Do you want me to share my screen? I can show one nice visualization. Maybe that might help. Yeah, definitely. But could you please hold a little bit? Let me finish this reasoning and then we'll see that in a moment so suppose you have three classes here now can we generalize the reasoning that we applied to for example two class cows and ducks can we do that maybe these are horses or whatever it is can we do that and it turns out that the answer geometry supports you because you can draw a perpendicular line here you can draw a line here you can draw a line here and what do we do we take the perpendicular bisectors of all of these and the perpendicular bisectors of all of these will meet you and you can convince yourself that this is always true will be like this and so what happens you can declare this is where the perpendicular bisectors meet and so you can declare this entire region to be whatever suppose this was cow right and you can declare this entire region to be let's say duck and you can declare likewise what what were these big like heavy but not well okay heavy but not big come again dogs dogs dog dog i'm not about a dog dog like an animal dog no dog will not do dogs are not that heavy like cows oh i see so tiger goats or Yeah. Let's say these are tigers. Right. And so what happens is that we just declared this region to be that. So what do we see? It looks very obvious, but it takes a while to realize that, hey, you know what? This discriminant analysis, and by the way, I do straight lines, and for quadratics it could be curvy. But let's stick with straight lines for the linear discriminant analysis you do realize that this generalizes very well to three classes naturally isn't it in one single stroke in one learning of the classifier of the gradient descent process you will be able to learn the red decision boundary and so be able to partition the feature space into the three regions. One for cow, one for duck, one for tiger. And from there onwards, it generalizes to four to five. You can convince yourself by taking a piece of paper that it will generalize to four, five, six, whatever number, right? And so one of the benefits therefore of a discriminant analysis here, additional benefit is it is multi-class. Yes, good. Multi-class. Multi-class classifier in a natural way, in a very natural way now would you agree isn't it that's one of the benefits and the other benefit was that it gives us dimensionality reduction but we won't talk about it now. I explained it, dimensionality reduction, dimensionality reduction. Potentially, potentially, it may or may not be possible because if the data truly uses all of the space, feature space, it may not be true, but it becomes a valid method for dimensionality reduction. So there is something interesting to it. While the history of this is to use it as a classifier, and your textbook discusses it in the context of a classifier, in machine learning literature, people often are split up. Most of the time, they tend to focus on the dimensionality reduction aspect of it, because it's a powerful dimensionality reduction tool. In classification, they have been, you know, with the coming in of very powerful tools like the kernel methods and the ensemble methods. this ensemble method. People, it is, let's just say that this is not the most commonly used tool for classification, right? Which is why I thought I'll do it on the Saturday, on the Tuesday evening session. But for dimensionality reduction, it stays quite relevant even today. So this is the sum, this is the gist of it, guys. I will summarize what I said. The geometry, first of all, let us say, what are we doing? We are taking the hills, hilltops. Let me do that. We are taking the hilltops, connecting them and just picking the perpendicular bisector. I can't think of anything that would be more intuitive than this. Would you agree? Right. And so it's a great classifier for that reason. Now when you do the hilltops, obviously their centers will differ. The center of the cows will be far off big heavy things in the feature space. The center for the duck hill, the hill of the ducks, will be closer to the origin. They're light, small things. That is true. So the mews will be different. But how much variance they have is your choice in the model. When you are starting out the model, you choose to make a hypothesis that you know what, to the first approximation, maybe they have the same variance. Or you say, no, they have different variance. Then what happens? If you say that they are the same variance, how many parameters do you have to find in the model? How many parameters do you have to find in the model? Let's think through it. This is a point we're thinking through. Oh, why am I all over the place here? Oh, okay. Linear. Linear. First is that you have to have the cow, mu of the duck, the center of the cow's distribution, the center of the duck mu cow, right? Mu duck, sigma cow, sigma duck. Isn't it? Because we agreed that even these are different. This, of course, has to be different. But besides that, we were looking for a common sigma. We were just assuming that they are the same spread of the hills. And for quadratic, we are assuming that there are different spreads of the hills. And so you have a four parameter model. So if you have to ask in terms of complexity, which is more complex? Quadratic. There's a little bit more complexity to it and as always you should try to fit the simplest model try linear try quadratic if linear and quadratic are giving you almost the same accuracy or within shooting distance of each other then you might want to consider just keeping linear because it gives you very straight decision boundaries in the feature space especially if your feature space is low dimensional and you can do the visualizations for people and show them what it means so this is it guys this is the topic of discriminant analysis a very simple intuitive idea that is used which is why it's one of do i understand it's one of the one of the older algorithms that are there so any questions what library do we use to i mean in sk learn oh yeah sk learn has linear discriminator lda lda and qda okay linear detrimental analysis quadratic it is already there in the notebook we covered it on saturday but kyle you you had gone to sleep i think oh you were very sleepy yeah i guess when i was running the classifier notebooks i was noticing that i think i don't have the i i downloaded the most current common support vectors common notebook yes but there's a few oh misclips that are missing okay i'll take i'm tracking them down unless you've already got enough no no no please do tell me and i'll upload the latest version of the common thing so So, Asif, how is this different from the PCA? Or the intuition is similar? The intuition is quite similar. See what PCA asks is that, see there's a subtle difference. The way PCA argues is it says that we assume that there is this data exists in lower dimensional surface. Let us go find that lower dimensional surface, hypersurface, right, on which most of the hyperplane, on which most of the data can be embedded, or it is near that hyperplane. or it is near that hyper hyper plane right so it is looking for all points to be as close to the hyper plane as possible in lean in discriminant analysis you realize that you're trying to find that boundary decision boundary such that the clusters the the the gaussians the class data is as far from it as possible right on opposite sides do you notice that this is in some sense a direction of maximum scatter if you take any other direction you'll realize that they are not as far separated out in in terms of perpendicular distance as this no other plane will maxi will give you as much maximum sub uh you know so we can run the river and make it like a support vector machine like on this or that is separate like because it's a between the two distribution like if you look at the mean yes so if you mean that is like the maximum margin right yes so if you touch the support vectors on both the so what what you're now bringing about is the ideas from something we'll we'll cover so uh the answer to that is there is a connection right these are all very related to each other and so yeah for me i don't understand the don't understand the differences between all these three. We are not looking for margins. Margin means points on the edges. In linear discriminant analysis, there are no edge points. It is a continuous distribution. It assumes a continuous distribution. It assumes a Gaussian distribution. Our maximal margin hyperplane, our margin classifiers, they don't assume Gaussians at all. They assume data is whatever it is, but there's a river flowing through them. As wide a river as you can flow. But here you reason from assumption that they are a Gaussian distribution separately, and then the furthest you can make each of the distribution, the plane that they'll be furthest from is literally the perpendicular bisector of the line joining the peaks. And so you're looking for that. Now now what happens in practice is all these linear methods they tend to produce results very close to each other almost identical results quite often