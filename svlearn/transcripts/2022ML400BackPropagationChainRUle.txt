 Because today is a little heavy on theory. Are you guys game to moving forward? Or you feel that you would rather just review what you have done? What are you trying to cover? Now is a big topic. It is the back propagation. Now is a big topic. It is the back propagation. Okay, let me introduce it. I won't do it fully. Let me just sort of introduce the topic and we'll do it a little bit. So imagine a neural network. And I'll today do it without mathematics and next time I'll do it with mathematics. How about that? Because I think today, this late, your minds may not be fresh for mathematics. So imagine a neural network. There is a layer one, and this is the input, X1, X, let's say P dimensional input is going to all of them and so forth going to each of these what are these things called what are these circles called inside each layer neurons right so neurons so it is there and then that goes to layer two which will have god knows how many layers how many neurons of its own i'll just make it four of Raja Ayyanar?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam and the final layer is layer L, right? So this kind of a thing. And it produces the output. Y hat comes out of it, right? So when you give it the X vector, some vector with some data points, so some data instance, and I won't put the subscript because it gets to be quite noisy with lots of subscripts. So assume that you have given one data vector X and outcomes of prediction because this will keep going down. So what happens? You give it X. There'll be weights here. It will then it will go through an activation here. Remember the basic idea? weights here it will then it will go through an activation here remember the basic idea each neuron is made up of what an activation function you know a distortion function which I write it as Sigma but it could be a Z then they have their weights W right and w's and then whatever input comes x1 x2 x3 let's say and if you were to write it you would say z is equal to and there is a bias term b bias plus w1 x1 plus w2 x2 plus let's say w3 x3 in the case of this to produce the result y hat in the case of a single neuron this is how you would think about it you would say first of all compute z and once you compute z uh give me a moment guys i'm a bit tired standing so i'll sit down i'm lowering my table. Little bit more. Okay, this is good enough. So you can write it like this. Now you realize that in a single neuron, the Y hat is the activation of Z. Isn't it? This is the relationship for a single neuron. That example. But now when you create a complex neural network, things become very interesting. Let's say that this entire neural network, you give it X, it produced Y hat, but it turns out that the value was Y. And so Y hat is, let us say, much bigger than Y. It has y hat has overshot the real value of y, right? So how much would be the error? Error would be y hat minus y. And of course, I'm not summing over all the data points, et cetera. Just for notation, I'm keeping it simple. And so your loss would be, let's say, if you square the error, y hat minus y square, let's say, for simplicity's sake. Of course, there is a summation symbol to sum over all data points, et cetera. Once again, as I said, I'm going to gloss over it for the time being. But now you say, well, you know, my loss was too much because y hat shot y. So how do I decrease it? How do I decrease my loss? Because this is what you're trying to do. So you can say, well, that is very easy. Let's draw the diagram. Let us look at the final layer. And now let's bring in our subscripts. Let's say that there is a neuron here, right? Let's call it jth neuron in the layer this. Then we can say, and let's say that this is producing an activation of j, right, activation j. Now you can say that, and let us say that, you know, all the other neurons are also producing their activation and you are looking at the, somehow, and this is not realistic, but I'm deliberately creating a loss function because there are many of these j's here, sum squared. So what happens is that the y that you get, it is made up of, it's a vector. And you expect this to be, let us say, 0, 1, 0, 0, 0, something like that. It's a vector. But what you produce is you produce light bulbs lighting up to produce certain values. produce light bulbs lighting up to produce certain values. Now in the language of machine neural networks, people often call it as the energies, what the output layer produces, sometimes are called the energies, right? Or the potentials or something like that. People use physical intuition, but whatever it is, and let's say that you do this. Now, what would you like? Let us say that you produced, let's look at this value, this value, right? You have neurons, and something is burning very bright. It produces a massive activation a j is bigger than the other activations to the extent that these things are being fit here this is nothing but what is y hat it is the activation j the prediction j activation minus y j square right this is what the prediction Raja Ayyanar?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk?nk 0.01, but this is 0.9. What does it mean? This is contributing a lot to the loss, right? And you are better off making changes to this guy than small changes to this guy, to something that's practically a bulb that's not burning at all, isn't it? But that is what it means so you want to if you want to really fix your loss you are better off making some small change to this activation but how do you do that that's the problem right so i will solve this problem first for one neuron remember going back to the picture of one neuron and we'll develop the idea of back propagation, is equal to the activation. This is also called the, in the new language we'll call it the activation from this neuron. We will say activation is equal to sigmoid of Z. is equal to sigmoid of z. Now, what is z? z is w1, w2, so w1, w2, etc. And the inputs that are going into it, x1, x2, etc. So, this is a bias term, bias plus w1, x1 plus w2. Let's keep it in two dimensions, this. And your loss is equal to y hat minus hat minus y squared, which is the same as activation minus which is activation is this minus y y is being the reality square which is the same as like basically sigma z minus y square that is why you see that if you want to make a change in this what do you want? You want to, the loss is very sensitive. Let's look at the derivative of this with respect to A. If you look at this, sigma, what do you get? You get 2 sigma z minus y. Does this look, this is basic calculus. Now, think about it a little bit. What it is saying, so now let's interpret it. There's a very nice way to interpret it. It says that if you change this A, actually let me use the loss. In other words, if activation goes through a minor perturbation, minor perturbation, or think of it as wiggle, wiggle, you wiggle it, you change it by, and this is very sort of a notation, a tiny amount, then how much did the l change that is delta l right how much did it change right so for a tiny wiggle of a if the change is lot loss changes quite a bit if if this is quite a bit, what does it mean? L, if. So that is W, that is WL, WA. How much did small change of A cause a change of L, roughly speaking? This is a hand-waving argument. And so gradient or derivative or derivatives one intuition you can carry sensitivity to input here input being sensitivity to the denominator or whatever sensitivity if you make small changes here how much does it change so the bigger this value the bigger the gradient the more it changes the more it changes see the intuition is very simple imagine our y and x if y and x are like this x at this point a unit change in x delta x causes a this much change in delta y whereas here the same unit change in x does not cause the y to change much, isn't it? So you say here the gradient or slope is high, here the slope is low, right? That's what it is. Slopes measure sensitivity of the response, how sensitive is the response to the input. That's one way to look at it, right? But now, if you were to do that, you ask this question, when I do gradient descent, what will happen? I can't change activation. Activation is not in itself something that you can change. What you can change in this neuron are the Ws and the bias terms, isn't it it if you want to make a different model you have to change the parameters so the way to think about it is to say and this is the part pay attention you say that if i change the w a little bit what i really want to know is this because the gradient descent steps is this. And so what I'm really looking for is to compute this. Isn't it? How much does the loss change when I change the weight, any one of the weight a little bit, right? And of course I've reduced the subscript. Maybe I'll just put the word, the subscript back, w i. Likewise, the bias is for the w i tilde is equal to bias i, bias tilde also tilde is equal to bias minus alpha to the bias term. This also is true. So now let's figure that out. Let's make, if I make a, let's go back to the picture of a neuron, I'll reproduce it here, w1, w2, x1, x2. And let's see if we get this intuition right. And this is the activation, the distortion function to produce y hat is equal to a of the z, which is a is equal to, well, I'm calling it activation and y hat interchangeably. This is the sigmoid of z. So let's do one thing. Let's see if I want to see the sensitivity of the loss to w. Let's change the w a little bit, one of the w w i if it changes well if i change this what will get affected w z will get affected because z is b plus w i xi right summation of w1 w11, plus w2, x2. This is what it was, right? So if I change any one of them, one or two, this will change what? It will change z, right? Because the inputs are constant. Input you can't play with. If you wiggle around the w's, it will end up wiggling around the z, right? So let's write that down so this will get changed z will get changed but if z changes if this changes what what change will it cause so let me make a causal change change in this let's take the one let's be very explicit w1 if i change w1 z will change right so the causal relationship goes like this this will affect this does that make sense guys if you change w1 it will have an impact on z right because of this formula because z by definition is this but if you change z Z, what will get affected? If you change Z, what will get affected is the activation, this A. A is nothing but activation of Z. So A will get affected affected isn't it so this affects this this affects the activation function or the output isn't it so far so good guys and what does change of a affect well a is the output if you change this your loss will get affected isn't it your net loss will get affected so you can say uh this part now change in a causes a change of the loss function itself and therefore a small change in w1 caused a change in z change in z caused a change in a change in a caused a change in the loss function and so changing w affected the loss function totally and so you say this is w1 is equal to this. So far, so good, guys, for this one neuron. And let's generalize it now and write it as WI, some ith W for that neuron, is equal to dzdWI, because it could be one, it could be two. The argument would still be the same. be one it could be two the argument would still be the same d a activation d z d z sorry dl the gradient with respect to the activation function and this is a more general result, right? Likewise, you could have said the same thing with respect to B is equal to DZ, DB, DA, DZ, DL, DA. So this is called the chain rule. What we just did is in calculus, you call it the chain rule, that how does the impact propagate? You must have used the chain rule in your high school at some point while doing calculus. But this is a result. Today, what I will do is not go any further beyond one neuron. We won't go to the full neural network because that will take more time. I can see tiredness now. But what I will do is I will present you the chain rule using a different intuition. Right. Let me do this and I will bring that intuition forward using sort of a joke. There is a TV show for kids, very popular in the US. It's called Psych. Anyone has known that? Psych. It's for teenagers. It's for young people, children. Typically, the age groups between six to 13, 14 like it very much in the US. So my children used to like it very much. And one day I was sitting through and listening to, watching it. And one of the episodes had, and there are two characters. One is a very rational and sensible person named Gus. And one is a crazy genius, but not terribly well educated or this thing, his name is Sean. So they obviously, it's a mystery. They go solving mysteries, often murder mysteries and so forth. So think of it as Hardy Boys or Annette Bighton or something like that. So at one point, Sean, they decide that they need to become, they need to run for the mayor's position in the city to solve a murder. So when they do that, Sean wants to, has to give a speech and he has to have a meeting with the press. So Gus gives him a very good piece of advice. He says that, see, if you are a politician and a journalist asks you a question, you don't have to worry at all. Don't answer that question you don't want to answer. Answer instead a question that you want to answer, a somewhat similar question adjacent to it, and answer it confidently and you'll be fine. And apparently there's a lot of truth to it because if you ask a politician a question, you always see that they're answering a slightly different question, the one that they want to answer. And the purpose of a good journalist is of course, to continuously bring the person back and force them to answer the question they do want to ask. So it's somewhat like that chain rule i often think that in calculus is like that so i'll take an example and try and make it real i just cooked up some weird problem so suppose you have a function that is y let's say y of x is equal to sine of e to the minus cosine x squared. You realize, and I say find dy dx. Now, this hopefully looks scary enough to you, but I'll give you a little table that will help you solve this problem. All that you know is that if you take x to the power n, its derivative is n x n minus 1. If you take the derivative of e to the x, a x, its derivative is a e to the power A x. Simple. And if you are given a sine, its derivative is cosine x. If you are given cosine x, its derivative is minus sine x. This is all you know, right? And the thing is, you're supposed to find this derivative and you say, oh gosh, this is hard. So let's try to solve the problem that we would rather wish we had. And see, chip away at this complicated problem and see if we can solve it. So what we will do is, first we'll pretend that this entire thing is some other variable called w. that this entire thing is some other variable called w right and we will write y is equal to sine w if we write it like this can we find d y d w what would that be cos w cosine w isn't it but that that is not what you were looking for you were supposed to find dy dx so well some partial credit let's move forward but w now is a smaller problem w is equal to e to the minus cosine x square oh boy that looks scary if only this part was some other variable let me just call it p so suppose w could be written as e to the minus p then if it is so then can we do dw dp what is that minus e to the e power minus p to the minus p well that looks like an improvement but then and so now we can put these two together multiply these two together and we came up with dy dp but then p is equal to cosine x square that still looks. We can't solve it using a table. So let's make this inner part as q. And so we can write this p is equal to cosine q. Now can we find dp dq? Minus sine q. minus sine q oh and what is q is equal to x square so can we find dq dx 2x this now now this thing is beginning to sink because now i can say what happens if i do this, dy, dw times dw, dp, dp, dq, dq, dx. I know that small change of x causes q to change, small change in q causes p to change, small change in p causes w to change, small change in w causes y to change, and therefore small change in x causes w to change small change in w causes y to change and therefore small change in x causes y to change and so this is d y in response to small change in x and i've just found that and we can start plugging in values now we can say cosine w and and minus e to the minus p, right? And times minus sine q times 2x. And now let's start resolving it. First of all, we can get rid of these two minus signs. And so we will end up with this. We can solve this by saying 2x. Now, what is w? W, we realize, was this thing, right? So let's do that. Two X cosine E to the minus cosine X square. Wow. Pretty complicated thing we did. E to the minus P. Now what is P? P is cosine X square. p. Now, what is p? p is cosine x squared. Then so we can write cosine x squared times sine q. What is q? So we have sine x squared. And lo and behold, we solve this problem. So we say dy dx is equal to this. And you realize that we sold it by simply looking at this simple table of derivatives isn't it and applying what is called and what we did is we broke the problem down successively into smaller and smaller problems isn't it we chipped away at the problem right and so this is the way you can use the chain rule and so this is the way you can use the chain rule to solve complex problems now so as if i think it's e power minus cos x square i missed e to the power somewhere right w where is it cosine w is this after cosine e power minus oh right e to the minus so this is even more complex you're right e to the minus p e to the minus cosine x square but see what a complicated derivative we solved. And we solved this complicated derivative. Thanks for correcting me here. We solved this very complicated derivative by just chipping away at the problem slowly. And now I will, today we are almost getting over time. It's a complicated thing. So what happens is that there is a final layer producing y hat from the responses here and from the y hat you get the loss change the loss is response to the activations which is the y hat is the activations from the last layer these activations are affected by inputs from the previous layer not only by the weights but the inputs from the layer minus one, right? This is the layer L. And those inputs are affected by layers before that, to before that, and finally to the input, the actual X. So do you see the relationship between the complicated problem we solved and what we are going to do? Right? What we have to do is we have to find the loss so what does the loss depend upon let's see at the last layer the activation depends upon a so suppose you take any one node here let me call it the ith or or the j-th node. This j-th node contains some weights coming in. Let us say that, let's look at this weight coming from the i-th, this one. So W, actually, the way you do it is the opposite, i and j. And so this notation, by the way, you'll find counterintuitive. You put the target first and then the source. W, I, J is the connection here. But what is this node producing? It's producing its own activation, A, J, coming from the layer L minus 1. And this is the layer L. So now we are throwing in all sorts of subscripts and superscripts here. This is it. This belongs to the layer L, right? And this also has some bias terms, B, I of the layer L, right? B, I of the layer L. So now you realize that this is producing its activation. that this is producing its activation, activation of the ith node in the layer L is equal to the sigmoid of whatever z is here, sigmoid of z i, sigmoid of z, I mean, not sigmoid, activation of z i, it need not be sigmoid, is equal to a bias i plus, what is it? It is all of these um w i j a j summed over all possible values of j like different nodes that are feeding into it right so i will i will whenever you see some subscripts repeated assume that you are summing over all possible values right this is the so-called einstein's summation notation so that you don't keep putting sigmas everywhere it depends on there so loss depends upon loss is a function of a i minus the final actual output, right, square. Ai depends upon sigma of Zi. Zi depends upon, up to a bias term, it depends upon Wij and activation of the previous layer, this activation. Now, this activation in the same way will depend upon activations of the previous layer this activation right now this activation will in the same way will depend upon activations of the previous layer and so what happens is that if you want to find this what what you have to know is i j l for every possible value of L, you want to find all the gradients. And how we do that depends upon, so there's a systematic way to keep on. First, you find the gradient here, use that, you find this AJ, AI, and you use this AI in the layer L, and you use this to compute WL, W, A, J in the layer L minus one, and you keep on moving back. But because you can compute this, you'll be able to compute these things. And there's a little bit of a mathematics. So what happens is something quite interesting. In the forward pass, you just do matrix multiplication and activations. come up with the output the output is wrong you compute the loss and then you find the gradients in this layer then you back propagate use that value to create the gradients here and then you back propagate and you use that to find gradients in this layer until you're done till you have exhausted the layer so it is like forward layer until you're done till you have exhausted the layer so it is like forward is just the compute network compute x backward is gradient back prop and that's why it's called back propagation gradient gets propagated the gradients get propagated are we together gradient respect to the activations get propagated back and it is nothing but the simple situation it is the chain rule and while complicated as it looks there is not much more mystery to it the only thing is when you are doing the calculus or writing the formula down exactly, you have to be a little careful of the subscripts. Now, which node is being activated? Which rate are you looking at? And so on and so forth. So you have to keep track of the subscripts. Other than that, there is no magic to it. That is all that there is to it, right? Next time, we'll do it a bit more carefully and do this, finish the back prop but for today this uh this is hopefully um what we say and this becomes very simple if you look at a very simple situation so suppose you have only two neurons this is input and single input if we so you have a weight w1 and single input. So you have a weight, W1 of neuron 1, W2 of neuron 2, right? Bias B1, B2. Let's think about this. This produces, what does it produce? This neuron will produce an activation a1 correct this will produce an activation a2 now let's work out loss is a function of a2 minus y square a2 is a function of activation of b2 plus w2 a1 the input to this is a1 the activation of the previous neuron and what is this which is which is the same as z right so you can say a2 is the sigmoid here z2 is of course by definition w2 a1 and now what about this guy z2 is done now what is activation a1 a1 still needs to be found out it turns out a1 is the activation of z1 this thing here where z1 is equal to b1 plus x times w one right so x being the input so you see how if i make this w what does this respond to it responds to changes in this, this. But the way it works is, if you look at the last layer, A2, first of all, let's compute Z2. Z2, it is made up of bias B2, activation A1, and weight w2 do you see this by formula just by this these three elements go into this does this make sense guys and from z2 you can compute the activation 2 and the actual data value you can compute the loss is equal to a2 minus y square isn't it so a loss depends on a2 a2 depends on z2 z2 depends upon b2 a1 w2 but what does these are things these are parameters you can const control but what about this guy? This guy, again, depends upon Z1. And what does Z1 depend upon? It depends upon B1, right? Now, the input for this is fortunately just X, right? And this is W1. You see how it goes back? And suppose this was not this. Suppose there were many, many layers. The same picture now, let me make it lots and lots and lots and lots of layers, right? So what will happen is this would not be the X, it would instead be A, the layer even before that, right? So suppose I'm looking at this layer. Now make a small picture. Z of layer L produces activation of layer L, the final layer, which produces along with Y, produces the loss function is equal to AL minus Y square. is equal to a l minus y square but z l comes from b l a l minus one and w l's the weights in the in this particular layer they produce this right but this comes from But this comes from, again, a tower of things, the same tower. It comes from Z L minus 1. Z L minus 1 comes from B L minus 1, A L minus 2, W L minus 1 and so you can keep going back and that process is the back prop when you do this when you do the gradients because you have to compute the gradients like little change in this is affected by this by this by this you ultimately realize that all the W's and the weights they they wiggle and they contribute to this you have to shake them a little bit if you want to get your loss right and so this is your back prop journey journey for gradient This is it guys, this is back propagation. This back propagation today we know was discovered many times actually, and it has its roots, computer scientists, I believe around 1979, 1980 is when the real value of back propagation was discovered. And so it has been used. It has been a workhorse of the neural networks for a very long time. So this is the important topic of back propagation. We can carefully do the math, but we are practically there. We can work out the indices and do it a bit more carefully and so on and so forth. Or we may not. We can just skip it and move on depending upon it. Now it turns out that there's a... I deliberately followed the convention of a certain video. Because this, by the way, when it is explained in books, unless you're very good at calculus, you don't see it. If you're good at calculus, you see it immediately. But if you're not, it looks like a bit of a struggle. So there are two lovely videos. Three blue, one brown, three blue, one brown. These are people who make beautiful visualizations of the mathematics. It's a delight to watch their videos. So they have created two videos on back propagation and I've deliberately kept my convention aligned with them. So do watch those two uh youtube videos kyle could you please add it to a class resource uh it's already there it's already there very good so guys as a review please do watch those videos watch those videos and also um go take quiz one you're ready to take quiz one go many of you have not taken quiz one go take quiz one even if you are reviewing this course go take that quiz and you'll benefit from it right two videos. Follow through that. Now, the main textbook that I gave that is there on the course portal, there the back prop is a little bit harder to understand because it uses the full glory of matrix mathematics. So you may get a little bit lost with all the symbolisms and all these indices and matrices flying around and transposes of matrices and so forth. So I would say it doesn't add anything new to understanding backprop. It adds the mechanics of the computation with the mathematical details, but the intuition is exactly the same as what you get using a much simpler notation. So review backprop now. And we may either continue on or we may get into the, you know what, let's do one thing. The mathematical details of backprop, let me make it as an extra session, because it is nothing but repeating the same thing, but being very careful with the indices and the layers for the w's and the a's and that's all it is. i'll do it in a separate session so that those of you who are interested can attend it those of you are not can sort of skip it and stay at this level of understanding. those of you who are not can sort of skip it and stay at this level of understanding. But this is how neural networks work, guys. You need to update all the weights. To update all the weights, you need to take the gradients with respect to the activations. Layer by layer, you need to back propagate that. And when you back propagate, you are able to find the gradients of the weights in the previous layer and the layer before that and so forth. So a wave of back propagation takes place in which all the gradients get evaluated layer by layer. That is back propagation, a very, very important method, right? The workhorse of the neural network, it's what makes the neural networks work or learn, but that is it. Any questions? So to begin the back propagation process, are you changing the weights of just one of the output neurons or all of them? No, no, no, all of them. So what happens is that, so suppose you have a big neural network, right? With layers after layers after layers. Input goes in. And if you think of this entire thing as a function G, what will come out is the final layers activation, but essentially a Y hat will come out, right? Now this prediction, you can compare to the reality y to create a loss which is a function of y hat y right whatever function is here i took y hat minus y square some squared error loss but whatever function it is doesn't matter when i can compute the loss now i can compute the gradient of the loss with respect to the activation of the last layer right and from there what i really need is somewhere in there are lots of these weights sitting there at some layer and what What I really want is this, and backprop gives me a way to compute all of them. Ultimately, what we want to do is compute this guy with respect to weight, which is this giant thing, W, every single possible value of these Ws, Ws and Ls, IJL, because that's all the weights in the system and the biases, include the biases. But once this is computed, the rest is easy because W, IJ, L. So first, next stage is compute the gradients. Once the gradients are computed, the next thing is just to take the learning step, which is, because this is the only thing that you need to know to be able to take a step of learning or wiggle the weights a little bit to improve the loss. So the steps are forward step, forward to compute loss, forward will compute y hat, compute loss, three, compute back prop, and compute gradient, gradient, four. gradient gradients for like learning step that is it learning step that is gradient descent the learning step is of gradient descent that is it okay so so it's just one whole congruent process that's why all the weights are all the weights and all neurons are making change okay that is it and that is it and that's that is that is all there is to this and now the pythor when you do we use the pythor's framework in this course you will see that this is very elegantly done So suppose your network is called network, right? A network function is there. You give it X to produce Y hat. Code will look like this. Then you would compute loss is equal to whatever loss function it is. Loss function, that is LF. Let me just and i'll represent it as l y hat y label this is the label value you know the actual label what it should have been then you compute the gradient and then you do the step literally the method is called step. Actually it's called backward. I think the specific method is just called backward. Backward will compute the gradients, and then step learns. That's it. Four lines of inner code. That is the inner step in the neural network training. And we will walk through the code on Wednesday, and you'll have fun with it. Once you understand this backprop theory and everything, you will see that the code is literally these few lines in training a neural network. And it's quite straightforward code. All right, guys. So with those words, it's only 10 minutes to 10. I'll leave the last 10 minutes for questions. Any questions? Sorry, one second. Kate, you have a question? Why is the label data? It's the actual reality, the ground truth. People often use the word the ground truth, the label, the actual value, so forth. Kaya, go ahead. Yes, so it would be best if people learn about the autograd feature of PyTorch. Yeah, that is in the lab. We'll do that next time. Autograd is what we will learn. Okay, so and if you can scroll up the part where you wrote this inner loop in detail, I just have one question. The inner loop, where is the inner loop? Yeah, this inner loop. I still can't see it. You can't see that ah yes it's finally scrolled up uh so the backward passes should be the third point right and the fourth one should be the optimization compute gradient is called backward and a fourth is called a step right okay now you do um fourth is called a step right okay now you do um oh sorry actually the word here backward learning step backward pass now this is this is the backward pass this is right backward pass and this is the learning step And this is the learning step. This is where we use the optimizer. Yes. The optimizer we use right here because I see here, you use the optimizer to compute the loss. Okay. Right. So mean squared error loss, uh, or cross entropy loss and so on and so forth. So once you compute the loss, the optimizer is the atom. Like, yeah, your atom is used here. Your optimizer comes here because this is the optimization step. Step number four. Let me just call it optimization is step number four. Okay. Is that clear, Kyle? Was today's session clear to you? Yes. Any other questions, guys? Is this all clear or was it like information overload today? Anyone? Any feedback? information overload today anyone any feedback this is about as hard as it gets guys after this we'll have a pretty smooth sailing for some time it's good maybe once we repeat it a little bit summary, then it makes sense. Yeah, and we'll iterate over it. Yeah. Alright guys, if there's no other question, then I will end. Then I will end for today.