 So now we are going to start with a new topic. It is a new architecture, different from what we have been dealing with so far. It is called a convolutional neural network. The convolutional neural network, they use a concept which is almost like a mathematical concept of convolution almost like that. networks. new neural networks. Just as an aside, if you remember, what have we been dealing with? We have been dealing with hidden layers, right? And then another set of hidden layers in which every one of these is connected to every of them, right? If you remember, we from this i don't know i'm just trying to make all of these interconnects by now you see that this is a mess this is a very very densely the word is densely connected so when you take a layer how many layers did i take just five and five and you can see that there are lots of things let's simply use the word lots of interconnects each interconnect interconnect has a weight parameter the weight parameter isn't it so this region if you look at it it looks dense and then you can imagine another layer and then that also has a lot of dense connections and so these sort of networks they are often called dense neural net or feed forward network and if you remember the word in Python to use forward why because the input goes X here it goes through the first layer, produces an activation, goes through the second layer, produces an activation, and it keeps moving forward till you get the final Y hat, the final prediction. word net not work this is a synonym for it and the classic pictures of urine that's that we see in our mind you see all of these dense interconnects between them the problem is evident from this picture the main problem that you see here is there's so many parameters can you imagine that if you make a dense layer that has let's say a thousand parameters thousand nodes and it is connecting to the next hidden layer which has a thousand nodes you're suddenly looking at one million parameters just off the bat just between these two hidden layers you have now one million parameters this becomes really problematic when you start dealing with complex objects like sound, like images and so forth. So we'll take an example of an image. Let's say that you have these days when you take an image, what is the size of an image? Quite often your images can be 128 by 128 minimum, or maybe like 256 pixels. If you look at the typical image that you take on the internet or something like that, let's say 256 cross 256. And then there are the three RGB channels, red, green, blue channels. So what it means is it is a grid of, it is actually a grid of three grids, three layers, and each grid layer, let's say this is the red, has 256 and 256. And there is some integer value, let's say, sitting there. Let's say four is sitting here like 11 is sitting here and so forth this is how the data presents itself this is how you store images or that's how you visualize images but now imagine that you're feeding this into a neural network of a dense network the first thing we'll have to do is we'll have to flatten it this into an input vector this into an input vector isn't it so how would you flatten it you could you could read it literally row by row right row by row by row by row you could read the data and then you could go from red to green to blue channel so you would end up with 256 times 256 times three if one of you wants to do the mathematics could somebody tell me what number it comes out to be about two mega 196 608 196 608 so let us say 200 200k right approximately 200k right and now let us say that you try to feed it into a neural network you when you see this vast number of parameters let us say that you feed it into a deep neural network a first input layer is the first hidden layer so hidden layers are often called fully connected layers for the reason that everything is interconnected to everything else. So you will see this terminology all over the literature now, fully connected. People also call it synonymously a dense layer for reasons that we just talked about. And then because these are feed forward layers, data linearly moves forward, like they are also called linear layers. So, for example, you see this literature, if you're doing TensorFlowflow or pytorch they use the word dense if you are doing pipe I mean say tensorflow or a keras TF keras they use the word dense in the library and if you look at pytorch it uses the word linear right and if you go back and look at the code when we build our neural networks, we put all these linear layers together. So let us say that we create a linear layer of a modest 1024 nodes, which are 512 nodes or whatever you want to pick. You realize that you're multiplying this by 1024. So the number of weights that are there, because each of these things is interconnecting to thousand and there are 200k things. So what you end up with is 200 million parameters just reaching the first layer, just in the first layer. And that is when we haven't included the bias. So whatever this number comes to, plus you can include another for bias terms, which is insignificant, but it is there. So you realize that you're looking at something in the order of 200 million parameters. And so by the time you feed in a few more layers, you have a whole lot of parameters. So when you have a neural network that has a lot of parameters or any machine learning model that has a lot of parameters, what could go wrong? Would one of you like to take a stab at it? What possibly could go wrong? It will crash. No, that's a computational thing. Imagine that you have your your filthy rich and you can afford huge amount of hardware and i hope you do become that i just run slowly it will run slowly that's one uh see there is a more it might not converge yes yes see Yes. See, data point. Think of how many data points you need to train a model like this. We are talking of hundreds of millions of parameters. When you have hundreds of millions of parameters in a model, you need a vast quantity of data for it to learn properly, for it to get properly trained. So it will tend to have problems of overfitting because lots of models will overfit. And in terms of the visualizations that you saw, what you will end up with is a lost landscape that has a lot of non-convex cities and God knows where you'll get stuck. If you just run a few hundred iterations or a thousand iteration so it gets hard to train these things so then comes an idea how do we deal with this the this idea actually dates back to the 90s and it comes from researching it comes from you neurobiology and neuroscience of cat brain people studied how a cat looks at their image i whenever i think of it somehow it creates very rather cruel pictures in my mind how they must have figured it out by putting all sorts of electrodes in the cat or whatever they did but the fact of the matter is that they found that when you look at an image when the cat eye looks at the image and can I just imagine that this is the cat brain this is the cat body this is a cat so let's say that this is a cat eye it is looking at something it's looking at an image of a tree let us say and there is a squirrel here that that it is interested in. So what, sorry, did I? Then what you notice is this part lights up one area of the brain. This part lights another area of the brain and different parts are lighting up different areas of the brain. The entire image, actually, you can think of it as a plane here, some sort of imaginary plane here. And as it looks at this picture, different parts of the plane light up and they respond to different parts of the image. Have you got it? And so that inspired a different architecture that is for that is quite interesting actually and widely used when you think of self-driven cars and whatnot and this it all begins with this so let us build the idea so guys are you getting the main idea that there is a spatial orientation which part of the the brain responds to which you know there's a mapping this part of the image is being responded to by this part of the brain are we getting that as simple as that so you can almost imagine that this image is being projected somewhere in a conceptual sense. Imagine that there is a screen in the back of the cat's brain, and this image is basically being projected there. So each little piece, this squirrel is activating here, and this little thing, whatever it is, is activating here and things like that. So based on that intuition, we created an architecture, but it needs the notion of something called convolution. Now convolution is an interesting concept. It is, it's a mathematical concept. I will give you the formula, but I'll deliberately give it to you in a wrong form. The reason is the way we use it in neural networks, we use what mathematicians would say, not exactly the right thing, not the strictly correct correlation. It is a slightly modified or a simpler version of it. It is in some sense slightly wrong, but we use it in deep learning because it really doesn't matter. There's no difference in using the right form and the wrong form. And the wrong form is more intuitive and faster. And it just saves us a couple of operations. It doesn't matter. So what it says is that it is this, the convulation of a function. So suppose you have a function, which is this The convulation of this with some other function is this. So imagine that there is some other function G and that other function, I will say is like this. Let's say that this is GX and let me write F in a more, something like this. This is Fx and this is Gx. What it does is when you multiply these two things together, you will realize what will happen. This is the peak of G. So this area of F will contribute a lot more. The product of these two will be much more than the product of G with respect to this or this. Isn't it? Do you see my point, guys? G is stronger here. So at this point is not a shining bright as far as this condgulation product is concerned if i look at a b c look at a g at a is just this much let me just call it g of c is also small. g of c is small. But g of b is pretty big. So when you multiply this function with this, what will happen is, do you realize that beyond a certain value, this function is essentially zero? So you can imagine that this result that you get is result will have effect only in this area more or less right it will this area a little bit in you know this part how should I say it only you know the major parts are here so only this little part of fx will get highlighted am I making my idea clear guys so I'll give you an intuition of what it is which is a physical intuition so imagine that you have a torch the torch is casting a light here is the torch and it is casting a globe and the globe is the intensity of the glow is like this if you were to measure the intensity of the globe then you would agree that it is very sharp here and less here if you are measuring the intensity of this am I making myself clear guys it will be very sharp in the center very bright in the center this is the light of the torch coming through when you're looking you're measuring on a screen you're measuring on a screen you would agree that it would be brightest here in the center and taper off gradually as you move away from it is this picture is this idea of a torch easy to understand this is a speaker torch and shine a light on the wall you notice that it has a focus light and then gradually it thins out from there did we get that intuition? Yes, sir. So now imagine that this intensity pattern, if it was one dimension is your g of x. So what happens if you think of your function as a wire hanging out there? So imagine that you have let me use a different color for this function. Suppose you have some function, right? And you use your GX. Suppose you focus your spotlight at this point, the same thing. If you have your GX shining light here, this is your GX and this is your FX. Now tell me what will be the product? The product of FX times GX. What will it be? It will be very little of this would you, so if I have to write it in red color now, you would say wherever this peak is, this value, it will really shine through, right? It will be thick. Then as you move away, the light will thin out. Another way of saying it is that in some sense what you are seeing is only this little bit. This little bit is visible. Yes. Isn't it? So the, the congulation of this, which is this integration, forget the integral, basically think it's multiplying it together. It is, it is essentially shining light on a little bit of that function, probing it. That's why you often call this the probes G X is the probe it probes it the function at a special a specific point and now imagine that you're taking your torch and you're moving your G X back and forth right so suppose there is a translation parameter here G X minus theta and as you increase theta what happens different parts of it keep getting highlighted do we agree with that but as you move your gx from here to here to here to here to here, all these little bits of the wire that you get to see, of the function you get to see. It's a little bit off screen. What's that? The function is a little bit at the very bottom of the screen. Oh, okay. A little bit off. Oh yes. So what we are saying is that this GX acts as a torch that shines light on bits of the function everywhere. To do so and just to take the, in some sense, the accum some accumulated like you say that a total somehow aggregate this value right is to take the integral right uh and we won't go into that part so the definition of the function and this also by the way not correct. So you say that at this particular value of theta, the, the con, right, of x at theta, this point theta right what is it at this particular point at point theta let's say it will just be a function of theta this location what is visible intuition would be what is visible what is the sum total of everything visible and so now let's relate this idea back to that cat so again to recap we had a cat and there's a tree and there is a squirrel and so forth and there is a cat looking at this squirrel and then in the back of the cat's brain is a plane in some sense right and let's give it a tail also there we go and this little cat is staring at the picture so you would imagine that it is as though this area of the brain it is ignoring the entire screen but it is shining a torch only at one region of the scene does that intuition make sense guys yes area so it is almost as though this has a GX right And you can have the theta parameter to move it back and forth, but let's forget that. We'll be sloppy in our notation. So this is shining a light here. This region is shining a light here. So shining the light and observing different areas of it, mathematically, it is the same in the mathematical language of it is you're convulating the scene with the filter what you have this torch the formal word of the the formal name that people use in the in the literature is they often call it i call it torch but people don't call it talk so you won't find the word torch in the books this intuition you'll find the word filter or kernel these are filters of kernels and in a way you can relate to the word filter so what this filter does is it will filter and look at only one region of the landscape so what you do is you convulate use the different areas different regions are convulating or they are convulating across different scenes or shining light across different scenes or different parts of the scene right so once we get that intuition into us now actually congulation neural networks become absolutely obvious. We just try to reproduce that in a computer, in a deep learning architecture. And literally that's exactly what we do. So well in this world, computer science is discrete, right? So what we do is, let's say that we are looking at a scene. So first imagine that the scene is, I'll just draw the scene out, right? Then I'll write it in numbers. Let's say that the scene is, there is a shift from black to white. Are we together and then there is a also just for fun let's say that this is white and just for fun let us imagine that there is another diagonal black line going across so these are colored regions regions. These are colored regions. I'll just take a very simple scene and now let's see how would we detect it. So what we do is we take, so this scene would be a bunch of numbers. So blacks are usually, let's say that I'll just simplify it in a very simple way as five, five by five by five so imagine that the scene is written by five pixels by five pixels one two three four five so what will happen is a blacks will have values close to zero white will have values close to let let me just say, 10, just for the sake of argument. It could be F, but okay, let me just say 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, and then I might as well add a diagonal also of black. So what we do is, let me just draw these lines here. And now might as well nuke out the diagonal ones also. So this is black and this is black. Black to zero. And then this is not 10. Actually, I shouldn't have used ten. I wish I had used something else because one and zero look so similar. Ten and zero look so similar. Suppose this is nine, right? And this is, let me convert all of these to nines so we can easily 999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999 of this this particular image mathematical representation of this image this is how you save the image in grayscale you give it some value so now let's think how can we shine the light at places and detect things okay so the way you do that is slightly different actually uh cat brain is pretty smart it can do a lot of things but we do with very simple ideas let us say that we take what is called a filter and that filter has property like this it looks for vertical edges so let us say that I put one maybe zero my this is a comb by the way this is the cliche almost everybody starts out with this example so I'll also start out with this example something like this for some reason 1 0 minus 1 1 0 minus 1 is pretty common so this is what you call a filter right or your torch I think of it as a stencil you know for another analogy that I give it is stencil so you know children do this they'll take a page right and suppose you you have a V cut out on the page and you ask the child that on this wall there is a V go match it what will the child do suppose this is the wall and somewhere there is a V let's say that the sizes match for the sake of the argument. How will the child do it? The child can't see. Let's say all it can do is if these two match, he will know because this is all dark colored, let's say. So his way is, this is a cutout. He can take this page here. See, are you seeing the inside area? Is it all black? So it is not all black. So the child says, no, this is not it, not it, not it, not it. And ultimately the child will end up bringing the filter to this region, to just the right place at which it would have an exact match. You will have an exact match to the stencil. Would you agree? This is how you can just scan all over and you would know that at one location you'll hit it. And then this would match this is this intuition clear guys yes yes it's a simple intuition so this is exactly what you do this this filter or this is a stencil that is deliberately looking for a vertical edge. And let's see how does it do that. What you do is you take this stencil and first you put it here. And this is the convolution part that comes in. Remember I said that the convulation is F, the initial image, Fx is the initial image, the actual image times the, well, I shouldn't say the actual image, I shouldn't say filter, because people in this literature use F for filter. So let's say that you conjugate the original image let me call it the original image O O with this at this particular location it multiply the two and the integral becomes the summation so let us do that what we do is we multiply one here with zero here. So it becomes. Should the window size match? Yes. So you notice that three by three. So I just, oh, sorry. You're right. I made a mistake. Thank you for pointing out. This is, it should match. Because this stencil, you're just bringing this stencil, this filter and putting it here. So let's see what happens. So thanks for correcting because this tensile, you're just bringing this tensile, this filter and putting it here. So let's see what happens. So thanks for correcting me. So now what happens? Zero times one is the first row. This would produce zero. Zero times zero is zero. Nine times minus one is minus nine. So we got a positive value. And then once again, you can continue that and you'll realize you'll get another minus nine, so we got a positive value. And then once again, you can continue that and you'll realize you'll get another minus nine. Are we together, guys? You add both of these up, minus 18. So what you do is you go and you start building a cell. Here you put minus 18 as your answer. Now you take this tensile, you bring it one step below right and you would realize that if you bring it one step below actually actually yeah okay so you bring it one step below it will still be minus 18 you bring it one more step below it will still be minus 18 right and now you you take the stencil and you shift it let us say it has gone to this location oh sorry let me make it wider yeah let's say that the stencil now moves to this location it is still your 3x3 stencil right so now what happens is you have shifted it by one and you again try to start multiplying these two so the first column is once multiplied by zeros you don't get anything the second column is 990 multiplied by 000 you don't get anything the third column you tend to hit a jackpot you get 999 times minus 1 so you come up with minus 27 right and now you bring the filter down one level and one more level you'll realize minus 27 minus 27 do you agree guys do you see how I got minus 27 here? Yes. I think it will be 18 I think for the next one because this is zero right? Oh no, you are right, like minus 27. And then you move it one more step in this direction. And now what happens when you let me use a different color. So guys, I'm a little bit sloppy. So sometimes I make mistakes. So beyond the watch out, I can make mistakes. So isn't it minus 27 minus 18 minus 18? Yeah, because this is 0 in the fourth. So it will be minus 18 minus 18. Here is minus 18. Yeah. Minus 18 minus 18. All right, I'll take that. So it means that you guys got it. So now let's look at the green one. 1, 1, 1 multiplied by 9, 9, 0. So you get 18. Yeah, right. So you get 18 plus, well, 9 multiplied by 0, you don't get anything, plus 9 multiplied by this. So that will be minus 27. So 18 minus 27 is minus nine. Do you see that guys? It's minus nine. And then you bring this filter one step down. I don't know. The zero is coming in our way, right? It's screwing up our computation. I feel like cheating and putting a nine there. No, no, no. It's a zero for that stuff what happens so let's think about it if I bring this here the filter comes down here now you're multiplying 9 0 9 with 1 so it means that you'll get 18 once again plus next column is 0 and the next column here is 990 990 would be minus 80 so you would get 0 here and I believe you would similarly get 0 or something like that here you'll get a 9 positive 9 a positive 9 right you get a positive 9 so now guys look at this image if you look at this result at the same pixel width and pixel height in the convoluted image also do we have to you know create borders on both height and width. We'll come to that. We'll come to that. Hold on. So what happens, guys, is if you do that, some things begin to stand out. Do you notice that they- Can you scroll a little up? Yes. And by the way, there may be some little mistakes here. So one thing you notice that all of these are vastly negative numbers relative to this column. Isn't it? So if you were to actually attach colors maybe gray, neutral, positive. So what happens is that, but this band will show through, right? You will see that this is a rather dark band. And what you're detecting is an edge. And if I had not, if I had done like a simpler version, like if I had put a nine here, and nine here and nine here, in other words, if I had made it very sharp, these values would be even more extreme. You can play around with it and you can see that if you try to paint this the result the congelation result what you have detected is a sharp contrast as you go in this direction in other words you have detected vertical a vertical edge. So do this guy is a simpler version. I shouldn't have stuck in the diagonal. Suppose you have this, you have a one, two, three, four, one, two, three, four. And suppose at zero zero zero zero zero zero zero zero zero zero zero nine nine nine nine and see what happens when you convulate this with this filter the way that we did as a practice thing this will be one one one zero zero 0 minus 1 minus 1 minus 1 just see what does it produce you want to do we want to go through the simpler exercise all over again and see what it produces let's do that so this when applied to this region the first column produces 0 second column produces, third column produces minus 27. Then you move the filter one step below, minus 27, minus 27. Now you move this filter across to this place and what you get is, suppose you have your filter like this, you multiply this, so you will get only this part. Again, minus 27, minus 27. That's what I seem to be getting. Correct me, guys, if you see something differently. Yeah, minus 27, minus 27. And then you move it one more step. You come here, third step. You get 999, that is plus 27 0 and minus 27 so you get 0 0 0 do you see that guys so you end up with this so when you do this like do it in such a clear way do you see that the edge is immediately visible if you were to paint it giving these color values any kind of a image renderer will make this black absolutely black or very negative and this as a different light lighter let me not call it black but let me call it darker can you scroll scroll up a little bit so guys what is happening You guys are not seeing the full image or what's happening? The full screen in the picture in your screens? No, just one line is all that's going. See if it's not... See the grids. So you see that you see a vertical horizontal, the vertical edge. So if you look at the history of the the computer vision community people have taught a lot about these filters they are filters this this detects a vertical line another thing can detect horizontal lines you know if you do the filter one one one zero zero zero and this is a three by three filter right you can have filters of different sizes typically you take an odd number times odd number you can apply this filter and if there are any horizontal lines you can detect so why does it matter to be able to detect those lines so suppose you have a picture you have a house what happens a vertical picture vertical edge detector will go let me call the vertical edge detector it will go and detect all of these lines would you agree these lines in the image. And then a horizontal line detector will come and it will detect all detect these lines. Do you see what it is all leading up to? That between the three filters, the horizontal edge detector will detect some features, vertical features, vertical edge features. The horizontal edge detector will detect all of these features the green features and the diagonal edge detector will detect the the basically the slant of the house the roof but between the three of them in some sense you can logically think that they have pretty much captured this picture captured the image or the essence of the image. They at least have the specific ingredients. And so you can say that if I could feed this into some classifier, all of these features, detected features, you could feed somehow into a classifier. You could be able to tell that this is a house and not a not a dog are you getting some sort of a general sense of where we are proceeding yes the main idea but here comes a beautiful thing You don't do it at one stroke. What you do is something quite interesting. And I'll just make block diagrams before we get into the nitty gritties. So suppose you have an image. Now, image typically comes with, actually, image comes with, I represent the image in this way, input. The reason is there is the rgb channel the three three here and then images whatever let's say that your images are even to be modest 32 pixel by 32 pixel right suppose you do that then you realize that your image is actually a three-dimensional object if you think of it as that because there will be a grid of numbers here and then there will be grid of numbers here right it's a cube of numbers what happens is that you you take filters you take one filter let me call it vertical edge another filter called horizontal edge another filter called diagonal and then you can create more and more filters these are all filters like this is this this and each may detect its own little piece so what have they done they have been there when you produce this output the congulation they have done primitive geometric pattern geometric detection or geometric primitives they have detected the other way around geometric primitives you know little edges little diagonals free natives so it's a just a fancy word of saying, these little pieces, you know, the little edge, the little corner that you can let detection. It has detected that, right? This is the lowest level features you can do, a very low level, level, or detailed, detailed feature detection because so now what you do is you repeat it you imagine that these little primitives they feed into another layer of filters congelation filters con layer and what it does is maybe it recognizes a box right there is one filter that will tell you that is there a box another filter and i'm just making it up another filter will tell you whether you have uh this another filter will tell you whether you have a circle right because cats will have circles things like that so it will tell you and those detections feed into higher level of abstraction and ultimately you go a few layers and what you see is that the layers many layers later of compilation later one of them is clearly forming a picture of a house right things like that are we together or another one if it sees a cat it's forming a picture of a cat and things like that so if it is a house then all of these filters they will all begin to agree with each other they will all be recognizing the house in some way or the other they'll begin to do that because they will all begin to put the literally the word is put the pieces together very much like if you think back to the cat and the brain of the cat what is the cat doing we believe we don't really know how we see, but the belief is that in a very rough sense, the external object, the external world projects a sort of an image as though on an internal screen, there's a bunch of filters. it sort of puts it together, puts the pieces together and constructs recognition of what it is. In the same way, we are doing it here. So when you do it here, there is a nice term for that. It is called representation learning. It's a very important term. And in a way, it's a better term for deep neural networks. presentation learning the presentation learning and in many ways a lot of people prefer that instead of calling it deep learning this subject it should be called representation learning and the main idea of this representation learning is that as you don't, you don't do all of learning in one shot. So imagine that you had just a logistic regression model. Input goes out and output you have to tell whether it's a cat or dog, right? Or a cat or a house. You don't have a choice. You can't do systematic learning of primitives then higher order features than higher order features and so on and so forth but if you do things like that in congulations what you're doing is you're first picking up basic geometric primitives then you're taking next order permits next order and the next order and the next order now what i've done on your website is i've put some couple of sites and resources and compilation links. You can see this in a very visual way, how a neural network is recognizing that. Now there is a little bit of a mathematics of basic arithmetic. Like if I start out with a filter, like n by n cross number of channels, by the way, number of colors is called number of channels. Right? Initial input is this. Suppose I start with filters. So what is the filter size is typically written as F. How many filters I have in there? So this you would call the zero layer, the input layer. This you would call the first layer, remember? And then you would have a second layer. So in world of neural networks you would say that let's say that the filter is three by three if i say f3 it means is three cross three filter right now how many filters i have this is the n1 so i may choose to have a certain number of filters the minimum number of filters that i need how many filters would i need input right so each of these filters by the way has to be 3d why because it has to match the input channel but i can have lots of filters you know lots of these filters matching the dimensions the channel dimensions of that input and i can have this so n1 could be anything let's say that n1 is equal to 10 right and so your n times n and usually what happens is as you scan there might be a couple of nuances or extra bits you can do what you can do is if you look at this picture five by five let me just take four by four suppose you put a maybe let's make it five by five suppose you took a three by three filter so now this filter as it moves in this direction you realize that this guy it was was visited only once, but this guy will be visited twice. This guy will be visited twice, three times over. Do you see that guys? And this guy would be visited six times because it would also be visited in the downwards journey. And this guy will be visited nine times right so what happens is that you visit different pixels a different number of times which is not good because suppose something very crucial is there on the edge you may miss it so what people do is that they go and pad it they will say for example that suppose i were to pad it by two right and fill it with some numbers just maybe fill it with zeros i extend it and then the image in mathematically and just pad it just stick zeros into it zeros and so now you have to start your filter from here. This one. You have to start your filter from here. And you have to skip. And so all the elements would be equally visited or at least better visited when you do that you are the the process is called padding so when you do filters and you use filters you typically specify how much is your padding usually determined by this padding right so there are two kinds of paddings that people use right so there are two kinds of paddings that people use one is called valid padding valid padding says that because we really don't know what the value outside would be so we are not going to guess we are just going to say padding is equal to zero in other words no padding right when you do no padding then your image shrinks you know the next layer image this When you do no padding, then your image shrinks. You know, the next layer image, this output of this, when you conjugate this, you will end up with new images, new things which will be smaller for valid padding or absence. And then there is another padding called the same padding. These are, these are little technical details, guys. You don't have to remember it because as you do, it comes almost by osmosis. The osmosis, this padding says that you need to pad in such a way that it is the size of the input and the output remains the same. This layer, the layer one size, the first layer size, the output. So these are filters. Could you please scroll up? The first layer size is exactly the same as the input size. The size is preserved. right so input and output of the filter con is the same that is why it is called In same pattern they are preserved. Okay. And give me a second. Yeah. I have a question. Yes. How does the number of channels of the RGB colors map into the filters that I didn't understand. What happens is that each of your filters,, I just do it as a two dimensional object, but it is actually three dimensional. It is the let's say this is three cross three, but how many channels are there? Let us say that there are three channels, then here there's a third dimension three also here. That's all it is. So you're applying a three-dimensional volumetric you're matching see this is a volume right in this beginning place so you're matching that whole volume filter is a volume because your input is a volume so it is volumetric matching That's all it is. Nothing mysterious about it. But then how many filters you have here in the first layer, that is totally your choice. So that is the concept of padding. Then there is also the concept of stride. And it also adds another degree of flexibility like what it says is that when I slide the the the filter forward do I need to just move one step or can I jump two step or three step so that is called the amount of jump jump while moving the filter that's all it is how much should I move by the way the same padding right there is a simple expression the amount of padding that you should give is the size of the filter minus one over two if you give this and typically you take filters as odd numbers so minus one will be even and so even will divide nicely with two sorry even will divide nicely with two and so this quantity will come out as a nice round number, nice integer. Typically, it will be 1, 2, 3, 4, whatever it is. For example, if you take a filter 5 by 5. So 5 minus 1 is 4 divided by 2 is 2. So your padding is 2. Are we getting the idea, guys? So that is it. Now, stride is typically written as S in the literature. Everywhere you'll notice that the word, the letter S is reserved for stride. It is literally how much you jump. So for example, you have a situation like this. So suppose you have a three by three filter. Let me just give it here. Three by three filter. Suppose your x is equal to two. Then what it means is the next will not be here. The next will be here. Do you see this edge jump to this edge it jump two steps so that is the stride and so you can play with different sides and so on and so forth then the question comes that suppose I choose a certain filter so at any given layer let's design a neural network suppose i have an input and n cross n image size let's say 32 cross 32 i'll take a question in a minute uh cross number of channels let's say three rgb right so suppose this is it this is your input Right. So suppose this is it. This is your input. And suppose you create a filter of size F. Filter F, which is F cross F. Cross F, right. You can say five cross five or whatever it is that you want to do. Typically you start with small filters. Let's say three cross three and then your padding is padding is p and your stride is s and the number of channels that you have the number of filters that you have in the first layer is whatever you want it to be so the question is what is the output if you apply this filter and go over this image, what comes out and what is the output. The output of this will again be again a great three dimensional grid. So now what happens is this will be of size, size of this will be there's a formula here you can convince yourself that this is true I'll just write it here and you take the floor notation in other words if it comes out to be 3.6 keep it 3 so that will end up being the size, the new size of the output. So that's how it goes, guys. The size depends on this. You can work it out. Like, for example, in our case, we started out with, let's go back and see whether this matches or not. We started out with 5. This was 5 cross 5. This was three cross three. So let's work it out. When you do five n is equal to five. If you go back to the formula that n is equal to five, right? A p is equal to one which is equal to one we took p is equal to one the stride is equal to one so what does our thing say now what does it say n minus n plus 2p minus f over stride plus one right ceiling of it let's see five padding is well let's say that we didn't give any padding. So here, actually padding was zero because we didn't pad when we were doing this. Plus zero minus three divided by stride was one plus one. So what does it come to? Two plus one is equal to three. Is that correct? This is three plus. Now you can look at this image and tell that it is indeed three by three. Now you can look at this image and tell that it is indeed three by three. Do you see that guys, that the output you could tell using the formula that this is what it will come out to be. And that formula is just common sense. If you just think about it, that is exactly what you'll come up with. So now comes an interesting thought. So far you may say, okay, where is the neural networks? Where is the machine learning? And in fact, there is no machine learning in what we have done so far. Now let's bring the machine learning. The machine learning comes with this interesting thought. How do you design good filters? You know, you can say, I will have a vertical edge detector, I'll have a horizontal edge detector, diagonal detector and so forth. But see, in a given image, suppose there are no horizontal edges. For whatever reason, there are no horizontal edges. Then your horizontal edge filter is thoroughly useless. Right? In the same way, if there are no diagonals, your diagonal filter will be useless. So a lot of your filter will go useless if or in the same way if there are no diagonals your diagonal filter will be useless so a lot of your filter will go useless if you hand design it which is what used to happen then comes an interesting thought at each layer so what happens is when you apply a filter what do you do you take a filter which is let's take this weight three cross three filter uh let's just call this weight w1 w2 w3 w4 w5 w6 w7 w8 w9 right and this is just your filter one f1 you can have another f2 right have another f2 and then f3 more filters and each of these they can have let me say w1 maybe 1 1 1 1 1 1 1 1 and this could be w1 2 w, W3, 2, and so forth. You get the idea, right? These are different weights. And now you can ask this question. But ultimately, what we are trying to do is do a classification. Let's say we are trying to tell whether this is a house or a dog or a cat. or a dog or a cat so why can we not use a loss function and back propagate back propagate and learn the losses so in other words can we learn the optimal filters for a problem, for a problem to solve a problem by learning in the, in the backdrop, the in the backdrop drop gradient descent sense you know all that we have been learning so far can we do this gradient descent step and the answer to that is actually that is the breakthrough that's what makes it into a problem in deep learning so what you do is typically you go like this. You take the input image. You put a corn glare. Let me, let me just put it as a corn glare made up of so many filters and I'll just write it as this NC and one and one filters each of them are F cross F or whatever it is so this goes in so this is the corn one layer you get the output this is your Z this output that you produced so in other words uh going back up to our story uh this thing in the in the language of machine learning this thing that we produced this is the z this is the input x this is the filter think of it this is your weight vector because now remember that we are saying that these are weights that have to be learned would you agree that Z is essentially X convolated with W it's almost the same thing the inner product Z is this you see how the ideas come together guys this is deep and it is literally the convulation this convulation is the inner product, right? In some sense, in a convulation sense, not a direct inner product, because these matrices are not matched. But in the convulation sense, if you convulate it, it's not the inner product. It's sort of a convulated inner product. But it is as close as you can get to inner product. And now this begins to sink. We are already familiar with this whole notation that Z is equal to in the dense neural networks. We were doing this all the time. W. Dot X. Remember to produce the the inner product of this Now the only difference there. So this was for a dense net. dense layers. was for a dense net dense layers now the only difference is that we are changing the vocabulary and we are saying Z is W modulation X this is the con deletion symbol concept are we together it looks uncannily similar so what do we do to z do you remember what do we do to z z goes and gets what distorted using an activation function do we remember that a traditional and where sektron was this right the weights go in x z z is equal to w dot X and then there is a distortion function activation of Z right this output is that some activation function applied to Z do we remember this guys yes by now it should be familiar so let's do exactly that you take the Z and you apply some activation function I'm putting Sigma here just to say activation function so let's say that you produce an output, A of Z, let's say, A Z, right? Apply an activation function. Are we together? You apply the activation function here. And so this together almost makes, actually there's something else called padding another pooling a max pooling And there is also something called average pooling which we don't use very much these days but max pooling There is something so I've sort of glossed over something but this part a convalescent Layer is made up of this so let me write it this way one pond layer just like you have a dense hidden layer in a dense net one collier is made up of a the filters and con and convulation with it on go you should it B activation function applied and this looks very simple X becomes weight on related with X convoluted with X input and this that that leads to Z is equal to this right and then this is just applying some activation function of Z if you remember that's what you were doing in the dense net also very similar now there is one more thing that I glossed over that people do after this after the activation or this or before the activation, or this, or before the activation, they do one, it's actually after the activation, they do one more thing, they apply the max pooling. Max pooling is very interesting. It is like, it's this. So let me take this example. Suppose you have numbers. Let me take this numbers. I'll take a 4x4 gradient 1 4 0 2 and 5 2 3 and 1, 1, 7, 2. I'll just randomly throw in some numbers for no better reason. What you do is you take a window. Let's say you take a window, and this is a common window, 2 by 2 window. And you frame that window on top of this when you frame it on top of this now you look into the window and say which is the biggest value so here the biggest value is what is the biggest value here in this window, yellow window guys? Four. Four. So you write four here. Then you go to the, you take the window here and you ask in this window, what is the biggest value? Nine. Right? Then in this window, what's the biggest value? Eight. And in this window, what's the biggest value? Seven. the biggest value eight and in this window what's the biggest value seven and you say that i if i do max pooling this operation is called max pooling for historic reason means you pick up the maximum in the olden days what people used to do is that they would take the average it was much more common to do average till somebody noticed that you take the max is actually works better so the way people justify maxes here is one intuition I'll give you for max it is that see think of each of these numbers as a vote for a vertical edge suppose a filter is a word that filter is checking for vertical edge. Each of these numbers is saying around here, I see a vertical edge. So when you put a window like this and you take the maximum vote, what you're saying, how confident in this region are you that there is a vertical edge? Here, here. Do you see what I mean by that, guys? So that is the intuition that you think of it as. Maximum vote. see what i mean by that guys so that is the intuition that you think of it as a maximum vote vote in that in that neighborhood so it's like you're in the forest and some of you see tigers or see an animal right one says it's a deer another says no it's not a deer it's an antelope One says it's a deer, another says no, it's not a deer, it's an antelope. Another says it's a tiger. Now all things said, who are you likely to take more seriously? What can have the most consequences? You'd say, well, I'm safer if I believe it's a tiger. I don't know, is this a hand-waving argument? It's not very persuasive, but something like that. Actually, the honest truth is that why max pooling works as an extra step in the convulation, nobody really knows. We are just guessing. We're just throwing sort of heuristic arguments or post hoc reasoning. But the fact of the matter is we have found that if in the one con Blair, you put max pooling at the end of it, it just leads to stronger signal. It just leads to better results. So these three things make up of it. You, you convulate with filters, you activate it, use activation functions, your max pool it and so forth right so you do all of that then all three things you do then so now what you can do is you can have just as and typically in the literature you'll notice that convulation layers are typically represented as solid cubes rectangular cubes or something so output of this one one one one will go into the next conv layer right uh we'll go into next conv layer a con two so whenever you make you see these diagrams remember that it includes all of this condolation activation pooling actually I might have gotten the order of these two requests okay so you do that and so for example one of the old architectures is lean it now lean at five and I'll come to that but one rule is there as As you convulate and move forward, here are some rules. Successive convolars should have decreasing size and increasing number of filters number of filters and number of filters people also call number of channels this really should scroll up number of channels so the why number of channels increases and is the idea is that we are hoping that you're recognizing more and more things. Each filter is recognizing something. So as you learn more and more in different layers, right? But the size of all of these each of the layers it decreases the idea is that Now you're discovering higher order things In different places so that is that and when you do that at the end of it, it's a con metric So at the end of it suppose you end up with I. So at the end of it, suppose you end up with, I don't know what in this particular case we end up with, but let me just guess that you end up with 17 cross 17 cross, let's say that you have 20 layers, something like this. So you realize that this is a pretty large number isn't it a 17 crown 17 is the size of it and there are 20 filters 20 filters or channels there so then what you do this result you can flatten into a you do a flattening operation you can flatten into a single vector why you start reading it like this and then layer by layer you read it and you lay it out and you will get a single flat vector. Let me just call this flat vector F vector. Do you agree guys? You can take this three dimensional box of numbers, cube of numbers, and you can flatten it into just one long vector of numbers. So that is called the flattening layer. This is a layer in itself. And then once it is flat, remember that this is beginning to look like the X input of dense layers, or fully connected layers or fully connected layers, fully connected layers. So technically you can just suppose you're trying to distinguish between a cat and a house. You can use your softmax right here with obviously size two, dimension two. One will try to detect a cat. Another will try to detect a house, right? And you could feed all of this, this flatten directly into it because you're used to that theory by now. But what people do typically is they put some intermediate densities, fully connected layers FC1 FC2 In between and then they put the softmax layer So overall to summarize you have an architecture that looks like this you have some These are usually represented as cubes there goes to another little cube, which is smaller in size but deeper right which goes to another cube which is even smaller in size but really deep lots and lots of channels and this goes into a flattening layer and flattening goes into some fully connected layers FC one thenc2 and then let's say that you have a soft max for classification does this make sense base and so this sort of picture they are iconic you find it very often the moment you talk about congulational neural nets or image classification object detection so what is object detection you're looking at a scene it's a picture you have to identify all the objects in it and it is of paramount importance you know when you're driving when you're trying to do a self-driven car it is very important that that the camera the image that comes from the camera the the ai system is immediately able to recognize a pedestrian who is there in the middle of the road isn't it or some obstruction that's there a car that's in front of you and things like that. So for all of those things, this is your basic architecture for object detection, image classification, cats and dogs, or cats and house here and so remember that in the really first class we dealt with a couple of architectures we talked about Alex net VGG you guys remember that the very first lab we did we tried out many architectures on the image net data set right and then i asked you to just try or then we tried the c far also and i said that see just put your own cat or dog there and see how well it detects in the image that it's able to do that right so those architectures that are there they all look like this except that their convolation layers are not just one or two layers there this is by the way what i just put is almost the same as a famous and old classic architecture called linux 5 that's what it is two convulation layers from the input and then uh flattening into dense layers right so and then the soft max is the final layer and flattening you don't call it a layer you just call it an operation so that used to be the linux 5 and so for very successful actually in its days so but now we live in the world of VGG and Alex net and rest net and rest net as we just saw in the visualization we are looking at restness 56 and rest net 101 those numbers tell about how many layers there are and most of those layers are quantities now so we I would like to end at this moment the theory part by the way guys all of this will become very real when we do the lab but at this moment I'm just putting the ideas here one more thing we have a whole month that we will give to this topic so today as you can see I'm skimming i'm just introducing the basic concepts the reason is we this is it in one theory session i'm giving you a flavor of it so that i can get you started with the lab but later on we'll have one whole month to congelational neural nets and we'll do it really deeply both from a practice and from a theory perspective we'll redo everything that we talked but we are just planting seeds in your mind at this moment think of it that if you are somewhat familiar or you get the basic high points or ideas then later on it will become much easier to learn are we together guys so it raises one question so why why use pond nets at all? Why can't I just use dense nets? Those dense layers. Would anybody like to take a stab at that? A hint, I'll put something in front of you that should be a hint. Let me see. Let me go back up. Let's see. hand let me see let me go back up let's see yes look at how dense nets work and see if that tells you something yeah it's much more efficient as you know uh what's that see look at it guys densenet has so many parameters just the first layer itself had 200 like close to 200 million parameters right that kind of and then imagine if you created four hidden layers of five hidden layers you will be looking at billions of parameters isn't it it is extremely hard to train a neural network like that with so many parameters unless you have ginormous amounts of data it is not feasible or like dennis says you use the word inefficient so inefficient in that sense in contrast let's just see how many parameters are there in a filter let's go back and look at it so you're seeing that three by three is a filter so you have how many how many parameters are there nine just nine parameters isn't it just nine parameters isn't it like the basic idea is you make this filter that you apply it has just nine parameters and so suppose you add the RGB the three color dimensions it becomes well nine times three twenty seven parameters and let us say that you have ten or ten filters that you start out with which is a good number to start out with start Start with 10 and keep growing the number of parameters, 10 to 20 to 40, whatever it is. Sorry, number of filters. So let's say that the first layer has 10 filters. So now you have 27 weights per filter and 10 filters. So how many total weights do you have, guys? 270, isn't it? 27 times times 10 a plus if you add biased term to each of the filters if you add a biased term then another 27 so you have 270 plus 27 right not a big deal under 300 compare under 300 to 200 million you see the contrast the tremendous difference yeah 300 parameters are easily trainable but training 200 million parameters per layer is hopeless is that coming across guys that's the basic idea so people say that why is that why is this more efficient because the same weights we use and slide all over the image right so this is variously called by two names people call it spatial invariance means the same weight is sliding all over the image spatially to recognize edges wherever it is in a given filter so you have spatial invariance that gives you some people also call it weight sharing and all these ideas but all it simply means is you you don't change the filter when you're computing in the forward direction you just move the filter slide the filter around all over the image and the weights remain the same because weights remain the same the number of weights are fairly limited up to 300 per layer so what happens is that in an architecture like linearity sector remarkably most of the learning takes place here in the congulational layers. But in the end, you flatten and people add a couple of dense layers just to catch the non-linearities in the data right before making a softmax prediction and it turns out that most learning happens here here in some sense and most parameters exist here because even with two dense layers even if you take the dense layers to be uh let me say 512 and 128 or something like that you realize that still there are lots of parameters sitting there most parameters here you'll still acquire in millions like of parameters in your densities and that literally contrasts the efficiency of the congulation layers versus the denser all right guys so I'll recap what we learned and then I'll take questions what we learned is that on relation the idea was derived from studying the fact that different regions of a cat's brain are observing different parts of the scene, right? And so they're lighting up at different parts of the scene. Now you put that fact together with a mathematical notion of convolution. By the way, this is as I said mathematically I'm being a little bit sloppy. People would call it slightly differently but that's all right we won't get into that if you multiply this with this it's like shining a light into different areas as you slide your gx horizontally we are sliding lights at different areas of the curve and different areas of the curve get illuminated essentially that is compilation and that is exactly what we do with a this sort of a torch which more commonly is called a filter or a kernel when you shine it upon certain area of the image you get to know what is there if there is an edge hiding here if it is an edge filter it will shine the light on any edge that may be present there it will detect the edge and then you move on to other places shine the light on any edge that may be present there. It will detect the edge. And then you move on to other places. So that's what it does. A filter is like a stencil, cutout stencil. It's looking for specific things. But then comes the question, how do you know which things to look for and not look for? And that's where you say, hey, you know what? Let the machine learn what the best filters are, what the important features are that you should look for. By making all of these weights learnable in a machine learning sense, in a deep learning sense, let it be learned through back propagation and gradient descent. And so then you have the concept of representation learning. What you say is that the first layer, let's say it picks up little edges. By the way, we'll see these things in a very visual way in the lab. But at this moment, let's stay with the theory. So it can pick up little edges. The next layer will learn from those little primitives and maybe make bigger primitive squares and things like that. And the next layer will do something else. And by the time you reach the last layer, if it is a cat, you will notice that the filters downstream, and there are lots of filters, they all are beginning to say cat, cat, cat, cat, cat. And so it begins to get very clear that from many perspectives, each sort of filter in the later layer is learning in a different way and think about it in some sense that they're coming to the same conclusion. They're reinforcing each other. They're saying it's a cat. Right. And maybe I'll take a moment to show you a visualization of it. That would be a good way to end. That's be a good way to end that's a that's hierarchical learning and the rest of it are technical details like how do you do in keep increasing the number of filters decreasing the size and create an architecture don't forget to do max cooling those are minor details and so long as you get the concept those details you can pick up but i would like to end by showing you something showing you something and again guys all of these resources are there on your webpage in your course page I again encourage you to look into that so in in the fund resources where the fund resources experiments depart embedded projectors visualizing right okay let's try this one I think this one has something by the way this is basic javascript you can use it to detect things oh all right let's play around with how things work classify see you have this image input image right and it is trying to detect what it is go here and play with it you'll see how so just pay attention to this guys I wish I could stop it let me see if there's a stop button pause let me pause it okay guys look at this when you look at the activation guys, the input is this, right? When you look at the, the convulation layers, can you see some aspects of the horse showing up? Do you think these filters are telling you that it might be a horse? Correct. Yes. It is saying that that isn't it so it is then you pull it and now you go to the next layer do you notice that the number of filters increases and it keeps going like that right so you start out with basically primitives and then finally what happens is as you go down you do a fully connected layer and then you're trying to tell one of ten objects what are the ten objects these are the objects that it is trying to tell right so then it will come to the conclusion that this picture for example is neither a deer nor a truck nor a cat and this picture is a horse. How much of a horse it is? This one is likely to be a truck. Green is good identification. So you get this. Play with these pictures, guys. And the code is very simple. This is the beauty of it. You can take this JavaScript code, very useful, the content library. do you realize that you just need to add literally you can write it in a HTML file and run it if you know basic HTML you can do that you can even change the network and you can add your own so for example do you know you can go and change that this is a filter let's see if you understand it input is 32 by 32 and three colors This is by the way, the famous image net data set. So now let's look at here Condulation layers type modulation. What is it saying? This size is 5 Right. What is the size of what? size of filter it Size of filter. It's five. And then filter size in convolutions are always odd. How many filters you're having in the first layer? 16. Then stride is one, padding is two. Why do we have padding two? Because it will keep the size the same. It's a size preserving thing. the same right it's a size preserving thing so in the next layer again you use a filter size one five five five by five filter but you have 20 filters stride one padding two likewise against filter five number of filters is they haven't increased 20 and go change it you know just go change it and see how it affects your recognition and then as you can imagine you create the net and you just train the net this code without even knowing javascript i hope this code is looking fairly obvious to understand right and you can literally see how these filters are detecting little things And you can literally see how these filters are detecting little things, given the input, how they're going about doing their jobs. So, that's it. I'll end today with that. Any questions, guys? And there are many visualizations of the inner layers. I think on the webpage, I can't see the exact location. I have put many of those things here. Another of them is neural network paints an image. Let's look at this. How does it, the same, just using a density. It is trying to get a picture of the cat right it's trying to gradually. Make the picture. And I don't know if you can see that this picture on the right is gradually beginning to look like the picture on the left. So, You can play with different pictures. These are different demos, you can play with different pictures and so forth. These are different demos. You can play with that. And this is the MNIST data. We will deal with it in the lab. So you give it a data and you see how it is recognizing. Look at this. I wish I can pause it somewhere where is the pause button pause let's pause somewhere do you see that zero is the input and how each of the filters in their own different ways they are recognizing this yes they all are coming to a similar conclusion. And then after the re-low, this is the activating the gradients and so on and so forth. And you go to the next layer and the next layer and so forth. So that is convulation. This is a new architecture we learned besides the feed forward network. These are called convolation neural nets it turns out that once you become familiar with all these little bits of mechanics that is all there is to compilation viewers architecturally they are very simple and they're insanely effective for any kind of spatial recognition like image is a spatial object any kind of spatial recognition spatial classification convolutions do marvelously all right guys and so then i'll stop the recording and take questions Thank you.