 So today to recapitulate what we'll do is solutions to the homeworks we have the river data set the flat data set the California housing data set, the California housing data set, and the breast cancer data set. Today I'll walk through the solutions to these, first using just feature engineering, then using decision trees and random files, the two methods we learned in the last two weeks. If you missed either of the two sessions, remember the Decision Tree recording is online and did I, I believe I'm yet to put the Random Forest recording online. It's probably online because of the live YouTube. So I wonder if you can confirm. Oh yes it is live. I can see. It's live, yeah. So all of them are live and so please feel free to go and recap recap on the material or review the material after the class in case you miss it so with that preamble let me go straight into the lab part of it and I'll share my screen there we go Give me a moment. All right, guys. Are you all able to see my screen? Yeah. Yeah. Entire browser you're able to see, isn't it? Okay. And all right, guys. So let's get started. So you remember, we load the the data we drop the null values and so forth. Remember the pandas profiling is a great tool to quickly get Descriptive statistics or preliminary descriptive statistics, which is not everything don't always completely rely on this. This was the flag data set. We got the descriptive statistics on it and then it gives you quite a bit then remember the first thing we do is we extract we separate if you are doing supervised learning either classifier or regressor the first thing you have to do is extract out the target variable from the data and now you have a separation between the feature set the features that feed into or the independent variables and the dependent variables another way to say is input and response capital x by convention is the input capital and little y is the response why is that because x would be a matrix because there are many features producing the input and a row is like there are many rows so it's a matrix whereas y is a column vector. Matrices are capital in general convention people follow and why they write it as a column vector. Actually I take that back in networks, all those conventions go out of the window because everything is matrices. Alright, so we visualize this data. If you remember, this was the visualization of the data. And just to remind you, this was a theory of logistic regression. Basically, what we said is log odds is the distance from the decision boundary. By the way, I have a video on our website, on our YouTube channel, about distance from the decision boundary. You might want to consider watching that. It is quite relevant in the context of, let me show that to you. It's a very short video for 15 minutes. Why do I call it distance from the decision boundary? It is because youtube.com slash support. If you go back, go down here, you see linear classifiers, distance from the decision boundary. This is the video I'm referring to and you will realize that at the end of it, let me go straight to the end. Yeah, you will come to the conclusion as you do here that this equation or this beta naught, etc, etc. Here, w It represents a distance from the decision boundary. In other words, this thing is the distance from the decision boundary. from the decision boundary. All right, so that's a little bit of theory for those of you who remember your ML 100. We'll build a model. If we try to naively build a logistic regression model, as we realize it's a disaster. It's a baseline model. It just degenerates to the baseline model where the majority class is the predicted class or the zero R model. Next, we try it with polynomial regression, polynomial logistic regression. We expanded the feature space with polynomial terms. When we expand the feature space with polynomial terms, we get a pretty complicated model. You see how complicated this model is and the fifth degree polynomial. But then if you try to look at how good this model is still the accuracy is only about 69 percent for the zero which is the real positive test case here there are fewer zeros here 85 for one but uh it doesn't so then we do a more careful analysis by extracting the river modeling the river with the line oh where where's the line gone for the river with a line. Oh, where's the line gone? For the river, okay, seems to have not run the visualization. Look at the model diagnostics, model diagnostic look good. These were the model diagnostic. You remember we talked about having a homoscedasticity in the residuals and the residuals must show a bell curve distribution which is see on the side all of those are good things a good correlation positive correlation between prediction and reality which again is here the cooks distance to show if any points of high influence none of them seem to have the all green well and finally we are here it is we visualize the model over the data so we now have captured the river the center of the river and so it helps us extract the feature namely just the distance from the center of the river and we are done after that we just have to build a logistic regression in one variable just look at this we are using a feature space of just one variable we ignore all the input variables and we are just using D, which is Which is a new variable. The future that we extracted. And when we do that, we get a model that is actually already from the confusion matrix begins to look good. And when we do the classification report we suddenly see that it has very good accuracy 84 person for the zero class generally accuracy 84% for the 0th class generally overall precision the harmonic mean of these two is pretty good actually sees 89% so 89% accuracy is a pretty impressive accuracy even for the 0 the positive test case we get 84% pretty good accuracy we can see how the precision and recall is for each of the two classes. For the zero positive classes, this 85, 82, 83, and so on and so forth. Pretty good. The ROC curve, again, if you remember what matters is the area under the ROC curve. When we look at the ROC curve, the area under the ROC curve, it's an impressive 96% of area under the ROC curve it's an impressive 90 96 percent of area under the ROC curve pretty good how often do we get the mixed up so in for blue there we misclassified some blues as yellows the sand and some ones we misclassified very few number of ones we misclassified as zero. So this is it. So now the question is we learned about decision trees, isn't it? The theory of decision trees, just to recapitulate, we learned that a decision tree goes about recursively splitting the region. It uses either Gini or entropy as its impurity index or disorder measure and then it uses information gain as a criteria to split where to split i hope this time you all understood information that reminds me i have to send you a survey guys but do please attend that respond to that survey like you notice if you feel you didn't understand something i always repeat it right so that's how i discovered if you did so now that theory when it comes to practice what does it become it is a pleasant experience it becomes just three lines of code one line to uh this is just importing the library this is to build the classifier do you notice is to build a classifier. Do you notice that you build a decision tree and you fit it to data training and training and training data? You make predictions you get predictions and then what do we do? We look at how good those predictions are. We look at the confusion confusion matrix the confusion matrix looks pretty good the principal diagonal has most of the elements and off diagonal doesn't seem to have that many elements which is very good if we do a classification report you see a accuracy of 87% not quite as good as our feature engineering but still respectable so this model is not really as good as the previous are doing it carefully by hand but still it's a respectable measure now when I build this decision tree one of the things I didn't specify here is how deep was the tree so we can specify actually those parameters so there are a lot of parameters that we can use to tune the tree and perhaps it will make it a little better then once we do that we can create as you notice where my decision tree on the river data set right so this is it overall accuracy of 87% a very good accuracy and then you said it just shows how these nonlinear methods or decision trees are quite effective and you see the reason why when decision trees came onto the scene people they really with it, you know, they got exaggerated statements that this is all you need to do. And she learned again. So those things have changed. Of course, today decision tree is not considered the state of the art by any means, but it is there. Nonetheless, So, Yeah, go ahead. Yellow brick classifier any reason to use that? Yellow brick classifier? It is a library that helps you visualize models. Model diagnostics. It helps you visualize model diagnostics. So here we go. We look at this. So guys look at this decision tree produces a roc curve like this for a moment just stare at it and tell me is this better or worse than what we did in feature engine compared to this what do you think guys are feature engineering roc curves or better or the decision tree roc curve is better feature engineering one is better. Yeah, it's much superior. And it also shows guys that if you can do feature engineering, it is the gold standard. Machines will only try to achieve that, but not always be successful. Sometimes they are successful. So I'm quite off in that, but it comes close. When you try to visualize this decision tree, remember the whole value proposition of decision trees is that they're interpretable. I'll let you decide how interpretable this picture is. How many of you feel this is a highly interpretable picture? I'll open it in a new tab. Where is it? Yeah. And I'll expand it. Yeah, and I'll expand it. Even in its expanded form, how many of you are delighted that this looks like a very simple explanation of that data set? You wouldn't feel that, isn't it? It is almost a black box, though in principle decision trees are highly interpretable in practice the moment you give it a non-trivial data set you begin to say all you see all interpretability going out of the window whereas a feature engineering it was so easy for us to interpret we had an intuitive explanation we said distance from the from the center of the river that is the difference this is why good good feature engineering is the gold standard. We will do this now again with random forest. When you do it with random forest, once again, we studied the theory of random forest. You use a lot of trees and so on and so forth. You see me do that. We'll grow a lot of trees. How many trees will we grow? We'll grow a lot of trees how many trees will be grow will grow a forest of thousand trees when we grow a forest of thousand trees and by the way number of jobs in parallel this is the degree of parallelization how parallel you make it so the more parallel you make it the better so for example if I say number of jobs that will run in parallel is hundred right obviously if your machine has that computing resource then by all means you go into it and if you do that notice that this decision this random for is built on my machine in 1.6 seconds but don't expect the same on your machine if you have a gorilla machine it may finish even faster if you are running it on a laptop expect it to run for a minute or so let it how is that a standard for thousand trees or how did you pick that yeah that's a very good question see my basic rule is I never take more than 400 trees or 500 trees I count the number of predictors here the number of predictors were 11 right oh no sorry not 11 here the number of predictors is two right okay i multiply 400 by number of predictors so it's uh see we live in computational plenty. So we can sort of go overboard. There's nothing wrong with 200 times to 400 times to 800. So I approximated it to 1000. It quickly ran through. But do we really need 1000 trees? Probably. Maybe I could have gotten away with 400. I usually don't go below 400. Okay. The default is 100. And the reason that you could learn in all these things have a modest default is because they're trying to make sure that the code runs the library runs on people's laptops and be pretty underpowered So that's that Sorry, what did you mean by number of predictors is to already get that the What is the dimensionality of the Okay of the PK set, X1 and X2. Oh, okay, okay. That's cool. Okay. So that is that. So now that is all I'm talking about here in the random forest. And so you run it, this is the level of parallelization that depends upon how good a machine you have. So model diagnostics is again simple. The same model diagnostics that we have been doing. So do you notice that, guys, certain things become boilerplate? In every analysis, you'll do that. Once you build a model, if it is a classifier, you'll need your confusion matrix. You'll need a classification report. And you will do the ROC curve. Right? So once you practice this, you will use it for the rest of your life. That is the value of it. While this is good to do, I must also say that when you interview people and you see people from different programming houses or different data science houses come to interview with you, most often you find that people are sloppy. They don't actually Do the model diagnostics carefully. So you have to just learn to do it right once and then do it. Alright, so with random forest, you see that the RFC curve is it is it looking better than the decision tree guys Yeah. Yeah. So can you can you guess why is this better than the decision tree? What is wrong with decision trees? There is a limit to the extent or that the branches of the tree we are limiting it. No, actually the problem with decision tree is exactly the opposite. It tends to just go and overfit to the data. Remember, it fits the data hard, right? And so you have variance errors. But a random forest, because it's an ensemble of lots of trees, it reduces your overfitting problem, because it's a voting machine. lots of trees, it reduces your overfitting problem. Because it's a voting machine, different trees are built or looking at different aspects of the data. It is very hard to overfit because each tree gets to see only a small window of reality. So even if it overfits, it will overfit to a very small window of reality, never to the whole reality. So random forests therefore are much more robust against overfitting and it gives you a clue that the reason the decision tree failed is because it was an overfitting problem and that showed up right look at the tree it built when you look at a tree this complicated what do you compute from this very very complicated tree which you which is practically impossible to read what you conclude is that it has gone in over fit to the data yes that is it see complexity unnecessary complexity we are able to explain this data in one line distance from the center of the river but this decision tree needs such a complicated explanation to explain it right so that's a classic illustration of overfitting then random for certainly seems to do better but even if you look carefully you notice that the ROC curve of this is only 95% remember I hand created classic I mean your feature engineering gave you 96 person so random forest does come close and you can tune it and try to tune it and so on and so forth right so what you can do is a you can try to take one tree from the random forest. I just took the first tree from the random forest and said show me how does it look and you realize that each of the trees still looks pretty complicated, isn't it guys? But because you are doing an ensemble over this comp all these different trees your answers are better now one of the things that you get in decision trees and random forests is a overall very rough measure of how much did each of the features matter in helping you decide about make a decision whether something is river or not is is a river point or not. It is called feature importance. How important is some feature? Just to give you an intuition, let's say that you are selling ice cream on a beach, right? Then you may be, temperature is an important feature. Wind is an important feature. Day of the week is an important feature. Temperature is an important feature, wind is an important feature, day of the week is an important feature. But suppose in the data somebody has given you the number of penguins in Antarctica who jumped into the sea that day. Just because you have that as a column or as a feature in your data, it doesn't mean that it's a very relevant feature. So in feature importance, its importance would be close to zero and the more significant features would trump. For example, temperature would trump and so on and so forth. So that's about feature importance. In the same way, if you talk about classifier, we are talking about deciding whether something is a cow or duck. You would all agree that size would be a pretty important factor. You would all agree that size would be a pretty important factor, isn't it? But when you're looking at the cow or the duck, take something fairly irrelevant. Let's just say that if they have stripes on their body or some sort of a straights or color lines on their body, whether the color lines are horizontal and vertical perhaps doesn't matter some you can take or some other irrelevant feature like how much mud is there on their feet as far as you can make out probably may not matter because both of them may or may not go through mud right so that is the value of feature importance in determining the answer in predictive models now one of the things I gave you guys is the value of feature importance in determining the answer in predictive models. Now, one of the things I gave you guys is the flag data set and I give you a hint that flag is just a small twist on the river data set. So how many of you saw the flag data set. I did. Sure, that is you did. Anyone else who did? You did. So I'll walk you through actually. It was just a small puzzle. See, when you look at the flag data set, you visualize and it looks like this. Does it look remarkably similar to the river data set, guys? Yes. Yeah, it looks very much like the river data set so now what do I need to do except that if you are above there if you are on the one side of the river you are this greenish color if you are on the other side you are the yellow color isn't it so when we compute the distance from the center of the river, the rest of the feature extraction is exactly the same. But the only distance is, do you notice that you don't take the absolute value? This is the only one line change. In the river, this one was absolute. Here, it is just as it is. Distance from the center, positive distance will make it green line, negative distance will make it green line negative distance will make it yellow line that's all and when you do that you again do your analysis exactly the same analysis the same feature building of logistic regression you notice except that now it's a tri-valued thing and here is your confusion matrix looks pretty good i would say. The principal diagonal is heavy in red. The off diagonals are pretty light, right? The number of mistakes you are making are very, very low. And if you look at that classification report, you have a close to 89% accuracy, fairly impressive accuracy. So with feature engineering, this is it. Once you develop the art of feature engineering, you can use the intuition you develop in one data set quite often into other data sets. Obviously here I engineered it to be such that you could use the intuition from one to the other. Let's look at the ROC curve. Would you guys agree that the ROC curve here is pretty good? Yeah. Right. In fact, it is 94%, 98% and 99% ROC. I would consider those pretty impressive ROCs here. And then this is the beauty of feature engineering by hand. I mean, when you feature engineer, very rarely do more black box algorithms come as close to it. But here we go. It depends on the context also. Sometimes it's hard. The reason you use black box algorithms come as close to it. But here we go. It depends on the context also. Sometimes it's hard. The reason you use black box algorithms is you can't feature engineer. You don't have the intuition. You can't visualize it, right? So you let them do the feature engineering for you. So here we go. The number of mistakes, how often the river points are sometimes confused as green, sometimes as red, and so on and so forth one and two but there's a little bit of a mistake here a little bit of a mistake here no suppose i try the decision tree classifier on flag the code remains exactly the same no change except that I'm feeding in the river data set yeah when you look at this what is a confusion matrix is here looks pretty good you you give the accuracy do you know that the accuracy is only 86% guys it is simply less than your hand engineered feature yeah feature engineering exercise isn't it and when you do the classification that's your classification report and this is the visualization of it once again when you look at the decision boundary I mean the ROC curve you are 85% 91 93 good but nowhere close to the 98 and 99 person kind of numbers you were seeing with feature engineering these are the number of mistakes and I'll leave the last part doing it with random forest as an exercise for you guys from here onwards do you think you can complete the rest of the exercise base you're doing it with random forest yeah yeah so fill in the gaps I've deliberately left some things unfinished. I'll be posting the solution to Slack. And obviously, please don't post it on the public web. This is otherwise, I wouldn't be able to teach the next class. They would say already all the solutions are there. They wouldn't even try. They'll just take the solution and just say oh I'm done shouldn't be like that so alright this is it now any questions guys I sort of went through this this is the way ideally you should create your notebooks so by the way Dennis what method did you use to find? Was your method for the flag what I used, or was it some different method? Oh, actually, Cindy has our code on Slack. Yeah. No, no, but could you explain what approach you used? Was it the same or slightly different? Oh, for the flag one, I first classified, no, I changed the label for either one or two into a consistent label, like saying I changed all the twos to one or changed all the ones to two. And then just did absolute value classifications and I set of parameters if your lower ends of midlines and your either one or two. And then that way it was actually biased against, like two's apart, it's biased towards the direction of which way you initialize it as. So I did it twice and stacked the predictions together and then got like a 90% accuracy interesting okay I'm going to read your code so guys some of you have sent me your notebooks I apologize I've been rather busy now fourth of July is coming and I'll be reading it over that and I'll give you guys feedback one quick question on when you do random forest on river here how many how many subset of features one like there are only two features here did you yeah that's right So what happens is that in a situation like this it will pick up one feature at a time You can just make a tree of one feature And I suppose same question with a different part So when we select like thousand then thousand decision trees for the random forest Method how those trees are different like when we select like 1000 decision trees for the random forest method, how those trees are different? Like when we have only two features? Oh yeah, the reason is that even if 500 trees pick one feature and other 500 trees pick the other feature, remember they are being given different subsets of the data. Bootstrapping is happening, right? Okay. They're getting different parts of the data. That's where it is. So it was a problem. It was there just to teach you guys the importance of feature engineering. It was just a didactic data set I created to bring out the importance of that. How do you prune the tree for decision tree and code? Yes. So the pruning of the tree is built in here let's let's talk about it the next example which will be more complex next time next week let's come to that what you do is you force it to limit the depth you say max depth is equal to let's say four actually in this in this problem itself i'll do that ask if your file just says river but it contains both river and flag yes it can be called we were careful but it contains river and flag three methods feature engineering decision tree and random and maybe you can take up a task take this and then i've deliberately given you html so you can't quite run it so if nothing else you have to copy paste it into your jupiter notebook you'll learn something you'll get some practice then i have left holes in it for example i haven't done the the random random flag you can go complete it i haven't done the feature importance for the flag you can go complete it so there's a little gap try to go finish fill in the gaps guys and see how so can you please explain where you use the absolute function for you know feature extraction see in the case of river because whether you are above or below the river doesn't matter the only thing that mattered is how far you are isn't it from the river whereas in the case of flag it mattered whether you are up or down up was one color down was another color another class so you don't use the absolute function did you get that? Yeah, okay. That is it. It is as simple as that. Seeing guys, if I may say something, much of data science is about just thinking about the data. It's not so much programming. The programming when you enter this field, it looks all new and strange and so forth. But by now, just reading the solution you see, after a little while it becomes very easy and repetitive. You get one solution right, you'll be using the same methodology for your next and the next and the next. Quite often you use the same process, similar processes. But this field is all about thinking about data, cleaning, preparing the data, feature extraction. That is where the magic is. And sometimes you can just think and solve a problem rather than actually hack it through code. So, all right guys, spend, it is eight o'clock. Let us meet at... Asif, so what is the purpose of the HTML code in the, like the first line? Oh, good that you asked. It is one of my secret tricks. Okay. I don't know if it matters to you. Let me explain that. So is it for visualization? Like visualization work? No. See what happens is by default, the fonts and the colors the fonts that Jupiter uses I find to be not the ones that I like some people like they find it clean and nice I like my own fonts right so do you notice that my title is in a different color and font the text is in a different color and font like if you look at my text here right wherever the explanations are given I'll give you an example you look at this this is more of a textbook point isn't it are you are you seeing that in it yes so I like to see explanations in the font that i'm used to seeing the way you write in text so i like to put my own custom fonts and so forth you don't have to do that so the beginning part is my you notice that i've given my own font here laura yeah that is all i'm. I'm setting up my fonts. I like the Laura font much more. So that's what it is here. So I tend to go between the Laura and the literata font. Both of these are Google fonts, the beautiful fonts. And I sort of go back and forth through that. It's just a bit of aesthetics no other purpose you can ignore this even if you delete it the whole thing will work so how are we supposed to open it like using collab or no it's HTML I've deliberately made it hard for you so that you can't just directly use it you'll have to read the code understand it and then write your own notebook I need try you have to'll have to read the code understand it and then write your own notebook I did try you have to you have to write every line yes or you have to copy paste every line from the HTML yeah you can copy paste but the whole idea is that see if I give you that solution as a notebook then guys you'll only have good intentions of doing it you'll never do it right so I made it just slightly harder you need the whole solution but as I HTML and at the end of the workshop I'll give you the notebooks also when the class finishes at the end in three more weeks i'll give you the original notebooks so you can hold on to them but ideally you would you shouldn't need them because you should create your own notebooks looking at the solution all right guys so one of you who is looking at this notebook give me an estimate 20 minutes or 30 minutes how much do you think is reasonable to It's a pretty long notebook. 20 minutes. Okay, so we'll meet at 825. It is eight to five and we'll meet at 825. I will put the recording on pause for the time all right so I'll go from once again quickly so we're going to do the California data set much of it is familiar territory the one new thing that I introduced is how to style table any table these are CSS properties beautiful thing is that the funders data frame has an integration with CSS styles you can style the data set and it looks different it looks pretty I suppose then the vanilla version without any styling well my styling is not particularly pretty but if you are really good at CSS you can make this table I suppose even even prettier yeah with all the cool colors yes you can do it with all sorts of cool colors i put salmon here but you can do some different colors light blue or something yes so it's not a jupiter specific command right as if i mean uh is this like a HTML table that the Python is giving out with that command exactly so it is everything out HTML okay okay see one thing is there is you Peter and Colab Colab is based on Jupiter are pretty much the defector standards in data science if you do the the one thing that you may graduate to is pycharm right pycharm has a support for notebooks this style notebooks to be just time so all of them will respect this this html output and so forth they will all respect it so now moving forward one of the things you do is, this is a new thing I'm introducing you to. See, when you get data guys, you don't know that data is clean. You should assume that real life data is not clean. This is a real life data. So a little bit of a background on the California housing data. See in California, one of the perpetual obsessions especially in Silicon Valley's are the house prices going up or down right because most Americans and certainly most people in California their biggest asset or their principal night I mean the value or retirement money is often stored in the house it is the house itself so as the market goes up and down the individual people they see their fortunes go up and down right and people worry if the house price is crashing people get very excited if the house prices are going up unless you're on the fence wanting to buy a house if you want to buy a house of course you love a crash and but the moment you buy a house you want the house prices to boom so a real data set from the 1990s at in 1990 a data set of California you see how the house prices were then now 1990 is just 2025 I think is 1990 or 1995 I forget this data was gathered and what looked like expensive house then will look like the I mean for the same money today you could probably buy yourself a shed or something like that California houses the prices have really gone up so this is not a time series data temporal flow it is very specifically a snapshot in time data for california what were the house prices then this data set is available in kaggle i must have taken it i'm guessing i took it from here it is a data set that is published actually. And it is also mentioned in this book that I recommended you all to read. So it is the same in your textbook, the Python one, this is discussed. So you can go back and read a chapter there and see, and actually I have not, I'm embarrassed to say that I haven't really looked at how he has done perhaps he's done some things better you can go and tell me about it so well this is a command what it does is it finds if data is missing in some of the rows and columns it is real life. Whenever you gather practical data, you will find that some rows will not have all the values present. They will be missing values. It's the nature of it. So suppose you're taking the weather, your temperature, pressure, whatever it is, wind, and let's say that the guy is doing it manually, forget modern instruments for a moment so you can easily imagine that if it is too rainy the guy won't go out and take all the measurements he'll just run back some measurements may be wrong somebody may delete it and so forth so things like that sometimes you don't have all the data for all the rows all the feature values for all the rows so you the feature values for all the rows. So you need to inspect that first, unlike the river dataset or flag dataset, which were didactic data. And so there were no missing values. In real life data, you will get noise. You will get imperfections. So you need to acknowledge those imperfections and study it. First thing is just as an exploratory data analysis, let's look up here. What are the columns we have? Longitude, latitude. It gives you the coordinates of a house. Actually, it's not at the level of a house, but it's at the level of a city block. But it gives you the coordinates of a city block. House, excuse me, median age. coordinates of a city block house excuse me median age as the name suggests it gives you the median age of the houses in that block so some house may be very recent some may be older but generally in a block in US you know we most of us live in row row homes right cookie cutter homes so quite often the almost the entire neighborhood sort of comes off the ground at about the same time so median house price immediate side median age is a measure of what the middle ages sometimes houses have a huge variation new and old total number of rooms in the house again is a pretty good measures worth looking at. Now it looks very odd, the total number of rooms is minimum is two and maximum seems to be a huge number. Why is this possible? Because it's not the number of rooms in a house, it's the number of rooms in a house is the number of rooms in a block. And if the block has a lot of apartments, high rise apartments and so forth, tenement housing, you may have a lot of rooms. This is the total number of bedrooms available. Typically, number of bedrooms is a pretty good estimate of the number of people living there. You're right. Because you see the number of households is also too many that's right yeah that's how you configure okay right households and yeah so you can see that and then you have the households population median income like you know what is the median income of people living in that block? And the target variable is we want to use all this data to predict the value of a house. What would be the value, the selling price or the estimated value of the house, median value in the whole block, city block. This ocean proximity is important. As you guys know, in California California it is pretty expensive to live on the beach. We have a lot of coastline but the coastlines tend to be more expensive than inland. And so ocean proximity matters. If you live on the island, for example Catalina Island or somewhere it could be quite quite expensive it's worth looking at so those are the features just looking at this value can you tell which of them is a categorical possibility right so next thing you do you look at the missing values in missing values if you see a white horizontal line it means data is missing so which of these features has some rows in which there is missing data guys for bedrooms bedrooms is one and if there isn't something else it's barely visible how many rows of data there are and so on and so forth it's a measure then so it turns out that there is a total bedroom and total bedrooms that's the only thing where any data is missing right and if you have total bedrooms missing then sometimes have total bedrooms missing and sometimes it affects all of these that's about it doesn't mean much now I taught you guys about this thing what was it about profile the pandas profiling you can use the pandas profiling good thing with pandas profiling is some of the things that I did by hand for example missing value analysis you know abbreviated form it does it. So when you look through this, PANDES profiling is pretty good. It established that most of these are real valued, but it was smart enough to indicate that ocean proximity is categorical and there are five values. What are these? You are less than an hour from the ocean you're inland you're near the ocean near the bay or you're in an island so there are five homes or five rows of blocks that seem to be on the island so which is the island in california alcatraz alcatraz is one certainly yes but I don't think any, I don't know if anyone lives there. Catalina? Catalina, exactly. So then you can look at the interaction, for example, how does longitude and median income, latitude and median income vary, right? Clearly you notice that at well latitude and acid can you please quickly recollect interaction is that collinearity no it just is showing you some sort of a correlation between correlation on the okay that's correlation because there is correlation below so I'm that's right so from this it may not be very obvious but you can see that San Francisco Bay Area is filled with people with a certain income value and then there are people in Los Angeles who have that income if you go longitude men but you see that also it's a little harder to see longitude wise but at certain longitudes are the most Hang on media income here longitude and medium income. Again, you see this value that they So the two big income areas are Bay Area and Los Angeles, you can sort of see that this is the correlation. You can see correlations are there very prettily it draws out the correlations right and if you want to understand what these different correlations are you remember that I taught you guys Pearson correlation all you have to do is toggle this and it will explain to you what this correlations are Kindle towering so forth if you want me to explain it I'll be happy anytime to do it in Saturday and ask me But today I want to move for a little bit faster So you can see that it gives you some sort of a missing value analysis pretty much. It says that none of the rows are missing Except for well, why is did it not show total bedroom see there? Well, this does not look right. Okay It gives you a few sample rows last row this is it then let us so now I want to introduce you to some techniques one of the things is when you get geospatial data it's always a pleasure to visualize the data right and to visualize the data is actually very easy do you notice what do I do? Let's go through this code. I got the minimum and maximum longitude of the latitude of the longitudes, minimum and maximum latitudes. Using this, what can I do? I can find the center of the map isn't it yeah yeah it does this like these three lines make sense it's very simple all I'm trying to do is the center so what I will do now is I will create a map and folium is an excellent library to create maps you see in one line I'm creating a map I'm just giving it the central location longitude latitude and I'm giving it the zoom level you can start out with the zoom level of course you can zoom out so if you notice that i with my mouse i'm zooming out right this is interactive map these are live maps it's a pleasure to actually be able to create it you realize that with just a line of code actually be able to create it you realize that with just a line of code we are able to create it one line right and on that map which will be an empty map now i need to project our points each of these points i projected it our data points are projected longitude that color that color has to be like in hex code? No, you can put whatever color you want. You can put red, blue, green, whatever. I must have been fooling around with something. I like this color. These days with coronavirus, this color is a dangerous color. Things are not not going well anyway so this is it guys so do you see how nice it is how quickly you can create maps in uh in in data so you know this is these are all the tools of data science people use uh it is good to know how to make maps if you have geospatial data right but you don't need to make maps you can just draw a simple scatter plot if you just did a scatter plot of the data you would realize that you would get exactly the same thing as if I just made the scatter plot of points and what let's look at the scatter plot I'm making a scatter plot X and Y are the latitude longitude. I'm coloring it based on the median house value, which means that if the value of the house is High, it will be more red. If the value of the house is low, it will be yellow. So yellow, orange, red color Color maps again, let me remind you what color maps are. Whenever you create scatter plots or plots in your code, you have to be careful of multiple aspects. One is the aesthetics. Together they should look pretty, right? They shouldn't be jarring to the eye. The second is disability. You you know a lot of people are colorblind so people have sat down and they have created this color map of good color the color you know the color ranges which fulfill both the contrast they are enough contrast they have and the color blindness sensitive at the same time they give you the aesthetics and the beauty so this is nothing but a scatter map let's get a plot and yet and then the size of each point is how much population there is and the color is how expensive the houses so if you look at this map can you see guys that places so deep red is expensive places they also tend to be high population centers isn't it which makes sense cities are densely populated and the median how home values is very high there is it making sense days we all know about homes we all know cities so it makes sense can you tell very San Francisco in ways where is Los Angeles in this? San Francisco, 39 to minus 1.2. Yeah, that's right. You can literally see, see, this is not a map. This is just a scatterplot of data. And yet you can see a lot here. You can see the San Francisco area. You can see a little bit of the Santa Barbara area. You can see the Los Angeles area here and the San Diego area not only that you can also see interstate 5 run through it do you see it guys effectively this line here those of you who have been now let's correlate it to the map and see whether we are right so look here guys do you see this yeah all the cities are along this and actually it's not I don't know if it is interstate 5 I think it is yeah but yeah there you go I think there's also a California route that goes through this that may be more through the cities I think there's also a California route that goes through this that may be more through the cities I think five tries to avoid the cities you can overlay the map and the scatter plot right yeah yeah you can do that and you can do you can do so anyway I give you some at this moment I was just trying to give simple code so you see this is your standard scatter plot I just pretty fight it a little bit yeah right hand side that you know hundred thousand two hundred thousand that is that is basically your C PD and median house value see right okay yeah so in those days in 1990 perhaps a half a million used to be the cost of houses, very, very expensive homes in Los Angeles and San Francisco. Today, what do you think 500,000 will buy you? You have to go to Tracy. Yes, that's right. And then you need to literally add a zero there. It's close to 5 million in those places I was just looking at the house prices in a third and I think they all are six seven million so so by the way any rich participant here who lives in a third in or something like that so alright so I put this side by side now guys, you can literally see the scatterplot in this map. They look so close to each other, isn't it? Now the other one thing I realized that the ocean proximity is a categorical variable. So I'm explicitly changing it to categorical. Instead of a string, it's just a good practice to do that. Any surprises so far, guys? Anything that needs explanation? Does this statement look very easy? These two things? By now, is it looking very straightforward and easy guys it is a categorical so I'm making it a categorical yeah yeah then what this is the describe by now you must be familiar with it we are describing it then what do I do I draw histograms of each of the variables when I draw the histograms can I just shrink the font size a little bit so i want to see the histogram in one single shot yeah so what is the histogram in histogram you can choose the number of bins what does alpha 0.4 mean guys who would like to enlighten me what does the alpha is it like how transparent it is exactly the transparency level how saturated it is alpha one means completely saturated opaque alpha of zero means completely transparent invisible so 0.4 why did i do that by the way guys why did i not have it why did i if i don't put alpha what will happen what yeah these will be very saturated colors um it still adds aesthetics your heart style then now why did i do this x rotation is 90 degrees why did i do that can you tell me what it is doing x-axis is you know like the numbers are rotated 90 degrees so you can read it well yeah if you don't do that yeah otherwise exactly great and figure size 2020 means I'm giving it a rectangular shape just enough big you can decide what your figure sizes when you 20 here was good maybe I could have made it a little bit bigger now the pairs plot is interesting this is again those of you who did ML100 with me know what pair plot is. It is nothing but the scatter plot of two variables taken at a time. So the way to read it is a little more complicated. Let's take this. What is the relationship of longitude to population? Do you notice that there are two humps here? Can you explain why there are two humps here can you explain why there are two humps why is this peaked at two places San Francisco and the area and the LA area yeah the bimodal I model distribution California is bimodal in most of the things so it's quite interesting that most people of california live in these two cities and the rest of california is empty people often say california is very expensive very expensive actually if you go away from this to population centers california home prices can be quite quite reasonable yeah prices can be quite quite reasonable yeah yes so in case you have a urge to run away to the you know the middle of nowhere or maybe in the middle of Arizona or something remember you don't need to run that far yeah there might be people might do that if this thing trend continues like this people might even do that. Yes. So you can see once again, near the sea, how often are people this is a box. This is a color category plot given latitudes, how much of values there are. Once again you see that there people are sort of distributed all along. In land at certain, in land people are at all sorts of latitudes. Near the ocean people are at, they're closer to this latitude, the 35 degree latitude which is closer most likely to San Francisco area. I mean, the Los Angeles area. Near the bay is of course all San Francisco population and so forth. It's just a dish. So these are the plots guys I'm introducing you to in case you had not introduced to that. guys I'm introducing you to in case you had not introduced to that. What I would say is as you study this notebook, and those of you who did ML 100 of course have gone through this exercise at a more leisurely pace, but those of you who joined ML 200 directly, these are the landmark or that's the typical or ubiquitous plots that you make in exploratory data analysis. So become familiar with the syntax. In the beginning, it will all feel rather magical that you're invoking some magic mantra or some obscure shlokas and something is happening. But gradually this will all become very real. It will begin to look very easy after some time because just standard Python code. If you want, I can explain any one of these code to you. Like for example here, do you agree that this one line is self explanatory? It is saying go make histogram, do bins of 50. What is the bins is equal to 50 mean? 50 buckets. Exactly, 50. to 50 mean 50 buckets exactly 15 so when you make histogram of a category of a numerical variable you have to put values into bins because you're counting how many values fall in each bin you take 50 bins alpha is this this we already talked about figure size we talked about so there's no magic to any one of these likewise the pair plot this is a bit more going on here but still it's four line I'll explain it to you you make pair plots between this alpha size is 80 all of those are self-explanatory then you say that along the diagonal make the histograms do you see so these histograms are the same as the histograms here then the next thing it says is that what is that now what is the color in the histogram that I give you is the ocean proximity how far you are from the ocean that's the coloration then in the bottom upper diagonal on the upper side of the diagonal, the pair should be scatterplot. And in the lower part, there should be something called a kernel density estimators. Those are sort of, think of it as some, at this moment, KDs will deal with them much better. But think of them as, these are well probability density sort of thing. But think of them as some sort of a heat map at this moment a very rough idea would be heat map but we'll talk about this and learn about this in more detail when we do the workshop the boot camps are those in the heat are those numbers like table of numbers in between no no no these are just the legends this picture is very dense see the thing is you made this picture of size this size when you have so many variables you want to blow up the size of the picture so that thing is well separated and you can see everything so you would make this picture to be more like hundred by hundred and put it on the wall. Yeah. Then all the things will show up properly. But it also shows what happens when you have too many variables that you have to you cannot put everything in a single visualization. This is already beginning to put beginning to push the limits, isn't it? Yeah. Imagine that there were 30 variables, then the pair plot would be horrendous. So what you do is you make it in pieces or you pick a few important ones and you make the pair plots. And there is nothing like you have to see all of them in one place, right? I mean, you could when you're analyzing going deeper, you can always go one by one right it's okay and then make scatter plots absolutely yeah you can do that and now this is the this is these are called beautiful the if they look pretty these are called violent plots they literally give you the box plots but violence a little bit prettier than box plots they give you counts like median household age how many homes and this is why remember that ocean proximity is the categorical right so based on the color so there's value so this household median age you can pretty much see that the median age in this is the ocean Catalina is pretty old homes are there you don't have new homes most likely they don't allow new homes to be built it's an island there must have a strict regulations so this is it and the same data I just show you that you can do it as a regular box plot. See with all of these things, and when you present data guys, people underestimate the value of clean visualizations. Visualizations, if you clean it up, it sort of creates a bit of aesthetics. I haven't done much, you can do much better than this, but for what it is, now all of these things that I'm doing box plot category plot violin part None of them are hard These are all features there. You have to just go use it You have to know that those things exist and you have to go use it, right? Account plot how many homes are there? So most homes it turns out are less than one hour from the ocean in California which makes sense California is a long and narrow state most homes are inland not there are some this is probably the Central Valley maybe there are not that many then near the bay near the ocean this is it right now correlation you realize that any two variables could be correlated, it shows you the degree of correlation. Now we already saw that in profiling, in the Pandas profile. So I won't go more into that. Oh, by the way, here I wanted to show you something. Quite often, when you write documents, you want to write it in latex. People ask how do I put the output into into a research paper that I'm writing something so it's very easy any data frame in pandas all you have to do is do to latex now those of you who know what latex is would find this magic mantra very easy to read. If you don't know latex, don't worry about it. Latex is a language in which in the site in the mathematical communities, a computer science and math and physics, etc. Most papers are published So this is the syntax for that. Now the same correlations, we show it as a heat map here. We're looking what the high correlated as. I'll let you ponder over it. Median age is highly correlated, negatively correlated with, let us say, with many things. Total number of rooms. Does that make sense, guys? Median age is negatively correlated with total number of rooms and total number of bedrooms what does it say new houses are smaller no the opposite the bigger the house the small the older the house the smaller the lesser the number of rooms yeah yeah actually if you go to the older house you will see the bedrooms were much larger right so the number of bedrooms were small as a result yeah yeah but these days if you go to the houses you will see bedrooms are very very I mean they like to because the you know the price is based on number of bedrooms right they increase the number of bedrooms the number of bedrooms right so they increase the number of bedrooms increase the number of bedrooms and tiny bedrooms okay guys so that was it so now let's come to regression i thought i taught you guys decision tree and the random forest let's see if we can do that a very simple if you read the river data set you'll find this easy to understand so we need to do some data pre-processing first pre-processing we need to do is that we need to do one hot encoding of the data what does that mean one of the predictors was ocean proximity it's a categorical variable right so we need to convert it into a numerical variable and the way you do that is you you take all its values so for example are you within one hour of the ocean yes no are you within proximity are you within are you inland yes no right are you you know uh so on and so forth are you on an island yes no are you near the bay yes no are you near the ocean yes no so what have you done? You're burst out. You have burst out all of these into this different columns, different features. This is called one hot encoding. So you can do one hot encoding of the data. Once you do that, the other thing that I'm doing is I'm dropping the, all the rows that don't exist. Is that a good practice? Actually, you have the luxury to delete or drop rows that are defective only when you have a lot of data. Here we have a lot of data, 20,000 lot of data 20 000 rows are there like 20 plus thousand rows are there but if the data is sparse you don't do that you do something called imputation you try to fill in those values and keep those rows and use them anyway right it's like when your car has a tire nail in the tire there are only two things you can do, you know, if you're rich, you go and change the tie immediately. Well, if you're like you and me, then I suppose you'll want to take the nail out unless the tire is old. We would like to have it patched. So it's like that each of your row of data, you would like to patch it up. by imputing some value filling in those gaps with some value the imputation is a topic usually these little things or practices we do in the boot camp we don't so but here we just delete it gradually as we make progress i'll teach you guys in precision then we impute it now notice one thing this data do you see skew in the data is the data symmetric or you see a lot of skew in the data guys skewed yeah the data has a lot of skew there one more thing you notice that this particular thing household median age 50 50 plus at 50 they have uh sort of uh clubbed all data that are 50 or plus into 50 plus category bin so you have to watch out it will it will screw your analysis and you'll see how it screws your analysis after a little bit i i leave this as an exercise for you to deal with this part one way you can do it is not take the data 50 plus right take it out and so forth but we'll deal with it this data is skewed is it right skewed or left skewed remember the trick guys if you think of it as the beak of a duck right the beak is looking this way right the duck is looking this way, right? The duck is looking this way. So it is right skewed. All of these are right skewed. Some of them are very strongly right skewed, right? And median house value, the target variable also seems to be right skewed. What can you do about it guys? How can I take care of this skew? Well, you could do the whole power transform and find the right transformation to normalize it. But a cheap way is just take the log of these, this one and this one and this one and this one. Whatever you think needs a thing, you can do as a pre-processing. You can take a log. If you take a log if you take a log it's a trick and again i'm talking about things that at one point i've taught in depth to you guys but suppose i take the log of it that is the log transform by the way this whole thing most people don't pay attention to and you will see what happens if you don't do this your r squared value will be about six to 10% less. But if you're careful and you do this, and I invite you to do this guys, comment out this code, run the analysis, see what your model metrics are, performances, and then do this and see your model performance. It is quite interesting actually that most of the notebooks that I saw of California dataset, which have been posted in Kaggle and other places, they did not do this transformations. If you're using, it is always good to do that feature extraction or data preparation carefully. When you do it, you get the extra bit of accuracy the extra bit of performance not actually extra bit of performance you will get so now when you do the transformation do these things look much more normal guys are they better right this queue is gone right so now that it is gone what have I done so far I have massage a pre process the data the data pre-processing after the data pre-processing then I go and so what were the things that I did in the pre-processing I did let us summarize what all things I did I mention it here convert the categorical to one hot encoding drop the missing values extract the feature space and the target x y right all three things then finally i also do the log transform because i looked at that so this is the value of drawing those histograms they're not just one thing you do as a ritual you use it to make some decisions about data and once you do that you use it to make some decisions about data. And once you do that, you now take this and use... Does this look totally easy to understand, guys? What am I doing? This is training and this is test. And then I'm going to try a slew of models on it. First, I'll do a simple linear regression. When you do a simple linear regression, do you remember these lines of code, guys? This is straight from your river data set. You just see the linear regression fitted. Now you notice that, you know, things are beginning to fall into a repetitive pattern here. It's more about thinking. The code begins to look very easy pattern here. It's more about thinking the code begins to look very easy and familiar. It's exactly the same code. When I run this code, I get a mean squared error, which is a fairly low actually. And I get a coefficient of determination, which is 69%. 69% R square for this data is actually, I would consider fairly impressive. If you had removed the outliers, it would become even better. I leave that as an exercise for you. So one basic thing is that outliers, you know, there's not enough data, not enough information to make a judgment call for them. So you could do one thing. You can remove the outliers and say, I'll make a separate model for outliers something else and let me make a better model for the in liners in line data data that is not outliers let me leave that as an exercise for you guys so I say where do I see that outlier information in the graph oh it is all here see if I look at this graph here look at the histogram do you do you notice that all these outliers are there in the box, in the whiskers plot, right? So anything beyond the whiskers, do you see this is the box, these are the whiskers. Anything beyond the whiskers are outliers. So latitude has outliers, well, that's not harmful, but total number of rooms, total number of rooms has huge outliers well that's not harmful but a total number of rooms total number of rooms has huge outliers right do you see that the main box plot is here and then there is a long tail of outliers total number of rooms total number of bedrooms population they have outliers households have outliers and let us also go and income has outliers right so let us go and let us go and verify whether that shows up in the original plot do you notice that this tails off but it goes to six thousand why does this box plot why does this uh histogram go all the way to six thousand we can't see it but there must be some one or two outlier values here isn't it in all of these there are some outlier values sitting here like for example total number of rooms 15 right hang on households let's look at the household, right? In some city block, there are massive number of households. In certain median income, some people seem to have a fairly high median income, even after, you know, pretty high median income. I'm assuming that this is in thousands or something like that. But whatever it is, look at the population, it goes all the way up to here some blocks are really heavily populated so the data is filled with outliers do you see that guys do you see that it is a clue like you ask yourself why is this histogram going all the way till here it means that there is data that I don't see so if you chop off the outliers, guys, your analysis will become better. And I leave that as an exercise for you. It's also one of the things people forget to do, actually. In fact, I was just in the break, I was looking at the California housing and I was looking at some of the notebooks and what people have done, even the more celebrated notebooks, sorry, yeah. The kernels that people have done. By the way, you can learn, I highly encourage you to go and read some of the highly-voted kernels that people have created and see what things they did that you've missed because you can learn from it at the same time you can take pride that you did something more carefully that a lot of them missed so you you know this is the way for you to get a reference of how good you are yet these people certainly think they have done a great job which is why they have posted their notebooks here right so go check it out guys you'll be pleasantly surprised that by the time this workshop ends, you would realize that your notebooks look better than most of the notebooks there at Kaggle. Anil, do you agree? Yes. Begins to get better and so forth. All right right so we are making a linear regression model and we achieve an R square of 646 about 70% or 69 points up or say you would take at this moment guys remember what did I teach you in ml100 it is not enough to know that you have you're getting a decent R squared or a decent mean squared error what you need to do is what you need to be able to do a residual analysis and see all is well so you begin to see that this is your residual analysis when you look at it first thing you notice on the in the on this side do the errors have a bell shape do the residuals have a bell shape yes yes yes yeah here it all there is a little bit of a pattern here by the way ignore this this is coming from the limitations in the data itself because remember there was a hard cutoff in the data in one of the fields this way this is coming from you can ignore that but it is mostly homos uh there is mostly a sense of homoscedasticity except for these outliers are screwing it up they seem to be there more prominent in the lower predicted values than in the high predicted values so if you remove the outliers it would get better so it is not perfect but it is pretty good after all we are using a linear model for a very complex data set what is the relationship between prediction and reality guys this is the relationship between prediction and reality does it make sense is it a positive or negative correlation positively correlated isn't it why prediction in reality are positively correlated they are along the principal diagonal so you can see the deviation see a perfection would be if they were along this a gray dotted line and they are along this dotted line well not quite perfection but pretty good right if you're searching for points that can be of high leverage high influence it gives you a sense of high infant points nothing pathological all looks good now this was your linear model guys do you notice that just a good application of a linear model took you this far by the way? If you had not done all of those log transformations, etc. Your numbers would not be so good They would be much worse Try that out So sixty nine point one now you expect You ask yourself if I had regularize the data would I get a better answer. It turns out regularization does what it suppresses overfitting. I haven't Obviously, I apologize for those of you have just joined overfitting means you know the data has high variance overfitting. There is a better to suppress it. a method to suppress it and you suppress it by different degrees of suppression and see which one is the best and then you whichever is the best you use it ah it has gone to 69.12 well 69.11 versus 69.12 pretty much the same if you look at the coefficients of the model let's look at the first three coefficient, 0.165, 0.164. Do they sort of match the previous one? 0.165, 0.164, then 0.003 and minus 19. So 0.003. So you realize that the coefficients are almost all the same. It means that the data does not have, your linear model does not have overfitting, right? And it sort of makes sense. Linear models tend to have underfitting problems, not generally overfitting. So, well, that is what it is. If I had made it a polynomial model, so I leave this as an exercise, guys. See, somewhere, I taught you polynomial regression. See if you can extend this and make it a polynomial model. Be ready that it will be a big computation because even if you go to quadratic form, 11 square is close to 110 combinations, right? So you will have an explosion in the number of variables that will show up. Try it out. See whether it gives you anything. But when you do a polynomial, you must use ridge regression or LAS or whatever it is to suppress that. Better is actually use elastic net. It's a combination of both. Can I leave that as an exercise, a homework for you guys to try let's do that then we talk about decision trees now because we learn decision trees the week previous weekend random forest this week let's look at the code for decision trees guys I trust by now this code looks obvious I think I build a regressor I give it the data to fit and then I use the data to predict the levels so far so good guys yeah right these are your predictions labels are your predictions so people use y hat or labels whatever you like them levels is more typically for classifiers I tend to use why had I don't know here. I wasn't being too careful. So I just used the word labels. Any variable name. So there we go. And what is the r square I get from a decision tree? 67.72. Is it better or worse than linear regression? Worse. Worse actually. And look at the complexity of the model this tree built. Worse actually. And look at the complexity of the model this tree built. So it is an illustration of what principle, something about lunch that I keep talking about. Anybody can remind, what do I say? Free lunch. There are no free lunches. NFL, right? Exactly. It's an NFL theorem. No one algorithm is in quotes powerful more powerful every algorithm has a space under the Sun sometimes one works better sometimes others so don't let anybody tell you that decision trees are more powerful than linear regression or something linear equation is too basic it was a point worth emphasizing so I'm mentioning it to you. You can visualize this, by the way, this is hopeless. The built-in visualization from Python, this KickIt Learn is not terribly illustrative. For visualization of graphs and trees, graph is the best. If you use that, and I can show you what that becomes. Let me see what that becomes let me see that becomes Oh, I think I removed the old directory. Okay. We won't worry about it there was a PDF version a prettier version that I could show up this I didn't want to run this now it will take a bit of time this is our if I use yellow brick the same thing do I now see a pattern? Does this look much worse than the linear regression plot guys? It does right it doesn't look as nice as this it is actually a little bit worse But good news is that I still have a more or less normal distribution of errors here you can see that this lines is a little bit more off the diagonal than the previous one so let's try a luck with an ensemble method now remember I told you that ensemble methods can be more powerful let's try a random forest the forest is just made up of trees. I gained a thousand trees. 20, well, parallelization is 20. It executed in 5.79 seconds. 20 is something you guys can probably run on your laptops if you have multi-core. I typically run it with 100. It runs faster. It doesn't matter. So here we go. too much faster but it doesn't matter so here we go you go and do the predict again why not this code guys i hope it's beginning to look very easy do you notice that between the decision tree regressor or linear regression all i had to do is change this word the algorithm name it has become random forest regression i apply it get the labels, get the predictions. And what do I get? Ah, I get an R square of 83.6. Is it better than linear or worse than linear? Better than both. Better than both. Isn't it? That was 69 and about 70% and this is close to 84%. So for this problem, random forest is certainly the best of the three algorithms that we have tried isn't it guys okay now this is what people tend to miss they tend not to do the residual plot but if you look at it isn't the residual also looking much better now here yeah there's far more pattern plasticity the the residuals do have a normal shape normal gaussian distribution much better so here obviously the random forest is the winner remember that in the river data set the careful uh hand generated feature extractions were the winner here there are too many variables and it's harder to do feature extraction but there are feature extractions you can think of so I'll let me tell you and by the way one of the cattle notebooks did it brilliantly some fellow has done he said that one good feature would be or she said the one good feature would be the distance to the metropolis nearest metropolis so distance to barrier or distance to Los Angeles, whichever is smaller. And he made that a predictor and he came up with a beautiful visualization. Look up his notebook will be the classic. I wish I could have found an open and showed it to you. I leave this as exercise for you to find out. So you can do a lot of further feature extractions. You can make it out. you can do a lot of future or further feature extractions right you can uh and make it out and the other thing you can do is join this data with other data for example you can join it with educational level add at any given latitude and longitude or city block what is the educational level you would realize that education level has a huge influence on the home prices birds of a feather flock together right so you can go on creating more and more features by joining this data with other data sets and that is where the magic is most of the time if you're given data you get more data not by getting more rows of data but by getting more features into your data set somehow your manufacture features or you take data that is not there by taking you know view of labor statistics data or something like that economics data weather data something or the other and you bring it in and see how you cool rating go ahead I was just saying school rating school rating yes yes yes excellent school rating is a huge determinant yeah so we can get like the zip code and then based on the zip code we can get the school yeah exactly we can get the overall school rating in California the schools are failing so it's like almost an arms race whichever few school districts are good everybody wants to go there and the house price just skyrockets in my own case I have a illustration I have a house which is much more modern bigger nicer in Vallejo Hills and I have a house here in Fremont as you know proximity matters but also the fact that the school district in Fremont is superior means the house price is practically three times over for a much worse house actually much older house yeah so it's a illustration of that so anyway when you visualize so what one of the things I did is, a forest is made up of trees. So you might be curious, let me look at into one of the trees. So here it is inside the forest. There are lots of trees, they call estimator underscore. So member variables are always hidden with an underscore. Now there's a convention, the output variables have an underscore here at the end and in uh skicketland so i just took the first tree no reason why i should take the first tree could take any one tree but do you see how complicated it looks guys do you think there is any interpretability here do you think there is any interpretability here there is also I say what is it exactly swing so one tree means what is it exactly swinging oh look at this there is this root node splitting into two nodes splitting into two oh okay oh they have shown the trees like okay now I get it okay it is practically indecipherable so that's how it is and some pruning is needed yes so one of the things I introduce you to today is a very rough measure you may say all right what factors are really important what really which factor is more important than other in determining a house price this is in 1990s let's see if the results agree with our intuition feature importance now by the way remember guys that this is a very rough and ready measure right usually feature importance is more local phenomena in certain regions of the feature space, certain factors may be more important, which is why this feature importance, which people tout as a big virtue of decision trees and random forest, I obviously is worth knowing and I'm explaining it to you, but don't take it with a huge grain of salt. Remember that it's a very rough approximation. There are better means of feature importance and explainability and those are the Shapley values and making local approximate models and so on and so forth. We'll come to that line. In the bootcamp, of course, there's a few attended, you know that we covered a whole day to that for Shapley. And by the way, those are non-trivial things. Shapley got the nobel prize in economics for one for one statement effectively one big statement how how much value to attribute to a player in a team if you think of each of these features as player in the team that is together getting a sort of a home value high home value or whatever then how much credit should go to each feature that is shapely valued we'll talk about all of those of course in the boot camp uh or maybe next time uh see there's a this by now you must have realized that this is a lot to digest are you guys feeling that that we had a long yeah yeah it was very overwhelming today yes so I don't want to bring in even more so I'll leave it at the feature value so it is for me I don't know who others so yeah so let's keep it to that and see does it make sense so the biggest determiner of home value seems to be the median income of people living there. Well, that doesn't seem to be very useful because obviously expensive homes can be afforded by rich people. Isn't it? The home, like our income determines the house we buy, right? The banks will tell you that you can't spend more than 30% of your income as mortgage payments it's the upper bound it's a healthy bound after that you get into dangerous territory so obviously you expect a median income to be very crucially related to the target variable which is the home value it determines the home you can afford. The second is, and so people of the, if a house is priced at 1 million, you can ask yourself what is the mortgage on 1 million, right? And let us say these days interest rate is what? 4%, right? So 40,000 that comes to about $3,200 a month. $3,200 a month means your income has to be 100,000 or 100,000 plus, you know, 120,000, roughly speaking, for it to be 30% of your, I mean, 40, right? 40 times 3 is 120. So your income better be 120 to 135 for you to be able to afford that million dollar house minimum isn't it do you see guys how the computation goes so this sort of makes sense I mean I don't know maybe by the way is my explanation right I made a very rough and ready over the top of my head computation hopefully okay that's that so the second is ocean proximity in land are you in land or not we know that that is true the more inland you are the less the house price would you agree that that's true for california guys broadly then the next factor seems to be latitude very true if you this what does latitude and longitude speak about? Whether you're in the metropolis or not, Bay Area or LA area or not. Isn't it guys? Location matters. And anybody doing real estate will tell you. So what do these things? See, median income is more of a chicken and egg situation. Expensive homes are afforded by people who can afford them. So I would sort of discount that. But if you look at these three values, ocean proximity, latitude, longitude, what are they saying? They're speaking to the truism that any real estate agent will tell you. So they have a saying, a cliche. They say that there are only three factors that matter in a house for a house price. It's location, location, location and location. Yeah. So those are all the three factors. And you see that said in so many ways out here. So I will stop here. This is the last one. The other factors are there, so on and so forth. Median age of the house matters, population, number of rooms matters, and so on and so forth median age of the house matters population number of rooms matters and so on and so forth Редактор субтитров А.Семкин Корректор А.Егорова