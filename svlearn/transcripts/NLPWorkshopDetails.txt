 Deep Learning Part 2 Hi guys, welcome to Deep Learning Part 2 of the workshop. In Part 1, we covered the fundamentals. We covered what is deep learning? What is, in other words, a deep neural network? We covered many architectures of deep neural network. We covered the feed forward network. We covered congulational neural networks, recurrent neural networks. Then we covered GAN, generative adversarial networks. And we covered transformers and attention models. Along the way, we sort of indirectly came very close to autoencoders but didn't quite cover it. We learned about word embeddings, in particular, how do you do the skip gram model of word to work. Now, in this particular part two, there are many things we'll cover. In the beginning, we'll focus on natural language processing. When we focus on natural language processing, we won't just be focusing or doing it at the level at which we did in the part one. In part one our attention was on getting the fundamentals right, knowing the broad picture, knowing the theory of the subject. Now that we have the theory under our belt, our primary focus will be doing a lot of real projects, real life projects. And as we do real life projects, we will develop a lot of fluency with the subject. Now, what are the things we'll cover? In natural language processing, which will be the first at least four weeks, it depends upon how fast we move, we will take a few projects. Some of the projects are the bread and butter projects of natural language processing. For example, we will do sentiment analysis, which is a simple text classification problem. And it's a good hello world. It's a good entryway into the field. Then we will learn about the word embeddings in more detail. We'll learn about each of these concepts. We'll do actual labs in them, see how Word2Vec works, how GloVe works, FastText works. What is to create contextualized word embeddings by using the transformers and so forth. So we'll pay some attention to that. Then we will, while we are doing all of that, we'll also do the traditional form of natural language processing. In other words, the natural language processing as it used to exist before 2018. Before the transformers came about, and I will call it the classic natural language processing, the classical language processing has two aspects. One is with, to some extent, with neural networks and one even without neural networks and recurrent neural networks. So we'll go into the foundations of linguistics. We'll learn about grammar, we have all done grammar, we know what nouns, verbs, adverbs, you know, subjects and objects are and so on and so forth, objectives and so forth. But a review certainly helps at this particular moment. I'll talk a little bit about that, how to break up a text into its constituent parts, how to build a dependency graph representing the sentence. And these are very illustrative things. So our first week, the lab of the first week will be using traditional natural language processing. It doesn't mean that the problems we'll try to solve will be any the easier, but we'll solve real world problems using traditional techniques and these techniques are straightforward in the crossfit as we make progress with this workshop we will be dominantly like sort of the predominant theme in this workshop will become while we are doing natural language processing. It will be the various transformers. Now the transformers have been, there's a whole zoo of them, starting with the original transformer, then came BERT, then came Alberto, then the GPT, GPT-2, GPT-3 now, and then there are many variations of it. people have tried to look at the transformer and addresses fundamental limitations for example it while it can have attention or deal with longer sequences than natural language but i mean sorry recurrent neural networks and lstms nonetheless it still hits the limitation so people have asked, how can we make it efficient with long documents? That's one area. The other thing is, as we go back and review the attention mechanism, the basic attention mechanism is quadratic in computational cost. If you remember, and you may not, we'll review all this, so don't have to worry about it. But if you remember, we break up an input into the key, the value, and the query vectors. When we take the dot product of the key and the query, it's sort of like a kernel, the basic kernel, which is a dot product, it is quadratic because you look at every possible pair of words in the sentence so if the sentence let us say has 50 words now you're looking at approximately 50 square right it would take actually of to be very exact the number of pairs that you can create it will be in the 25 times 49 it will be in the 25 times 49 pairs and so you will have to compute the attention value for each of these pairs so the computational cost is quadratic it doesn't scale very well when you have very long text and you want to do a lot of computations so people have tried to come up with more sparse networks, sparse models for the attention and there's a lot of creative activity in this space that people have tried and we will look into those architectures and those breakthroughs they just keep coming. For example, there was a, as you know, we talked about some of those words, so the transformer ecosystem, there is obviously, besides the Bert and the GPTs, there has been the long former, there has been the distal Bert, there has been the Robert, then so on and so forth. Then recently, there was a paper called the reformer, which tried to optimize the transformer. And then very recently, as in very recently, as in I suppose I read the blog just a month or so, a month ago or this month, I forget which, on the performer architecture. So breakthroughs seem to be coming one after the other. A very, very active area of work. We will try to understand these architectures of transformers. This is very important because NLP has almost become synonymous with transformers today. So understanding these transformers deeply is important. But while it is important to understand transformers, it doesn't mean that all of NLP is transformers. You won't do well in this unless you have a broad understanding of two things. Linguistics, computational linguistics, are what people have done over the years. It's important to know that, the classical theories, the statistical approaches because they do help you quite a lot in building your model and machine learning that all the machine learning that we have learned in ml 100 200 300 and so forth because it will so turn out that you will always use these transformers in conjunction with the machine learning that you have done and we will use all those approaches for example we will use ensemble methods we'll use stacking we'll use support vector machines we'll bring in kernels and we'll bring in even even a logistic regression and things like that so we will be using a lot of machine learning from from the previous classes that we have learned in this particular workshop. So this would be a good opportunity to go back and brush up on, for example, if nothing else, your textbook, the ISLR textbook and the videos. All the videos for the previous topics are online. You all have access to it. You can, as needed, go and review that. But in this particular workshop, as I said, we are very focused on practice. The need for being focused on practice means, quite often, if you have forgotten the theory, I will just refer you to a video or a lecture of the past. If you don't understand it, you can again then sit with me on a one-on-one basis and I will explain it to you, but we will use the formal sessions for our practical, for a lot new stuff that we will learn, because we have a lot of new stuff to learn in this space, in the space of natural language processing. Natural language processing has become a vast subject. Today universities have started offering an entire master's program in NLP, natural language processing, so you can gauge how broad the subject has become, that it now is worthy of an entire master's program, a graduate school program as it is. This is true for many other areas of AI too. Computer vision and image processing are becoming some universities are beginning to offer entire master's degree in that. So it should give you a sense of what a broad field we are covering. Though my hope is that once you do all the labs, you will become certainly very good in this field. But the operative word is, once you do all the labs, this part, the part two, is all about doing guys. Now we all live busy lives, you all have day jobs to carry and you have obviously a lot of expectations from your bosses in this audience and some people are from my own team and I'm the cruel boss sitting on top of them, I suppose. So well the reality of our life is, jokes apart, that we all have day jobs to deal with. I have a boss too in my day job and we are doing all of this in our evenings. So you will not be able to keep pace every single week with the workload. It's not possible, right, as adults, as professionals, with families and so forth. It is not possible that you'll be able to carve out 10, 15 hours of work to give, to do the entire projects or labs independently. So I would strongly suggest that in the interest of caution and making progress, a pair up, find a partner or two, make small groups, but don't make groups of more than four. Two is the minimum and three is probably the optimal, four is pushing it. Form little teams, form little groups. Now, you guys have all been together for the last many months. You should have some idea who is it that you would like to pair with. So, do these labs and these groups because these labs will have the nature of projects. And, you know, if you are very confident or if you really feel that your learning will get know if you are very confident or if you really uh feel that you your learning will get impaired if you form groups then do it alone that would be great for those of you who don't who currently have decided to put job aside taken a break and are pursuing this there are some in the audience who are doing that that is a very very high level of commitment for those people i would strongly advise do it alone you'll learn a lot more if you just do it alone because you have the time to make progress on the other hand for those of you who are carrying rather heavy day jobs i wouldn't advise you that. I would suggest form pairs, form groups, at least form a pair, somebody that you can work very closely with, and then make progress. Now the projects, I would give you a broad idea of what are the projects you will do right now, even before we get into the subject, because these projects are very practical. The projects would be in natural language processing, the easiest is of course the sentiment analysis. The second thing we will do, which is sort of the twilight between image processing and natural language processing, is that given an image, can you generate a caption? So can you find the caption for the image or generate a caption for the image? That would be the second thing. The third would be, we will use GANs to generate more data, more text of a given type. If I give you a certain type of tech and a lot of instances of that type of text, can you generate more text of the given type if i give you a certain type of tech and a lot of instances of that type of text can you generate more text of the same type that would be the next would be problems in entailment asking you this question does the given two pairs of sentences does the second sentence follow from the first or not is related or is completely unrelated to the first sentence. Those are entailments. Then we will do completions. We will ask if I give you a few sentences, can you then generate more and more sentences? That would be it. We will also have a project in which you will have fill in the blanks, you know, certain words would be covered up and your job would be to come up with an algorithm that can fill in the blanks. We will do text summarization, quite a bit of that, right? And the text summarization is important, we'll do that. Then we will also do some unusual things. We will see whether from the well, it's we get into text classification then. The text classification is a broad area. Sentiment analysis is a part of it, but we'll do something quite interesting. We will start with the old problem, which is a classic in this field, and many of you may have done it, which is to ascribe authorship. You're given a text and you're given a certain number of authors. You have to establish who wrote that text. That is a classification problem of sorts, authorship established. You can do it as classification. You can try alternate approaches and you will do that. You will also do other things. Given text, you will detect what subject it is about. That also is a classic problem in various newsgroups. Which newsgroup does it belong to? For example, is it about science? Is it about religion? And so on and so forth. These are famous 28 newsgroups and so on and so forth, are famous 20 news groups and so on and so forth google news group problems and so forth but we'll go beyond that i will give you a lot of text and you will do it not just as a academic exercise you will detect subjects and we will run our own little Kaggle competition in the class. And there is a prize actually. Whichever group, well, whichever team happens to win that Kaggle competition, will each member of the team will get an iWatch. I hope that's the prize. It's a token prize, it's not something a big deal, but we will have an iWatch and a really big, I suppose, cup of achievement that will associate with the winning team. So which means that I take it seriously. You have to detect the subjects and so forth. Now, when we do all of this, none of this, we will do as just purely an academic exercise. We will do it as a broad. As though your group is a startup. Now what does that mean? It means that you will do these projects by taking the problem specification. So when I give you the problem specification now, I won't give it to you as a do this, here is the data, informally. I will instead give it to you as a business requirement. You will have to take the business requirement, take it apart, and obviously I'll make it easy, somewhere in there is the pointer to the data set and a clear articulation of what needs to be done. You will, in small parts, and obviously not all of it at one go, but slowly, you will build, you will train a machine learning model or more machine learning models, you will build, you will train a machine learning model or more machine learning models, actually a whole hierarchy of machine learning models to do a whole set of tasks, including a word embeddings and this and that and text classifications and sentiment analysis and all of that. You will create what is called a machine learning pipeline. And that is how things are taken to production. Now, when we create a pipeline, I want you guys to learn the right way to do it. To not be able to do a good pipeline means in production your application will crash, it will fail, it will have all sorts of pathologies. It's very, very common, unfortunately, to see this happen in startups and even in big companies and so forth. It reminds me of a saying of Tolstoy, who used to say that, actually not used to say, that the first line of his marvelous book Anna Karenina, if any one of you have read it, it's a great classic, Anna Karenina if any one of you have read it it's a great classic Anna Karenina by Leo Tolstoy so the book starts out with a remarkable statement it starts by saying all happy families are alike each unhappy family is unhappy in its own unique way is unhappy in its own unique way. Quite a profound statement. Now, in the world of software, in the world of data science, in our world of AI, it sort of can be readapted to mean all successful projects in AI, they sort of look alike, they have a certain pattern, a structure to it. A broad structure, broad patterns to them, which you can recognize. But each failed project in AI fails in its own unique way. It has its own special set of dysfunctions. So when you build projects for production, it is important to do it right. It is very important to not make the mistakes that others have made, isn't it? And over the years we have seen a lot of that. We have all learned from experience. It is always better to learn from the experience of people who have tried things in the past and seen it work or not work. These have been distilled into a set of successful architectural patterns or design patterns. We will follow some of these emerging design patterns in artificial intelligence in building a very robust pipeline. So when I say that we'll build a pipeline, what will it be? It'll be a pipeline that will train a deep neural network or many models of deep neural networks. Once it has done that, it will have a proper store for those models. It will store it. Then you would have a microservice which will scale which will be in google cloud and i will help you guys with skeleton code it does don't worry about it you i won't just leave you to do everything on your own the whole point is it's a guided workshop so you will have my help all along the way just as i've as i've been doing in part one, I'll give you a skeletal code to do it. But unlike part one, you'll have to write more code. In part one, you have to just do a little bit more. Now it gets real. You'll have a lot of fun. So you will build a microservice, which is Kubernetes, which is in the Kubernetes engine in Google Cloud, and you will use it for inference, like your AI models will be used for inference. Then you will have a UI which you are welcome to write in anything, though I would strongly suggest you write it in React.js. I will give you guys some React code. You can write it in React.js or you can write it with some basic templates, a very, very basic templates that we can create using any of the technologies. Those of you who like server-side technologies, you can do it in Java or any kind of a templating framework. Or those of you who like Flask, do it in Java or any kind of a templating framework. Or those of you who like Flask, do it in Flask. Django, you can use it. Whatever it is that you guys like, you can use it. Your favorite, Angular. Angular is very popular and quite a bit used. Or whatever, just plain JavaScript and HTML is fine for those of you who so prefer using any of the libraries for components like Bootstrap. Simple UIs, but the idea is that you're trying to, it need not be extremely pretty, but it would be, it should be clean. It should be usable. It is the, it is a UI with which the users will interact with you. The user will interact with the UI, will give you text, and you need to make a whole set of predictions on the text. You need to detect the subject of the text. You need to detect a sentiment in the text. You need to detect a whole variety of things from the text. You need to detect a graph of words, concepts, and so forth. You need to do word embeddings and so forth. And a whole slew of things that will gradually build up. So your application is essentially the product that you are building from end to end, an AI product. And if you do all of this together in this workshop, in your workplace, that is predominantly all you'll do you would have an end-to-end experience of whatever it is that most people are doing in their workplaces are we together guys so as you may have surmised by now this workshop is a lot about doing things. Now in doing things, I will be guiding you. So if all of this is looks intimidating, remember that divide and conquer. We'll break it up into small bite sizes and then each of the sizes will look simple. But at the end of the day, if you do all parts of the exercise, you will end up building a first class application. Now part of this is in this workshop, somewhere around in a month or two, you will start making a pitch for it, you will explain why, if you were selling it in the marketplace, why somebody should buy your your application, or why somebody should, for example, fund your little startup. The storytelling is an important component in data science. See, in our field, just about everything we do is abstract. The data is real. Then there is a black box and in the black box something happens from an outside world perspective. And then there are people like us who create an inference engine and that makes inferences and people are lost. How in the world did this black box and these people, how in the world did these people build this black box that is able to do all this? And what is its value? Quite often it's hard to articulate its value or explain its value, which also brings us to the explainable aspect and interpretable aspect of AI. So some of the things that we will do from the point of societal benefit and a technical perspective, we learn to provide detailed interpretation models which are sort of a meta model that explains our model in some sense. So we'll do that along the way. But at the end of the day, you come out with a product and all you do is you tell a story about your data and about your inference engine. You should be able to clearly articulate its value. Are we together guys? So that is the scope of this particular workshop that we are getting into. This workshop will start out with natural language processing. We'll do a fairly in-depth project for about four to five, between four and six weeks, depends. Let's see how far it takes. But the more you haven't taught you guys, important in architecture, which is the auto encoders and the variational auto encoders and restricted Boltzmann machines and things like that. There are a few more things. I would like to cover those as time permits along the way. The format of our course will remain the same. We'll have Mondays and Wednesdays 7 to 10 p.m., occasionally pushing into 10.30 p.m. Then we will have the Saturday for quizzes. I will release a quiz every week, Saturday for quizzes and for some help. And Sunday afternoon, Sunday noon, we will keep for a research paper reading. Asim Kadavig, PhD, Now, before I continue this format is Asim Kadavig, PhD, Sort of a negotiation. If you guys feel we should add or subtract something from our workshop. This is the time I would like to entertain some ideas. Does anybody have any ideas on what we can add or subtract from the workshop. Nisarg uh i heard you talk about uh you know building a microservice and ui so for people who are not uh used to that or haven't ever like done uh been involved in developing a ui or using a microservice uh do you think it will be a uphill task to go about all that you do like once i give you the starter code picking up the microservice will be a matter of 10 minutes okay right but it is a good thing to learn you know to be able to write microservices that will that will serve inferences from your model, it is an important learning thing to do. Okay, yeah, so I think it will be a good way to like spend some more time on that, just for people who are not exposed to it at all. Yes, certainly very much so. So you will do that. And also the UI, see the UI, you can go very deep or you can be minimalistic. You can have a plain UI, which is like a blank HTML page with simple text and a form, right? Just calling the microservice. That needs very elementary, very, very elementary understanding of HTML and a bit of JavaScript together. Or it could be, I mean, you can go or learn the basics of React. React is a dominant platform for UI development. By the way, just as a show of hands, may I know how many of you have at least some familiarity developing things in React? So could you please raise your hand in Zoom? I see only one person so far raising their hand. No one else? I started learning React. I don't think that quite counts. Okay, well, let me count you as half of half a person so that makes two anybody else react to something guys you can pick up uh maybe i'll give an ex how about this i give you guys an extra session we do a lot of extra sessions as as you know, in this workshop. So in one of the extra sessions, I bring you guys up to speed on react. Will that do? That'll be good kickoff. Good idea. Yes. And the resources. Yes, yes, yes, we'll do that. And then the microservices part, we'll keep it very simple. We will use just REST. We won't go reactive, we won't do a lot of web sockets. We'll just keep it simple and vanilla, REST microservices. How many of you know or have written REST endpoints? Please raise your hand in the Zoom. your hand in your in the zoom two people I see two hands is that it three hands all right so for rest two I will give you guys a little primer I will give you so there are four people I'll give you a primer in two languages. One is using Python, I'll use it. And one is using Java. So I'll just, for the four people who raised their hands, I want to know which particular framework they are used to. Anil, which particular framework do you use? So I mainly used, for workday I created like the rest APIs and then also at part of the certification for Google Cloud. So like I did like some of the courses there. Okay so no no which which framework did you use python java uh so it was python yeah python which which one did you use did you use flask uh i used flask yeah okay praveen pravin you're on mute uh you are most of the time. I used in the Java Spring Framework. The Spring Boot Framework. Yeah, Spring Boot. Okay. So there will be a little bit of a, then you, even you guys might find it fun because I'll convince you that in Python, we should use fast API, a new framework which is much faster and backed by C executables for microservices. We'll use that. And in Java, we'll use a much, much faster framework than Spring Boot, which is, Spring Boot is very popular, but we'll use a different framework called Quarkus. And we will compile the microservice or the java into native binaries and as native binaries it becomes quite fast we will do that right so fast api and quarkus and that will hopefully add some new value to your learning so So those are the two things we'll do. The rest of it, this workshop will remain predominantly PyTorch based. Occasionally we'll touch TensorFlow and Keras, though I would like to stay with one architecture, and Keras, though I would like to stay with one architecture, PyTorch. Once in a while, I might touch on frameworks that are built on top of it. There are frameworks like PyTorch Lightning, which is a template, which is a sort of a systematic way to build up your code. Occasionally, I'll talk about yet another framework called fast AI, Jeremy Howells, which sort of tries to make it very easy to get your foot in the door with deep learning. So we might occasionally dip into that, but broadly we'll stay with the core of PyTorch itself, using libraries like PyTorch, Vision, and transformers will use quite a bit. Now within deep learning for NLP, the dominant libraries that we will focus on are for the classic stuff, all the stuff that are non-transformer based which which i define as classic even if they use neural networks or recurrence and so forth we will use a spaCy now the textbooks all the textbooks are there on your website but let me show it to you here in this picture do you guys see this here. Do you guys see this? Natural language processing with PyTorch and SpeeC. It's a very thin book as you can see, very fairly easy to master, very readable book, a read-along. It is at version two, which is the production version of SpeeC. There is a newer version of SpeeC coming, SpeeC 3, but that is still not in production. That is still in nightly builds. So we will go learn, get this book. This book is an easy read. You don't have to know everything in great detail, but whenever we talk of Spacey, dip into this book and you'll be able to do the labs. The first lab is based on spaCy. The second lab, then the second thing, the second good book on this is this book. This is a slightly higher level book but still very good. Are we able to see this book? Yes. Yes. On the website, it is all there, guys. So this is a book that you should use as a reference. It's not a book to learn from, but it's a good reference. And then it's a slightly high level book in the sense that it gets into the practical discussions and so forth. But from a textbook perspective the pytorch aspect this is the book natural language processing with pytorch and then there is another book which is by manning i don't know where it is on my bookshelf it has disappeared it is a manning textbook on uh manning publishers textbook on natural language processing with PyTorch literally that too, but I've given a link on the course webpage. That is a very good book. So let me actually walk you guys through the important books while we are at it. It is all there in the course page. May I just share the... All right. So this is how this website must be looking to you. So you notice that right off the bat, I have the first is a Slack channel. Then you have the this is, by the way, a great classic. If you guys are very serious about a career in natural language processing, you cannot be without reading this textbook. So let's visit this textbook and see what it is. This is it, Speech and Language Processing. The third edition is in the draft form. Don't buy the previous edition. You can actually, but they have graciously made the chapters that are emerging available here. So if you read this textbook, you will have perhaps the best possible foundation into this subject. It's very well written. It's written by scholars and researchers, so obviously the quality of the book is very high. So for example, it is available online. You can read it here, and you can clearly see that just from the formatting, it is a very well formatted, very good book. I would encourage you to read this. The trouble is our workshop is fast paced. And so you won't be able to read it in depth as we move. It needs a lot of time commitment. So consider this as a aspirational goal that as you go along, you'll dip into it where needed. And later on someday you'll go back and finish this textbook. There is also this online book somebody is trying to create, I forget, it's a very good book actually, NLP Overview. So it's a, people are trying to create this and it's a completely online book. You can read it. I read through this. The quality is pretty good, actually. It's very well written. So you can use this as another way to quickly go and learn it. It's not a very big book in quotes. It's very, very readable. And if you read this, you would realize that most of these things I've covered in the fundamentals part, in part one of this workshop. So this particular book, the second NLP overview, this online book will be an excellent review of all that we have learned. Are we together? And it will be a good sort of, it will go along very well with you. We haven't done everything, for example, we haven't done reinforcement learning and so forth but for what it is worth uh use this okay then the third book is this which i like very much natural language processing and action an excellent book An excellent book. Right. If you look into it, let's go and look at the table of contents. So you will see the NLP overview of building a vocabulary, DFIDF and so on and so forth. Deeper learning, neural networks, word to web, getting started with congulationals, RNN, Sequence to Sequence Models, Scaling Up and Optimization. Now, when you look at this particular book, you will notice that it's a very good book that covers just about everything, but its emphasis is not on transformers. I'll get together. It is, I think at some point it alludes to it or has some of it, I forget where, but it doesn't do that much emphasis on the new way of doing things which is predominantly based on transformers. So consider this book as your guide to the prior way of doing NLP, But nonetheless, it's an excellent book. So this is your book. Then you have another book. This is the book I just showed you the picture of, Practical Natural Language Processing, the O'Reilly book. It's a good book. You can dip into it. But use it as a reference. It's not a good book, in my view, to learn from. Then this book, I would consider to be this Natural Language Processing with PyTch which I just showed you this book right this I would consider to be your main lab book for lab treat it as your primary book Kaiser are we together right let it be your main lab book. Then we have, this is the other. This is something we will get through in the first few weeks. This is an easy book. I'll expect you to finish it in no time. Natural language processing with Python and spaCy. So knowing that you will need it almost immediately, it would be a very good idea to go and immediately purchase this book right away or get the Kindle version of this book if you're in a hurry. And you will be in a hurry pretty soon. Besides that, I put together some resources here how to solve NLP problems step by step right so this was a good article this is from the medium website and as you know if you are not used to medium I would strongly urge you to visit medium daily I treat it like my daily newspaper. Instead of going to New York Times, I go to Medium. So this is just a very sort of a informal way to get you started with the world of natural language processing. language processing. So do this. Then they have all of these curated lists and they are very good. So one of them is the awesome NLP curated list. This is fairly well known. It is a collection of resources on NLP. So what are the libraries in NLP in different languages that we talk about? And NLP is broad. There are libraries to do NLP in many, many languages. For example, you could be doing it using the Stanford NLP, which these are all Java, a core NLP, open NLP, and so on and so forth. These are all very well-established libraries. Then you have libraries in Python and C++. And in R, by the way, my own thing, I used to do a lot of NLP in R. R is pretty good with this, the support for this. And then, so obviously you have it in Python and so forth. So Python's of course hardly needs to be told. Okay, the one library that I didn't mention is Allen NLP. Allen NLP is something that's sitting upon PyTorch and Spacey and the transformers. The transformers and we will use a lot, we'll use PyTorch a lot. We'll use Spacey in the early stages for the traditional stuff and we'll use Allen NLP occasionally. Now in this space one of the libraries that I initially introduced you to is the NLTK. NLTK is great to learn from. It's big. Unfortunately, it doesn't scale very well. So it's not very fast, actually, unfortunately. So be warned of that. Text blob is a simplified version of NLTK. Now you will remember that we did labs with all of this. And some of these are beautiful, like for example, this scatter text. I don't, I'm not, I'm not very in-depth familiar with it. I've glanced at it. I'll let it be for you guys. But they give you a lot of beautiful visualizations that you can use that makes all the subject very real for you. So use that. So I'll go back to this curated list. And see, see ultimately you may have information overload so stick to a few stick to transformers spacey by torch allen nlp right and dip into others if you need to because you can't become an expert in all of them and it's redundant because then you'll start getting mixed up between them so that that is about the libraries. Then research summaries. If you're looking for the NLP research progress, these things are very good places to go and see what is happening in each of these areas. So for example, you can go and suppose we do dependency. Let's take a topic here. Suppose you want to learn the taxonomy, right? So there is a whole body of work that most people don't know about and you will come to know about all the greatest work being done in that for text classification. What is the state? What is the latest, right? So for example, the Excel net is quite important and so on and so forth. So you'll come to know about it. So worth learning about, worth learning about. I'll leave it as that. And then of course there are efforts in different languages. We can go to that. There is an excellent, so I'll go back now to our website. So that is about the awesome. Then the YouTube. This YouTube channel also is worth mentioning. For those of you who would like to have some additional videos. This is by the same people who wrote the textbook that I mentioned, right, the famous textbook. So they have also created YouTube videos of their talks, right, of their book. So you can, these are the slides and the videos from them. These are very good, So you can, these are the slides and the videos from them. These are very good. I think it is really a good idea to go and obviously subscribe to it. Oh, I'm subscribed to it, but with a different YouTube account. These are really good. You may want to do that and listen to some of these. So I'll expect that as we, because we focus more on the practical side now, some of these things I'll refer to as we because we focus more on the practical side now some of these things I'll refer to these people to do then podcast this is a very good site as you can see I'm showing you naturally the whole universe of things on this topic right this is very good podcasts that keep happening and they stay pretty current. There are not that many podcasts coming out every, in a month, a couple of them, if I'm right, comes out, two, three come out and so forth. So these are worth listening to, at least I find them quite good and worth listening to, especially when you are just, I don't know, sitting there and listening to some very, very boring meeting in your day job. This is the Sebastian Rudis website. Again, a very good place to get started. He has a lot of good topics that he talks about. Considered very highly in this space, then the Twitter, if you want to follow Twitter, if you love Twitter, then there is a dedicated place which is fairly well known in Twitter. For all that is NLP and you can subscribe or whatever it is that people do. I'm not much of a Twitter person. So this reference I know, but I don't myself use it because I'm not much of a Twitter person. So you make out of it whatever you can make out of it. Then there's another curated list. Now for embeddings, we talk about word embeddings. We did one in detail, the skip gram model. If we do it in detail, if we go really deep, it's a field in itself. It's a subfield in itself. People have created wonderful, wonderful ways of doing word embeddings. And so this is excellent. Just to show you the richness of how much work is happening in NLP. This is about all sorts of word embeddings that you can have. So there you go. You can see it's a pretty long list of things people have done, right? And the papers, the research papers and everything. Improving the language. So that is that. When it comes to frameworks, remember Spacey, Hugging Face Transformer, and PyTorch or LNNLP and PyTorch. These will be our main things. Occasionally we'll dip into the rest. These in my view are the dominant things. These are some papers we have done, the essential word to work. We have done glove. I'll cover glove and I'll cover something that is less known but should be probably more known. Poincar√© embeddings, it's based on very geometric ideas. Then Burt Paper, you are familiar with the illustrated things. Transformers, once again, there's a whole flurry of activities on Transformers. If you go to just awesome Transformers, you will come to the whole world of transformer resources. So that is that. So that is about our textbooks, guys, textbooks and this. Now, we'll also talk about the RNNs, LSTMs and GRUs. They are important. They haven't quite faded out. They have their own use and quite often you use them in conjunction with the transformers. I want to ask one question on the books. Go ahead, Pradeep. I want to buy just one book for now. And so if you could advise like something which, like it has a little bit of everything, not a very detailed one, not researchable kind of, but you know, I can refer it for pretty much everything. If you want that, what I would suggest is from this, the book that I suggested, the the O'Reilly book this book okay this should be a core book like I said click because we are practical make this your core book right and for the traditional one are this Manning book, the green book, right? Make it your core book. And for all other things, just refer to the classic, the great classic, which is the first reference. See, it's free. It's online. You can just quickly go and refer to it, isn't it? This book, right? It's freely there and you can quickly keep referring to this. I would suggest do that. Now the thing with natural language processing is that we are heavily transformer based and there are no good books that address NLP from and focus completely on just the transformers right in fact maybe collectively we can write it as a class i'm writing a few chapters uh if you guys are interested you can help collaborate with me and help me finish those chapters yes yes all right guys let's take a 10 15 minutes break and then we'll get into the core of the matter.