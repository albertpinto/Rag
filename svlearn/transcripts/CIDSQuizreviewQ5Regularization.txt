 So this is quiz 5 and this is on regularization. Question number 1. Let beta, which is beta0 and beta1, be a hypothesis on the hypothesis hyperplane. which is beta naught and beta one via hypothesis on the hypothesis hyperplane. If we choose a circle of radius d as the constraint, the constraint equation g beta is given by. So we have these four options. Now beta is a vector. So this would just result in two times of beta. This is not the right answer. would this would just result in two times of beta so this is not the right answer uh this one is the right answer as we the equation of a circle is uh x squared plus y squared which is is equal to r squared so in that way this is correct and this one is also not correct because this is again the same thing as the first option this is uh addition of two vectors and now the last one is correct because this is a dot product and the dot product of the vector with itself will will be the square of the magnitude of the vector and that is this this is right so beta in our case our beta vector is constrained and it is restricted to have this radius so and because of that we know that the magnitude the maximum magnitude that it can take is d and and so we have this answer which is beta into beta equals beta times beta beta dot beta equals d square okay equals beta times beta beta dot beta equals d square. To consider the context of gradient descent based learning for linear regression given below is the contour plot of the error function where beta beta naught and beta one are the coefficients to be estimated. Let e a e b and e h represent the error values on the contour lines a b and h respectively so we have a b c d which sort of go outwards from the from the optimal beta star right so let beta star be the optimal coefficient minima learn from the gradient descent process this is not from regularization but this is from the gradient descent process so before regularization choose the right expression for the relationship between the error values of the contour lines from the options given below so if you draw these contour lines um as equator surfaces um on the z-axis, right? Depending on their error values, we know that from beta star is the minima, right? So, and then these contour lines, they go outwards and for each contour line, the error increases. And therefore, we know that E beta is the lowest. And from that, Ea, Eb up to Ej it increases from there so this option is right. Next on the hypothesis hyperplane where each point beta is the hypothesis the contour curve in blue of the error function e beta and the constraint circle in red tangent tangentially touch each other at a point p given this statement which of the following are true okay okay so let's just draw the tangent line. So here I've drawn the tangent line and let's go through these options. Now we know from the lecture that for any point P on the error surface or any curve, the the direction of steepest ascent or uh the grade the gradient points directly perpendicular to the tangent from from that point so if this is this is a point uh let me just so oh yeah so if this is a point and we have a curve like this, we know that it is directly pointing outwards and it is always perpendicular to the curve at that point. So, in that case, the first option, the tangent of the contour curve is parallel in direction to the gradient of the constrained circle. So, actually the tangent for both these curves are the same line so this is not true the tangent of the contour curve is normal to the gradient of the constraint circle yeah this is true the gradient of the contour curve is parallel and opposite in direction to the gradient of the constraint circle that is also true we see that both of them must be pointing in the opposite direction as the circle increases in the opposite direction to the I mean to the error surface this will be true for in wherever the error surface may be this would be true so this is true the gradient of the contour curve is normal to the gradient of the uh constraint circle this is false they are actually parallel and opposite in direction nice illustration so for that one i think if you go back to the previous one uh i think the second and the third oh one was tangent another is the gradient okay got it yeah yeah the parameter of a simple linear regression model beta naught and beta 1 are plotted on the x and y axis respectively on the base hypothesis plane the error function e e is plotted on the z axis the gradient vector of the function e at a point p on the hypothesis plane is okay so uh for this thing we know that uh gradient vector it actually just points it has a magnitude and a direction and it is a vector on the uh base hypothesis plane that's that it's that is its definition. But I guess a lot of people get confused between a tangent and the tangent here is just it's a plane so let's say there is a point that just touches that curve on that single point and it doesn't touch anywhere else so that is a tangent plane but the gradient vector is a vector on the base hypothesis plane and it points in the direction of the steepest ascent so in whichever direction you will have to climb up the hill that would be the direction so it will be like north or you know it'll point in a direction plus some it will have some magnitude which which tells you the slope at that particular point so in that case this for this we we know that it must be on the hypothesis plane gradient vector Vector. Given a differentiable function GX and the contour surface of this function, then the path of no change at a point on the contour surface is. Okay, I think this could have been worded slightly differently. The path of no change in magnitude or no change in the error function i think that would be or no change in the function would be appropriate but what it basically means is that at this point so now we consider these options we look at the base hypothesis plane and we look at delta g sorry we just look at the gradient this is the function g so we look at the gradient at that at that point so the first option is in the direction of sorry nabla g saying it wrong so it's nabla g the direction of nabla g that is not correct because that would be the path of steepest ascent in the direction along the contour surface well that is true because if we go along the contour surface we will not see any change in the direction of the vector k such that k dot nabla g equals zero now that is true because if we that would be directly perpendicular to this vector and we know from it is the value zero we know that they are perpendicular to each other so the the two vectors are perpendicular to each other so this is this one is also correct and we have on in the direction along the contour surface in any arbitrary direction away from the contour surface that is not true and in the direction opposite to nablaji and that would be the path of steepest descent so let's just next one given a differentiable function so the same question they're asking this path of steepest ascent steepest ascent is in the direction of nablaji Steepest ascent is in the direction of Navlaji. And the same question with the path of steepest descent and that would be the direction opposite to Navlaji. So after the gradient descent learning process, if the polynomial regression, if polynomial regression produces a very large optimal coefficient minima beta star then so the mean squared error of the model with the coefficients beta star is very large no that that may not be true on the hypothesis hyperplane beta star is very close to the origin that is not true on the hypothesis hyperplane, beta star is very close to the origin. That is not true. On the hypothesis hyperplane, beta star is very far from the origin. That is true. Almost everyone got this answer correct. The paraboloid error surface of the error function is far above the hypothesis hyperplane. This is the same as the first one. These both mean the same thing. That is not true the gradient vector of a function g with parameters x and y at a point p equals a random point x comma y is given by so we know that this point is on the input plane or the base plane of of this function g x y so now what is the gradient vector it's just the partial differentiation of g with respect to x and then with respect to y that and this is the vector That, and this is the vector. Okay, so we come to the CD. I really like these. illustrations made by us, so the figure below shows a unit circle of some arbitrary minkowski norm which of the norms is the figure most likely to represent okay so. Can anyone answer this simple. How yeah so we went through this in class if it's con concave you know that it's less than one and it as it gets convex you know it's it goes greater than one so now this is this is one right so this is the manhattan distance and a circle and we know that it's two euclidean distance norm and the square so now this is the most convex it can get and so now that is manhattan distance oh okay the answer is given as manhattan distance but that's not true and i think the answer is given as Manhattan distance, but that's not true. I think the label is incorrect, but L is equal to infinity, yeah. Yes, okay. I'll change the question. I told Ashif I didn't care, but I think we are. I think the other original round, he kind of had Manhattan Euclidean mixed. Okay. Everyone looks at the L number anyway, so. Yeah, okay. Even I didn't notice it until now okay I'll change it. If we increase the regularization hyperparameter lambda of a regularized regression model then during the gradient descent learning process what happens to the size of the constraint? so this could be just understood we could just think that as we increase the dampening of yeah as we could increase the dampening then the lambda increases and when we increase the dampening, the size of the constraint region decreases. But I've also made a small illustration to kind of understand it visually. So we, we know that as we go further away from this bottom of the curve, the gradient increases quite rapidly. So if we if we look at these different contour surfaces of the error function and different constraint regions that we are selecting there's this relationship between the optimal points that we would so a b c d are the optimal beta stars that we would reach if we choose say this particular yeah if we choose this particular constraint circle then we would arrive at a if we choose this then we'll arrive at b and c and d so on but we know that at this particular point at say a then nabla e is or is zero but nabla g is quite large because this is compared to the other circles the nabla g must be large because the circle is larger and so we we also know that nabla e equals minus lambda nabla g and this is just a parameter that kind of makes these two equal so in that case equal and opposite so in this case lambda should be zero so that they both will be equal because nabla E is zero. And here at point B, we see that the error slowly increases. The gradient at BCD, they slowly increase. But for BCD, nabla G will definitely slowly decrease because the size of the constraint decreases. will definitely slowly decrease because the size of the constraint decreases so in that case uh if we just look at this relationship we kind of see that lambda sub is likely to increase because half times of nabla g only that would be equal to nabla e so these are just uh approximate obviously this is just for understanding and they're not exact but we see that lambda is supposed to increase and only then these nabla g and nabla e would be equal so we see that as we decrease the size of the constraint or we increase lambda the size of the constraint decreases and size of the constraint or we increase lambda the size of the constraint decreases and that's how pretty much i think how we control the size of the constraint by setting a parameter setting our lambda parameter nice illustration the red constraint circles yes yeah the size of the constraint decreases. Consider that on the hypothesis plane, the contour surface of the error function E with gradient nabla E and the constraint surface G with nabla G tangentially touch each other. Then the following relationship holds. So this is the relationship that we just saw. So what is this called? This is just to make sure that you know what it's called. It's called the Lagrange multiplier. After the gradient descent learning process, if polynomial regression produces a very large optimal coefficient, minimum beta start, then the model most likely overfits the data now that's true we have a large quantity of data that's not true i mean it could be but that's not what we get from the question the values of the variables are large and we haven't standardized the variables before learning so this was discussed in class and i think premjit learning so this was discussed in class and i think pramjit raised this question if you if you want you can go back to the lecture and watch this again and understand why this is not true the models most likely underfits the data that is not true next uh okay so we have to find the expression that shows the lasso regularization loss function. Okay, so can anyone answer this? First let's review that lasso is the one that's not a circle and it's the norm is L equal 1, right? Is it c or? It is plus. Actually, it is d. It is d, yeah. Yeah. So what we do here, yeah. Yeah. Plus lambda delta g. Plus lambda, yeah. This is what we want. And so that's why 2 is the right answer. Also, that's in the textbook. Yeah. Lasser regularization is superior to ridge regularization in making an accurate predictive model because, okay, we can't blatantly say that it's superior because it might be superior sometimes. We might require reducing, I mean, removing a few parameters, but we may not also need it. So dimensionality reduction, we might need that, but we may not also need it so dimensionality reduction we might need that but we might also not so in that case we cannot say that uh what is the answer here lasso rocks lassos more modern and none i think yeah none of these yeah none of these losses not necessarily superlative age yeah because i said mentioned something about like nfl like no freelance theorem yeah yeah exactly which of the following is true okay so what about the first one? That is not true. Minkowski norm L equals one is associated with LASSO regularization. Yeah yeah correct. And this is not true. L equals two is not true. That is also not true. Yeah this one is true. Because ridge is Euclidean space and that is true. How is regularized regression different from ordinary least square regression? A regularized regression model includes a constrained term, the loss function, in addition to the sum squared errors, while the loss function of an ordinary the sum squared errors while the loss function of an ordinary v square regression model does not this is true yeah and the rest are not because there's just one option correct i'm not going to read everything the ridge regularization loss function for polynomial regression of degree p is given by. And Ridge is L2. So you can think of that to remember to square. Right, let us see here. Yeah, correct. So what is shrinkage in the context of regularization of a regression model? So we know that shrinkage, okay, so shrinkage the sample size, shrinking the sample size of the data set, that is not true, shrinking the training data set also not true, shrinking the hyper parameters of the model, that is not true, shrinking the coefficient estimates and bringing them closer to zero. So that is true because what we are doing here is at this point, the coefficients are quite large. And we are slowly, with our constraint circle, we are slowly bringing them and we are shrinking them and getting them closer to zero. So that is true. What is the purpose of regularization? It helps mitigate underfitting. It helps mitigate overfitting. That is true. A point P on a two-dimensional plane in vector notation is expressed as. So we remove these unit vectors and we just use their coefficients and this is how we represent a vector. The path of fastest ascent of a function is the direction along. So yeah, we're going back to this. It's along its contour line. That is not true. That is false. The presence of very large coefficient estimates of a polynomial regression model for a dataset D tends to produce wild oscillations in the model's prediction curve. That is true because each coefficient tries to pull the curve along its direction and that results in a lot of oscillation. That is true. You see the overfitting. Yeah, yeah, you see the overfitting. The presence of very large coefficient estimates of a polynomial regression model for a data set T signifies that the model may have significantly underfitted the data. So just as Kate said now, it has overfitted the data. Same and then we either should consider regularization or selecting a polynomial of lower degree that is true because we could either regularize or we should think of reducing the hyperparameter so true okay my it is slow. It's two more questions. Come on. One minute you can do it. Yeah. Come on. Okay, yeah. If the regression model performs very poorly on the test data set compared to the training data set, it is likely that the model has overfit the data. Okay, let me read it again. If the regression model performs very poorly on the test data set compared to the training data set. Yeah, this is true because the uh the curve kind of hugs the errors as well and it leads a lot into the noise instead of the actual signal so and that's why we see this okay then if the regularization hyperparameter lambda is zero for the same training data set rigid regularization lasso regularization ridge regression sorry ridge regression model lasso regularized regression model and the unregularized regression model produce the same coefficient estimates that is true because if in that case the regularization term becomes zero if lambda is zero and so that is true yep and we are done finish attempt and let's see how I did. Oh, I left a few questions but aha. Okay. Yeah, I think I did pretty well. Except I didn't choose this answer. Yeah. Otherwise, it's fine. And I left her two questions but I got. Yep. And we're done. Very good. Okay, right under the bell just in time. Yep. Let me stop sharing. Stop share. Hi, as if. Hi, Kyle, thank you for holding this session. Kate Kyle. How do you need? This is really great. And did you folks do you have any question doubts are you guys finding the quizzes useful useful for all levels yeah i'm asking the the participants if how they think about it yes it's useful so it is useful so i have been experimenting with a different writing software called Sketchbook Pro. It will take me a little bit of time to get used to it. So today I'll still use OneNote, but I think we have hope it might be a better thing to do to use that. So we'll see. All right. So I should share my screen. I don't start recording yet. Let me situate myself properly. And where's my writing pen? It's recording. I'll just ask Joy to cut it off. Yes, sure.