 Last time on Monday we did a lab on autoencoders. We learned many properties of the autoencoder, the ability to remove noise, the ability to come up with much more efficient compressed representations. The ability also to create in some sense a stable representations and so forth. We did all sorts of things. Then we studied the theory of variational autoencoders. We haven't done the lab on that so far, but there we learned that encoders can be generated models and we can use it once you train it on data. It can be used to generate new synthetic data. Like for example, if you train it on a lot of faces, you can then train it on, I mean, then you can generate new faces from that. And you can give two faces and you can then ask for a continuum of faces in between. So that's the power of autoencoders and creating that sort of representation of models like that. Today, we are going to get back to the concepts in attention and transformer. So last time we finished all the bottleneck methods. We did word embeddings, we did this, that, and we did auto encoders was sort of a digression, but we did it because we hadn't covered it in the fundamentals. Now we get back to NLP and we focus on transformers. We did a week of transformers, so I will expect that you have some recollection of it. I'll go a little bit faster and I'll cover BERT because I don't remember covering BERT. Asif Srinivas here. I have no idea of a transformer. I know I'm going to go faster, but just like... I'll make sure you understand. Okay, thank you. Any other questions, any other thoughts, guys? So stop me if you don't understand, and do please stop me. So all right, so let's get started. Today's topic is attention transformers and in particular birds. And what can we do with all of these things? It's remarkable actually how many things we can achieve. So to motivate that I'll bring back the concept of the encoder-decoder. This is a good starting point for these transformers. So suppose you have words, you have a sentence, starting point for these transformers so suppose you have words you have a sentence the cow jumped over the moon and let us say what you want to do is translate it into some language let's say you want to translate it into a pick a language you can try. I don't know if you can try. Spanish. Spanish, okay. Okay, good. And then you can tell me the translation. So what happens is that it will be, suppose you have tokens, one, two, three, four, five, six, and then full stop is it. This is the end token, so, separate a token. So suppose you have seven tokens. When you translate a sentence into another language, it is not necessary that in the other language it would have exactly the same number of tokens. That is one aspect to remember. Some languages you need a lot of words to describe something, and in some languages maybe they are more efficient representations. You can express the same thought with much fewer words. And then vice versa, the first language may be efficient at something and the second language may be efficient at something else. So that's a variety of human languages or natural languages. There is another aspect that you have to consider. Not only are the number of tokens likely to be different in Spanish, but if I were to convert each of these words into Spanish, words into Spanish. So the order won't be 1 prime, 2 prime, 3 prime, 4 prime, 5 prime, 6 prime, 7 prime. Suppose these are translations, translations of the words themselves, then these will not be, these will not be literally in the same sequence. For example, the French at least I know that they say quite a few things the opposite of us. We would talk of the Eiffel Tower. And I don't know French, but if I had to say, try to say it in French, it would be something like tower, the FL or something like that. So it's the other way around. So the sequencing also matters, the order matters. And so the question therefore is, when you translate from one language to another, it is a given that you can't just look up a dictionary and the translated meaning of the word. Even if you tried that, you would have difficulty because occasionally what would happen is you would encounter a word which is a homonym. For example, the word apple. It has multiple meanings based on the context that you are in. And okay, so... In French, the word for potato is something like earth apple earth apple is very interesting right so yes that's a variety of language so how do we translate from one language to another and how do we deal with the fact that some words have multiple meanings? Apple to the techies is a laptop or some company, a tech company that makes technical devices. To the farmer and to most common people, apple is an apple that they would eat. So, and like this, there are many, many words, right? You use the word stick, is it to stick to something or is it a noun, a stick in hand, right? So you cannot literally translate a word into another language, you have to be context aware. And that is a crucial word. Any word that you try to translate, Any word that you try to translate, you need to know the context. Right? Without knowing the context, you can often end up with bad translations. So let's take a word that has many meanings. Would anyone of you like to propose a word? Stand? Stand. Let's say stand. What meanings would you associate with stand besides just standing up? Like a music stand that holds? Yeah, a music stand. Yes, that's true. A music stand, let's say. Or I can't stand you? Yes, that's right. Stand. So the other meaning of stand could be bear, tolerate. Yeah, that's an excellent word. Let's take stand. You realize that we learned about word embeddings. Now you'll often see me use the word classic word embeddings. All those word embeddings were very exciting. Word to web, using skip grams, using continuous, bag of words, glove, fast text, etc. And they do have their place under the sun. But the main limitation is that they are not context aware. So there is one unique vector to stand for the word stand. But's not it the stand is actually three different words which happen to have the same spelling right homonyms i believe that's what they call them uh three three separate words that happen to have the same spelling in many ways and you can think of it like that so technically they should have had three different embeddings. So there was an effort to create context-aware embeddings and there were things like ELMO and so forth. I won't go into those. They tried to create those. One of the things that we will do in the current session is we will see how these things are automatically generated, a context-aware embedding of words is automatically generated as a natural learning step in the transformer. So just for those of you who are encountering the word transformer for the first time in natural language processing, transformer is a neural architecture which uses sort of either encoder, decoder or only encoders or only decoders, but basically it has a lot of what I call attention units so today i'll start by explaining attention units but we'll get there but let us just give you the overview hereafter like quite often we will use word embeddings that are context aware and that are generated as part of your exercise so for example if you're writing a sentiment analysis, a classifier of a sentence, positive, neutral, negative sentiment, and you have a lot of sentences, why not create a custom embedding, learn a custom embedding along the way as part of training the classifier. Are we getting that, guys? That is the goal, and we will do that in this architecture. But this architecture, the transformers that we are going to talk about today, they, so this is a transformer. So we need some building blocks before we understand transformer. And so we go back to something we talked about the encoder decoder architecture. But prior to transformer, this these tasks used to be done with RNN and derivatives of RNN, RNN and LSTM. The basic model was this. You take a box. So imagine that this is it. You have two pieces, encoder and a decoder. What you do is you pass the sentence, the cow jumped over the moon as a sequence of series of first goes S1, S2, SN. And at each step, this encoder, what it learns is it might emit something. It might produce, let's say, an output, but typically it doesn't. An RNN can, but here you may not you produce a hidden state h1 all the way to hn and any one of these hidden states becomes and that is the recurrent so you know you feed it back as an input to the box so this is the recurrent part of it strange you take the output in some sense one one output that you can have another output, but the hidden state that you have, you take it and feed it back into the box. Now, you say, why would I do that? And it has a natural explanation. slowly I say this word, the cow jumped, you realize that in your mind you are slowly constructing a thought, isn't it? There is a thought of what you have heard and that thought is accumulative and it is aware it also depends on the words that came before it for example if the word was the cow stood right now i'm using that word stand so when you you when you have the word stood or the cat then you because stood, because the two words, the cow came before it, you implicitly know what is going on. You have your noun, you have your verb, and now the cow stood, you wouldn't confuse it with a music stand and you wouldn't probably confuse it with may or may not be tolerate but pretty much not right and it is the act of standing that remains as a high probability a thing and the cow stood therefore is a thought it's a coherent thought right now you say over the moon as the words keep coming up, what happens? You have a thought vector in your mind. You have a thought in your mind. And that thought becomes richer and richer. Every new word that comes, it modifies the thought. But then the new word has to be interpreted in the context of the previous thought, the accumulated thought so far, the partial thought so far. Am I making sense, guys? I hope I'm saying something very sort of hopefully intuitive. So, guys, I will appreciate it when I ask, are we understanding it? If you guys can unmute yourself and give me feedback. I can't see your faces. So I have no idea whether you're following it or not. Following. Yeah, okay. Yeah, okay. Is it easy to understand it? So that is what is understood by this fact that the thought that has accumulated so far, H1, when the first word comes in, you feed it back. It is your context. And then you use not only the second word, but the thought from the understanding from the first word to understand now the phrase, the collective phrase, the cow, right? Jumped, for example, the cow jumped. Jumped takes its meaning much better when you, and the whole sentence, the phrase takes its meaning from all three of these. So that is why you feed this back in. And so these are called recurrent neural units. Now the recurrent units can be there in, and we didn't cover the recurrent neural networks so much, but this is pretty much it. At some point I'll do that. They still have the users, but transformers have come and pretty much dominated the scene. We still often hybridize them with RNNs and so forth. So in all honesty, we need to give a proper session to doing the RNNs and LSTMs, but we'll do that at some point. Now, this thing, you can unroll it to look like this. You can say S1 all the way to SN is going in. so the way it works is that the first thing you you have let's say sentence s1 went in it produced the hidden state okay let me just say trigger this is how the diagrams are made this is a trigger point this is s1 went in so not the sentence sorry the word s1 went in the word s2 went in and so it the cow right and at each state you're producing a hidden state it goes in to the next box so it is the same box unrolled but just for logical thinking you think of it like that the cow jumped so forth and then you continue on and what you have is this is your second thought vector context vector i often use the word thought vector it may be a bit uncommon but it helps me capture the intuition of what is going on so So jumped over the moon, final word. So what comes at the end of it? This entire thing is actually just one RNN, but we have laid it out, we have unrolled it as though it's many. It is the encoder. Now at the end of it, we'll have the final thought, which is, I believe, seven tokens. Seven. So you will end up with H7. Think of it as the final thought. Or more formally, the output context. And of course, because we are doing machine learning, everything here is a vector. Remember that vectors go in, vectors go out. So here, of course, you assume that you have taken the words and somehow, for example, converted them to the word embedding. So then something to done something to vectorize them or just do TF idea vectorization. I mean, just one hot encoded it or whatever it is that you want to do. In some sense, you have encoded this, not TF idea for this for document. You have somehow converted the words into vectors. For the simplicity sake, just imagine that you have done it with, let's say you use word embedding or something like that. So these things keep going in and this comes out. Now encoder differs from the standard RNN. One thing, standard RNN also produces some output. Here in the encoder part, we ignore the output part. We are only caring about the hidden state that we propagate forward and the next word sticks out. So then what happens here now, let's look at the decoder part. The decoder, the full decoder is like this. The final thought goes in, in this, and what it will do is it will emit the first, let's say that we are doing translation. It will emit out the first word, right? So since I don't know Spanish, I'll just use the word alpha. And then it will just emit it out. It's just a trigger. And there is nothing going in at this moment. And then what happens is this output is fed back into the RNN along with the output state. So once it emits alpha, it produces a hidden state of the decoder, right? So let me just call it encoder. It will produce the first decoder state of the decoder. And then that will go into the RNN again, and then that will go into the RNN again, as well as this alpha will go back in and it will produce the next word beta. And this process will continue. This hidden state will get, H2 will get fed in and this is a beta feeding it. And then gamma will come out. And let's say that this, at the end of it, the hope is that alpha, beta, gamma, right? Delta, epsilon, let's say. Let's say that five words are enough in Spanish to represent it. So the Spanish sentence. So the way you represent this is, obviously this is unrolled version. In reality, you're feeding it into the box as it is. Right. So in reality, what happens is that initially you trigger. Then what happens is that some output comes out, some hidden state comes out, and you feed both of these back in and it looks funny, right? And then yet another output comes out. I find that very non-intuitive to draw, so I won't draw that. I mean, let's stick to this basic diagram. And so by the time the last of these outputs, last of the words have been produced, at some point this will say stop, you know, full stop, separator, or something like that. And so you're done. It will be done. So this is your encoder, decoder architecture. Are we together, guys? Now, when people did this using RNNs, the recurrent neural networks, and the LSTMs and so forth, there used to be one problem though. LSTMs and GREs did better than the vanilla RNNs. They had more ability to remember and forget and all sorts of things. But essentially, the theory is that of a RNN, your feedback into the structure. The problem that we faced was that if you have a long sequence, imagine a very long sentence, then this sentence, this the final thought vector, H final, it remembers, or it is more aware of the last few words, last few words dominate. And very early words, early words fade out. early words fade out. So if you think of this final thought vector as sort of, you think of it as a gradient, as color, the early vectors were let's say blue and progressively the later words you paint them red. So what happens is when you look at the final thought vector, ideally its color should be midway between blue and red. But what tends to happen is it is more influenced by later words, so it is far more red than it is blue. Are we getting the concept guys? Yes. It happens to us when we listen to stories as children isn't it? That suppose there's a very complicated story and somebody has told. We tend to remember the last part of the story. So if you take a two-year-old and say this happened and he did this and X did Y, Y and then Y and Z, Z then did this. And then you go back and ask the child, let's say that X has done something, X has jumped and and Y has eaten an apple, and Z has done something, or something else. But you ask the child who jumped, because they remember the Z. They're much more likely to just repeat the last name. They'll say Z and the apple. I don't know if you have observed this with children. I used to observe this. So there is a slight, I suppose, it's a very hand-waving argument, but that's the basic intuition that as you train it, because the final, this vector, it has finite size, you give it a finite memory. Right, so many uh bytes of uh space that you have given so what tends to happen is ultimately so it is a vector basically of some dimension let's say that it's a vector x final it belongs to a vector space of let's say d dimension so you have d values the d floating point values here those values get much more weighed towards recent words because they come and they wash over the every time a new word comes it washes over and modifies this into the next hidden state so that's what goes on now though that is a limitation it depends upon what era you are in when the RNNs came out and all of the cellistines they were a runaway success they were absolutely amazing and people used to talk about the unreasonable effectiveness of RNNs and so forth so So very influential articles. And everybody was puzzled that it's working so well. And for short sequences and for many variety of situations, they do work very well. You could do time series with it. You could do many, many, many things people started doing with the recurrent neural networks. So they had quite a dominant, sort of quite a dominance for many years. And then people began to be more aware of the limitations, the more they used it. And the biggest limitation was exactly this, that given finite memory tends to de-emphasize earlier words in the sequence and be much more colored or influenced by later words. That is one limitation. But even more, and limitation which is not a conceptual limitation, but from a very practical perspective, from an engineering perspective, RNNs practically ignore the massive parallelization that you have in today's AI machines. So today's AI machines are usually stocked with tensor compute units or graphic units, GPU cards or TPUs, tensor processing units or GPUs, and all of them can do a lot of vector processing. They can do a lot of parallel operations. The problem with this recurrent or this particular way of doing encoder decoder was or is that we have no way to parallelize. You have to feed in the token one by one, right? And so it is a little bit slow process. There's some optimization you can get for vector processing, but not much. And so you had two worlds. In the world of image processing, there was massive progress because of the use of hardware, optimal utilization of hardware, this parallel compute capabilities. In the world of NLP though, we didn't quite have the benefit of that. So that was one aspect. Also transfer learning is not as sort of smooth as it is with transformers. So there's some practical aspects also that come in, but from a conceptual perspective, the sort of the fatal flaw with them, if you use the word, So there's some practical aspects also that come in. But from a conceptual perspective, the sort of the fatal flaw with them, if you use the word, for very long sequences is it tends to forget the earlier words or de-emphasize or not remember the earlier words. So then the question is, so let me summarize what we are doing. What we say is that we need something. The architecture of encoder decoder stays, though, encoder. So let us say that you feed it a sequence. Let's say that you have a sentence made up of word. Now I'll just write the word W. Okay, W1, W2, Wn, right? This sequence goes in, this is your sentence. Obviously you feed it the word, first word first. And what it finally comes out is people often use H as a hidden part vector, or they use the word context, right? Encoder context, and sometimes they say this is the encoder context or whatever or they'll use the edge like i am using h final the final hidden state that comes out of this encoder and then that state goes into the decoder and the decoder this is word decoder down here because this keeps emitting words, right? One by one, it keeps emitting the words, right, in a sequence. So to take it further, decoder, how does the decoder work? It has awareness of the thought vector, the final thought vector. It uses the words that it has produced so far, alpha, beta, as another context that it will use to predict the next word and the next word and the next word, right? So all of that, it uses. And then this is your encoder-decoder architecture, as it is called. This is what it is. is now keeping this idea here i'll take a small digression people asked this question that how do we address these limitations the parallelization aspect and uh the fact that it doesn't the attention doesn't stay with like you don't have long span sequences handling handled very well so then in this with this as a backdrop there was a pretty interesting development there was a development of a concept called attention this was in 2014 A concept called attention. This was in 2014 and then subsequently followed with another paper in 2015. Those are the so you can look it up in the in the. Concept of attention and the concept of attention is actually very simple. It sounds very mysterious and people get quite often frightened when they look at the architecture drawn on different PowerPoints and so forth. It may look intimidating, but the idea is actually very, very simple and hopefully we will understand it in a very simple way. So this was in 2014 and 15. And what those researchers did is they took the RNN, which was a dominant architecture at that time and some variant of it, LSTM, etc. And then they added attention to it. They started adding attention, this concept of attention to that. Now, what it is, I have not explained. And so then they started getting much better results. And they knew that something is up, something very interesting is happening. But so results. So the question is, what is attention in that thing? So before I do that, I'll do it. I'll peel the onion or the concept of attention in. I will treat it as an onion and peel the layers one by one. So at a very intuitive level, you can say that, see, if you go back and look, we have many concepts. We have, you know, if you look at the thought vector, we have H final. But before the H final, there was H final minus one all the way to H1. All those thought vectors were there. all the way to H1. All those thought vectors were there. The earlier thought vectors or the earlier context vectors, they were very aware of the early words in the sequence, isn't it, by definition, because they haven't seen the later words at all yet. Am I making sense, guys, in this? Am I making sense guys on this? Right. So with that, so given that, so why not take when we are decoding, why just take HF, the final thought vector, or why not feed it in a logical way or in a sort of a conceptual way, all of these context vectors, and learn which context vectors. And learn which context vectors are more important than others, right? If there's repetitive and so on and so forth. In some sense, in some way, clever way, learn to utilize all those thought vectors. Because then there would be a demo, in some sense, a democracy of influence. All of the words will have, will find their place under the sun and they won't get forgotten. The early words in the sequence won't just fade out. Are we making sense? Yes. So this was the basic idea. The concept of attention is, can we, while we are decoding, utilize not just the final thought vector but all the previous thought vectors right along with it as a context and obviously you have to give certain weightages to them like which thing to for this step of the decoding which one I should give more emphasis to and so on and so forth and you can learn those sort of a thing. And that's the technical detail of attention that I will get into in a little bit. So do we understand that, the concept of attention? So now comes, now we'll peel the onion further and we will write the sentence, as the cow jumped over the moon and try to build it up in a slightly different manner. The cow. So see, when you are decoding, you are giving, so suppose you have already generated alpha, beta, gamma, three words in Spanish. So they together form a context that's one context you're also feeding in this h h i uh the different values of which i which i so the question is how much importance etc you give to them you can figure that out you can train it and so on and so forth the you you can look at what to what to seek you can look at what to seek, right? And you produce the word. And you were doing it with RNN, it was still sort of a very sequential operation, right? You got away, you solved the problem of longer sequence, but you still had this big problem that the whole thing was not parallelizable. So then came in 2017, December, a landmark paper, which landmark paper which has become almost like a it's a sort of a favorite phrase within the NLP community, attention is all you need. Right, so this paper introduced the transformer, the concept of a transformer. And it claimed that you can actually do everything that you were doing with the RNN or RNN plus attention by throwing away the RNN altogether. That's quite a remarkable thing to say. You're saying let's throw the r in and out of the window and r n's were the dominant thing at the time so how in the world can you do that so the first thing is that you can't just push all the words at the same time through this encoder that would be disaster because the sequence the ordering of the word matters. So the first thing you need to do is you need to give a positional encoding to each word. In some sense, you have to say that this is the first word, second word, third, fourth, fifth, sixth. Right. So if you are going to feed it, you have to feed it as like this sort of a pair, a word and its position encoding. And if you can do that, then imagine that you feed it into the box and a box. All of these words at the same time, let's say that they are N words, you feed it all at the same time. let's say that they are n words you feed it all at the same time and what you do in some sense is you come up with these hidden states that we have to use right the h1 h2 etc the diverse thing except that the idea is taken a bit further you you say that if we consider each of these words to produce say that if we consider each of these words to produce their own value states, then how much, like when I am decoding, converting this word into French, right, the first word into French, this is number one and number two, when I'm converting this into Spanish sorry then how much should it care about what comes later right or the other words in there we need to figure that out so there is a very interesting analogy to this that I like to use well I don't know very interesting very obvious in here rather see most of you are familiar with sequel databases, data access, right? So in a data access, what do you have? You have some sort of a primary key, some key, right? K1, Kn. And then you have some values sitting there, a value vector, the rest of the things you can consider to be, like if you treat the rest of the columns together, they make a value. And so what do you do? You need three things. You need a query, which would be something like select star from table t, t where, now what happens? Now you have to say a k is equal to, and now you have to give it a value, a query value. Does the SQL sentence look very obvious? And this is what we do, it's a very intuitive thing to do with relational databases, for example. Am I making sense guys? You can say a k is equal to like I don't know if it was a number I could say k is equal to 15 or k is somebody's name and I could give the name there. So guys tell me that this intuition is very obvious. This is how we access data. This is data retrieval. Yes. So now this is an exact and deterministic process. Here, what are the things we have? We have, of course, the key. We have the value. Of course, think of it as the row, the rest of the row, the meaningful part of the row. A key is more or less like the coordinates. for the meaningful part of the row. A key is more or less like the coordinates. Value is the real deal, real thing in some sense, isn't it? But at the same time, you also need a query. Because if you're going to go and get a particular row, you need to have some query to put here. Query value, query like 15 or something like that. You need to feed that in to your SQL. So you need three things to do your retrieval. Key, value, so database is there. And then you need to do, I mean, you take the key, you take the query to find the best match key. Typically, you do a match to key and then retrieve associated value. Associated. associated value. Associated. Would this be a standard a pathway for us to do a data retrieval? We apply the same idea. Now, this is a deterministic thing. Suppose I slightly weaken it. So sometimes you say a k is equal to 15. We can often give it like name. K sorts of matches hammer ham hamlet all of those things will come back from this isn't it are we together guys in in select, select star from T where K is like that. Sometimes you give a more weak query. You don't know the exact answer. But let us say that you are fairly sure that only one guy will match, but you don't know what it is called. You remember only some near the key. Those of you who have done a lot of SQL, please tell me that this is very obvious. Isn't it? So now let's take that idea a little bit further and say, suppose, now get into the probabilistic domain. Suppose information was stored in encode stored somewhere by all the words were stored by the key. So every word I had associated with it, a k i and a value i and imagine that this is sort of the storage of the word right then what happens what you have to do is suppose you have another word you want to find out suppose you have a q j you want to find out, suppose you have a qj, you want to find out or retrieve a value, a sort of a value. What will happen is that you want to find the similarity between similarity between ki, qj, right? If it is not an exact match in in the probabilistic domain the most crucial thing for retrieval would be something like the similarity isn't it guys sanjay are you getting that yeah yeah it's a so you get a sim some a similarity is the best you can do. But if you... Okay, I'll repeat that. See when you do SQL access, so there is this the path query to match to a key in the database to retrieving the associated value yeah i got that even i got that wi uh you know for some retrieval you need ki and vi so i i got that piece right what you got last is you know qj yeah so look at this sentence select something something from right maybe i can say select value from from some table t where k i is equal to some query object some query value right something like 15 or something then if i weaken this in in databases you do that using the percentage symbol like that so suppose you know that there is a guy right whose name is whose name starts or has uh but you don't know what what is the letter, what is the last couple of letters, but you vaguely remember that this is there in the name. Then what do you do? You just say ki like and then you give in quotes some sort of a something like this in your SQL query, right? It is no more equal to, but it is a like. So now take that idea from deterministic domain to a probabilistic domain. I'm just trying to give you the intuition here. Take it now to our world of machine learning. Nothing is ever equal to something else exactly in our world, right? So the best we can do is a similarity between a key, Ki and some query, some query that you give. So let me give it a term, Qj, some query value that you gave, right? Qj. So the best you can do is look at, when you are searching or retrieving the data look at the similarity between them how similar they are this is the equivalent of your like now you got that right so now let's carry and that is this similarity now we will change the wording you are are retrieving. Now think about this. Being in the probabilistic word, what will you end up here? For every word, if you take qj as a probe, it is often called the query or the probe, just like in the database, you probe the database data. If you apply it, you will end up with all sorts of, and the basic similarities, let's say machine learning was the basic similarity that you can make between two vectors. Dot product it. So there you got that. Simplest similarity between two vectors you want to see just this, just take their dot product. Yeah, product yeah so you take the dot product of these two and uh there is more to it but let's say that for all of these you do a ki a qj and then what happens is this gives you and then associated here are those values v1 v ki so k this is finally kn this was k1 right and this is v n right suppose there are n words so you took one some particular probe word and you could do this similarity to this and now what happens is suppose you do a dot between these two. So let's imagine that you're doing a dot between the dot product. We haven't gotten to attention, guys. Don't think of this as the more formal definition. But we are getting very, very close. So now what happens? Suppose I retrieve data. And I say that we now do a probabilistic game. We don't just get back one row, but we get many rows, all the rows, but if it is not very similar, we'll just take a pinch of it. If it is very similar, then we'll take a lot of it. If you imagine these things to be data, just think that if the similarity between the probe and the key is weak, maybe a pinch, a little bit of a salt to add to your soup. But if it is huge, if the two vectors are very aligned, very similar, then take, you know, heaps of that material. Right. So what will you end up with? The value that you will retrieve, you will end up retrieving all of those values. V1, Vn, because no value will be left behind, except that each value would be weighted. K1 QJ isn't it okay to QJ V2 are you getting it this gives you how much of how much of it you go you retreat right this is like that in the same way. And then you will have KN, Q, J, whatever the query is, times VN. So, so far so good, guys. You'll end up with this entire vector, which is basically all these things for word one. Right. With me so far, guys, please, Please unmute yourself and give me feedback. Yes, please. If there's anybody who hasn't gotten this intuition, ask me about it. And now comes... So observe one thing. What we have done is, we have just done a weighted retrieval. Weighted. And in some sense it is like, it has the flavour, you know, of weighted averaging. Weighted retrieval. Like weighted averaging. You know, when you do weighted averaging, except that you're not dividing it with a denominator here. In a weighted average, you would divide it with the sum total of everything. You're not doing that, but it is that. So this thing, how much of each amount you get, there is a word for it. You are saying that this query, attention that the query J gives, the attention vector is this. The amount that it has retrieved, which is a weightage, each of the values weighted by the similarity to the query is basically your attention vector, K1, QJ qj now why is it called the attention vector we will just change our way of thinking about retrieving some amount from the database now we'll change it to we'll just put a small twist on it k nqj times vn. So by the way, this math is not exact. There is some exponentiation, there is some soft maxing and blah, blah, blah. So ignore that for the time being. And there's also a square root b and so on and so forth. So let me just not call it attention. So let me call it almost attention. The word for it is almost a. So almost a vector is like this so far. Right? So now before we go further into the math, let us ask ourselves this question. What is this? And what is this query term? And therein is the magical thing of the paper the main idea of the paper is that it is the word self-attention so if if take the sentence and all the words in the sentence and make the words pay attention to all the words in the sentence itself. So in other words, if the query term qj is a word in the sentence, and let's say that the sentence has seven words, qj will have a dot product in this thing with all the seven values, isn't it? You will have a q1, qj, the same the same thing right so here you'll have v7 right so but then you ask no no wait a minute uh one qj will have a lot of similarity to itself to its own key what you do and there's a bit of a clever thing you do not necessarily you uh you you train Not necessarily. You train the system in a very interesting way such that a word's query probe vector and the key will be very different because you don't want a word to pay attention to itself. Right? So you play with that. What you have just ended up is the retrieval of the data is equivalent to saying how much how instead of how much data am I retrieving from the database the the thinking is how much attention is this word a word J giving to the words in this sentence all the words in the sentence are we together So that is the way, that's sort of the moment you understand this, the rest of the mathematics after that actually begins to fall through very easily. So one of the things that happens is we don't take just, so now let's look at this term, K, I, Q, J. You know that some Q that some K-I will be very close to Q-J. Asif, can you repeat the last sentence, last punchline, what you said? What I said is that, you know, I'm saying that you use the query to get different amounts of values, the values like v1, v2, v3, v4, right? So those different amounts are given by the level of similarity. So it's just weighted retrieval. Now change the thinking. What if the query, so what if what you're retrieving, these values are associated with the words, right? The key is, so each word has a key and a value. So now when you come up with the query vector, obviously, and somehow, you know, you have those words, keys, and values, and then create its attention vector effectively, almost attention vector. It can do that. Now you ask this question that, alright, if the words have keys and values, what in the world is this query? Where does this come from? And the interesting thing, the answer to that is hey you know what whenever a word is searching is looking at other words so imagine that you have eyes right so with these eyes you look at other people isn't it think of each word having its own eyes and those eyes are the is the query vector. So each word therefore has three things built in. Word I will have a query vector QI, it will have a key vector, a value vector. So key and value vector is obvious, right? If you want to store a word, you'll want to have some key and some value. If you think in terms of information retrieval. But the QI, the fact that every word has a unique probe associated with it, right? And it probes other words in the sentence. That's where I believe the word self-attention comes in. The self-attention. The words are paying attention to each other in the sentence. Right? Now, the amount, so we are talking about how much of a V1 is sort of like the value, how much of that to retrieve from the database. So the shift in thinking is, it is not how much to retrieve, but how much attention to pay it's not a big shift if you if you bring a lot of uh from the database let's say that you have many things there you have rice and you have i don't know lentils and you have sugar and you have salt right and so a typical dish that you would do is you will pay a lot of attention to rice and a lot of attention to lentils you'll take scoopfuls of them bring it and you will take rather modest amount of salt you see that right because so in other words you say that your your query vector will be much more allowed aligned to rice and lentils to get your soup and food, a main staple diet, and less aligned to salt. You'll fetch less of the salt from the system. Or another way to phrase it is, if you remove this from the system business, because there is no real storage going on, now let's bring it to what we say. What we are saying is that we are paying attention, this query vector, this query of a word is paying different amount of attention to different words. Or if you were to think of those words as sitting somewhere, it's fetching to itself different amounts of it in many ways do we do we do we get that intuition uh yes yeah so that is the that's a crucial intuition and that explains why you have these three vectors k q and v are there and this self-attention thing comes in it's quite interesting but now i'll just if you if you this much guys, the rest of it is just the mechanics of that. So what happens is when you're looking at similarity, so let me use the word Sij, right? So suppose you have lots of similarities, S1, J, S2, J, S, and J. For a query, J, Q, J. Associated with this, you have this. Suppose you have, actually, let me say, say arrow, right, it's not a dot product. It's a square bracket. So suppose you have these values that you have similarity of the query to the keys. These are the similarity levels. Quite often, we have been using in machine learning this technique over and over again we want to emphasize the bigger value how do we emphasize the bigger value waiting uh exponent we do softmax we exponentiate it we exponentiate ij right ij right and then we come up with numbers which are all positive which has also really amplified the bigger value out of them all so whosoever is the tall guy you have made it really tall right but given given the person let's say a 10 foot a pole to stand on right so the person stands out. But then it's good too, because when we talk about attention, if you think of attention as a pie of size one, what would you like to do? You want to sort of normalize it, right? So the way to normalize something is, you remember how do we normalize it? The way to normalize something is, you remember how do we normalize it? Summation over e to the i j sum of all values of i, which basically means is, suppose you're looking at the qsij, similarity ij, 1, 1, 1, similarity. one similarity one will be it will be similarity ij similarity ij So the new value let me just call it the S tilde, is this value, Sij, sorry, similarity exponentiated ij, or in this particular case, 1 and 3, divided by summation of all the similarities. So, for example, here you're looking at the probe j. So let's just say that you do s1, 3 plus e to the s2, 3 e to the sn, 3. So I won't go into that. We know how to do this summation, right? So you divide it and what do we call this kind of an entity when we exponentiate it and normalize it we call it a soft max soft max soft max right that's the purpose of it right it sort of takes a bunch of numbers make the tall guy much taller and then makes them all positive and then at the same time essentially converts them into probability or proportions if you want to think in terms of that so softmax is a good thing to do just generally to amplify the signal if you are really paying attention to something just make it stand out that it's paying a lot of attention to it right so that's why you do the softmax and so now you have pretty much gotten to everything. So I will say that the attention, let's say that you bring in a probe J, the attention vector, the attention it is giving to each of the words is you know, the softmax of similarity. So s tilde is approximately softmax of Sij, right? Is basically something like that, similarity softmax. So this is literally what it is. It is. So any one term, this is Ki and Qj dot product, right? And then you can do it in matrix notation. You throw in your transpose and so on and so forth. This. And then what do you do? So let's work out the pieces. No, I'll take a little bit more space here, one second. So I can write it in bold. So you softmax what? The similarity, which is the, so this is K, I, Q, J, right? I'm using Q because the paper uses Q. Why confused? Let's continue to use little q, right i'm using q because the paper uses q why why confused let's continue to use little q right it is the soft max of this except that they do at times the v what is this vi right this is the value how much of the value you retrieve the only thing that i missed here is that you also reduce divided by the square root of the dimensionality d is. Let's say that kq, all these vectors, k, q, v, they belong to a d-dimensional space. In the original paper, it was 64 dimensions, right? So square root of 64 would be eight. So it's just a technicality that it just makes it better, more stable or whatever, to divide it by the square root of D to normalize it a little bit. But other than that, it is nothing but this. So if you look at the attention vector, if you think of attention as how much of a certain value to retrieve, this vector tells you how much of it to retrieve and that brings you to the very celebrated, perhaps the most celebrated equation at this moment in the field of natural language processing. This is your attention. Are we together? Asif, how do we know that you know the value of the, you mentioned in the paper a 64, right? So is that a variable or it is like a constant? No, no. See, I can imagine that when the first attention is all you need paper came out, there are a lot of things. And as I explained that, you will see that there are a lot of things it has. And see, frankly, when a paper like this comes through a breakthrough, there's a lot of experimentation, hack, trying this, trying that. You know, you have a vague idea that this will work, but you don't know what the hyperparameters are. What is the dimensionality you should reduce it to? Right. How many layers you would need? The basic question in neural networks, the perennial question, how many layers? Isn't it? So how many layers do you have? And things like that. So a lot of that were experimentally or empirically decided upon to converge onto things that work. And since the original paper, they have changed. So for example, BERT base and BERT large, they use entirely different values. And so the whole thing is fluid. I mean, think of it as simply a hyperparameter of the transformer. Okay, so does it mean that, you know, for our understanding, you just take the square root of 64 there? Yeah, yeah, so you just took a square root of 64. Okay, so there is no need for us to get into more details as why we got specific for that. Yeah. See, like I told you, right, it's just a constant. And for understanding, here's a basic trick, the tip that I told you guys for understanding anybody's paper or theory. The first thing you do is go erase out all the constants. Then you will be left with the core or the essence of what what they have written and now try to interpret that so suppose you saw this equation like this how would you do that you would say that this right it's a v vector so this is a vector like vi and so all of those elements are there this part what is it this is a similarity Then you're doing a softmax of similarity. So what are you doing? You're making it stand out, really stand out and making it into a zero to one scale. And so what you're doing is you're in some sense taking the weighted amounts of this, different values, right? Almost as a flavor of a weighted average or something like that. So that's what you're doing. Effectively, you're doing some vague way like that, and you're creating your vector. And that is the amazing thing. So that's how it is. Once you get the idea, you get that. So the rest of it is mechanics. If you understand what attention is, now comes the question, how do you do that? And the original paper has quite a bit of things to it. So a unit. So what you do is, first thing you do is, let me lay it out in the way they lay it out. The first thing is you have an embedding. So suppose you have a sentence, the cow jumped over the moon. The beautiful thing is that you have to, you have no choice but to give it in parallel. Why? Because, what did I do wrong? Why the cow jumped over the moon? Why did I end up with only six words now? Okay, so let's say that this is it. Oh, the period. The last one is the period, W7. Period is the W7. W7. That's the W7. W7. That's the separator. All right. So what happens is that because each of these words, each word has a query. it needs to go pay attention to other words right so you have no choice but to feed all of these words in parallel to the embedding but they independently traversed but now here's the interesting thing this embedding is not your word to work or glove or anything you say in in this neural, let's let it go and learn what is the best embedding for the words. You feed it lots and lots of words and sentences and it soon figures out embeddings. And one of the lovely things it does is this contextual embedding. It knows how to distinguish between the three meanings of stand right it will do it and it becomes fairly implicit so this is the word embedding and then you do a position embedding so you you take the position of the word uh and we went into the position embedding quite a bit on the sunday class so i don't know if i want to give time to it i noticed we have spent an hour but imagine that somehow you need to inject the position information here. The two together, the word embedding and the position, word embedding, position embedding, makes the complete embedding vector for each word, right? Each word embedding vector goes. And then comes the interesting part. The interesting part is it goes through this so-called attention layer or self-attention layer. What it does in simplest form, it multiplies these values. It will take a word. So word embedding. Actually, I know that there's words I've used w for which i regret now i wish i would just use the word okay we will just use that so what you do is you mul you multiply it with a query matrix q to this word e i such that it produces the query. So this is just a weight matrix. So think about this. Suppose the dimensionality of this typically is 1024, let's say, right? Thousand dimension, give or take. Now what you want to do is you feed this thousand dimensional vector, but you want to produce a vector which is only 64 dimension, right? So all the query vectors 64. So what happens? You need a matrix, which is 1000 cross 1024 cross 64. Isn't it W? Does that make sense, guys? Right? So you can multiply it with the right matrix of the right shape. And now the whole question is, wait a minute. Where did that matrix, what are the values of the matrix? And here, the answer in deep learning always is, we don't know. We'll learn it. So just create a matrix of the right size, multiply it so that what comes out is some vector which is of the right size, multiply it so that what comes out is some vector which is of the right dimensionality, 64 dimension. Then you train another matrix key such that when it is multiplied to EI, it produces the KI vector. And another, likewise by now you get the drift matrix such that it produces VI vector. And of course VI, KI, QI all belong to a d-dimensional vector space. Typically 64 was the original idea initially taken. So you end up with these vectors are we together and then what you do is from these you you produce for each of these words you realize that each word j you can now produce the this, first KQV comes out. K, I, Q, I, V, I comes out. Then these things can together produce what? Your attention vectors, right? A1 all the way to A. Suppose there are N words. You will produce all these N vectors so what happened seven words went in and seven attention vector came out what do these attention vectors mean the attention vector for the word cow right probably is paying a lot of attention to the word jumped into the word moon right so you will get the you you have a different attention vector between for each of the words and the intuition is this so let's look at the word let's say that the cow let me let me write it like you know now i'm going to complicate this sentence the cow ate uh ate her treat and crossed the fence and jumped over the wall. Suppose I maybe I'll make it even more complicated. It's just I'm just saying the sentence on there that Sally gave. Sally gave it jumped over the moon. Then it jumped. Maybe then also then it jumped over the moon. So think about this, all these words are there. The word it is there. Right. When you are thinking about the word it, you need to know what is it? What is the word it referred to? And the way we do that is that we make sure that the it attention vector is giving a lot of attention to cow. It's like heavily, most of the weightage is given to the word cow. It is pointing to the cow, that the it refers to the cow. Am I making sense? And of course, every word is paying different levels of attention to it, but I wanted it to stand out because this is crucial. This is ambiguous. Does it refer to the moon? Does it refer to Sally? Which hopefully not. Does it refer to cow? Isn't it? So that is the value of attention. That's why you call it self-attention. The words are paying attention within each other. But at the end of it, what you end up with is everywhere, you end up with A1, A2, A7 vectors. And now comes the magical part. So you have one layer of attention. In the first layer, word embeddings went in. And what came out are the attentions, attention vector. Then what you do, because for each word, each token, there is attention vector. You feed it to another layer of you do the same game all over again. You make the attention vectors pay attention to decide how much attention they need to pay to each other. Right. So that goes here. That goes so this goes here. This goes here, that goes, so this goes here, this goes here, and what they found is, for no, I think, big reason, but again, one of those things, they just experimented it, they put six layers in the original paper. Right. And so, but that is not enough. So this entire thing, the word for that is, it's called a head. This is a head. In the world of attention. And guys, this, the reason I'm going into this mathematical details is, trust me, this is when people, they just go for interview for some reason. Details of this paper tends to come up quite a bit in these, can you explain what this is and so forth. So all of the six layers put together is a head. And now here comes the interesting part. You have another number of heads. The total number of heads that are there are eight. I think the original paper used eight heads. These days they use even more heads. So what it means is that, you know, we had one weight vector, WQ here, but this is associated only with the first head. This will have its entirely different WQ prime associated with the second head and so on and so forth. Now, why in the world would you complicate your architecture so much? What they found is that it is somewhat like convolation filters. See what happens when you use many convolation filters. Each filter detects optimally something different. One detects, let's say an vertical edge, another detects a diagonal and things like that, right? So they all learn to pick up things that the other guy is not picking up. And so the same thinking goes here. The head is somewhat like a convulation filter and a rough analogy. And what you are seeing is you need a whole lot of filters so that they all pick up different aspects. So when the words go in, one maybe if I have to just caricature it a little bit, one is more sensitive to the grammar, one is more sensitive to some other aspect of the sentence and so on and so forth. So they will pick up on different kinds of signals they'll pick up from the words and they'll pay different kinds of attention. The words will pay different kinds of attention to each other in the different multiheads. And at the end of it, all of it put together, and obviously not to forget that there will be the activation functions and the fully connected layers and so forth. And there will be a joining of these two. And then the residuals, all the things that we have learned in our fundamental deep learning, there is a lot to that. So all of those things are there. Are there residuals there? Yes, they are there are there residuals there yes they are there are there do we have feed forward layers of course we have that all of that is produced and the whole thing is called the encoder and the the transformer the original transformer is actually fairly complicated it is made up of two things it produces the encoder so this is the encoder basically words go in right and i will i will ignore the position encoding in something i'll just leave it as that and with the multi head and all of that and layers and layers and layers and fully connected layers at the end of it what happens is it will produce the last layer. It will produce its own KQV. This is the output of each of the words, K-I-V-R. You just stop here. Then what they did is they have a decoder, which is very interesting. You take in the beginning you have a trigger which is nothing but a start trigger. You put it through again some attention heads right. So some initial word you get an encoding of it right and what you do then is you put it through a decoder. So this is actually still encoding. So decoder actually has encoding built in. These are called masked encoders and for reasons that I'll explain in a little bit. So what happens is that this is one input. So into this attention model, or sort of a self-attention model, the attention, I don't know if you should call it self-attention. I'll just leave it as the word attention. I don't know what the paper calls it. But it's basically the same thing with one interesting caveat. The query from this inside thing the vector that comes out you treat it as the query right and from the decoder from the encoder what you've taken is you've taken the key and the value right so key and value you you think of it as the real Right? So key and value, you think of it as the real summarized thought or encoding of the sentence. Think of it as stored data identified where the coordinates of any value is given by its key. Right? You feed the key and the value from the encoder and you take a query from here, right? By encoding some inputs, a starter input. Let me just call it starter input. Then what will happen is it will produce one word of, let's say, Spanish, the first word of Spanish. Then you do something interesting. In the beginning, you took a dummy token. Now you take this and feed alpha here, right? So alpha and start together will create the query vector the k and the v's are pretty much frozen this is your encoded thought vector is essentially a kv's now this query changes right you have a different query vector because now it is a composition of alpha and start it goes into the attention it will produce another word beta, right? That then next you take that and now you feed alpha and beta into this word to get gamma, right? So your query that here first starts with nothing, right? Just a token, empty token. Then next time it starts with the first word, this Spanish word that it produced. Next it starts with the first word, the Spanish word that it produced. Next, it starts with the alpha beta, right? Next, it starts with alpha beta gamma, right? So what happens is as you train this, what happens is suppose you know this, because when you're training, you really know the Spanish that should come out. So suppose the Spanish was alpha beta gamma delta epsilon. What are you doing? When you want to train it on alpha, you're masking this all. Mask is everything. Alpha, beta, gamma, delta. They're all masked. In this case, while training it, you're masking only beta, gamma, delta, epsilon. And then here you're marking. So in other words, whatever word there is, you're masking all things to the right of it. And you're only looking to the left of it, isn't it? So suppose you want to, you're at gamma and you're looking to produce delta. So obviously, your delta and epsilon at that moment are masked and you're saying that i'm feeding alpha beta gamma to produce the query vector and that's what it is so it is called that's why it's called the mast encoder right mask or mask attention people often i love to look at the exact word use mask attention and then you feed it in and you go away and so when you look at this architecture, the first thing that occurs to you is, my goodness, isn't it? There's a lot happening in this particular paper and this particular and it has must have arisen from a lot of experimentation, lot of experimentation, and trying things out and doing it. And obviously, this had a profound impact on natural language processing. It is commonly believed that it was a watershed moment. After that NLP, the effectiveness of MLP went up significantly in many, many areas. And the whole way we write codes changed. We use transformers now. And the whole way we write codes changed. We use transformers now. So I talked about, for example, I taught you GloVe and this and that, but now I'm saying, hey, you just learn the encoding as part of the transformer training itself. And the most remarkable training is all of these W vectors in the, even within a head, there are six layers of W query vectors. vectors than their six matrices of key vectors than six matrices of value vectors and that is just within one head, and if you have eight heads, now you go multiplied by eight right, so you realize that they are 48 weight query weight matrices and another 48. 48 weight, query weight matrices and another 48, this key and per value. And so by the time you're looking at 150, just 150 matrices are there to learn. And each matrix is approximately 1000 by 64. So you realize that it's a pretty huge model, lot of training to be done in this model. And that happens when you train the transformer, it's not easy. You need to feed in a lot of training to be done in this model. And that happens when you train the transformer, it's not easy. You need to feed in a lot of data and then it sort of gets trained. It's very hard to train it from scratch if you don't have sufficiently many data. So what we do typically is we start with a model of a finite size. I mean, so you take a pre-trained model, that model is the transfer learning concept. And really that is the way to go. You take a model, you chop off the last layer. And when you do this thing, by the way, you can now use it for all sorts of purposes. So you can then for your problem, just apply, first apply directly to your problem by applying a classifier or whatever it is, the last layer to it, the head, or maybe fine tune it, relax a few layers, let it get trained or maybe relax it and get it trained, but start not with a blank slate, but start with a pre-trained model and let it just fine-tune its weights when you train it on the data that you actually have. These transformers are trained typically on massive machines using vast amounts of web data. So usually you don't want to train a transformer from scratch unless you truly have a huge amount of data. What you want to do is you want to take the transformer and do the last mile training for your particular domain and it works very well. So guys this is the the concept of attention and transformer. I know that I took you through a long theoretical journey today and you might wonder that what was the whole point of it. It is the thing guys, is practically in NLP, it is the big gorilla at this particular moment and there are lots and lots of variations and implementations of the transformer. I thought it was worth it for us to understand all of these words. These words get thrown around quite a bit, attention, self-attention, transformer, etc, etc. So that is that that we are coming upon a break we are more than one and a half hour through let's take a 20 minutes a break but before i do that any questions guys did you understand by the way for many of you it was a repetition i hope it wasn't a useless repetition see if i have one question on understanding the the purpose of the multiple heads so there were six layers making one yeah in one head and there were eight heads right now could you give us the intuition in terms of the training process happens see what what each, let me, I'll put the question out completely there. When the training happens, why would each of the heads learn something different? Where does that difference being built? Yes, it's a very good question. See what happens is it goes back to our congulations, like, oh but but you didn't take the cnn you didn't take part did you take part one you took part one right yeah yeah i was there yeah you were there through the whole apartment okay so remember we talked about the congulation filters right so in the world of image processing people used to hand build filters it is a good idea to look for vertical edges. It's a good idea to look for horizontal or look for this or look for that. You know, there's basic primitives in the first layer when you encounter an image. And you go over the image with that filter. You scan it. It's like a stencil, you know, cut out. I'm looking for this shape. Is it there? And the field was that. this shape? Is it there? And the field was that. When deep learning came up, it basically said, you know what, let us just use this thing. Neural network, let it learn the filter weights. So the question naturally arose, well, if I have three filters, won't all the three filters shown the same image, won't they learn the same thing? But that is where the gradient descent and the loss function magic comes in. See, what happens is when you back propagate, if two filters learn exactly the same thing, there is redundancy, right? So let us say that they have learned something and the loss has, there is redundancy. So let us say that they have learned something and the loss has come down a certain level. But the moment one filter, even with a small fluctuation, learns something different, its values change, what will happen then? Quickly it will move in a direction in which it's picking up something new and then the overall loss is coming down the the training loss is coming down and it will drift and soon the the two will become almost as i mean they'll become as different as possible but they will pick up different things from the system from the image literally different things because it's almost as though they're talking i am taking care of this you don't take care of this you take care of something else but it has to do with the magic of very straightforward magic of gradient descent and uh loss see the moment you keep pushing down the loss right at the end of the day these filters in some sense they have to become orthogonal to each other. Otherwise, if there's redundancy, there is not enough learning. But there is the mechanism to learning is there, it's just being wasted. So soon the training will force them to diverge. In fact, they never converge. I just took the example, what if they started a line that was only theoretically, in reality, these filters will start with random values and gradually as they take shape their weights move they will all end up with orthogonal things they'll pick up literally different kinds of signals so that's what happens with convolutions and the idea is that each of these heads is like a congulation filter and these multi heads see these multi heads are acting in parallel right each of the heads has its own the same words are going through it but as the training happens the loss the loss minimization process and all of these gradients will force these heads to learn literally different things so that they don't repeat what the other guy has done, because then you don't minimize your wasting resources, right? So they learn different things. And so also within a head, you know, the information flows sequentially, attention of one layer goes into the, you know, gets into its next breakdown into KQV and so on and so forth. It keeps propagating forward. But the point is multi-head these heads are completely independent of each other they're like the filters they all are doing their learning independently but because of the magic of this optimization process the random weights will flow in opposite directions they will sort of make sure that they learn different things okay and that is it see ultimately this is the magic it is all calculus that's all you know calculus linear algebra matrices and so forth but this whole thing works okay i think i might come back to this question again later i'm still i'm taking it for granted right now and then let's move on right Yeah, it takes you, see, when you think about why or how these things work, it does take a lot of thinking just to figure out why in the world did it work. Can that not be practically checked also by having that same less number of beds and then increasing the number of beds and saying that. Sanjay Gupta, I'm sorry, please say that again. Sanjay Gupta, Now what is asking, can that not be practically checked also that you have very less number of heads and that last maybe more and as you increase the number of friends in the last Sanjay Gupta, Oh threads you will last oh absolutely absolutely so see something somebody must have had a eureka moment let's add one more head and gotten even more accuracy or even better performance right so then you can imagine that these things are team effort so people must have been trying different things. The whole thing is experimentally. I mean, there's a lot of experimentation before you come up with an architecture like that. What you don't see in papers are all the ways that didn't work, right? Or the halfway steps that you have to talk to people to figure out, to get the history of it. But any good paper is a result of months sometimes years of effort and often it's a large team effort so as if in in the practical context when let's say in your company when you're using one of these transformers would you ever tweak any other number of heads or so on or you take that and then just work a few things on the last layer alone uh no no no what happens is see imagine that the weights are trained your problem still deals with words the language is still english a lot of the learning that the transformer is done is still applicable so if you just try to retrain the whole network transformer, but you start with a pre-trained transformer, what will happen is those weights will just jiggle a little bit and adapt to your problem. But what you're not doing is, you know, learning all the weights from random values. They're already near perfect for most problems. So let me rephrase the question as if like so we had six so eight heads six attention layers right now when you say you will retrain something what would that be would you drop a head or would you drop no no no no you don't do that what you do is you take the transformer first of all you don't uh you you freeze transformer. But at the end of the transformer, you'll put your normal classifier, right? Okay, okay. So that classifier needs to be trained. So you'll screen a classifier at the end of the transformer and you say, okay, this is my model. But then the classifier needs to be trained. You'll train it, see how the performance is. Okay. Suppose you said, maybe there is scope for improvement if you have enough data then you say okay let me unfreeze the transformer okay And see, there's the fine-tuning very much at this moment. While we have a certain amount of theory, in practice there's a lot of experimenting with things, seeing that they work. And then once they work, now sitting back and saying, why did this work? Post hoc reasoning in this field. Okay. Thank you. You're welcome. Alright guys, if there are no thoughts, it is 8.40, 8.40, 8.50. We should take, by the way, those of you who have been through, I mean, you guys have all been through the earlier transformer talk, I deliberately talked about it differently. Was it useful? Are you felt that was quite repetitive? How do you guys feel about it? That is useful. Yeah. Okay, good. So. So this is it actually is one of the things it takes time, like, what is attention. Papers are written very condensed and a lot of people have explained with, I sometimes feel that many explanations are very, very good, but some explanations just repeat the words from the paper. And it is worth asking, what is this KQV and things like that. what is this KQV and things like that sorry guys it is 850 let us regroup I'll stop the recording we will come back Yeah. So which part are you referring to? So if you can come down. I think I understand this softmax a little bit now. But then what I was last on this diagram, a recorder and then there's two boxes yeah so let me explain that so words go in here in let's say one way to think of it is let's say that english sentence goes in here right what will happen is that whole sentence will produce its k vector v vector and the q right at the end of it so now we have to translate that one sentence what happens is this is the encoder part. This is the decoder part, the right-hand side. So the decoder works in an interesting way. You only take the key and value, not the query, not the probes. You take the key and value, and you feed it into one attention layer. But then the question is, if you're going to do that here we need the q also right to create the attention because attention needs kq v right softmax so where is the q and that's that answer to that q is what you do is you create a an encoding layer here, a simple, a few attention layers. These are called masked attention layers for a reason. What you do is in the beginning, you just give it a dummy token. So it will take a dummy token as a word, a single word, and it will create its own KQV, throw away the K and Q, just take the Q, KV, take the Q, and you take the K and V coming from the encoder. And now you have a Q. So you can go ahead and produce some word, right? You can go ahead and encode and ultimately apply some classifier, you know, fully connected layers, the usual thing. So you come to the conclusion that alpha is the best Spanish word to produce. thing. So you come to the conclusion that alpha is the best Spanish word to produce. You go ahead and produce it based on the attention. All right. Then once you have produced alpha, now what you do is you take alpha and the next time around, it becomes, this is the recurrent part. You put it again and now you feed alpha into this lower box, this masked attention. You feed alpha into it. So alpha will go in again it will go and meet the knv which is coming from the encoders which are pretty much at this moment constant and then this attention layers will do its attention business and it will ultimately point to a new spanish word beta right so beta gets produced now you take both alpha and beta and put it here right so beta gets produced now you take both alpha and beta and put it here feed it back into the system and you go through the same thing see what you do is as the spanish words are being emitted that partial sentence becomes the input here at this level at the bottom to the decoder so the decoder input starts with the partial translated word as well as the encoded sentence coming from the encoder of the original language the kv comes from there and the q comes from here but the kv is going to be constant right when you look to be confident yes that's it but you'll be looking to alpha beta gamma where do we stop what's that so your k and v are constant to the decoder box right right and your decoder box is looping around yes it produces alpha yeah you fit alpha into this again as a to produce q uh it's the group prime and then that works with kv that's right this is beta and then now you've got alpha and beta yeah so when do you stop it like this is what you will ultimately emit the stop stop token okay when the stock token is emitted you stop the sentence but how will it produce the stock token because you only pass K and V right so here's the thing a kv are coming from here right that is they remain constant as you keep reading alpha beta at the bottom what are they doing they are generating the probe or the query right or query vector each probe or query vector will go and match the this value vectors based on the key dot product it will it will be similar to some key, different query vectors will be similar to different keys because in the beginning, the query vector was nothing, it's just one start token. Then it is start in alpha, then it is start alpha beta, then it is start alpha beta gamma. You see that, right? So what happens is that as you keep feeding all of that in through this layer, you keep producing the query, different query vectors. Eventually, this query vector at some point will produce, let's say that there is an epsilon is the last word. Once you feed this entire sequence, this decoder, it will start pointing to the end of sentence. The next word that it will produce is end of sentence. Right? And the moment it produces end of sentence, you say, okay, fine, you're done. Okay. So, Asif, I think intuitively something is missing, right, in that application. Because, to say, because I saw K and V, Asif, let's say for first two, the cow, and then the K and V are for those two words. And how can I get to the end of sentence? No, no, no, see, all the KVs are there, K1 to K7 are here there k1 to k7 are here v1 to v7 are here remember okay when i wrote k i wrote it more as a matrix all the k's and all the v's are being passed okay so you're passing the complete matrix of k and v okay that's that's why i was asking the question right are we passing the whole matrix or just passing k and v what happens is that k 1 k 2 these are vectors and then of course the k itself in the literature right they often use capital letters for matrices so i pointed out that there's capital remember that i use little k ki but then capital k when you use a v that when you use like this with caps it's just an implicit Raja Ayyanar?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi?nilavakshmi? will talk about is one particular, very popular implementation, two actually. So what happens is you notice that this is a pretty complicated transformer architecture. So two interesting things came out. One is GPT and it's very, very GPT, GPT2. Now, of course, the whole world is completely excited and frightened by GPT-3. What the GPTs use is they only use the decoder part of the architecture. And what they do is they don't have one decoder, they go on stacking decoders. So I won't go more into the GPT because I had to pick one and go deeper into it. So we picked BERT because B bird is something you'll use quite a lot right so this on the other hand if you just don't throw away all the decoders and you stick to only the encoder part of the transformer and you just go on stacking a lot of the encoder that's it you get something called the bird bi-directional cetera. But I won't go into the full name. I have to myself keep reminding myself what is the full name. And the full name is, hang on, I will remember. I have to look it up myself. Bidirectional Encoder Representation from Transformer. Yeah, this is it. BERT, Pre-Training of deep bi-directional transformers for language models okay this is invited but bi-directional transformers so we will do bird so what does bert do what is the interesting thing actually when it was created do, what is an interesting thing actually, when it was created, it came from the Google AI language lab, when it was created, it became quite another sort of a success moment for the transformer story, big success moment, because what they showed is if you use BERT as a transformer, a lot of the common things that you could do. So let's think about the common things you can do. Take off a pair, a pair of sequences, sequence alpha, sequence beta, right? You train it using the sequence and then you can have some output. So let's think what are the common things you can do first of all if you just don't oh sorry sequence beta so suppose sequence beta is not there you have just sequence alpha and another you could do for example uh sentence like a sentiment analysis you could just how by attaching a classifier at the end of it, at the end of the transformer, right? You could do, for example, sentence completion. So for example, you can ask if this sentence alpha is there, what is the best beta that completes it? What is the best sentence that follows it? So what does that do? That gives you generation, right? Next sentence, next sentence. The next sentence generation or completion or something like that, right? Next sentence completion it gives you. Or you could ask this question, suppose I have S alpha and I give you the coffee shop and the sentence would be, and bought myself a cup of coffee. That would probably be a good following sentence as opposed to, and the cow jumped over the moon. Or, and something else, some arbitrary sentences. So you can train the transformer to pick this because it is just a thing of all of these, you train it with, you say this is a positive case and the ones that are wrong are the negative cases. So now you can train it just the way you train classifiers or things like that. And you can make the transformers do all sorts of things, right? Things like that. So the transformers, when you went to the transformer website, you know, the hugging faces, we did the lab, if you remember, and there were many applications. We used it not just for language translation, but we used it for many, many purposes. So these transformers can be used for many purposes, in particular BERT. So when you train BERT, the two things you do, you only take the encoder layers, lots of attention, encoder attention layers. And how many attention layers? Now, one of you asked this question, why eight heads and why six layers? And the thing that I wanted to mention is there's nothing magical about it so let me show you what it becomes i will uh hang on let me do this somewhere in here in the paper is a discussion of how many it is yes so what they did is there are two models of BERT or transformer they trained. One is the base model, one is the large model. So here, if you look at it, the number of layers, remember we said that there are six layers in the original transformer model. Do you guys remember? We're talking about it. In any head, there were six layers in the original transformer model. Do you guys remember? We're talking about it. In any head, there were six layers, six attention layers. So here in the bird base, they started with 12 layers. Then in the original one, the hidden representation was 64 dimensions. Here it is a huge 768 dimensional representation. Do you see that? The hidden size is 768. Huge. And how many, so when we talked about multiheads, how many heads are there? 12 heads. And so this entire model with their many layers has 110 million parameters. So guys just think for a moment what that means. If you remember you all started your workshops with me. We started our journey with linear regression and there used to be what one two three or maybe up to 10 parameters, like 10 dimensional or 20 parameters. Do you remember Boston, Boston crime data and so on and so forth. And all of those data sets, California housing data, there were like 10, 20 dimensional things. If you made linear models, they would have 10, 20 parameters. Guys, do you remember those days yes yes so see where we are now the state of the art these days is a very small the smallest bird model has 110 million parameters as a more useful larger version of BERT, it has 24 layers per head. And the internal representation is 1024, like basically a thousand dimensional representation. And there are 16 attention heads. The end result is the total parameters that you get is 340 million. And now we are talking about models like GPT-3, which are apparently so big you can't even have it on your machine. So these models have become ginormous. I don't know how valid it is, but somewhere I read, and I hope I'm not misquoting it that the amount of energy needed to train one of these giant transformer models is about the same as the energy that would that is used to build about a hundred cars and let those cars live out their entire lifetime of gasoline consumption. Think about it. How much energy goes into manufacturing a car and then in this lifetime of 20 years that it runs, how much gasoline it consumes and now take 100 of those. That much energy, electricity ultimately goes in training these transformer models. So this is where AI is currently. Actually, GPT-3 is 175 billion per annum. Yes, 175 billion. It just adds more, goes on adding two, three orders of magnitude to it. Shocking, right? Just think about it. It's so big you can't even download it. So anyway, what is BERT architecture? What I will explain to you is the BERT architecture was trained in a very interesting way. It used unsupervised learning. It didn't use label data and train it. It actually did something quite remarkable. And let me, it used two things, next sentence prediction and masked language model. These two tasks it used to train a bird, which is only made up of encoders, no decoders, layers of encoder. So how did they do it? What do you do? And I'll just explain it on a piece of paper because then it is much easier so you take the sentence the happy uh jumped over the moon the the mask masked the masked language model like i think there's a word close close or something like that is a old uh literally what this paper mentions it i keep forgetting but mass language model what it basically says is that in the first bit you take a sentence and maybe you can take because we take pairs of sentence if you want alpha beta you can take pairs uh two sentences with a separator and what you do in a bird is you always start it with a not only a ending of a sentence but the entire sequence you start with a special token that is called cls now why the word cls because quite often this token the final representation of this token as it goes through the encoder, you feed into a classifier for some of the tasks and so forth. So that's why the word has stuck to it, the CLS. So now you have this token, all of these tokens. What you do is you randomly go and you mask some word, you hide it. So let's say that you go and hide it. Are we together? Right? You hide this. Maybe this sentence is too small to hide more than one. So let me take this example. You hide one word. Then what happens is you pass it through the transformer layer, right? The transformer encoder layer. Encoder layer. And what will happen? Each word will have its final output, you know? Encoding will produce a final output for each of these words. So let's say that it will be A1, A0 for CLS, A1, A2, all the way to A7. It will produce. So then what happens is you ask this word, 1, 2, 3, 4. So A4. You feed it. Now you can do your normal thing. You can feed it into a softmax classifier and see which word in the vocabulary, the target variable are all the words in the vocabulary, which of the words it is pointing to. So for example, is it pointing to the word truck? Is it pointing to the word house? Or is it pointing to the word truck? Is it pointing to the word house? Or is it pointing to the word jumped? Right? And the idea is if you train it like this, you want to make sure that it points to this word. You train it in such a way by masking some words and making it guess it. The more it guesses the correct word, the more it gets trained. All those parameters get trained. So that is one task. Now you notice something very interesting about this task, that there is no label data you're waiting for. You're literally creating your own label data. You just take vast amounts of text. Just go take Wikipedia text, right? And what you do is it is filled with sentence. I think it is like 150,000 million words or something like that. Genome is number of words and lots and lots of sentences. And you play this game of masking some words and making the transformer learn it. And then gradually those parameters will start learning. And so what happens is when you take a really long sequences, you imagine long sequences, you go and mask 15% of the words. And you don't just mask it. Sometimes what you do is you play games. Instead of masking it, sometimes you go and replace it with a random word. And yet you expect that for that random word, the output will point to the correct word. So you either mask it, you altogether mask it, or you put a random word and things like that. You play these games, but at the end of it, what happens is that, so 15% word masking is what the original paper did, word paper did. And they got very good results. Then the second thing they did is the pairing problem, next sentence prediction. This is, again, very easy. And so you don't need vast amounts of label data. You go to Wikipedia, you find a sentence, the cow jumped over the moon. Somebody remind me what's the next sentence, Kate? Kate, what's the next sentence? So anybody remembers? And there is something ran away with the, I don't know what. So anyway, so suppose there is another sentence. The dish ran away with the spoon. Yeah, the dish ran away with the spoon. Ran, the dish ran away with the spoon. So what you do is you first, this data, this is S1, S2. What's the next sentence? Oh, the next sentence you have there is the dish ran away with the spoon. Okay. Yeah. So this is a correct. So what you would do is you would take a valid two sentences that follow each other, treat it as a positive result. Output is positive. And then you would take S1 and just put some garbage, like random sentences taken from the corpus. S random, random. And then you would just, you know, create a lot of random. Typically, I think for every correct sentence, they throw in five random sentences. You mark them as negative. So now what do you have? You have essentially created label data of your own, isn't it? This is the input. This is the output or the target. And then you start predicting that. So what happens is by the time this model gets trained you can tell whether sentence given a pair of sentence it can tell whether s2 follows s1 or not right so this is the second thing that you use for uh for this purpose you keep uh predicting that for this purpose, you keep predicting that. So these are just means to train the bird. I won't go more into that. At the end of the day, what happens is the bird got trained and there is a benchmark called blue and there are quite a few tasks. Basically, the rest is history. It turns out that it went and wiped the sled clean. it won or became it won almost all the benchmarks suddenly in one shot and so bird has become obviously a runaway success a lot of people are using it in a lot of different contexts in my team we use but quite a lot variants of birds quite a lot and for all sorts of purposes and the good news guys is this was the theory this is theory worth learning once in your lifetime and i really feel that you should learn it because for multiple reasons you should know what it is that we are doing and secondly for those of you who will be job hunting these are some rather common questions that keep coming up in interviews, right? Somehow transformers and attentions, for I suppose valid reasons, they are a staple diet of interviews to explain how they work and so on and so forth. So it is worth knowing that and now you guys know that. Now in the lab part, if you remember when we did the lab with transformers, it was perhaps one of the easiest labs because there were already pre-trained models. We just used it and got away with so much functionality just freebies that's the power of transformers but there is a other side to it also which is that we can use it to do more complicated things in which we actually train the transformer so in the coming weeks we will deal with a little bit harder problems where we take the transformer and we don't just use it out of the box as a pre-trained model, but we do fine tuning of the transformer for our specific purposes. And we'll take a variety of NLP tasks and we'll apply it to that. Does it make sense guys? So this is all that there is to this transform. I mean there are little bit of details you can hop across and so on and so forth. So for example, when you do sentiment analysis, so you have this sentence, right? The cow jumped over the moon, the cow jumped over the moon. And what happens is you put a CLS token and the second sentence will be just null, empty sentence. You feed this as input to your bird and then you're doing a sentiment analysis. What will you do? You need to feed one vector as input remember a classifier takes x vector as input and it produces a y hat which will be positive or negative negative let's say for sentiment i'm just taking an example simple example but then each of these tokens are producing their own vector so which vector we will take so this this is the convention. This is why we put this extra block in the beginning. We ignore the output of all the actual words and we instead take the output of is CLS token? It is just a token we insert in the beginning of a sentence while we feed it into the transformer. And when it comes out already transformed and all sorts of attention and everything fully through all of those layers, we use that thing. Now, another thing is why would you use that? The thing is that the reason you place that block is, it is sort of paying attention to the whole sentence, right? And in some sense, it contains the essence of the sentence, right? And so that becomes our input vector, goes into the classifier, and then you can use it for it's a sentiment analysis or all sorts of things. For example, at this moment I'm trying to train we are actually using my team is trying to train see if we can take texts which are very opinion heavy and text which are very knowledge heavy and try to differentiate between the two right so we have some early success let's see how it all works out so we do all sorts of things people use it everywhere and in the next two weeks i'll give you guys a flavor of the kind of things you can do with transformers. Asif, can you take a moment to explain, like you said at your company using GPT, right? But- GPT-2, GPT-2. GPT-2, okay. So the part that I was getting to is, we know GPT-3 definitely cannot be downloaded. It's like a billion records in terms of the billion weights in the... 125 billion weights. So when you're working with it in your company, what would be the, let's say the computational complexity of doing any change with it in terms of time or computational resources needed what's the magnitude you're describing it you do stick in the transformers and we do train them or fine-tune them for our tasks and for many tasks especially for classifications for our specific classifications we do do the tuning and the stuff. Generally, it can take anywhere between, some of them train pretty fast. The data is very clean, signal is very obvious and half an hour they'll get trained. Sometimes it runs for the better part of two, three days on powerful hardware. So the back propagation, does it go through all these weights everything back propagation is the one trick in the game okay it's all see in this deep learning right pretty much the back propagation is the king of the optimization process of this optimizes there are certain methods that have recently been proposed, predictive coding and things like that, which researchers are showing are equivalent to backprop and are closer to neurobiology. So one of the problems with backprop is that, see, it's backward and forward, right, in our neurons. Signals can go backward and forward. Forward is the prediction part or the inference part. Backward is the learning part, learning part, updating of the weights. But in biological systems, signal goes only one way through a nerve, it doesn't go bi-directionally. So one of the things that people have wondered is that while it works very well backprop mathematically, how do we build something equivalent that is biologically plausible? And so there are some alternatives people have come up with and they have shown that it works. But leaving that entire area of research aside, pretty much you have to make sure that throughout your end-to-end system, back propagation, the gradients can propagate all the way back if you if they get bottlenecked then you have a problem and the the computational resource that you allocate for these kind of problems what is what is the how would you describe those computational resources see in my team right they're very gc heavy. So they will all go and pick up these GCP instances where you can configure your GPUs or GPUs. And the way we do that is we expand the hardware based on need. If we notice a computation is taking too much, then we add more GPUs, more GPUs, more GPUs. And they get very expensive. Ultimately, you end up paying $10, $15 an hour. And imagine a computation that runs for three days. You already have run a few hundred dollars through that. And now you imagine a team of about 20 guys, each of them fiddling around with something and running their model, you realize that suddenly the costs are very high. So when you do that, my experience has been that it's cheaper to buy hardware for development. Take it to production for inference in GCP. But if you can afford to put hardware under people's desk or something in your premise, it is far cheaper. The per hour costs amortized over the cost of the hardware comes out much cheaper. You pay a capital expense, but it is much cheaper. Which is one reason these guys who make this Lambda Labs, et cetera, they are selling their machines like hotcakes. A lot of people are selling a lot of hardware nvidia charges for one of its hardware you know some ridiculously high price 150 200 000 for one machine and they get away with it because even at that price when you amortize it over the lifetime of the machine it turns out cheaper than the cloud so that's how it is at cheaper than the cloud. That's how it is. At this moment, the cloud is expensive. You don't feel it. It's a nickel and diming business. When you're doing little things and learning, it feels cheap. The moment you make heavy use of the cloud, that's when the cost comes. You want to go to production with your application in the cloud. It's when the cost comes. So inference, you see you want to go to production with your application in the cloud. It's a given these days. Otherwise people don't take you seriously as a company. Quite often, I don't know why it's become like that. It's sort of an indoctrination that you must be in the cloud. So you go to production in the cloud. Yes, you do get a lot of facilities, a lot. You basically save on IT resource, IT staff. You don't need your own IT staff, big IT staff. You can have a much smaller IT staff. And you're basically buying IT services from Google, or face, I mean, Google and Amazon and so forth, but it's expensive. So go to production in the cloud, my advice. Do your development on something that you can actually see and touch. It will also be less headache. See, when you have a machine that's totally in your control and other users are not logging in, weird things are happening. See, this was this industry work with one engineer one workstation so i my argument is that still that should continue in the deep learning world too at the end of the day like for example you notice that code tends to run on my machine but we have this problem when i give the same code to to run on my machine but we have this problem when i give the same code to you guys a different problem different people have different issues right so at the end of the day like in my case i don't get much time i have a full day jobs that i like to teach in the evening but i have only limited time to give to it so when i sit down to write code i need to make sure that the first time itself i'll get it right i won't be fighting the the environment itself and it helps to have a very powerful machine the some of you who work at support vectors know my white elephant and black elephant patrick you know it very well i think yes Patrick, you know it very well. Yes, Asif. Asif? Asif? Yes. So regarding explainability, Asif, is there any way we could still salvage some form of explainability by building something in the last few years? Yeah, it is an area of active investigation. See, some people even, people often used to say, throw in the towel, you can't explain what a deep learning model is doing. It's too black boxy, right? In fact, some people even used to postulate in physics, there is something called the heisenberg uncertainty principle. Raja Ayyanar? which says that for elementary particle if you know its position reasonably well you won't know its momentum its speed. If you know the momentum, how fast it's going, you won't be able to tell where it is with accuracy. So there is a, so people often used to oppose that maybe we have our own uncertainty principle here in deep learning, in machine learning world. Either the model, if you can explain the model reasonably what the model is doing, it probably doesn't have that much accuracy or performance. But if the model is high performance, then probably you have lost interpretability. So a lot of people talk like that. I don't know if I completely agree. I personally don't agree, actually. I believe that everything is a work in progress. We need to build tools to explain it. There is a lot of research going into interpretable AI. If you remember, I wanted to. I said i started this workshop by saying i'll put a lot of emphasis on interpretable ai right and automated machine learning both of these things at this moment seems a little bit distant because we we have a lot of material to cover and we don't seem to be reaching to those points. So it becomes a choice. Do I do interpretable AI and automated machine learning or do we do time series, for example? So we have to make certain sacrifices because we have only five more weeks left. But maybe I'll make it some extra sessions after that. But interpretability is big, otherwise society is doomed. We don't know what these black boxes are doing, they're destroying. I mean, so anyway, let me just stop the recording before we talk for one second. Thank you.