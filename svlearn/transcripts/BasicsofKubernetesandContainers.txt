 So today is January 1 and we have gathered to discuss about the project that we have been doing. There are two things on the agenda. First, I would like to introduce you to some technologies which you can start looking into. I will be giving out source code and instructions later on. But before those instructions make sense, you need to go and familiarize yourself with those technologies. The second thing is we'll go over the project specifications. If you recall, on Wednesday we had agreed that today you folks will come back with what you understood as the functional spec of a project, the image processing project. The idea is we'll follow the complete pathway of taking requirements from business, first creating a functional requirement, then creating a design understanding of how we'll go about it, the systems design, and followed by iterations of implementations, rapid iterations, and followed by actually taking it to production after that. And one of the things I've requested is in getting ready for production, some very common sense things, go get a domain. Otherwise, you'll go to production on just an IP address. Domain is a couple of dollars these days. If you want to go take it, it's far less opaque than just an IP address. The second so with that, actually, what I would like to do is first part, I would like to see people's function specification. Let's quickly run through those and then I will get into. So the topics that I will cover today are things like the compute engine in Google. We'll use Google standardized on Google for our deployment because otherwise we'll end up wasting time covering both AWS and Google. Let's take one and we'll do that. So we'll use the computer engine, we'll use the Jupyter notebooks, the AI stack in Google. We'll make use of all of that and proceed from there. So and I'll introduce you today to do things containerization and Kubernetes, orchestration of containers. Those are the two topics to be introduced. to them and what technologies have emerged in recent times to make it very, very easy to do rapid cloud deployments. Let's do that and then we'll cover some of these technologies. So with that being said, let's start with various teams. Who would like to volunteer first with their function spec you are also welcome to show progress on the eloquent transformer project anyone would like to volunteer? Dennis? We haven't gotten to do the functions back here. Biology? So not it actually. Prachi is out in India, so just kind of a little discontinuity. Anybody who has done it, Kate? Kate, how about your team? Kate is on mute. Sudeer? It is on mute. Sudhir? We haven't done anything on that, Asif. OK. Praveen? No, Asif. We didn't get time, so we will not start. OK, guys. So what I will do today or tonight is, at some point I was going to release my version of the function spec, but read that, but do write, do get into the habit of taking notes from, you know, when businessmen talk, when a business says or enterprise says, this is what we want done. It is a skill to be able to listen to what the person is saying, come up with, capture a function requirement, and essentially align it with the subject, with the domain of AI. What does it map to? Are we talking classifiers? Are we talking recommenders? Are we talking regression? What is it that we're talking about? So that's part of the skill set. Do please pay some attention to it. But I'll release it tonight. I'll release for you guys the specs for that. I have it. I have it. Anybody else? Anil, what about you? Nobody seems to have gotten time. It's too quick. Okay, so let's leave the specs aside. Do we have some progress on the project, guys? Anybody would like to show some further progress? Yes, we made some progress. How would you like to speak about it? Or yours is that clinical part, isn't it? Clinical part, we kind of know that we have to debug that one issue. But we wanted to develop something related to eloquent transformer so that we can participate in the deployment part of it right so we we did spend yesterday and today on building that eloquent transformer so we want to show just that oh yeah sure your expectation in terms of usability at this point but at least we have something that sure why don't you show it okay okay so a few things as if we are trying here um one thing is um i will also pull up look like you have so there are quite a few features that you wanted us to implement we did implement some of it not all of it yet okay okay and uh the way we thought that to learn more ways of implementing this, we wanted to use at least spaCy and Transformer as a model. So here you can see that from the left side here in the pane, choose the model. Not all of the features are implemented in all of the models here. So choose the model not all of the features are implemented in all of the models so this drop-down basically you choose the model then this drop-down is driven from what you choose the model and those implemented good nice and then if you choose translate then you will see this further okay so for example you can see here right now it's been translated into Hindi and I can change it to Spanish. And if I change this translate to, let's say limitize, then this disappears from here and you will basically see this. And then let me just pull some text so that we can test other features as well. No, this is very good. You have made quite a bit of progress in a short time. So this is really good. Also worry about the aesthetics of it and add some pictures and themes and so forth. Sure, we'll do that. Were any one of you successful in adopting a css so i just copied here some pegs from cnn trying to do so i used um transformer to do summary sort of do that so we can do a sentiment analysis as well. Very nice. This is using Stimlet. Very good. Anything else? That's pretty much it. So we, I was trying to show this, you know, not just the numbers here, but quite a few things that we can do. Yes, this is a pretty negative sentiment for everyone, I suppose. Want to walk me through the code or this is fine? No, no, this is fine. We can walk through the code separately, one on one. All right guys, since you don't have, nobody else is willing to show, just as I have a question, did anyone make progress with the themes with CSS? Style their page a little bit, beautify it? No, I think. Not yet. Okay. All right. So what I'll do then is I'll move on to two things. First is I'll just give you a summary of what the project is with the trees. The project is to distinguish between two types of trees. One of them is called the pepper tree. You see them all over in the Bay Area. And another ubiquitous tree in the Bay Area is the weeping willow. And you have to distinguish between them. So you have to write a classifier. Given an image, you have to tell, is it one or the other or none or not? Maybe it's neither. Then the second thing is to do neural style transfer. You have to take a few great artists, like Van Gogh and Monet and Picasso and a few other artists of your choice. And you have to take an input picture, and you have to render it into that neural style now people can either upload a picture or they can just give you the url of a picture and you have to download the picture and do the neural style transfer but the that's the second thing the third is you have to do object detection in an image and you have to itemize what is a narrative, what is a text that you can do. This last part is a stretch goal. And the other thing that is important is whenever you create a classifier, you must give, explain it. So you say it is a pepper tree, by which regions of the picture convinced you that it is a pepper tree, but create your saliency maps or your attention maps. Right. And display that. So the three things classifier, explainable AI, and neural style transfer. These three are core parts of your project and a stretch goal, if you can do it, is to do object detection. Right. And given the object, you can put a narrative. There is a boy and there's a, you know, there's a child under the tree or something like that. You can do that for any given image if there are multiple objects in the image. you can do that for any given image, if there are multiple objects in the image. So see if you can do that. I'll release the function spec tonight, my version of it, and see what you guys can think about in implementing this. What would be the production level issues? How resource intensive would it be? And so on and so forth. Those are things to think about as we go into production. So today, what I would like to do is take an hour that we have remaining and introduce you some basic things which many of you may know. By the way, how many of you, by show of hands, tell me how many of you have used Docker fairly extensively or use Docker or use Docker on a regular basis? If you could please raise your hands. What? Anyone else? How do I use the raise the hand? Oh, there is a someday I don't know, click on your name and say more. In your name you can do I think I think you can right click your name and do more. I don't know. What does it how does it work? In fact, I don't know how those of you who are raising your hands how are you doing it reaction so oh in the reactions okay at the bottom there's a reactions button click on that so at this moment it seems that two people are familiar with docker so not many of you so this time wouldn't be wasted let me start with first principles let me start with the basic concept of docker and kubernetes okay so i'll start sharing my screen so as if kubernetes is going to stop support for docker uh like this is pretty soon that is right that right. So I'll use a more generic word. I'll use the word container. I thought Docker is the most famous type of. No, Refik. There are many things that have come Docker. docker well okay there's a lot of peculiar dynamics typical open source things competing containers and whatnot going on so in fact it's hard sometimes to keep track of everything that goes on so all right let me are you guys seeing my screen let me take a pen and start writing up okay guys see our understanding okay my goodness this is gone or something in an Our understanding, okay, my goodness, this is gone in an entirely different direction. Okay, our understanding of a computer is like this. We have a computer. It has a CPU. It has memory. It has a hard disk. This is how we think of it. And then you have peripherals like the monitor. Monitor. Keyboard. This is how we think of it. And then you have peripherals like the monitor, keyboard, mouse. These are peripherals and whatnot. And like, for example, my writing pad. But the core computer, you think of it as these, and then of course there's networking, and there are other things that work. Now what happens to the computer is that these days the computers are very very powerful, and people have often tried to do like shared computing. That can you take this machine and take a powerful machine and in some sense, so you do sharing of the machine across users. Across users. So how would you do that? The old way used to be that you would create if it is a Unix, Unix or Linux, which was the original sharing model. Each user, many users, share the operating system and machine resources. You protect the file, the isolation between two users is through an access control list. So users are separated by permissions, partitions separated from each other's resource using Unix permissions, using Unix permissions, which acts like a ACL, access control list. So given a resource, who are the people who have access to it? Every user, group, everybody in the machine, things like that. So this is your standard Unix model. Now it turns out that this model while it worked very well when you could strictly control the machines and even today if you go to universities you'll often find this powerful machines and you get a unix account on it and you do that the things are moved further we live in a world of huge security concerns but even more than security concerns they are very real problems very simple problems for example on a machine if i use a library let's say a java library or a or a linux library you have your library let's say that you install something you install pytorch right and you have one version of PyTorch and I have another, I want to use another version of PyTorch. You need to have some degree of separation so that I don't go and screw up your project, right? Now, Python has a really nice way of doing virtual environments, right? So in the Python world, Python world, you create the virtual environment, which is limited only to Python libraries. It says that you can create your own special sandbox. And in that sandbox, you can install whatever libraries you want, and then do whatever you want, you will share the computer compute, you'll share the memory, you'll share the file system. But in your particular sandbox area, only your libraries will be there. And when you activate your virtual environment, that your system will see or your virtual environment will see the base Python and the libraries that you have installed in that particular virtual environment. So this is a simple example of containerization. You have contained the impact of this user to this environment. You say that this environment can't contaminate other environments, Python environments, on the same machine. Quite often what people do is that for different projects they will have multiple different environments because they need different versions of the library, right, and these libraries don't always work together. So this sort of containment or sandboxing of environments is very essential. In Linux, well, first of all this idea came about with the concept of virtual machines, which was a rather coarse grained way. What it did is that within a machine, it created another machine, quite literally. It had the concept of a CPU, the virtual box when you use an open source project or VMware, etc. What you do is a sort of a file that acts as one giant isolated box. This box, you give everything, you say number of CPU that it can have, and it could be obviously some number you can limit it to from one to N, where N is the number of CPUs in the host machine. You can dedicate memory, quantity of memory. So you can dedicate a small bit of memory from the host machine. Let us say that you say 4 GB. And then you can give hard disk. And you get your own hard disk. And when you fire up this virtual machine, you get to install your own OS. So for the long, so this virtual machine concept became a runaway success. Everybody loved it for some time. Why? Because now many, many things uh virtual machines give you a way to checkpoints so suppose you install something you ran a test code or production then you want to go back to the clean state right the original state all you have to do is either checkpoint the virtual machine or keep a copy of the virtual machine, which was your original pristine state. And so you can have multiple copies of your multiple states of your virtual machine. And it's very easy. Some installation screwed up, you regret that you installed it. In a typical Linux environment, you would, or operating system environment, you would really sit and cry over it. But now you have to fix your environment or start all over again, erase everything and do it. In the case of a virtual machine, that is not really needed. All you have to do is, well, you know, either go back to your last checkpoint or just pick up a new copy of the virtual machine and start all over again. So it sort of speaks tremendously to productivity. You can experiment without fearing that your machine will get screwed. The second benefit of virtual machines is that virtual machines are portable. They're just some files. So whichever machine can support or has support for virtual machines, you can just copy these files, these instances, these are called virtual machine instances onto those into that environment and fire it up. When you fire it up, it will behave exactly the same as it was behaving on the other machine. And so it brings about a very interesting possibility. You don't have to go through the tremendous headache of installation. You can start, for go through the tremendous headache of installation. You can start, for example, let us say that you want to do Python development and you're struggling with the libraries and so on and so forth. And not only Python, you need some other Linux libraries, maybe you want some particular version of Emacs and whatnot. So what you could do is, one thing you could do is get a machine and install everything from scratch. But a better thing you can do is you can just go and look around if people have virtual machines they have already created successfully and you can just copy the virtual machine and there is quite a marketplace or open source collect repositories of useful virtual machines. Quite often some of these open source projects are very, very hard to install. The instructions are not so clear. It struggles. So somebody would spin up a VM, then go and spend a lot of time making it work, perhaps reach out to the original developers, spend some time on the developer mailing list or user mailing list. And after a lot of struggle, will get something to work. Now once they have made it work in a virtual machine environment, all you need to do is freeze it. Just keep a copy of it somewhere and share it with other people, and arbitrary many people will now benefit from the work that you have already done, the hard work that you have done. So it leads to a lot of efficiencies. And you don't have to become a deep installation guru, a system admin guru. Today, for example, in a very practical sense, you can go and get the whole Elasticsearch installed in a VM, download it, that's all, and you can do it for anything, web server, etc. You don't need to know any system administration at all. You just go download the VM and fire it up and you're done. So virtual machines actually, one of the reasons that, I mean, one of the underpinnings of the initial cloud revolution, when Amazon Cloud came up, it started out with VMs. You could just get VMs. Even today, VMs are the dominant thing. You can have lots of VMs. And you can purchase VMs of different sizes, you know, extra large this and small that and whatever. And you guys are all familiar with spinning up instances in the cloud. So you're familiar with it. Runaway success. But it turns out that the concept of virtual machine has some limitations. And the principle amongst it is that, let's say that you have spun up a VM that takes 8GB of RAM and your machine has 16GB of RAM. Now what happens is you can only spin up another machine, maybe with 6GB of RAM or something, leaving something for the base operating system. You can't run too many VMs on your machine. It is limited by what resources you have given to the running VMs. The other problem that happens with this is the hard disk. There are two things you can do. Well, nowadays the VMs have gotten smarter. You can say, I want a VM with let's say 100 gig, but you keep it elastic. Unless you actually consume the 100 gig it will not it will start out with let's say just some 10 gig or whatever use and then whatever you utilize and then transparently expand with the initial versions you had to just go and allocate yourself 100 GB of disk space and that would be the internal sort of virtual physical disk of that VM. And so soon you would start running out of disk space also. So people used to maintain on their big hard disks libraries of VM and they would go buy more hard disks and keep even bigger libraries of VMs. And it was very common if you worked for big companies very common to check in and check out and just download those VMs and you know do things with that and so on and so forth so all of that is wonderful but they have one this limitation is there a resource constraint so people began to but what is the benefit of that your Your VM is your VM, nobody can screw it up. So whatever else is running on the host machine that cannot come and interfere with what you are doing. The only way it can interfere is maybe it can hog up the CPU or slow things down and so forth. And that is one of the things you observe in these cloud systems. You must have noticed that when you run a workload sometimes your work your programs run very fast at other times the same vm instance runs sluggish and part of the reason is because uh typically like on a typical machine they will put close to 5200 VMs of reasonable size, if it is tiny VMs, then even more. So what it means is that most of these VMs you hope are not active, they just sitting idle. But if all of these large proportion of them all become busy at the same time, then they will hog up the CPU resources, it will get sort of, there would be a starvation, and whatever CPU resources there will be shared amongst all these active computations. And so things will move slowly. So you keep hoping that you run your job, or when you're running your job, other VMs on the machine are not hogging up the computes. It's a fact of life with VMs. And that problem sort of stays. You don't have a way around it. So that is one. People began to ask, why are we creating VMs ultimately? You get isolation, you get a sort of a hardware profile and so forth. So can we do something more lightweight and still have the isolation from a practical programming perspective? So to get that isolation, people created the concept of containers. I believe it was a Linux that introduced the concept Cgroups, which was to contain, it's another isolation level baked into the operating system to create that isolation. So what you could create is something, a very lightweight VM. It's called a container. There are quite a few containers now and people compete on which container technology you should use and so on and so forth. And there is a lot of fun things happening all over the place. So let me talk about containers. Docker is obviously the most popular example of that. Containers. So what the containers do and how they differ from the VMs is this. When you create a container, let's say you declare a container, let's say you declare a container, let's say in some sort of a manifest, you say that, let us say that you start with, now typically you go to production on a CentOS. So let's say CentOS, right? Version, whatever it is today, it is seven or eight. Eight. So we are on CentOS 8, let's say, 8 dot whatever. You say that, take a base image, which already may exist some somewhere. On top of that now install, let's say if install, B, Python, right? C, Apache server, web server. I'll take an example. And you declare that this is what my Docker image should be. And what you do is you run it. When you run it, you materialize it, materialize the docker as an image, what it will do is it will, at that particular moment, go and create a CentOS environment and so forth. So the way it works is that, first it looks around. If you're working on CentOS itself, there is no reason to further do that. It can be pretty lightweight. It can just go and install this, this, this, right? It can go and do the installations of this extra software. And what you do is you can think of this container, you activate, you enter the container and it's your isolated environment. It's a level of isolation. This isolation is very much like VM, except that you don't, you will traditionally, you can, but by default, you just say that, I will share the host operating system resources, and it is a more lightweight isolation or containment. This is Docker. This is containers. Now what happened is, with this coming in, many good things happened. Containers are much, in a way, more portable or more easy to scale out and replicate across a cluster of machines. So it made possible things like staggered deployments in production and blue-green deployment. So let me tell you what blue-green deployment is. So suppose you deployed your software. Traditionally, you would deploy your software on a hardware, but with these containers coming in, you don't do that. You test your software, you deploy inside a container. You take the container, typically Docker or something, and you deploy it to your hardware. And what you typically do is that your bare metals, you don't put anything except the operating system on it. So for example, you can put the, and that is why you see this lovely distributions like CentOS minimal, and then there's CoreOS and so on and so forth, or various versions of it, operating systems, a minimal version of operating system. Why minimal? Because you want to keep it absolutely rock solid. The more you install, the more vulnerabilities, the more likelihood of, the more the need for maintenance and so forth. So you ask yourself, what is the core essential of an operating system that I need? You just take that on the bare metal. And on top of that, every software or everything that you do you sort of do it in a Docker container, right? And we'll do examples of that. So once you do it as well, let me not use the word docker, but in container, docker is a popular container. You do it in a container. Now what happens is suppose your software is in production and you come up with another functionality that you add to your software, to your product. Or your application. What can you do? You can compile your code, you can deploy, you can create another, deploy it into another new container. And now that container, that materialized container image, what can you do with that image? In the production system, you can bring this into existence. running you can bring this into existence do a sanity check to make sure that all is well and what you do after that is you redirect traffic you take one down and you redirect traffic through the other typically you can do it like for example in our cluster we do it but through just the load balancer the load balancers are aware of these two ips or the other technologies or many many tricks to do it but you just divert traffic now to the new instance. And there is no downtime. So you call this the blue-green deployment that at any given moment, you have a blue instance or a green instance. So if you're on the blue instance, when the green instance comes up and runs successfully, it gradually takes over the load from the blue instance. And the blue instance then gradually is taken down. The old version is taken down. Then after a little while, when the green instance has been running, you want to bring in one more instance, you call it blue and you bring it back. Right? So it gives you a way to continuously evolve your software in a very rapid manner. And this very rapid evolution of software, the whole ethos is called continuous delivery. Continuous integration refers to the fact that you keep running your code in an agile way and keep integrating within your team. Don't hold on to the code for long. And it's okay to break each other's code and buffer it with unit testing and all that, that whole agile testing movement. But continuous delivery refers to the fact that whatever you guys are building once it reaches a certain level of certification or internal validation you just deploy it and deployment is easy now what has happened is we deploy vast on vast number of servers it is very common these days to deploy 100 servers at a go. And that's where another set of technologies that, you know, in the ecosystem comes about. These are called infrastructure as code and they're all of these things like Salt and Ansible, SaltStack and Ansible and so on and so forth. And then there are things like spinnaker and then there's jenkins and whatnot there's a whole ecosystem of things some of which i'll introduce you guys in the labs what they help you do is they make the whole process very very smooth of how you can scale out and deploy your container across a whole slew of servers without so much as a human intervention in a purely automated way. Just click Deploy, and in one click of a button, your container now gets deployed to hundreds of servers. And you can follow whatever methodology you want, blue-green, or people do staggered deployment. So what they do is one machine will first get the new software, then another, then another, then another. So you sort of one by one keep replacing each of the instances with new container instances. The fact that you can do that makes for a very, very powerful use case. Today, it has become common for big companies to brag that they have thousands of releases a day. I believe it was Netflix, I believe, when they were asked how many releases do you do in a day, they said that across all the teams put together, we do close to 4,000 plus deployments a day, releases a day. What does it mean? It means that every little, every part that is building its own unique functionality, whenever they are ready, they just go and deploy. The whole architecture is very modular, it's very microservices based, and they can all take their things to deployment and they all use all of these technologies of container container-based deployment and we will do that in this particular workshop hopefully by Wednesday I'll bring in examples and I'll take you guys through that process of container deployment and we'll do some deployments to GCP and so on and so forth right so in other words what it means is today, it's a lot easier to deploy. Furthermore, suppose you want to just add one more thing. Let's say that a container is there. It has Java there, Python there, Apache web server there. All you need to do is create one virtual end in which you want to install some specific python libraries which is not there in the base system and fire it up right so what can you do you can actually there's a notion of inheritance solving this out of this you can create another container let me just call it c0 you can create c1 which will have plus extras, whatever extras you put. And usually when you do that, this container technologies are pretty smart. They will look into the library and see, this is already there, right? C naught. And it will just take this image and add this extra things, bring it up and add these extra things. And you're ready, right? It is just a bunch of files that gives you an isolated file system in some sense with your own binaries and so forth. So you don't have library conflicts and all those problems that you used to have, that used to be held before all of this containerization came about. So that is the magic of containerization. Now there are more to it. People compose. So suppose here is something that gives you a database. Here is something that another instance that has the web server and you want to put these two together. So most of these container technologies, they give you some form of composition of the two. You can take this and this and create a mashup, essentially, a union of the two, compose the two together into one big instance or something, image. So all of these things happen, these things are best seen through a lab. And we will do some simple experiments with containers in our next lab. So that is container technology. But one question here remains suppose i have a hardware and this is a question of orchestration before i go into orchestration any questions guys if these are very new concepts do ask me if you understood that did you get it guys Do ask me if you understood that. Did you get it guys? I'm having flashbacks to the big data workshop. Please say that again, Kate. I'm having flashbacks to the big data workshop, CentOS. CentOS, yes. up sent to us sent to us yes right sent to us for big data and ubuntu for uh ai sort of a defective standard okay so we go to the orchestration part um again any any other questions on continuous values did you all understand it if you if this is the first time you have encountered this concept and surprised no one is asking the question. Asim now. A little bit louder. Yes, go ahead. Am I okay now? My voice is okay? Yes. You said by de facto for Big Data is CentOS and Ubuntu for AI. Why is that? I know for Big Data though, but what's the difference that drives the difference in technologies for Big Data versus ai so there are multiple reasons or co-factors first is that red hat took the approach that they wouldn't install any proprietary libraries people they were very strong believers in open source and they said if you don't have a gpl or a lgpl or bsd license software if you don't release the source code we are not going to install it right now these video card makers now they all are extremely reluctant to ever ever release their source code their driver you know video driver source code the reason for that is they know that there are a lot of ideas shared between two companies and the thing they fear is that there are a lot of ideas shared between two companies. And the thing they fear is that there would be a patent war. Oh. Patent war between companies, right? The last thing you want to do is show that, you know, you did something and the other one will claim prior art. Oh. So they are in a fix. Much as they would want to, they can't release their source code, right? And so they give you binaries which are optimized but by default you know centos never made it too easy to do that what centos does is and what they do is open source community reverse engineers the video cards whether it is nvidia or amd it reverse engineers those cards somebody will sit and do that and create an implementation based on whatever you could understand as the behavior of those cards. So these are the Novo drivers for Nvidia and so forth. Their performances are very suboptimal for AI purposes. You can't really do it. And AI, the GPU is pretty much central. AI runs on GPUs, not so much on the CPU. Whereas big data so far does not care so much about the GPU. Like you can have the lamest GPU in the machine and it doesn't matter. Most of the servers have the lamest possible GPU, a GPU chip. So that is the differentiating factor. Ubuntu, on the other hand, was very sort of much more permissive. It says, you know, if the software is good, it's okay for people to give proprietary binaries and they would install it. So when you install Ubuntu, what they do is they don't actually install the Nvidia driver, for example, for you, but they'll bring you very close to it. Right? You are you in the command line, you say they'll bring you very close to it right you are you in the command line you say what are my choices and it will tell you this nvidia is one of the choices and you just go ahead and install it proprietary driver it will just say do you agree to its license so it becomes very painless now what happens is because they have the driver, people have written the CUDA drivers, the CUDA and all of this GPU, all of these libraries, CC++ libraries that optimize on the GPU and move the computations there. They all therefore work well on Ubuntu because everybody is testing it on Ubuntu. That seems to be the only platform. So what does Google use? They use Ubuntu, their own older version on Ubuntu. That seems to be the only platform. So what does Google use? They use Gubuntu, their own older version of Ubuntu that they have highly customized. So all of these technologies, when they emerge, the developers who are creating them are themselves sitting on Ubuntu boxes. You see me, right? The same problem comes to me. When I teach to you guys, have you noticed the very frustrating experience that I'll release the code happily and confidently, but it won't work on your machine? Why does that happen? Well, because I work on Ubuntu and I wouldn't have the foggiest idea that it wouldn't work on other people's machines, but it doesn't work. And so reality is you can sort of get away with Mac, but you won't get the CPU optimization. But okay, Mac is trying to do its own walled garden, they have come up with their own new chip. And they're trying to port over all of these libraries, TensorFlow, etc, to their to their chip, with some degrees of success, and they'll succeed eventually. So they are a completely different game. They're a full stack, but they have their own CPU, their own GPU chip and now they have the APU, everything put together and whatnot. But outside the Mac world, you don't have much choice. You use Ubuntu. Windows is literally, picking Windows for AI development is a bit scary. You know, mindshare is not with you. You become a minority. Right. You can get away with it for so long, but when you get stuck, not many people are around to help you. Yeah. So that is the community. Ultimately, it's the community and it's a networking effect. Everybody who seems to be doing AI is using Ubuntu. It essentially ensures that anybody who will want to do AI will sooner or later throw in the towel and go install Ubuntu. So that's the reality. And Ubuntu is amazing actually. I find that, see, it is not as, the UI is not as rock solid or as beautiful as the Mac. There's no doubt about it. But everything under the covers is really, really rock solid. It's just core Linux, right? Absolutely battle tested and rock solid. Much more so, I would say, than Mac, as far as AI is concerned. So that is that. So, so. Sorry, I have a question. I thought on the containers, we don't include the OS part because that was there in the VMs. Okay. So Rafiq, here's the thing, you do have a choice. Suppose you're running on windows and now you want to run a software which is with centos or what it will do is declaratively when you state it and you don't have that unfortunately the whole vm the whole os will come with the package it will become a giant image right you should natively run on your machine so for example if you have sent to us try to create docker instances align with that or compatible with that so you your images don't contain the os i see okay but there is an option to include the os you need to do all sorts of things you can do a lot of mischief yeah quite people have done a lot in this space as always right but somebody you know somebody has an itch and somebody will fix that itch scratch that itch so that's how it goes and that's like so it's a rich world guys if you haven't used containers the one thing that i would say is start using it in fact i was very tempted at the beginning of this class to make you guys do all your installations in docker containers and even to give you just docker images with everything set up the reason i hesitated was exactly this i can give you a docker image associated with ubuntu and so forth, but most of you don't have it. Also, it's a learning experience to install, become familiar with all this and use it. It is the world, the present is all productionization is all container based. There may still be some, you know, dinosaur companies hiding under a rock who are not using this all sorts of containerization. But one would assume that they are few and far between. Or these are the companies that run absolutely mission critical software. So for example, when you do stock trading, you want absolutely no latency, no layers, nothing. And you sort of optimize it down to the bone. So they don't even use a standard operating system. They use their own weird operating system and weird applications sometimes. Right. So, barring those people, most people go with containerization. So become familiar with it, guys. This is the homework I'm giving you for today, tomorrow, and so forth. Obviously, I'll help you guys do that. But see, if you come to the lab on Monday, but you come without any experience whatsoever, without any reading, you can find it a tough go. Go ahead, Kate. Did somebody ask a question? Wednesday right? Wednesday yes. So guys this is the homework, learn about, start by learning about Docker but and then go to, okay well I'll give you some exercises but let me come to the orchestration problem. See today a lot of the software that we create are what I call stateless. Stateless means it's a, you know, you get a request, you send back a response. And then you forget about it. You do something and you send back a response. And then nothing is saved. There's no memory utilization, nothing. You clean the slate. You have a clean slate after that. or stateless servers or whatever. That is the meaning of the word stateless. Now, when you have stateless, then there is also one more thing, it's called microservice. The micro part, let's talk to the micro part. The micro part alludes to the thing that these days, you don't make a full stack application the way we used to do. Remember the J2E application or the .NET application. In one giant application, you would have everything, the web tier, the business tier, and lots and lots of functionalities all on one giant machine. So we have sort of departed from those used to be called the monoliths. Those used to be called the monoliths. And monoliths have become sort of a derogatory word in the technical landscape. Nobody wants to claim that their software is a monolith. And every company is doing hand-wringing and has a plan to migrate from the monolith to microservices in production. So what do microservices mean? It means that one thing, do one thing only, but do it well. So let me give you an example. You have created this application in which you have translation, you have many things, summarization, you have your NLP application. But now what it means is that in a single runtime in a single python code as different users are using it your memory footprint will increase because you're loading so many models each of those models data is flowing through right and so it you will start having performance and scalability issues. Do you see the point, guys? You're loading giant models, and those models are being fed these amounts of data, which creates large matrices in memory before the inference finishes. memory before the inference finishes. So now suppose you have a pathetic bug in your model for or in your request processing for translation. What it can do is that bug, or maybe there's a memory leak or something, it can go and chew up and corrupt the rest of the functionalities in your application. Do you see that guys? At this moment, almost all of you have written your functionality as one, a single application, one single Python file in which you load these different models. Are we doing that? True. Yeah, we're doing that. So the difference, you would say that in effect you have created a sort of, well, this is not quite a monolith, but it is a small monolith you have created in the parlance of the new world. So what you instead do is you say that, OK, here is one team that does translation, which will be a reality. Within companies, there will be a group of like four to five like you know between four to six seven people they call it parts the word people often use are parts and those of you who are working at this moment does this word resonate with you guys parts small team with a single responsibility. Yes. Technical. Yes. Sounds like an extension of those solid principles applied to Yes, absolutely. Okay. Yeah, in fact, the word here is single responsibility pattern. Exactly, the s in this order that's right so you apply both to people and to their software one pod has one responsibility and it it produces one functionality and that one functionality it will create a mini application that only serves this functionality. So imagine that you have a service that only gives you language translation. Right. So now in your runtime, you will load one model, isn't it? Yeah. And your entire responsibility to make sure that that is that model is as good as possible. It works with as efficiently as possible, scales as much as possible it works with as efficiently as possible scales as much as possible and so forth right and it will have one one or two few rest endpoints or graphql endpoints or web sockets or the grpc these are the okay so let me talk a little bit about the communication protocols, communication protocols. Underneath all of this is the HTTP, 1.1 or 2.0. This is the old, this is the new, HTTP 2.0. So this is there. Beneath that, over, and HTTP itself is layer 7. It sits upon TCP, which sits upon IP, transmission control protocol, and internet protocol. A very, very quick summary of this is internet protocol just says take the message, chop it into little pieces, step them into envelopes and mail the envelopes to the destination. That is the internet protocol, right? And it doesn't matter in which order it's received, just throw it by whichever routes possible and let it sail its way to the destination. And in between there are all sorts of optimization algorithms. The old one used to be the diastereo, but now people have created clever tricks to give the shortest or the optimal path to the destination and the network is dynamic you know some some place that looks fast now will certainly have congestion and so forth so ip takes care of that tcp now with ip packets can be lost some of the pieces can be lost right uh so you don't want that, of course. So what do you do? How do you have guaranteed message delivery in its totality? That is the transmission control protocol. So how do you do that? Very quickly, I'll explain the TCP. This is obviously a diversion, but let's go through it. So suppose there are two, imagine that you are having a battle and you have two battalions. So let's say that you are the good guys, A and B. And let us say that in between is the enemy trench right enemy is sitting upon a hill or is sitting in between and your entire goal is to take over the hill as a as yellow people the good guys but there's a problem a cannot attack the hill alone and win and b cannot attack the hill alone and win so B cannot attack the hill alone and win. So how in the world you want to give a message to exchange between the two yellows, two battalions, that let us attack tomorrow. So let us see how that message will go through. A can send attack message with a messenger so that messenger can go one of two things can happen it may either reach b or it may get killed by the enemy in between isn't it guys a will never know unless uh just by sending a messenger you have no idea whether the message even reached b so if you go and attack the hill tomorrow and B doesn't know and has no plans to attack, you'll get slaughtered. So well, that we can't do. So what do you do? The B then sends back an acknowledgement. Now acknowledgement also may or may not come. The message on the way back can get trapped by the enemy or the message can come back to A. Now, when the message comes back to A, this is an acknowledgement, what does A know does a know is a sure that b has gotten the message a just received an acknowledgement to his message yes at this moment a is pretty sure that b knows but does b know does b know no a is informed of its decision. B doesn't know. So what do you need to do? You need one more. You need, let me pick a color. You need acknowledgement of an acknowledgement, which again may get killed or . So when B also receives an ACK successfully, right, then you know that the message, both sides are sure that the message has been successfully delivered. Are we making sense? Right? So it is a three-way journey, This sort of methodology is called the TCP protocol. But Asif, how does the A know that the third time acknowledgement has gone to B? Yeah, so what you keep doing... So there's more detail to it. You caught me in the right field. There is more to it. i have oversimplified it what you do is you keep windowing you maintain windows and if you don't receive a acknowledgement you keep sending the message again and again but till you receive acknowledgement and b receives an acknowledgement of the acknowledgement you keep on doing it both sides maintain two windows of unfinished business so there's more to it but let's not get into that. But roughly speaking, this is the big idea. Now, this is your TCP. It guarantees you exact delivery of message to the other side. Over that is HTTP. What HTTP stands for is Hypertext Transfer Protocol. It answers a very simple question. It says that, see, transfer protocol it answers a very simple question it says that see can i have a server which is a minimal which does very little a request comes in it processes the request and it responds over tcp but the moment it responds it entirely forgets about the client let's say that the client is here. You don't maintain the connection. What you do is you go and you break the connection right after the response has been sent. The server is not obligated to maintain the connection. This is different from FTP. In FTP what happens? You open a session to the server, but till you close the session, you have a connection, isn't it? Likewise with telnet, you have a connection. The trouble with a connection or sessions is that soon you'll run out of resources on the server. You'll run out of sockets, if nothing else, right? But with HTTP, the benefit was that these connections are very transient, very quickly you could break them. Now, HTTP 2.0 and HTTP 1.1 have further optimizations. People, when they load a page, not only one image or one HTML file, they load literally a few hundred items from that source. So for a very short duration, the connection stays open. source. So for a very short duration, the connection stays open. But then HTTP20 quite literally has gone back and says, well, if you really want a persistent connection for some time, we can give it to you. And you can do a two-way communication. The client can send a whole lot of messages in a stream and the server can keep streaming back a whole lot of messages. And both these things can happen in parallel right and so it has made possible technologies like web sockets grpc and so forth so the basic technologies that we talk about are one is rest and the examples that you will do uh in this and in fact you did when you created that little app from the html page in that flask is you created a rest endpoint rest stands for representational state transfer it's a it is a layer over http what it basically says is that a layer over HTTP, what it basically says is that you can take a server world you can either give it here or give it as part of the for the get and for post you can give the body right and so you expose the rest endpoint and things going in the rest world it is very common to have JSON in. There are only two sort of very common approaches. So other ways are also there. You either do string or text in, like as in string, and text out again as a string. Or, which is still what you can do is you can give formatted text a text query maybe let's say and xml out you can do it like this or much more common these days x XML is not as prevalent. It is text in, text query, and a particular format of text formatted, JSON result response, out, out, in, right? Like that. And then finally the combination, in fact, the one that I tend to use quite a lot is JSON in, JSON request, JSON response. So you can literally return objects, right? Object in, object out, which makes it very object oriented actually. So in the lab, I'll just give you guys a starter code and deliberately to just for a change of pace, I'll give you these things in Java and just get you started it's very easy actually very very easy very much like class to create a high performance rest of microservice in that so rest is one these days there has been some change and people do instead of textual input output they use binary and binary data tends to be much more compressed one particular format that is very popular these days and i use it a lot protobuf it was it it's a google's open source project internal that they open sourced protocol buffer it declares a schema for objects and then you can compile and create clients and server code etc but your objects are binary so then once you have binary objects you can use web sockets or g rpc this is pretty common very popular these days because of its big it's really efficient and fast when you do grpc calls your request goes in as protocol buffer the response comes back as protocol in a web sockets though you can do both text like json or binary you can do whatever you want yeah grpc though is very strict about it it just takes a GRPC though is very strict about it. It just takes a protocol buff in, protocol buff out of objects request and response. So this is the landscape of new things. So you'll see me create microservice in the lab. I'll give you a little bit of a flavor of all of this, WebSockets and GRPC and REST, right? So that you become familiar with that. Are we together, guys? And so you create your microservice around just one functionality. Let's say translate. You load one model and you handle various requests and so forth. And then you deploy it. How would you deploy it? You just need a simple container in which just your microserviceervices deployed. Microservices deployed, right? So this is your container. But there comes an interesting part now, because this, so suppose So suppose a request comes. Now, soon what can happen is you get more requests. The load increases. Request load increases. You start getting lots of requests. Now, what happens is if you are taking a model and each time you run an X vector inside and you translate it and get Y hat, you will create a lot of w matrices in between and those matrices will occupy memory are we together guys and with some of these giant transformer models easily it can be up to one gb or half a g. It's a pretty large amount of memory you can end up consuming. So what does that do? On any given machine, you can handle only so many requests at a given time, otherwise you'll start having out of memory errors. Does that make sense guys? Yes. Okay, so what is the solution to that? Well, here is one solution that you could do. You can go and create, because it's stateless, it doesn't matter. What you can do is put a load balancer here and put instances, C1, C2, C3, C4. And using some way, it does a load funnel. Like based on the load load a request that comes in it will give it to any one of these let's say four instances right and whichever instance gets the request will respond so now you have divided the load in this particular case by four isn't it and you can go on let's say that you can go to C100. Well, your peak load, typically, if you look at your load profile, typically load profile looks like this. This is your load. What do you notice here? At this point, what do you notice? A huge peak. So till now, the normal load is like this. This is a little bit maybe more than normal load. And then you again have normal load. And then suddenly you have this huge anomaly right you have this huge peak load that is taking place here so when you buy hardware or when you deploy containers you cannot you cannot ignore the peak suppose you buy hardware only up to this level. What will happen guys? What will happen when you get peak load? It will crash. Yes, you'll start having denial of service. You have either a choice to crash these instances or you'll throttle it down to only so many requests per per use you know concurrent requests per instance and so you'll start having denial of service issues right your system will start denying users the service after a little while on the other hand what happens if you take if you accommodate this high the peak load you you instantiate 100 containers on a lot of hardware and you load balance across all of them. What happens now? Is that a good idea? Underutilization. Yes, you'll have a massive underutilization problem. So these were real problems, guys. What happens is that today in data centers, most machines are run at 30 percent load just to take care of peak load etc but even then you in spite of everything you try to make sure that you run at you hum along comfortably at 30 40 percent load anybody here who works in site reliability engineering or has production experience experience? Yes, as if. So you agree with that, you guys, isn't it? Typically, you run production at 30-40 percent. You never want to exceed that. At 50 percent, you start raising alarm bells. That's great. So let's do that. But the thing is, can we have the cake and eat it too? So here is what you can do. What if magically when the load is low, there were, let's say that only C1, C2, C4, right? Or when the load is very low, let's say that there's only three instances, two or three instances, right? As the load increases, you go to, let's say that there's only three instances, two or three instances, right? As the load increases, you go to, let's say, C10. And as the load suddenly peaks up and you have a huge peak, right? So let's say that you are at the Politico website or something like that, or what is it called? Five 30 something, 38, you have one of those things, and then suddenly, you get its election season, or what do you need now, you need a lot of capacity. And you can auto magically suppose the number of containers. If the if only the number of containers could elastically and dynamically adapt to load go Now that would be magical, isn't it? So what can happen? You can have a lot of hardware to deal with the peak load, but on the same hardware, you're running many microservices. It is unlikely that all the microservices will have peak at the same time, isn't it? Typically what will happen is one microservice out of 100 will be peaking, the others will be running, humming along at a baseline level, right? And some of them will be just sitting dormant. Does this make sense, guys? So things that have very low utilization, you can just give it one container, two containers. Things that have moderate load, maybe like three, four containers. And things that are moderate load maybe like three four containers and things that and that one thing that is peaking can use up the pretty much the whole hardware but after the peak load is over it will give it up so that some other microservice might be having a peak and that will go acquire the hardware much of the hardware by scaling out the number of containers and doing it okay so if that were possible, it would really be optimal utility or near optimal utilization of hardware. It would be better than this. Would you agree? Yes. Common sense. You have 100 microservices. If you go with the belief that typically when one is having a peak, most of the others are not having a peak. So you let it elastically grow and take up much of the resources. Lots of containers it can spawn. And the others are still humming along with low number of containers. Then when the traffic dies down, it shrinks back to a minimal baseline number of containers. Then something else peaks and that takes up the hardware. What you can do is support a very healthy amount of workload so the company that there are many guys who tried to create this thing there was the mesos os and so forth resource and whatnot but the one winner the one that got done really to perfection and pretty much took over was Kubernetes. It turns out that Google had implemented because they really needed it. The internal architecture, some microservices driven and whenever you fire a query, your query is actually processed by a consortium of microservices or a coalition of microservices, which all come together to help process that query. They add different aspects to it, personalizations and indexing and whatnot. So in view of that, they needed this technology and they created internally a technology called an orchestration. So this is an orchestrator. You need a puppet master. These are called orchestrators or puppet master of the puppeteers of the puppeteers of the puppets puppeteer of the puppets isn't it each puppet being a container it is just managing the load and also it gives you the load balancing feature so that is kubernetes what you typically do in your kubernetes is in a kubernetes a cluster you can declare your parts one for your microservice m1 m2 let's say and do etc etc and each part you give the elastic limit you give the min max and the highly configurable parameters and what basically and you have all of these containers and what kubernetes does is it will just run your microservices in a rock solid manner. So well, if it can do that and you don't have to worry about it, and also it does all the infrastructure stuff. Like for example, you can have one public IP and that public IP automatically gets fanned out to all these virtual IPs, et cetera, and the load balance is there. And all of those things are taken care of. So for example, if this container dies, doesn't matter, another C101 will come up, right? So just one fault in one or one container becomes unresponsive, no problem. Automatically new containers are spawned, traffic is routed to them. It's a tremendous benefit guys. So for example, sometimes people have memory leaks in their code. So the container will after a little while just throw up. So what happens? No problem. Because the other container is taking the load and this container, once it stops responding, the Kubernetes will anyway get rid of it and spawn another one. Do you see that guys? So all of these features come with Kubernetes. So now let's map it back to our problem. You are creating an application, the Eloquent Transformer. What can you do? You can make it out of microservice one, M2, MN. Each of these micros microservices a different team can be working or you could be working in different projects well in this case it was a simple example so you did it all together but nonetheless you can package it all so each microservice would be what will it be it will be its own let's say in this case, a Python or Java code compiled or JavaScript code, JS code, whatever. These are the popular ones, compiled. And you would have a model, AI model, PyTorch model, model file. So what will this do? It will rise and read this file and it will provide you the service, right? And there'll be some rest microservice frontend, either REST or rest, gRPC, WebSocket, or maybe just plain HTTP, who knows? Whatever you want it to be, it'll automatically take care of you. So all you have to do is bundle your code into a container or the image, container image, and let Kubernetes take care of that. So today, next time we are running out of time next time what we'll do is we'll talk about i'll just for a change of pace take one particular framework in java which is very very cloud native and gcp or the kubernetes native you write your code typically you would run with the same ease with which you can run the code locally Typically, you would run with the same ease with which you can run the code locally. With just a command, you can run the code in your Google Cloud with almost zero effort. And you can deploy it to a GCP, to a Kubernetes cluster and the scaling and everything is taken care of. It's really a pleasure to be able to use technologies like that that take so much of the grunt work out of the picture. So that is orchestration. By the way, Kubernetes claims that they orchestrate planet-scale services, and it does. Usually open source projects take a while to be matured. But this project started out the other way around. It completely baked in inside Google, became rock solid. It was running their planet scale operations and then they open sourced it. Therefore, it's a gem. You should definitely learn about it and become familiar with it and deploy to it. As of this moment, I would say there is the reigning king and other things may come and topple it over. But so far it seems to be the reigning king. So we learned two things today guys. We learned about containers. Docker is an example of that and we learned about orchestration of containers which is Kubernetes is an example, is the only one we'll consider. And lastly we learned a little bit about communication protocols so i will stop there guys any questions on kubernetes and orchestration asif can we bring the count to zero like minimum can it be equal to zero or it has to be more than zero really um you should not bring it to zero. I don't know, maybe there's a configuration you can do that. We don't do that in the microbe. We never bring it to zero. It's a good question. I don't know the answer to it. You'll have to read the documentation for each of these guys, but I don't think so. I don't think you can bring it to zero. To bring it in AWS, these are called lambdas. Lambda functions. In Google, in GCP, these are called literally just functions. This is called serverless. You just write your functions and in Azure, I think these are literally called functions. There what you do is you write your code, your microservices, just a function. You don't worry about how it will be deployed and what your container will do, what this clouds will do, is when a request comes for your service, it will actually bring up a container with that function in it, a mini container sort of thing with your function on it. It will make it work and return a response. Then it will keep alive for some time, anticipating that more requests can come. If they don't come, it'll go get rid of this thing. And this one, it may scale out. If there's significant load, it may scale out a little bit. So that is their feature. They call it the server. There's a hot word these days, serverless architectures. And you can write your code as lambdas. There is, it's very high. It has its use cases for ETLs, for big payloads is very good, but it sometimes gets abused. People treat it as though it's a web server of sorts. It has actually limitation because the load time is pretty high. The first time you get a request is pretty darn slow. Of course, there's a billing and cost aspects to it. If you expect sustained traffic, you just go write a proper containerized app. Asif, how do they communicate? For example, one microservices running on one container, another microservices running on another container. And if they have a need to communicate between the two. Right. So what is the discovery mechanism? How do you discover? There are multiple ways. One is through the FQDN. So many ways of doing it. You can maintain discovery registries and microservices can look up in the discovery registries, where is the other guy, right? That it needs to. So that is a, so suppose you have a discovery and there are many mechanisms of that. And they're all these projects, console and whatnot. So it can be in each of these microservices, like it's a hub and spokes model, Right? So if this guy needs to communicate, so suppose A needs to communicate with B. A will ask the hub, right, or registry, where is B? And then it comes to know where B is, and then it talks directly to B. But first it needs to ask and get the, get the address of B. Are we together? This is one way, a simpler way if I may say, is just that for each of your microservices, if you can afford to have your within your internet just internally visible FQDN, a fully qualified domain name, a fully qualified domain name, which basically means something like this is the translator, translator.yourcompany.co. And this is your internal. Another one would be summarizer so suppose i ask you to create a summary of an english text in hindi so what will you have to do first you'll have to call the translator and the output of the translator has to go into the summarizer and there will be a third micro service which calls the first the translator and then the summarizer. Isn't it? And what happens is that what it will do is if you map it to this fully qualified domain names, easy to remember, then in Kubernetes, you can map it. You can say, okay, here is this part, here is this Kubernetes thing for translator, the FQDN maps to this and for this. That's how you can do it, simple way. Or you can use a full-blown registry kind of a software. I see, okay. So even the registries are actually maintained by the Kubernetes? Sort of, yes and no, yes, pretty much. It gives you a lot of extra features and so forth. And sometimes people use their own third-party registries to discover. See what happens is that if you look at this mouse garden is very clean so I'll just use this. So think about this. I'm just moving it with my finger here. So think about this. Request comes to this. So it has to come to a very easily understandable IP, right? It will come to translator, right? Translator dot, I don't know, xyz.com, let's say. That maps to a public IP, a so-called public IP. This IP is sort of a virtual IP, right? It's not a real IP because this is not doing anything. It is just picking randomly one of these, that's why you call it a VIP. A lot of people call it a VIP. It will just pick one of the containers and under the covers route the traffic to that and get the response back, except that you from outside won't be able to tell. Isn't it? So as long as you have given this sort of logical name, you get away with it quite well. it will okay yes understood so as if i have a kubernetes like installed in google cloud and also in let's say quite another company that have a kubernetes but it is in microsoft azure or aws can it work uh like out to cloud we do interconnects all the time it doesn't matter see when one microservice calls the other using a fully qualified domain name and that fqdn is public it doesn't matter whether it's in a data center whether it's in a private data center so long as it is accessible whether it is in the other cloud. We do a lot of cross-cloud interactions. Microservice in one cloud calls microservices in the other cloud. How does it take care of the scaling? Like if it is in two different cloud? No, each microservice is its own partner. They are not the same Kubernetes cluster. You don't create a Kubernetes cluster across clouds. Each cloud has its own Kubernetes cluster, but the microservices running in their pods can certainly communicate with each other. For example, your microservice, for example, any that you wrote, is most welcome to go and make a call into a Google, for example, because nothing prevents it from the code can just directly invoke because nothing prevents it from the code can just directly invoke the other public service. Suppose you need mapping service, you can call the Open Map MapQuest API. Just like that, if you can call Open MapQuest API, you can call other microservice also in the same way. So long as there's a public IP, you know, visible that you have exposed, why not? Ultimately, it's just a REST endpoint, right? Publicly available REST endpoint. And you control access to it using some JWT token or some OR2 and all of that authorization. Am I making sense guys? I know you got that, right? Yeah. Any other questions, guys? So what about App Engine standard? See, see, Compute Engine is like, yeah, they're related things. They're very, very closely related. You know, these are minor flavors, like the lightest, weightiest functions, just like the most microscopic coding is give it to them to run as a Lambda or a function serverless architecture. The next is you just write an app, but you don't worry about anything else. You give the app to them. The third is you do a little bit more. but you don't worry about anything else. You give the app to them. The third is you do a little bit more. You create your own container image, and then you give it to Kubernetes to run the container image. There's a next level. And obviously, the last level is you go yourself, get yourself a VM and do whatever you want in the VM, and there is no control, no further help. So these are different progressive levels. at each level you gain something, you lose something. For example, if you write it as just a function, you don't even have to worry about parts and orchestration or anything. All you need is just your code following a certain API, a jar file, you give it to them, and you have an IP address. And on that IP address, your service can be invoked. That's it. But IP address or some mechanism, some discovery mechanism, and your service now, by some way, it can be addressed and invoked. By name or something. That's the easiest. But then you don't get much flexibility. This is it, you must exactly conform to that API. Next year, you can do App Engine a bit more control than you can do. By the time you get to container, it's your container, do whatever you want. But Kubernetes will still manage it. And lastly, go to VMs, well now you're on your own. Any questions, guys? to VMs well now you're on your own. Any questions guys? For those of you for whom all of this is new do you think you understood it Harini? Harini is not here. So I'm here. No not much but little little i can understand but i think i need to read get you know understand better yes all right guys so let me give you the homework guys go to the google kubernetes engine jakey it is a simpler version instead of doing a full-blown kubernetes deployment on your local machine which is a pretty full blown Kubernetes deployment on your local machine, which is a pretty bit of a hairy process if you're not used to all the terminology. Just go with the Google's computing Kubernetes engine. Quite simple. Go create a little app, play around with it and see how well it goes. Create a little app, a little container, play around with it. Now also, see these are big technical stacks. Create a little app, a little container, play around with it. Now also, see these are big technical stacks. I'm doing it as part of a deep learning because it is essential to have an end-to-end story and be able to take your things to a part of your own. But people who do this, each of this is a world in itself. Just as I said, tons of paper on explainable AI or tons of paper in any one of these topics that we covered here also. On Kubernetes, there's a tremendous amount of work, containerization, orchestration, and continuous delivery and all of this infrastructure as code. It's a whole world in itself of automation so infrastructure automation and there are tons of books that if you go to coursera actually google has i believe it's a free course i don't know for me it's free because i pay them the annual fee i think but there's certainly a course a very popular course which tens of thousands of people have taken, or which trains you on the Google Kubernetes engine. I would suggest it is worth enrolling, especially if it is free, or pay that little bit. Go through that whole exercise, create containers, deploy it, play around with JKE. It will make you very strong guys. Now, when it comes to me at this moment, see, I have explained to you all in high detail and I'll take you through it. But at the end of the day, while I know all this, this orchestration and the infrastructure part, the reality is, well, I have a large team that handles it for me. And so I stay more on the research paper in the technical R&D side. And occasionally, I do my own deployments. Actually, I still do my own deployments, et cetera. But I know the standard stuff. There are lots of extraordinary features people have created and plugins and whatnot. It's a whole zoo, many, many things that I don't know. So I would call myself not a power user, but a user of these technologies. Get into the world. Being a user is enough to take your things into project. But if you really want to get into it, go take the course, get into it fully. It'll take you two, three months to become an expert at it, really good expert at it. And this technology is in very much demand. Right? And especially with AI, people are still figuring out what is the optimal flow. So they're all of these projects like cube flow and whatnot. We're trying to create proper, you know, two production pipelines, continuous delivery pipelines and the orchestration. So a lot of the things are still fluid. People are still figuring it out and interesting projects are coming up. All right guys, that's all I have for today. Thank you.