 So what is information gain in the context of a decision tree? We know this is just a review question to make sure you haven't forgotten from last week. It is the differential between the entropy now and the if you split on a region based on the size of the region like size weight proportion rate entropy of those regions. If you look at the expectation value of the list not use complicated was is basically the entropy of this split regions. So you have information gain. information gain when the entropy of the two sub regions properly weighed is less than the entropy before you split otherwise it's not worth splitting there the information gain is the criteria you use and maximum you split at that point that gives you the maximum information gain so that was a weak learners are learners whose predictions are at par with random guesses and thus the name weak learner obviously this is false a weak learner see you can't make an ensemble of idiots right because that leads to chaos the there is a theorem the con the Condorcet jury theorem says that you need people when you when you're taking a jury, the jury should have the ability to be right at least 50% of the time. If it is right less than 50% of the time, then the noise will accumulate. If it is right more than 50% of the time, that is when you get a strong learner by taking an ensemble of weak learners. So that is the whole point. So you can't have a bunch of people who are just making clueless guesses and therefore get a good idea by taking an ensemble. So that's a point. It is not random guessing. They have to be at least right half the time. Next question is the term random in random forest is present to signify that some trees are assigned to make, so obviously none of this, it was meant to be deliberately confusing. Each tree is given a random. So this is the most enticing one that people end up choosing. They think the random comes from the fact that each is given a random. So this is the most enticing one that people end up choosing. They think the random comes from the fact that each is given a random sub sample of the data set. No, that is true. Random sub sample is given and random subs, but random sub sampling is called bootstrapping. Those are bootstrap samples. And if you, if all you do is random sub samples and build trees the algorithm would be called bagging trees not random forest bagging trees existed when ensemble came out one of the first things people discovered is that let's go in and sign let's go in bag use bootstrap aggregation bagging of the trees and we'll get good results they did get pretty good results better than individual trees but then the breakthrough that came with random forest is that you also took random sub samples of the feature space at each time you would split in a tree by the way this is a good point for me to correct a mistake I think I made when I was talking about random forest I said that each tree not only gets a random sub sample of the data a bootstrap sample of the data it also gets a random sub sample of the predictors that is not true for the standard random forest it's true for something else where it's not true here here what happens is you take a tree but when it is about to split you have to choose between which of the axis which of the features it will split along you don't let it choose from all the features you let it choose from only a small set of features random subsample of features that is random subspace in other words you limit its knowledge to a random subspace of the feature space. That is where the word random comes from. And it is interesting to know that because people by and large confuse, they say random comes from the random sampling. Now, you might see if a joke when I was first using random forest in our team, there was a new person who had come who didn't know data science and her job was to write you know documentation on some feature i developed and it was rather amusing when in a documentation for the work we had done the statement was and then we we randomly take some data and make a random prediction on it. That sentence stuck with me. It is none of that. So obviously the person was new and gradually learned. So it almost seems like provocative to say that you made a random prediction. That sounds worse than zero R. That's right it's sort of weird and I was like oh boy is that what these young people are thinking about my work. Alright the next one consider a random forest where each tree is just a stump in other words it makes only one split only one feature and it makes a split off that's it just one feature and one split this would be rather useless since the trees tree learners cannot learn anything that was the statement and it seems very enticing to say yes but actually no this was one of the rules I kept telling you about the one-hour rule, that actually, it is a surprising observation that in embarrassingly many situations, it suffices to build a random forest of stumps, and its model performance is almost as good as that of more complex models. Another advantage is the sheer computational speed of building such a simple random forest. So sometimes start with max features is equal to one. The observation that simple classifiers can do surprisingly well was first made by Robert Seaholt, a professor at New Zealand University and it is called the 1R classifier. It led to quite a bit of debate. So the effectiveness of random substans is actually an extension of this observation. When one hour was discovered, people were a little bit shocked. There was a bit of debate. Is he really right and so on and so forth. People tried to prove he's wrong. Today we know it is correct actually. You can try it. One thing that I would invite you to do, go back to your California dataset, set the max features to one. See how much accuracy you lose. It's a very quick one guys, one line change in your California dataset. Try that experiment out and see, do you really lose a lot of accuracy? Maybe we're doing it, try that out given a probability feed of the entropy finders this of course a negative of p log p y negative because log of number between 0 and 1 is negative and a negative of a negative will make it possible sir can I go sir 1r and 0 are are they synonymous no 0 R is to say forget the feature. Forget all information at all. Just look at the time and pick the majority. So, okay, got some examples. Like I give you the example of a doctor quack right? comes to a quack and says I'm really sick. I don't know what will happen to me. The quack will give him a patent that I can say my son you'll be fine and gives him some fake medicine and sends him home that's zero why because most most of the time diseases are self-limiting and people cure themselves so that's zero all right zero r yes got it that's that um then this is the magic i hope you all got it right. Weak learners are learners that predict levels correctly at least half the time. They are not random guessers. An ensemble is a crowd of learners. You can make an ensemble of strong learners also, but generally it's better to start with the weak learners. Aggregating for regression simply means taking the average of the predictions from each of the weak learners. One predicts 3 1 predicts 3.5 another predicts 4 you take the average and you say 3.5 is the prediction uncorrelated learners are learners whose predictions are not correlated in other words it shouldn't be that if you look at the predictions for to learners they're correlated they shouldn't be there aggregating for classifiers means taking the voted, the most voted level. Most voted level need not be the majority. So suppose you're classified between a cat, dog, horse, and a duck, it may easily be that none of them would be the majority, but one of them will get more votes than others. So that's that. For example, in the elections in some multi-party democracies, US is not really a multi-party, it's a bi-party democracy for all purposes. But I believe in things like Italy and so forth where you have multi-party democracy I think it used to be true in India also but I don't know if it is still true but what would happen is a lot of candidates from different parties would stand the winner would actually not have the majority vote you would get a lot of votes so then obviously coalitions would form to achieve majority but you don't do that here you just take whichever is the most voted bootstrapping is the process of taking random sub samples from a population with replacement this needs explanation see the gold standard for statistical analysis is something so is something called a magic word called IID. Independent and identically distributed. Like if you have samples, they should all be taken from the same underlying ground truth and the sample should be independent. You can get those samples. That's the whole idea of bootstrapping. You try to approximate IID. You take random sub samples from a population and you replace it so in other words two samples may have something in common one of the inherent advantages of a decision tree is this robustness against overfitting to the data actually quite false that is as one of its weaknesses it overfits to the data in a random forest one can get a measure of the relative performance, yes, true. You can average over either the Gini index gain or the entropy gain from the splits along each of the axes and you can therefore tell which is more important than the other. This is again the last function for a forest. We won't go over it. We did it last week. In the context of Bagic with examples, the individual learners are mutually independent. It must be true, in fact. If the learners get correlated, not only should they be independent, if they get correlated, generally things look pretty bad after that. A decision tree is in general a more interpretable model. And by the way, random voice improves the situation of overfitting. It doesn't cure it. Random voice still suffers from impractical problems, many practical problems, but it still has some degree of overfitting. Are we together? So the decision tree is in general more interpretable than a random forest too. You can draw out a decision tree, you can see it in a picture, we saw that in the lab. You can't do that with random forest. You can pick a tree from a random forest and visualize it but that wouldn't be representative of the forest itself. A decision tree is inherently more powerful than a logistic regression. Hardly needs to be told. No. Based on the situation, that's a no-freelance theorem. Which of the following are typical uses of a decision tree, both as classifiers and regressors, of course? Bagging is the process of packing decision tree estimators in a small collection of bags banks this is useful for random forest actually this is a good giggle little eggs I have question regarding 13 question number 13 you're asking for one or the another like typical the word you typically use so are are we using 50-50 for regression and classifier see what happens is people who are used to yeah yeah it's quite often used the trouble is that the world is filled with mostly classification problems and fewer patient problems but when it comes to regression the proportion of times you can use decision tree is quite high they use it just as they use it for classification. Decision tree has one advantage. Remember, it is a nonlinear and it is somewhat interpretable. You can draw out the decision tree. So amongst the interpretable models, this is another few models that is nonlinear, that can capture nonlinear decision boundaries, which is why it is liked a lot by people who tend to use it a lot yeah I would think you know if we if we are doing regression wouldn't why not you just you know linear model or something like that so yeah we'll not capture nonlinearities you'll have to go polynomial got it yeah so what does Janine the decision tree refer to? It refers to of course this great mathematician and it's a measure of impurity of the tree is summation over impurities over obviously this with my sorry it doesn't refer to the mathematician it refers to impurity and Gini the word comes is eponymous with the great italian statistician and sociologist in a decision tree what can measure a relative importance is certainly true the prediction of each of the weak learners should be correct at least half the time for example in a binary classifier that is certainly true in fact that is this theorem that called dorset theorem so guys what i'll do is because we have limited time for theory, one session a week, I'm not able to cover all the nuances. So some of the nuances like for example this, I will cover through the quizzes. I'll introduce these things through the quizzes. Let us say that there are a hundred features in a data site p. While building a random forest classifier you have to decide how many features should each tree in the forest consider for deciding on the split. The answer to that is in general the best classifiers will be created if each of the tree considers all the features while splitting. Now it's false. That will lead to possibly trees getting correlated. You want them to be de-correlated and you don't want that. So for that take no, generally don't take more than the square root of p. I think that the site had learned for the longest time, the default value was all the features, which isn't nice. So you should, this is one of those things where you shouldn't blindly use a library. You should remember to the minimum, know the theory and set the parameters carefully. Decision tree is inherently more powerful algorithm than linear and point-angle regression once again no certainly not true we talked about it if your decision boundary happens to be or your regression plane happens to be a hyperplane that is slanted decision tree will not not in general outperform the simple linear regression for example bagging leads to the creation of a strong learner from an ensemble of weak learners true in fact that is literally the condorset up here and then random forest always outperforms logistic regression quite a few of you got this wrong no it does it does not. It does not. Once again, based on the situation. So guys, that is it. That was the quiz today. I hope you guys are benefiting from the quiz and not finding it too theoretical. Makes our brains pay attention. Good, very nice. Any questions guys before I end today's session? Hãy subscribe cho kênh La La School Để không bỏ lỡ những video hấp dẫn