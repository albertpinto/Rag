 So, first of all, good morning everyone. Some people look sleepy. I noticed Dennis is still waking up and some people look bright and eager. I hope the sleepiness is from doing this project till late at night. It's a beautiful fall season, September end, Saturday morning. Nice, for the first time, we again have a fog and cloud, mist on the mission peak. I don't know if you guys noticed, it's looking absolutely beautiful, all of this mist. So, and it's a rare treat at this time in California. So once again, thank you for being here on such a beautiful day instead of being outdoors this is day four of our bootcamp we have been spending time building the foundations so far what we built is um ai delivery platform all of you have created an AI architecture that is a close approximation to what you can take to production. You have a data ingestion pipeline, an AI data ingestion pipeline. Most of you have, most teams have figured out that given a directory of files, how would you parse the text, parse the files in the directory, extract text. And remember the two axes that you need to be aware of is the text can be in one of the many languages. Try to support at least 10 languages. Make sure that you have support for English, Chinese, Hindi, and Hebrew, Arabic. That should cover, and obviously once you have English coverage, European coverage comes pretty easily after that as an extension. Spanish, yes, by all the European languages, they come easy after that. And Thai and so forth. So do give coverage to those languages. The other aspect is give coverage to at least a dozen file formats. You should have support for PostScript files and PDF files and Word doc and all sorts of HTML doc and XHTML docs and so on and so forth. Documents come in all sorts of file formats, markdowns, whatnot. are trying to have support for all of those things. So how many of you have support for multi-document format? I would imagine everyone. Which team does not yet have support for multiple formats, just has one format support? Okay, this is awesome. Everybody has multi-format support. How many of you do not have multilingual support? Okay, yeah. Yeah. So I'll spend a minute about multilingual support guys. See what happens is when you do multilingual support and you use AI with transformer models, you will observe something interesting. Your performance for English goes down. Has anyone noticed that the quality of results, but just English corpus is not as good as using a language specific English specific trained model. So what you can do and perhaps should do. I don't suggest it for this project, but in production, a good be that pick a few dominant languages in whichever geography you're located let's say that you're located in korea then pick korean pick english as two dominant languages whenever you encounter content in those languages maintain a separate index a separate vector embedding for that those languages and then for all others use a separate vector embedding for that, those languages, and then for all others use a multilingual embedding. When you take an approach like this, you will actually have the best of both the worlds. You'll have multilingual support. At the same time, you'll have a pretty good accuracy in the dominant languages, a pretty good performance in the dominant languages a pretty good performance in the dominant languages just a brief interruption um we don't have kate over here to take the meeting notes like actions that we need to do can we designate someone or people collectively like take these action items that what you're suggesting and we keep a running thing at the end of the course we kind of at the end of this lecture we say what all stuff you have asked us to kind of future looking things because these things get lost afterwards so we don't uh it's very hard my memory is very short so that's why i'm so i need a volunteer guys okay patrick patrick has volunteered so two items guys make sure you're multilingual and make sure that you are multimodal. And these are, see guys, these are production tricks. You don't want to have wide coverage, but lose performance in a dominant language because people will perceive that your site is not as good or your application is not as good. That is one aspect to it. The other thing that I noticed is, one good thing I noticed is that most of the teams have moved over to Rayserve. How many teams, a couple of teams are still on FastAPI. One team prefers Flask. So be on the journey gradually to do a race of or any AI serve. Over time, what you will notice is that I will be changing the infrastructure game a little bit. Today I mentioned that and maybe I'll take some time to walk through the code. Last time, I think I gave a session walking through the inference code. This time I'll walk you through the five spark code if you're interested. But as we go, as we make progress with this workshop, you will notice that I'll ask you architectural questions. And those questions, as you answer them, you'll realize that whatever we have done is a milestone in the journey. It is not the perfect architecture. And we'll see the limitations of what we have built, and therefore we'll introduce one more technology. So today, I'll ask this question that I've been asking in different rooms. Today I'll ask this question that I've been asking in different ways. See if you use a model server, we realize that architecturally the biggest hardware constraint is the bank of GPUs you have, the GPU-enabled nodes. Because they are finite and very terribly expensive. Right, a typical, for example, an H100 is 30 to $40,000. In 30 to $40,000, you can buy more than 100 processors. So these are terribly expensive. Their price ratios are one is 200. now the question comes if you're sending traffic to that we all realize that you can overwhelm the gpo bank right you if you send a lot of traffic to it so how do you protect it that's when you use an ai server dedicated to it it will protect protect those GPUs. You can set the max concurrency, max traffic, etc. You can still get a distributed compute over a distributed serve model serving over the many nodes that you have GPUs in. It will balance the traffic across them. them nonetheless it will throttle it down and make sure that they don't get overloaded if you configure it properly right because the most common mistake that you have is or the what is the most annoying bug you see when you run with your computer notebooks and your ai any ai model it ran one moment and the next moment you have a CUDA error, CUDA out of memory error and so forth. So this is what that would protect you from if you configure it well. However, by protecting it, it is not solving the problem that you are getting a lot of traffic. So then what happens, they have some basic primitive queuing built in. That will develop a back pressure. Right? Requests will start queuing and you'll develop a back pressure. So how do you, how do we handle the back pressure? Anyone? How did we handle the back pressure? Using a queue. You realize that if that lay server goes down, the all the jobs waiting in the queue will die. Right. And obviously, a back pressure these and these things were not meant to handle that deep level of back pressure. So you need two things you need a queue. Now many of you implemented for example the optimus prime transformer has done a very good job implementing the whole of the Kafka queue. And I believe the Oppenheimers have also done that. Kafka queue? Not yet. Oh, yeah, we have used the Nats. RabbitMQ. RabbitMQ. Yes, so different teams have implemented different queues to handle the back pressure. When you do that, to manage the queue on the client side, often you use libraries, excellent libraries like, for example, Celery, that helps you integrate with the queue very easily. queue very easily. But on the back side, what you have to do is you have to write workers, consumers of the queue, whose job is to take items from the queue and then post it to your influencer. And that is what all of you have done. So now I will ask you two questions, architecturally. This is the way to sort of grow into the right methodologies. way to sort of grow into this the right methodologies isn't that dumb like every single like what you have done there is nothing specific to the search project it could have been any ai project that's taking web scale traffic isn't it or even enterprise traffic so why is it that you're having to do something every single guy is having to do that? Shouldn't there be infrastructure pieces that does that integration between Q and the model serve frameworks? And the answer to that is yes. You notice the model subframeworks, for example, Ray, it now has, and this was as Parveen was pointing out, it has excellent integration now with Kafka directly and messaging queues directly. So this in-between connector, you know, pulling from here and giving it to this guy, the model sub, and then keep doing it. You don't need to do it. It gives you a support for that are we together so so now in as you move forward we are going to go deeper and deeper into ai so you won't change your infrastructure too much your architecture is late our movement will move go from ai engineering to ai research now in the rest of the boot camp. But as we move from AI engineering to AI research, AI engineering will not go away. There will still be AI MLOps activities, in particular LLMOps activities. One of the activities that I'm now suggesting is go and bring more of the infrastructure or whatever you want to use, any framework, but I see if it helps you. Number one. The second thing, go ahead. Oh, what is the password. no no yes once again guys i've tried just about everything maybe i need to bring in some world-class experience for now we should just consolidate in terms of the interactions with you right let's do that let's do something okay let's go here you updated the firmware on that mesh network you have maybe there is and let's test on the wipes here it is we are in the room about 30 people there are 108 devices connected right we have 18.99 GB of traffic already happening. Active devices. If I look at the table, network traffic table, something fishy is going on. See if any of the networking gurus can identify. Oh, all the four hot spots are, see, there are four access points. Each of them should take 200 users. They seem to be in good state. I'm not able to tell where the screw-up is. Shana was having a problem, I can see, in the MVP. So, guys, one thing you could could do if you're in the room, maybe you don't need to, all of you need to be on the Zoom. We are talking, so it may not be necessary. And one thing I should have done is just projected my screen here. You should just reduce the remote people. No, Asif yeah you know what just switch off the other uh three four uh access points and just keep one just one and the main one in your building I think that's good enough these things are just hitting into each other so to coordinate with each other not hit with each other but I could try that i mean let's get a screen here everybody yourself that's right i will do that guys in the lunch break while you guys are having your lunch i'll go get another tv should we just gather in that room no no in the bedroom big screen okay guys bear with me at this moment I'm not casting anything. I'm not showing much on the screen. Or if I am just between like pair up one of you connect one disconnect sitting next to the laptop if you can please do that. The other option is go to YouTube, that is a little bit delay, so you're listening, but at least you're able to see what you're saying. Guys, would you like to do that? Like just go to YouTube and watch. It might be a Zoom issue in particular, because see, if you look at the signal strength, all the devices seem to be having a pretty, you look at this table, right? What does it say? Everybody has a good to excellent connection, but some people have poor connection and it's going up and down. If you have your cell phones for some time at least guys we can reduce traffic by disabling your wi-fi on the cell phone for some time um all right so with that i apologize we'll soon figure out go ahead mr question on this mq and um like kafka and so forth so you are saying that we could do something like a date and like Kafka and so forth. So you're saying that we could do something like a date framework instead of- Which gives you a native integration with the queue. So instead of you writing the in-between code between Kafka and yeah. So you're the coolie picking things up from the Kafka and- You have to write the same logic. Dumping each other. Why do it? Every house is doing it. So obviously you should expect uh uh uh inference framework uh inference serve framework to have taken care of what everyone needs and it seems that they have they already have go ahead so why do you have ravers let me just stick with Kubernetes and can't can't Kubernetes solve your problems across things how what is very different from Kubernetes uh yeah Albert is asking a question how is Ray different from Kubernetes? Yeah, Albert is asking a question. How is Ray different from Kubernetes? And why do I need Ray when there is Kubernetes? So see, there are two approaches that people have taken. Some of the inference server frameworks, they leverage Kubernetes, Kubeflow, and I believe KSERV and a few others, they're all very good. They're saying Kubernetes is battle tested. It runs Google's infrastructure on planet scale. Every guy, every enterprise uses Kubernetes. In AI, MLOps, why not just liberate that as an extension so there are frameworks that do that their framework has taken a slightly different approach they're saying that hey what if we wrote the entire thing ai centric thinking from the ground up right and they have done a pretty good job and i'm going to talk about that so good news it's a framework. So if you look at the strength and weaknesses, as a very recent framework, you're thinking from the ground up, attuned to the hardware realities of today, not 10 years ago. So that is one thing. It's written in C++, which makes it very fast. The downside is it is still an evolving framework right it doesn't have the maturity of communities and so forth or things built out of communities so there is a i mean see the ai community is split which is why if you remember i'm not i'm whenever i'm saying ray i'm also saying or another framework that you use so use whatever you do but use an ai centric framework use cubeflow use case uh use ml flow use ray but in each of them you'll see that they're beginning to bring in these pieces of integration by the way also has now aq just like oh the the bridge the gap to communities okay there you go so everybody seems to be making bridges to everybody else. There's articles about Qflow with Rayserve. How we go together. I mean, this is all in active development. Yeah. Yes. So, Satyam points out that now there are articles saying how can you hybridize a race array with cubanities and make the two together into the same architecture and as he points out everybody is trying out all sorts of combinations so see guys at this moment there is no maturity there are no final answers they are not dominant like for example for containerization and our container orchestration Kubernetes emerged by far as the champions today 95 of the deployments are on communities in the cloud I don't think at this moment in AI we have that level of uh a dominant you know a layer. And it's a very experimental stage. So use what you can. A re-serve I gave, I suggested to you guys, because if this is the first model serve that you're encountering, it's a pretty easy one. How many of you felt it was, it was a very simple learning curve? Very easy learning curve. And was it worth learning? Like, was it better than using what you were using Prakash before? Yeah, it is. So it's good. So do that. Come again? You know, Celery is like it submits to a messaging queue and it has its own worker notes, but definitely better than Celery. Celery is a different purpose. Mainly for a web frontend messaging yeah so you can still use the salary on the submission part of it and waiting for the response but at the back end you can directly the consumer part of it you can directly use the rate and do that the other aspect i want to bring about is something that uh that i learned actually thanks to sachin i didn't know that uh he pointed out and then i researched and it's true so i'm going to learn it along with you that I learned actually, thanks to Sachin. I didn't know that. He pointed out and then I researched and it's true. So I'm going to learn it along with you guys. See, for the last 12 years, I myself, I have sworn by Spark, 11, 12 years now, right? I've always, yeah, actually 12 years now, I've always used Spark, actually 12 years now, I've always used Spark. From its very early days, Spark was a huge improvement over Hadoop MapReduce. Anybody remembers Hadoop MapReduce and writing code in that? It was literally like doing a root canal. And Spark came and life was much simpler. The same code that would run on your laptop would do just fine on the server on a big cluster and it has been the workhorse of the industry for a very long time but again it is a framework that came from uh amp lab of working and was rooted in the architectural considerations of 10 to a decade I think, decade ago, right? Optimizations that are a decade ago. But now I'm noticing new frameworks come up, and in particular, Ray data. Ray has come up with its own compute framework. Now you would have imagined that, well, it's a good beginning, but how well will it do? And I saw some articles, and as Sachin pointed out, it actually blows Spark out of the water, right? Especially for, and because it's very tightly integrated with AI workflow. Yeah, I forwarded. Oh, you forwarded that. Yes, that's right. It was Praveen who mentioned that. So that is amazing. So Praveen, you mentioned about the data, then maybe it was Sachin who told about theen, you mentioned about the data, then maybe it was Sachin who told about the messaging or you mentioned about the messaging? Yeah, I mentioned both of those. Okay, nice. Yes. So Asif, just for completeness for Albert's question also and what you're saying, I am personally building a three node Kubernetes cluster with two control planes and one the GPU node that sits back into it and deploy three on top of it. And so essentially what happens is that when I want to do a development, I'm just doing the CPU node. But when I want to go heavy GPU, I basically integrate that node and that will come up. And so eventually once that's ready, I will show it to the other people and share the learning. So it's very painful to get all of that up and running and using container D with Podman and Ubuntu. Yes. So just to get Albert's thing complete. So I've explained to Albert before also, and I shared a book also in our group on learning Ray ray so that's for the benefit of the other guys also if you're going with ray so that they have a good understanding of i second that by the way the the o'reilly's book learning ray is an excellent book it's just a little bit 257 pages or so short book so what use is sunday just read it it's available three Oh, it is available. Okay, I didn't know that. Yeah. It's like you're waiting for some say something has been asked like so for this class, maybe not for writer but wanted to better if we offer something like a day to this team and then everyone doesn't need to like go and oh you mean array cluster yes you have you have 10 machines there right so one of the things that you will have to do i'm not doing it this way at this moment each of you got your machine some of you who are not used to working with data centers got a machine in your room right uh those of you who have teams that are more savvy who know how to connect using a console to a data you got a much more powerful machine back there. The Dell servers. Now here's the deal. As we make progress, you will have to make a cluster out of all of them. But I won't offer it and leave it to you to build it. I'll give you the hardware and all of you have to make a cluster out of it. But it is coming just right so um so just to add to the experiment i don't know who was talking about such it so uh the data point is instead of having multiple control games kubernetes natively supports something called node pools so you can have a single kubernetes cluster you can say these are my nodes for Spark, these are my nodes for Ray, and these are my nodes for inference. And it just helps you manage that. So it reduces the work significantly. supports the notion of uh pools node pools and instead of control plane you may also consider using a node pool so you can have a node pools for the gpu enabled uh and gray stuff and you could have a node pool for the usual microservices so the i'm aware of that but the use case i'm doing personally is basically getting my laptop I should be able to demo somebody independently and then when I come home I need to have all these three guys come up back again and failures so independently I should be able to run the control plane on each one of them and sync up that's okay looking at failure scenarios but that's a very good point that he has mentioned so guys you guys I don't know which team i believe homie baba team has sachin you guys are really lucky because he runs a whole entire cluster in his garage and you have access now it seems you guys will have access to it yeah so question on kubernetes uh what will be the file system he has chosen up to you no no as a shared file system uh sachin do.: To you know as a shared file system. Sathyan Kulkarni, Ph.D.: I said, do you have a preference for a shared file system. Sathyan Kulkarni, Ph.D.: For the etc do which one are you seeing. Sathyan Kulkarni, Ph.D.: No so. Sathyan Kulkarni, Ph.D.: let's say you have multiple nodes on your. Sathyan K a file system like you know a distributed file which is the distributed or shared file system i'm not doing that so i can do zfs those things i can so right now it is more of a proof of concept for myself to get how strong this ray is in terms of failures and all that's essentially what i'm trying to do i'm right now almost at the point of getting the kubernetes cluster up getting podman and things like that I'll come back to you with that question when I set it up so okay no no sir I'm just going to go through searching Apple and Amazon applications, OpenAI has claimed the largest model on Ray and Apple and the internet is important also with Ray. They all raise, yeah. So what she, Bosni made a comment for those of you on Zoom, that she found out that OpenAI is rather big on ray and so is hugging face and so so it's coherent so ray seems to be the the at the moment leading the path right the good thing is open ai was on kubernetes okay interesting so now they're moving today she's this variance is the winning horse at this moment let's see who the winning horse is next week. Ray on the Humanities. It's more of the orchestration on top of it. Okay. Ray is just a Sahil. Okay, guys. So with that there, I will uh I'll do one thing and for me every boot camp every workshop is I I often feel that I learn as much as I learn from as I have to share so I'm going to do a ray data work implementation of the same thing and release it in the solution I want to give some time so today I mean obviously we have been speaking for some time what I would like to do is walk you guys to the solution. How many of you have looked at the solution? How many of you feel, no, don't show me the solution yet? Well, now it's a bit late, I have to show the solution. Are you sharing with me? Yes, I am actually. So one easy way is guys, instead of Zoom, just go to YouTube. Support Vectors Streaming, YouTube. Are you all able to go to YouTube? YouTube.com slash support vectors. reading up the press paper where they were saying is that if you batch all the queries then you can do the in the sense that you are amortizing the cost all over them here in the kafka case what you're doing is you're doing one query inference at a time correct no you shouldn't do that in fact it's one of the things that i wanted to point out, all of these have support for grab, see any messaging queue system. Most of the time you take one message at a time, but you don't have to, you can do batch retrievals. But then you can get up to 12 messages, up to 16 messages, whatever your batch size is, 8, 16, whatever, you get up to 16. If the queue is empty, or there's only one message, you'll get one message. Otherwise 16 if the queue is empty or there's only one message you'll get one message otherwise if the queue is full you'll get up to let's say 16 message batch okay guys so i'm going to do something can we all go to the course portal and in vector databases project vector databases uh section you will see a solution here sample solution are we seeing that guys yeah if you do that it is what going and well I'll try to download it and I hope something comes down yes something got downloaded and I will do it along with you guys let's see yeah here we go I know not video surely I'll go there now downloads and i'll just unzip it guys and sorry as if i missed that what are we downloading uh we are downloading the solution okay the llm if you go to the website, the, okay let me show it again. On the course portal there's a sample solution to the project. Please see if you can. Actually posted on the channel. Right, no but that one is a bit old. Okay. It's easy. Today. Please download and unzip it and when you do that you should see something like this you should see this you can you can include it in your vs code pycharm whatever it is that you do and then but i'll just walk through the documentation that I generated out of it. So you will see that there is a doc folder here. And in the doc folder, if you just go there and open, I hope there's an index.html, there's an index.html. So you will see, this is read the docs. How many of you, how many teams did read the docs? Only one team. The rest of you haven't done. So guys, if you do a read the doc, it just is a common way to document whatever you have. Good thing with read the doc is you can do not only that, you can even put, like for example, the Jupyter notebooks that I gave you, they are all in the read the doc, elastic search setup, Python ES setup, sample Python code. You notice that you can add your Jupyter notebooks also here and you can do, but I'll take you guys through the modules. But i'll take you guys through the modules. It is in the docs for them. Docs index dot index dot html. So guys, these are the modules, and if you look at the modules, you can. A good thing is that you can go and let's say that you want to look into this module yeah and this is this is obviously the comments if you have commented your code all your comments would be extracted your pi docs would be extracted go ahead i'm asking um what i did is i went to this you know when you unzip this it becomes a file and it becomes a directory llm bootcamp lab one solutions and there you find a directory called docs do you see a directory in there called docs guys how many of you are seeing it uh mostly you put it no you have to download the latest one so if you're sitting on an older version uh don't use that i just simplified it by making it into the root directory. Once you're there, then you can see this. And so, yeah, I keep releasing, I released an updated version. So go to the website and get the latest one. When did you release us today morning yeah so guys those of you are sitting on old now the good thing with this guys the reason i encourage read the doc is if you want to see how have i implemented any one thing let's say that how have i implemented the that how have I implemented the config, right? You can directly click on this and you can go to the source code. Do you notice that the source code is right here? And you can. So it gives you the ability to have a pretty good dashboard on your project and a source code. Those of you who are familiar with Java docs right and extract of source code. It would be familiar with that. Doxygen. Anybody here with a Doxygen has used Doxygen? Yes, there you go. Can you show us one support factors where is the link where is the link oh here we go so first of all you go to my courses when you are in my courses you go to the llm projects bootcamp and then you are seeing your course okay now once you're in the project uh are you in the in the course portal yeah yeah then you scroll down you go to the vector database because and this is the solution for the vector database right here you'll see project specification doc followed by sample solution that's the one so you download it yeah if you click on it it will download it's a small file 35 mb yeah and then go to and once you have unzipped it go into the doc yeah and then you go index.html. Are you there? Good. Anyone else, guys? Anyone needs help? Yeah, okay. So that is that. And I would strongly encourage you because not many of you have done the read the doc. Please do it, guys. It's basic hygiene. Nobody will take your project seriously unless you have a read the doc. In open source world people will just say where's the read the doc. So let's have that and again it is missing some packages as I can see. The text package is missing and I myself have to do some cleanup but uh current state is this and again guys be in the habit of putting documentation like comments in your code etc all right so now i'll come to today's project can we talk about today's projects guys right um it is to do with something called RAC. And for this on the course portal, I'll put some resources. I was going to do it today, but I was worried about networking and all sorts of other issues. So I didn't get time to do it. Oh, network is still doing well well but some people are having trouble okay you're not able to tell me so yeah wi-fi is unstable i apologize guys youtube is okay yeah just gonna keep it so that's not yeah so guys um there is something called retrieval augmented generation. It's a big thing. It came out of, this is the research paper. I'm going to give you the main idea of the research paper. So guys, here is a reading right after lunch. I want each of the team to sit and study this paper as a group can you please do that it's important right i'll explain to you the main idea and when we do that you'll realize that the paper is very easy to understand after the explanation and this is a paper thank you yes certainly uh Yes, certainly. Let me do the links. Give me a moment guys, Facebook's own website and things like that that I explained. Are we familiar with that? So I'll give you an example of why it is useful or what it is. See guys, if I were to ask a question and let's just okay i'll let you guys finish your discussions and we'll start uh some of you do your setup like whatever setup here how about do we do this we take a five minutes break and we start yes right because i noticed some of you are still downloading and setting things up uh finishing then we'll start so i said your wi-fi speed is really slow seven point eight we promise week this Wi-Fi is not going to be problem. Monday we fix it. 100%. Even if I have to pay out of my pocket. 100%. So by the way, if Wi-Fi has a problem, blame it on Sukhpal. He's the one who told me not to buy new routers yeah but we have a pretty strong gateway i was confident that it would all work so we'll see all right guys five minutes break and i'm going to pause the is there a way to pause the streaming i don't believe we can pause the streaming i'll let the streaming happen but take a quick break five minutes no more than five minutes guys so uh i said what what are we discussing when we come back what's in the agenda we are going to do the topic of that uh money you said you will touch on pi spark would that happen today not walk through the code but maybe architecturally like explain some of the aspects of let me make an optional session in the afternoon because a lot of people here have worked for spark with many years i'll make it an optional session those of you who want to learn can join me because part is uh work learning properly correct it explains yes yes that will be helpful because i i don't yeah some of the questions i have is how does it work internally like how many parallel connections does it spawn and uh should like considering data frame should how do we how do we work with it like things like partition and those kind of things yeah i'll talk about all that okay sure sure thank you thank you before you understand before you understand pie spark once you understand spark pie spark is nothing but a shim on the spark layer yes yes i meant spark yeah which is not very obvious from the get-go. It's just like, everything is done by Spark. I think this is PySpark as well. But once you understand Spark, say 90% you understand Spark, PySpark is 10%. Correct, correct, correct. Agree, agree, agree. I tried to read a lot but yeah because after the water material is there in the internet it says how to use it but I am not I am not like really concerned on how to use it. What are the details about it that will help us optimize, use it in an optimal way? I shouldn't be using it in, like for example, last week when I talked to you, you said, you should not be calling on each row of the data frame. I need to have an understanding of how what kind of optimization price takes part does and what kind of optimization it leaves to the user to be done thank you um i had to like Thank you. Hey, Asif? Asif. Talking to somebody. Asif. Asif. Asif. Asif. you you you Thank you. you you you you Thank you. Hey, Asif? Asif? Asif. Asif. Asif. Did we lose him? No, he is muted. Somebody can ask him to unmute. Okay. Hey, Asif. Asif. Asif. go ahead sachin to have i can't hear you is it i said that is another breakout session I would like to have that is related to basically usage of Podman, the container D and the ecosystem and those kinds of things, what people are using. I say that in a breakout session on one topic. Podman. Podman. Podman. Okay. Container D and all so what i did i want most of it working on ubuntu but these people are doing rail and source they are that but none not on any of the other operating systems yeah it was very painful to get it up but it's working i wanted to see if there are other people who have used it actively and what is the experience with it going forward is that what it is going to happen the uh demonless uh docker essentially right that's that's what it is no that is great actually would you like to give that session who me yeah no i want people to give me input that's what i'm doing because i spent like almost five six hours getting it working on ubuntu so it's ordinary organization people will jump i want people who have been using it in production they're what do you call uh feedback and things like that because it's a evolving ecosystem the two and everything are different so that's why i said yes yes yeah because if i am praveen those guys know the thing pretty well saying that because i feel that the world is coming towards a point where we are going uh demonless the container d that is picking up quite a lot of steam compared to what right close so uh sources kind of in some sense so compared to docker exactly yeah yeah so that's essentially what it is happening yet and it is really fast what it is happening yet and it is really fast nice no i no first of all i'm a big no doubt right so where i'm right now is to get the gpu access from the container d that's where essentially where i'm fighting right now once i get that going then i'll be able to because you have to do the nvidia smi all of that runtime all of that things you have to do so i'm very close to getting that with the roof all right guys uh let's let's gather rose in the break room please come back All right. So I would like to now introduce the topic of retrieval augmented generation, RAC it is called. Now it is one of the dominant methods these days being employed. Let me set the context. You know, this happens quite often, especially with parents, with dads. But dads are supposed to know the answer to every question. So your two year old comes and asks you, how many countries are there in the world? And you, you of course, very confidently said there are 164 countries. Well, that was what you probably knew from your book, way back in the 1980s, 90s, right? And as you know, the countries, the number of countries keeps changing, countries keep merging and splitting. And I could be wrong. Sometimes dads are known to make things up too, isn't it? In answering the question, and how would the two year old know? How many of you have had that experience? Nobody is facing up to it, okay. Oh yeah, how many states are there in India? India, that's the real question. When I was at school, it used to be 22 or 21, I forget. Now it is probably close to 30. It reduced also. New states became union territories. Union territories and things like that. Or a basic question. Let's say that you ask a question. How many planets are there? Well, based on the current state of affairs, and it keeps changing, Pluto is demoted out of planet status to planetoid status and then becomes a planet again when people protest. And then there is even a suspicion of a planet X, a 10th planet. It's a black hole. It's a black hole. All right. So, or how many moons are there in Jupiter? And it's a question that is very hard to answer because every time you think you know the answer, there's some more moons. Right. So, I believe the count used to be around 50-60 but now it's close to 160 or plus. Right. Moons have been found. And the whole question is what is a moon? Like at what size of a speck of dust does the speck of dust have to be before you call it a moon? Like how big does it have to be? One kilometer wide, two kilometers wide, half a kilometer wide before it's called the moon. So therefore imagine these large language models they have been trained they are like us they have been educated but our education is dated isn't it and like most most people who uh promptly take a shower after graduating from college and uh sell their books at the college bookstore, the relationship with learning ends for the large language model. A trained large language model will always refer back to what it has learned to answer your question. But it doesn't work for realities which are fluid, which have evolved since the last learning, isn't it? And so the question remains, isn't it? And so the question remains, what will it do? How do you solve that problem? The other problem that comes is, how do you prevent a large language model from hallucinating? It's somewhat like a confident dad. No matter what a two-year-old asks, there's always an answer, right? And even when you tell it, dad, do you really know? You say, of course I know, Right? And even when you tell it, do you really know? You say, of course I know. Right? And that's how the LLMs behave. I can see a lot of guilty smiles. So well, now, how do you prevent that? So one of the paper, one of the breakthroughs that came out and was quite effective from Facebook research and the University College London in New York collaboration is this idea called retrieval augmented generation. It is, I would classify it in the context as one of the in context learning sort of prompt engineering and so forth. So see, there are ways of doing prompts. So just to use the clarify the vocabulary, do we all know what a prompt is? Whenever a generated model can produce something, generate, generally in a model that produces some input goes in, some output comes out. We use the word input output but when you are looking at generative models people tend to change the vocabulary and use the word prompt for the input and what comes out that is often called a continuation so a prompt goes in and a continuation comes out the output that's sort of the way of looking at it now prompts have many parts to them one is very simple if you go to i don't know chat gpt or whatever and you say who is the author of tale of two cities it will say charles dickens but i invite you all to go try this experiment right now. You all have access to chat. Ask it, who is the author of The Snail of Two Cities? And see what comes up. Not aware of any book. Now it says it's not aware of any book. I guess I must have asked that question too many times. It has wisened up. But it used to always come up with a random name. It used to cook it up. Isn't it like I asked Claude the same? He said, unfortunately, i do not have enough context even in the world of scale of two cities but there is a book called tale of the skill cities okay it's it's getting smarter i guess right i think though because they what they do is openly they look at those prompts that have been like wrongly answered and then they plug in they plug in things yeah there's a lot of improvement continually improving data like in the last week somebody did they checked for the reverse statement so let's say charles nippens is the author of the electricity but if you asked him like the reverse the reverse and it's not evident it doesn't automatically Patrick, thank you for pointing that out. I was going to just mention that. The asymmetry problem there is, what it has is you can say the same thing in active voice and ask the question in the reverse order. Usually LLMs do a pretty poor job still. The state of the art as of today is they still do a poor job of it. And so there are a few limitations, well-known limitations of that. So one of my favorite jokes is now gone. I used to always, if you ever get bored, ask it, who wrote The Snail of Two Cities and some fictitious name would come up then i would search for that person online so oh you forgot how insulting i am the author of this. You should know that. Okay. Who's the author of the Lord of the Wings? It kind of gives you a random answer. Yeah, there you go. A slight variation on the Lord of Rings and the Lord of the Wings. And there is an author for that. It turns out. Yeah. So it will hallucinate and you won't know so guys when you this is all right fun and jokes and we all know the limitations but how can you use such things in a legal domain or in a medical domain or in a domain where authenticity or correctness is important so there are two ways of doing it uh like like kid would ask, and actually my kid at some point used to ask, Papu, have you checked the latest information on this? They used to ask me, my kids used to ask me. And then that would be a clue that they think I'm wrong. And I would go back and learn the latest and sure enough I would be wrong. So based on that idea comes Rag. What Rag is, and it's a very good continuation of your project. See the reason we started this project of hybrid search, hybrid search is good in itself, but its main value is that it is, it augments or complements a large language model in an amazing way. So from now onwards, we will be dealing mostly with LLMs, and today is your first introduction to LLMs. So here is what we do. What this paper says is that, see, when you write the prompt, we know today that writing the prompt is much better if you give clear instructions. So if you say, finish this sentence. It was the best of. And then leave it blank or something like that, or fill in the blank, you tell it what to do. Otherwise, it will just sort of come up with something that you want to hear. It won't be as precise. So giving instructions itself. Now what you could do, suppose you could, you had a question and you knew the answer could be there in a corpus, in a body of documents. It could be there. By the way, when I use documents, I use it in a general way. Today, I use it that the answer could be there in a photograph. Right? The answer could be there in a video. Right? Because I trust all of you have finished last week's project images and sound and so forth so anyway the answer could be there in some body of knowledge so now that you have the ability to search in it what can you do you can take the question you can do a vector embedding of the question and you could do that again do the same thing you could do your whole hybrid search into elastic search, etc. What would you come back? A search results. Those results are body of knowledge that is relevant to answering this question. a question based on what you come up with by executing this search on my system use that incorporate that in answering this question it will be relevant because see large language model may have been trained two years ago or one year ago much may have happened but your corpus may be more recent you realize that right and that is why it is called in the in this paper they use the word uh non-parametric right uh memory for language in other words it's outside your system it's not it's not embedded in the weights and biases of the large language model of the neural network itself right it is outside it it's a it's a adjacent a complementary body of knowledge that you query into. Now that body of knowledge needs not just be, and when I use the word search, guys, I am using it in a truly generic sense. What other complementary body of knowledge can you do? You could have a proper knowledge graph, an ontology, right? For example, if you're in a medical domain or form a domain in some specific domains ontologies make a lot of sense so you could save your question through an ontology now what it is essentially saying is that there are techniques that have matured over the years for question answering isn't it so what you do is you pre-process your question by getting some of the answers from your prior techniques. Bring those answers or those results, body of knowledge, forward. And then you say to the LLM, go use all of this in answering my question. So what will it do? It will use its own, what in the paper they call the parametric learning in other words whatever it has learned from the vast web of data that it was trained on it will take also and give emphasis to what it is getting right now as in context learning right and put all of this together and answer your question so some things it may not know for example the number of states of india may have changed right or number of countries may have changed isn't it uh so it will look up the latest body and now you can hook up you realize that it opens up interesting possibilities it opens up the possibility that you can hook up wikipedia right there right and therefore that is your project guys. I expect now you guys have all been none of you have really set up a good corpus, but RAG at least I want you guys to complement it. One of the things that you would search and integrate into it is the Wikipedia. right please incorporate the wikipedia in your rag right and i'll try to also incorporate as another source of information a new source right so there are a lot of news things they give you an api right or you could integrate twitter or whatever it is integrate a couple of sources into it no no chat gpt knows wikipedia as of the last so x date when it was trained now the only way and by the way chat gpt has started doing the same bad thing what it does is before it answers your question it actually does a search in wikipedia in the latest google search and gets uh information and then answers your question and it's now it's not behind two years it's up to them what are they calling it gpt4 or gpt4.5 okay yeah so we are in an evolving landscape guys anything i say you know you you you realize that 10 minutes later it's out of date right at that time i'm giving you out of date information it's how far is the world is evolving we do need to focus on the ontology and the relationship stuff yeah for this specific exercise if you do it would be great because most people don't know ontologies and knowledge gaps so i i make that part optional but definitely integrate in your model and which llm would you, guys? How many of you are planning to use OpenAI LLM? No. Do not. Do not use that. You have to use an open-source LLM. Because like you guys just pointed out, if you use OpenAI, they are just up to date. How would you know whether your information was actually used isn't it and don't do that see you will realize that you get essentially the power you come within shooting distance of the power of open ai we're just using an open source model in your own corpus right and that is your paper that's the main point so i'll come to it guys give me a few minutes i would like to read the intro the abstract and just give you the main ideas of the paper so are you guys able to see my screen let me start i think the new model is mr ms israel is this french yeah yeah it's outperform the llama yes it's outperforming the llama models doesn't mr mean the agent the guarding angel So, guys, what I'm going to do now is quickly walk through this paper. So let's hold our comments for a little bit later. Did I walk through this paper? Now, I'm going to go through the paper. So, I'm going to go through the paper. So, I'm going to go through the paper. So, I'm going to go through the paper. So, I'm going to do now is quickly walk through this paper. So let's hold our comments for a little bit later, till I walk through this paper. Now, I just read the abstract of this paper. Are you all seeing my screen on YouTube? pre-trained language models have been shown to store factual knowledge in their parameters and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. By now we know that, right? You take any of these large language models, it has some form of, some ability to have a statistical understanding of whatever data it was fed, says that it looks, it is equivalent, it seems as though it has memorized. It can reproduce facts. And it is all there in its parameters, weights and biases. However, their ability to access and precisely manipulate knowledge is still limited. And hence, on knowledge-intensive their performance lacks behind task specific architectures. So, for example, if you look at the news, obviously that would be a disaster. Because news is a fast evolving knowledge base. It's limited. Additionally, providing provenance for their decisions and updating their world knowledge remains open research problem. What does that statement mean? It's a polite way of saying additionally, the dad can just make things up, right, as dads do. So pre-trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. That seems heavy language. What does this word mean guys? What is an explicit non-parametric memory? For example, vector embedding. It's not in the model. It's not in the model. It's not. So we know that you can you have pre-trained models. Those models have been trained on, like, for example, contrastive loss. Right. So they have been through a differential, a differentiable. And then they have a mechanism that you can use to access it we explore a general purpose fine tuning recipe for retrieval augmented generation and this is the term that literally starts from here the first time this term is used is in this abstract rag right models which combine pre-trained parametric and non-parametric memory for language generation. Simply put, it uses the model to say something, the knowledge what is encoded in the model, and external, an external agent support for it, right. Non-parametric would be, for example, your entire hybrid search engine that you created is a non-parametric Raja Ayyanar?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam which is a LLM, which today you just say LLM. And see, remember this paper initially came out in 2020, v4 has come out in 2021. So at that time, people weren't using the word LLM. It wasn't that common, that word was yet to come. People were just using the word sequence to sequence model and the nonparametric memory is a dense vector index of Wikipedia. What's a dense vector index that you guys created? The FAS, the FAS index of your document, ANN, right? Vector index. Right, accesses with a pre-trained neural retriever. You guys have a retriever, of course. Now here, think about one thing, guys. Things have moved forward. Today, in your project project you're doing better how are we doing better we are not only using a dense encoding but indexed but we are also using a sparse index which sparse index are you using elastic exactly which internally uses uh many things but in particular bm25, sparse encoding. It's basically inverted, roughly speaking, conceptually. It is an inverted keyword index with these encodings like BM25, et cetera, techniques applied to it for data retrieval. We compared two RAG formulations, one which conditions on the same retrieved passage across the whole generated sequence, another which can use different passages per token. We fine tune and evaluate our models on a wide variety of knowledge intensive tasks and set the state of the art on three open debate outperforming parameter models and task-specific retrieval and extraction architecture. In other words, if you hybridize, if you put together your hybrid search, or at this moment just dense search in those days, with LLM, you expect to amplify the strength. You get the benefit of both. Whatever you create will beat each of them individually. Does that make common sense? Today it would look like common sense, right? And of course it does happen. For language generation tasks, we find that, yeah, and you realize that this entire paper is just one idea, right? Let's put two great ideas together. Let's not do this or that, but do both. Right? And I would consider it as one of the early good users of proper prompt engineering. Patrick, would you agree? This is it, yeah. Two hands, okay. Mohsini, go first. . Oh, yeah. Albert. But I have a follow up. Okay. Yes, of course. Excellent thing. Can you do it with the database? Actually, one of the projects will be... See, we haven't done with tabular data we're still dealing with unstructured at some point you will one of your projects hint hint will force you to understand tabular data retrieve from a database and help use that to answer your questions and this is not happy at that idea idea why why is that it is difficult well there is a reason i call it the boot camp right so you will be doing it guys but we'll do it slowly i hope guys by now are you finding it extraordinarily hard or is it just right pace right pace and right difficulty give me some feedback how are you guys doing okay give me some feedback how do you feel is it getting too tough it's really it depends on where you are But it's amazing. I think it depends on where you are. Yeah, I can. Anyways, yeah, we learn different from different. I think that you just maximize that's right here. Go ahead. It's very good. We are getting a lot of loss. It's just that we don't have enough time. Yes. That's why it's a boot camp. Yeah, that's why it's a boot camp. You know, guys, I'll tell you one thing. If you look at the top US universities, and the university ranked 4,000, right, or whatever. Let's take, I don't know, well, I come from Urbana-Champaign, so I'll take that. And you take another university, I don't know, let's say South Dakota, something, I don't know, university which is ranked low, some university that's not ranked so high. They all teach the same body of knowledge. Why is it that the top universities tend to produce little bit like overachieving people, at least in the early years, right after graduation? The answer boils down to only one thing. Because of school age. Yeah. Partly the students, that's true. Partly the better raw material goes in and gets cooked there but there's another reason actually which is proven which is known to work over to make your silence start having a challenge they basically forget the fact that the day has only 24 hours they dump so much on you that if you're sleeping you feel guilty during graduate school and you can see that at three at night two at night or one at night you could go to one of the engineering libraries and you would not find a desk to sit on in any of the seven eight floors it was uh one of the first places that had gigabit network way back in the 90s reaching every single desk in the library right yeah when i was in high school yeah we used to fight for the seats in library that's it yeah you wake up early go put your books in the sequence that's right yeah so right there so i'll say remember um i'll say i will sit sitting hold on i'm getting a question from the audience. Discussions what we have. yeah. What happened to other presentations are expecting that to that you learned something from the presentations. Absolutely. The whole afternoon is presentations. Today. Today. Yes. Sachin, you were saying something finish the point. I was saying that I run a robotics club at home and there are kids who come over here and I see their peak performance at 12 o'clock, 1 o'clock in the night. And there are several times where they have stayed up till early morning so it's very good for these kids they're pretty high achiever kids also and they're very motivated when there is motivation they work all together so I think that's what what asif was saying I can't I'm like 200 agreeing with him so yeah guys there is a saying I learned in my childhood it's been there I suppose my life has been tough i've always been working very hard but voluntarily most of the time uh so there's a saying that says if the going is easy you're surely going downhill right so be aware of that so this boot camp is I want you guys to go through the hole and come out feeling that it was worth the experience. Everybody who comes out of top universities at the end says they'll be totally dead tired and then they'll say what a great experience. Also sir, Stanford, Berkeley, they have Nobel laureates that are teaching there. That also motivates me that does i mean let's see but those noble laureates are produced in the same crucible of hard work right so you realize that in the bay area we are sitting in a place which has more than 100 noble laureates around us just the bay area itself you know in berkeley there is a car system if you are, undergraduate student, and even if you pay for parking, you'll get parking way out there. If you're a faculty or lecture, you'll get nearer and nearer, normal staff nearer, an actual professor then nearer, and then really at the heart of the premium location, there is a parking lot only for noble laureates it says noble laureates only right so the problem is even the noble laureates complain because if there's an interesting seminar they say there is no parking spot left for them and it's a pretty big parking lot actually reasonably big so you can imagine what a concentration of intellectual horsepower there is out here in silicon valley right anyway so guys I hope you guys get to do great things but this picture says it all you get a query you see this query what are we doing a typical query would jump straight in a normal model into a llm isn't it whatever question you ask but you brought in this box you take this you are doing now this looks very obvious to you right you guys have brought in this box, you take this, you're doing now this looks very obvious to you, right? You guys have been doing given a query, you create a vector out of it. But we do something more, we also go into Elasticsearch, you know, the thing, then we do something which has a very complicated mouthful name, MIPS, maximum in a product search. Are you guys doing that? Of course. What do you think you're doing with your ANN and so forth? You're doing that. After that, you get a lot of documents. Is this what you're getting? You're already doing this part. What is the additional part you need to do in this? Feed it into an LLM. what you're getting you're already doing this part so what is the additional part you need to do in this feed it into an llm isn't it that's all it is feed it into an llm and have the llm answer and of course there is more happening in this paper what you can do is you can jointly learn like for example you can jointly learn across the encoder and the llm and so forth and fine tune it you can do some fine tuning across the board but we'll skip that part i will leave you guys with the idea that you don't need to do any of that i mean you this this particular project one second uh the this particular project just integrate anM. Let's do that part. We'll come to fine tuning, etc. later. And that is your project for the week. Very easy project. Trust me, if you have come this far, this part of the project should be very, very easy. It's just plain prompt engineering. The reason I'm keeping it this simple is I noticed that most of your teams are about one week behind right you haven't done the multi-modal multilingual learning right and search indexing uh go ahead shadow waste after the cross encoder the results that you get, use that to prompt a gene. And then have the results come out of the LLM be your answer. So now notice that you are in the question answering mode. Sorry, Junaid, go ahead. You have a question? Who has a question? Yeah, go ahead. uh you're thinking along the right line you're definitely thinking along there see guys now you realize that there is a chain of ai activities now isn't it and so whenever you have a chain you're looking at a language chain so you can use lang chain in the concept of agents the agents are much more effective when it has to decide how to build a chain dynamically. In this particular case, the chain is sort of baked in. It's a static chain, but still you can use it. I mean, go ahead and use that. Somebody who's asking any more questions. Albert. So I'm really confused. So is RAC a dynamic form? Doesn't matter how you get it through an llm through lang chain through your database yeah and then you've provided the dynamic prompt to an llm so that's what that's what it is so that's what it is yeah so how you get that information retrieval many ways you could get it from a knowledge graph you can get it from your hybrid search right or you could use whatever like if it if it means making a phone call to your friend on the fly that too is fine it's anything that qualifies as information retrieval sheena if i just use the search and ensure and which the prompt can i call it right yes it is right it is right literally stands for retrieval augmented generation answer generation. Can we use just the embeddings? Yes you could potentially but that gets to the I mean you can do that but I think at this moment you'll find it a bit hard to do. Guno Park Yes, go ahead. Yeah. And then that's your organization and feed that to an element. Feed that to an element. It can't do any transformer, right? Yeah, so long as it is open source, you're not allowed to use a cohere, anthropic, open AI, anything that comes as an API. You can't use an API. use a real model integrated into your. So this could be what you did in the. That's for pipeline. Yes, you could do this. You could do that. That is one possibility. One possibility. The only rule is, guys, you are building a product. Remember, this boot camp is building a real product not using somebody else's product are we together it's okay you can use the components yeah so it's just like you know you're building a computer you are allowed to order a gpu a cpu but you're not allowed to bring somebody's computer and say here it is right go ahead summarize now yeah to answer question. So suppose the question is, let's say that the question is, describe, right? Describe the state, tell me about the state of India that has shown the greatest economic development in the last five years substantiated with facts now your llm may cook up something potentially but whatever it does it may know the answer already but if you do information retrieval into for example the indian government publishes all these documents five-year plans and progress reports and whatnot you have done information retrieval from that you and use that so firstly what you're doing is you're searching for the answer that traditionally and whatnot. You have done information retrieval from that. You have used that. So firstly, what you're doing is you're searching for the answer that traditionally, imagine you're like Google searching this, you take all the answers that you got and then you say that here, here's the question, this body of knowledge may be relevant to answering this question and answer this question. So what may happen is the LLM may know things that is not there in this body of knowledge. And it may use that also in answering the question to come up with a final answer to the question. And that's the point. And not only that, the LLM will synthesize all of these things because you just got search results. See, if you think about it, Praveen, you mentioned that you have almost completely stopped using google why because google comes up and you're using uh chat gpt or some of these things most of the time why because chat gpt gives you a coherent answer whereas what what search engines give you is a listing of research results and by the way it's getting very annoying days, I don't even get search results. All I get is a list of advertisements. You have to practically skip the first page. Medium or medium articles. Yeah, right, or something. So I mean, look for anything and 20 advertisements come first. I didn't expect Google to degenerate to this level, but I suppose that's what they've done. You scroll down and somewhere way down there after many many things you finally come up with some sensible suggestions go ahead. So. I have done this. In terms of the models so can you do something like a link in agent and everything to. Do that, but don't use open Ai so if I see that you're using. So can we do something like a link chain agent and everything to do that? But don't use open ai. So if I see that you're using one here, you can use like Open source. Yeah, the only question is, see, use whatever you want. It's free for all so long as you build it and use open source tools. students as if sir i have one question yes yes yes so i'll come to that question emma money is uh has been waiting for some time money go ahead yeah hello assistant so you said uh the recommendation like the document index that is being fed into generate generator that is actually taken into the result how does the no you come back with documents you you you come back with actual documents remember this your application produces real documents or snippets of the documents you feed that in that's correct but let's say five minutes break five minutes break and then i'll take more questions i forgot to call up the viennese shop in orderly land let me do that five minutes you you you you you you you you you you you you you you you you you you uh let's have a finish our discussion and then we'll move forward ask you sir can i just continue the question just one question hello can you just need 10 minutes of your time please Please come back. Asif, can you hear us? I'm sorry, please go ahead. Yeah, Mani was asking a question. Okay. Just wait a minute, guys. I know that lots of you have questions. Just wait a minute. I'm waiting for everyone to come back guys please come back patrick anyone else there okay guys uh let's start with the money so as we are continuing with the question so let's say we we are querying about 2024 election but the model llm model is trained up to 2021 so it knows 2020 election now the document index may know something about the 2024 elections but you are feeding those information into a 2021 model how will that work no the language models are i mean the way this works is that when you give it new information, this in-context learning will always prioritize the facts you've given. Okay. Because you're telling it. So suppose you said that giraffes, giraffes are now 10, like 20 feet tall, right? And here is a table of giraffes and their heights. Who is the tallest giraffe? It will answer it okay got it so it will try i think okay but won't it get confused which is which correct answer versus which is which is it i mean it is trained to give a certain answers and then you are feeling no that is reinforcement learning reinforcement learning has taught it that it always prioritizes that which the user says over what it knows, what it would have said. So see the way it works is that it's not magic. Look at this part, they use the word conditioned on. If you look at the mathematics here, on. If you look at the mathematics here, they say the magic sent, I mean, so this is the reading the paper more carefully and I asked you guys to do that. See, given a query and a generator, which is a generator that would have generated a response in view of the query, some token in view of the query, what you're doing is you're saying let's not let's make it conditional on multiple things make it conditional on the documents that return and on the query so you're double conditioning it and when you train the model like that it inherently learns to give emphasis to the information in the query. Are we together? The instruction gets emphasized automatically. And anytime there's a contradiction, it tends to pick up the instruction. Got it. Thank you, Afif. So, before any more questions, are we all clear in this thing, guys? So, there is a mathematics there. Is that always the case? Yeah. To pick up the instruction? Yes. You can try that. Actually, try that experiment and see. And guys, it all comes down to look at this. Look at these equations and you'll get the hang of it. It is conditioned learning. It is conditioned on the thing that it does is is response that it produces now is conditioned on the information that it was given and it tends to emphasize that quite a bit uh okay aditya you're waiting oh sorry not it yeah so um uh help me understand clarify the flow so i mean in the first uh project for example first or the second one we have a corpus of data, we give it a phrase or keywords it comes back to the answers. Now for this one we pass on that same query to an LLM which has let's say based on its pre-training has its set of response, but does it also concern this corpus and somehow has a way to identify? It's not like that. You do it. See, this is the pipeline. What you do is you take the query. You don't go to the LLM at all. So it's like this. So typically, the old way, let's say the old way, was given a query, you had two approaches. You would give the query as prompt, would be the query itself. And LLM, it would feed into the LLM and out would come a continuation, a response. This would come out, isn't it? Now, what this paper is saying is don't do that. Do this. Don't make a prompt out of it. Take the query. Right. First, what you do is you go and actually these people in the paper use the word x right x x is the query you create the query vector out of x which you have learned to do that go to your index ann what will you get you will ultimately come to know documents right z1 all the way to let's say z10 you you got the 10 documents now you create a prompt like this you You say prompt, the question, go back to your X. And then use the below facts. And then you say D1 to D10. So this is your total prompt. This is your new problem. You feed that into the LLM. And whatever continuation you get will be superior, will be better. Potentially, not always, but potentially, if it's a trivial question, then you won't gain much. One second guys. Yeah. So Prakash, does this answer your question? It does. Right. So if you think about it, what have we done? We could have just stopped by getting search results. That's how search engines work. We could have only gone to LLM, right, which is how you talk to ChatGPT. But RAG is saying put both of those together. Right. Now we use the word. Now this paper use only dense search, vector search, but in this project you have already used more than vector search, you're using hybrid search using a cross encoder. You can go one step further, you can take this not only get the results d1 and d2, this comes from search, but you could also append to it some things from your knowledge graph. If you have an ontology, you could bring things from there also, right? Potentially you could make a phone call to your friend and get half the answers and feed it there also. Pretty much programmatically, but of course you can't do that programmatically. But basically what you're saying is you can augment your prompt by retrieving information from a variety of sources. You see that, right? And when you do that, you get a much better response from the LLM, much lower chance of hallucination than you would get. So I have a question on the prompt. I think there is a limit of 4K character or something like that. Yes, the results are maybe hundreds of them and they cannot produce a product. Tough luck. No, sorry, the question of Prakash is, oh no, Prashanthi, sorry. Prashanthi is, Prakash, sorry. Prakash Ranade and Prashanthi are there, sorry. I apologize. So is that if you get a lot of results, if you get a lot of results, won't your prompt exceed the prompt length limit? You actually have much worse problems. What happens is that the longer your prompt is, even if it fits in, and these days the limits are pretty generous 8 000 8 000 tokens million tokens this and that problem is not problem is beyond a certain length the prompts you not only reach a point of diminishing returns you actually start harming the response the quality of the response degrades my general thing is that if your prompt is exceeding more than a thousand or two thousand token you're already pushing it right uh there's an entire paper that's called land in the what is it lost in the land of yes lost in the middle yeah lost in the middle like if you have very large content prompts you remember how much attention like you know ultimately this is made out of attention think of the quadratic problem you have 8 000 tokens you are computing 8 000 squared attention 64 million attention values as a minimal get computed in a single head right and you're going doing it over and over again. And so it's a process of focus getting diffused. It's not good. In fact, the paper that one of you posted to our bootcamp yesterday, the LLM Infinite, literally, there are many books that are trying to address this problem. That was one more work that was trying to look at this problem and say how can we not have that right so the the more practical answer to your question is guard against very long prompts so i have a question related to this right so as a let's say there are a lot of data analytical applications and i've worked in a couple of them where you actually give some information about the data to the LLMs, which is part of the prompt, but it's input data, right? So if I say, hey, look at the data and I actually use a variable there and the data is there, use the column names and tell me how I get, say, some information which records complex SQL queries, right? So, since the input is very long, this also needs to be in chunks, right? And then how does it come up with complex SQL queries? Because if it is standard Python, you could say that there are so many python code that it has been trained on like stack overflow or any other forms but sql it does really come up with very complex operations perfectly that is specific to the data so it i mean these are not even like directly uh identifiable in terms of sentence vectors or something like that. So how does it condition that way? Haripriya, you brought up the question of the SQL and the SQL generation. So there is an entire subfield of generative AI, which is called NL to SQL, natural language to SQL. Now your point is valid when you use, for example, a land chain to first understand a schema, an agent to understand the schema, and then to infer what SQL query to run. Sometimes the SQL queries are complex, or if you're doing search and you're trying to feed in a SQL query itself in the prompt you're getting into areas where you're pushing the limits actually llms don't do as good a job they do good enough that you can make a huge difference but the let's just say that nl2 sql is work in progress at this point okay Okay, so guys, hold on. So I want to hear questions from people who have been quiet so far. Anybody else with questions? Yeah, so one question that came from Dennis is, can we get for lunch, can we get eggs instead? I'll ask for it next time. I don't know if they, if they increase the price, then the answer is no. If they keep the price the same, then Dennis will have his eyes. Any other question. Right. So generally, I mean, so we've talked about hallucination, but I can imagine two ways to identify that, say, it is indeed the model is hallucinating, like either you know the domain, or you have some like test inputs and outputs. So are there like the general recommended guardrails to prevent a model from hallucinating? So Prashant Yadav asked this question, how do you prevent LLM from hallucinating? This is the central question of research today. What we cannot do, we don't have a bullet proof way. There are two ways of preventing it. So many ways of preventing it, of mitigating it. Don't give, be very precise with your question, be very precise with the instruction and clearly say that if you don't know the answer, say so. Things like that. Even then, it may cook up things. But RAG goes very far. If you say, ground your answers in this knowledge facts that I've given you, then you mitigate it. So that is one way of doing it. Another way I was going to joke and say, a support vectors gives you a service, literally consulting service, to help your model solutionate less. Put guardrails around it. So we are the guardrails company, hire us. And also you can- So what is the related, sorry. So it's a very related question. So there is a knob called, a parameter called temperature, right? right in chat gpt which helps you get the most deterministic output if you have the temperature zero there is uh least hallucination possible so what is that what is that relating to no that is i mean see i don't know temperature is uh i'll tell you what temperature is so yeah good question whenever you take a model there are two parameters one is is the temperature, one is the P. But let's focus on the temperature. See what happens is that, let's say that you have a softmax, right? You got some larger values, 41, minus 26, 3, 2, 1, something like this. So what happens is when you do a normal softmax, this will become extraordinarily large, this will disappear, these will be too tiny. So the probability that this answer will be picked, let's say A, B, C, D, E, when you softmax this mathematically, this will stand out as your highest probability by far. This will stand out as your highest probability by far. It won't just be, so suppose this was plus 26, so it was here. It won't just be twice that of B, probability of A won't be just twice of B. It'll be actually a huge amount, right? Because you have exponentiated, it will be about roughly speaking eight times or something like that, right? So to prevent that, what people do is they use the heat of the, like this modeling comes from statistical physics, like you heat it up. In other words, when you heat it up, then these guys begin to look a little bit taller. So if this is here, this becomes like this. a little bit taller. So if this is here, this becomes like this. Now the size asymmetries get muted down. You realize that you have dampened it down. The big guy doesn't look so big anymore. Now what happens is once you've done that, even then you sample, you pick a sample solution. You say which word should I pick given the fact that A is more likely than B is likely. So what it means is it may still, for example, end up picking D. It has a small probability, but it may still pick up D. But as you increase the temperature, the unlikely tokens get picked up a little bit more often. do with hallucination remember the next word that gets generated the next token uh token next its probability is conditioned on is conditioned on how many things all the previous tokens t1 to tn minus one it is conditioned on the information retrieval the retrieval facts and it is conditioned of course on the parameters of the model and the input right so sorry the query given given the model the parametric part the probability is determined by the query the retrieved information and all the tokens that it has generated now what happens is if you make unlikely token come up, the error rates increase faster. So I'll give you a very simple mathematics. Let's say that if the temperature is very low and you're much more likely to pick the highest probability thing and it is 99% you would have picked the right thing. There is only 1% error rate right so by the time you generate a hundred tokens 99 percent picking up the right answer is not good enough because 99 0.99 to the power 100 is anybody can guess what is it just guess don't don't do it yeah what is 99 percent to the power hundred so what yeah it is basically zero right so the probability that you are picking up the right answer after a little while becomes zero right that is why whenever you do talk to these llms try to you know the shakespearean simplicity and brevity are the soul of wit. It truly is true with large language models. Make it give concise answers. It's more likely to be right. Otherwise it will meander off and you'll feel that it's making things up. So, Taripia, does that give you a simple intuition on how this goes? There are other parameters, the nucleation focus and things like that, but we won't go into that. Go ahead. Sure, thank you. I think Mani has asked this question. Let's say LLM has been trained on it is. The. Then we are reading the. From this new model and how does it figure it out. The data based on the current context? Because, you know, the example which you started doing that it has to know the how many countries exist as for the latest. You give it, you search for documents which say that now in 2023 there are as of 2023 there are 187 countries right i see so that will be the uh it is a part of the problem okay and it will always take whatever you say in the prompt as the truth and so that becomes the memory of it it becomes okay guys i know that there are more questions but but I have a small announcement. What are we going to do? You guys are going to do your projects, but today is the day of presentation. I do want every team to present. I hope you guys are ready. We will have our Viyani, and after that, I would like each of the teams to start doing their presentations. And we will start with the teams that ask the most questions no i'm kidding go ahead no nobody is asking questions well you don't want me to answer all the questions do you you have to figure that out that's what the project is about see guys what i'm doing is i help you with the data engineering part and so forth now we are getting into the research areas i want each team i said that each team needs each has its own tribal culture its own way of approaching solutions its own innovative solutions that's how you'll do your startup. So I deliberately won't answer the question because it admits many answers. And I want you guys to discover some of the answers yourself. Shivani. So just going back to the idea. So let's say if there is the hybrid source that you have, is there some pre-processing that we would do again on that 5000 output that we do and if yes no no no think about it what are what are ai models capable of think of any llm technique that can reduce your 5000 token to thousands of summarization you can summarize it if you want what would summarization do it will pick up the most important statements in the text and give that to you right okay guys so uh many questions anybody how many hands more all the hands have disappeared okay yeah okay yes guys patrick uh you brought a great Guys, I have a request for all of you. See, there is a glossary on our course portal. Did you guys know that? All of you can contribute. Could you please contribute to that glossary? But please, please give accurate, useful. Yeah. So somebody told me a joke that if you want to confuse your peer reviewers for your code, make sure that you do import pandas as NP. Import them, yes. And then have them read the code. In addition to what you said, I think you should normalize that as part of the presentations, because I think we are all over the place. That is very good actually yes, let us all create and let us also in the presentations each team from next week. Tell what what what you added to the vocabulary, what you added to the classroom. So guys, you have one new task. Everybody needs to add something to that and they need to present what they're at. OK, so with that, it's time for lunch. We are. Are we all clear what the project is? It's a fun thing, guys. And now we are using a real LLM to solve problems and you're in the territory of residence by the way hint hint you might have to use an llm for the parts that you have already done but uh no more hints for the parts that you have done you could have used llms for the parts that you have done you could have used lls I won't say anymore any other questions any other requests any other things right yeah question I raised my hand but nobody saw it I think um so I've been looking at quite a few data domains and how do you know that basically the model is not hallucinating because you need to have an independent way to check and make sure that you have the guardrails but it is actually telling the right thing that it is supposed to do so when you look at these exams and things like that all the chat gpt and all are clearing and it seems like every time they're getting better and better but i think that target that they go towards is a fixed set of questions and people are just kind of fine-tuning it to make those marks as opposed to generic questions that it is able to solve and answer so you're saying that making taking a summary or concise thing is a better stuff but if you look at a medical thing if somebody has to elaborate what is wrong with you you want as much as data as possible as opposed to saying oh you're just sick that is true see such anaman, Ph.D.: Is working progress there's a whole body of research that's trying to expand the token land without degrading the performance effect, the paper that you said specifically. R. Vijay Mohanaraman, Ph.D.: So it's at this moment, everything is fluid and every all of this is working. R. Vijay Mohanaraman, Ph.D.: no there are no winning answers people are trying all right guys so i'll end today's session i'll end the recording and uh i wish you guys thomas you are here are you uh it's working with them probably yeah yeah you will work with thomas hi yeah yeah i've been working with them thanks yeah can i make a can i make a request that uh afternoon if you all have the five spark session can you all record it to share it because i think afternoon all i i can't attend yeah yeah promise all the sessions will go live stream to youtube and stay there so you can always watch them i'll make sure that i uh i have all the recordings on youtube okay thanks so much and should we bring a laptop for the presentations to view or we're going to project it so uh i will i will yeah okay let me solve that problem i will solve the problem so you can bring your laptop but i think i will be able to put together a pv on the screen on the stage soon okay thank you oh