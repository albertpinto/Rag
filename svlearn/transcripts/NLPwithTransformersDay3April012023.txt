 All right folks, welcome back. This is a beautiful morning in California. We're having sunshine after a long time, isn't it? And today is Saturday, Isn't today the April Fool's Day? Yes, it is. It's time for me to play a lot of pranks on you guys. All right. Let us recap what we have learned so far. We started out with a case study. We said, and the point of the first case study was, what is the value that natural language processing, the AI version of it, not just linguistic sharing of documents, but natural language processing, what value does it bring to a lot of common things we do now? Search was something we have been doing for a long time. Linguistic search keyword based search has been around very mature engines are there. Apache leucine is an absolutely awesome library, very mature, with lots of features and bells and whistles on top of Apache leucine. For example, there is the Apache solar solar and then there is the elastic search which both of these are industrial strength very heavily used search engines they search through enterprise documents at quite a large scale and people create entire clusters of people and so um people create massive deployments these uh search engines are run on large clusters but at the at the heart of, the core idea remains the same, though it goes through a lot of enhancements. The core idea is you take a document, find all the important words in it, the key words, then you create a sort of a dictionary list. The key of a dictionary or that hash table or hash map is the word itself, the keyword itself. And the value is a list of those documents that contain this keyword. And people augmented, of course, not just by storing the document, but how often it was in the document or how relevant it was in the document and many, many things. You can go and extend the idea further, but that's the basic concept. If you can keep what is called the inverted index of a keyword and all the documents that contain the keyword, the basic idea remains the same at heart. If you can create an inverted index from given a keyword, what are all the documents that contain it, you have the germ idea or the kernel of all linguistic or keyword based search engines. The problem with this, obviously, is that it is so different from the way you and I search. When we ask a friend for things, I joke that sometimes in my absent-mindedness, my wife says that, where is it kept? It is right there, you know, you just go and then turn left and it is there, right? So the thing is, unless you're very contextually aware, go where and turn left where? So, and yet, that's how we talk. So it is the context that tells us what is meant by a certain thing. We don't do keyword search, like we don't label all the locations in our house by certain keywords. In corporate world, we do that. Every room has a name and we'll say, look in this conference room or discussion room. On the table of this discussion room, you'll find something. At home it's much more amorphous. So that's sort of where AI search comes in. What if you could capture not the keywords, not force the user to bend and adapt to the technology, but have the technology infer what the user is trying to say in a very natural way, in a way human beings are used to speaking about or searching for things. way in a way human beings are used to speaking about or searching for things then that is ai search you say that you understand the semantics of what the user is saying rather than just take the keywords and go searching by that go ahead albert so why is it called an inverted index the linguistic search the keyword search is inverted index because see usually a document has words so one way of keeping it is that given this document these are the keywords it contains but if you invert it what do you get given a keyword what documents contain it think of it as the inverted map right hash table so that's what it is and obviously the values are the list basically like then goes looks at the documents and then kind figures out the index that is right so what you do is you take a document see through it extract the keywords and then in the hash table for each keyword you you put this document id saying that this document is there right you you add it to the list of document IDs that are there. So that's what a search engine traditionally has been. But with AI, we have a different thing. And we saw how dramatically enabling a semantic search is. We did it with the classic case, text in, text out. You know, you give textual prompts and then you get give textual prompts, and then you get back textual response, the AI searches through a whole corpus of text. But then we extended the idea further, we realized that the semantic search we were doing through this particular case by using sentence transformers, which creates sentence embeddings, it embeds it into a hidden vector space and so you're doing vector searches. Now vector searches is good but you realize that it's quite effective. What do you lose? Now I want to give the big picture. See, think back to the bird. big picture see think back to the bird the bird transformer if you remember it took two segments or two sentences as input right and it passed them passed it through the layers of the encoder and then finally it produced the transformer heads and so on and so forth. And finally it produced, well, obviously the output, but one of the hidden representation outputs was the output for the CLS token, the initial token. And now from that, you can do many things. So for example, you can see, if you treat it as a softmax input, which of the words in the vocabulary, right? or like, I mean, you could ask from that, a very simple thing to do, not even the words in the vocabulary, you could ask this question, what is the probability, just simple logistic regression, what is the probability that these two sentences are close to each other? Right? Because you use probability as a similarity measure or something like that. You could do that and you can train a bird transformer to be very, very good at it. Right? It's one of the classic things that could do, similarities between the two sentences. Now, why do we not do AI search using that? That would be the gold standard, isn't it? So imagine you are sitting with a corpus of a billion texts, which is quite common. For example, in the previous company that I worked for, literally that was it. You had a corpus of a billion pieces of texts, more than billions and billions. If you give it a query text to pass a few billion inferences through a transformer, for every query is not feasible. It will take arbitrarily long time, whereas the expectation in search is that it should come back in the blink of an eye. And it wouldn't happen. So now from that gold standard that AI can do, let us break it down and solve the problem piece by piece. So the first great milestone is the sentence bird and today we'll do the architecture for sentence bird. We didn't discuss the architecture for sentence birds. So far we understood the classic transformer, encoder-decoder architecture, attention is all you need. Then we discussed the second paper, the BERT paper. Now today we'll discuss the sentence BERT paper, but moving a little bit ahead to what it produces, what the sentence BERT produced for us was embedding of a sentence into a semantic vector space, a semantically relevant vector space. It understands two sentences will be close together if and only if they're very similar, right? And it somehow learns similarities based on the semantics of sentences. So when you do it, and how it does that is the context for the afternoon, but given the fact that you can do it and you have projected it into a very high dimensional Cartesian space, then the problem becomes very simple. This query becomes a point, a vector, and around it you have already all the documents pre-indexed into the vector space, projected into the vector space. Just go look for the neighbors, right? Now, one way of looking for the neighbors is the k nearest neighbor again take this vector and compare its distance to all the billions of other vectors that while it is cheaper for example we use cosine you can use cosine distance vector similarities are far cheaper than running sentences through transformer to look for similarity. Isn't it significantly cheaper? But still you're comparing against a billion, let's say, documents, a billion vector points that they're vector representations. So that is exact k-nearest neighbor search. so then comes to our rescue and before we get to how that problem is solved remember that in very high dimensional spaces you tend not to use the euclidean distance because of the the curse that data scientists face which is worse than any harry potter curse and what curse is that the curse of dimensionality right in higher dimension spaces. Everything becomes very far off from everything else right or if you look at any one point relative to this point, all other points seem to be at the periphery of that whole space is just way way out there. So you tend to use cosine distance or dark product distance to recap the advantages and disadvantages of cosine and dot product is cosine distance tends to favor similarities that are close by, they're short sentences, they tend to be more similar. But if you are comparing larger pieces of text, they tend to be favored by dot product distance are you together they get they get favored by that it's just an empirical thing you need to know then another approach which is sort of a composite is what you do is you take all the vectors and normalize it when you normalize all the vectors what are they they become a point in a n-dimensional hypersphere just now hyperspheres are hard to imagine just imagine a globe they become points on the globe and so you can talk about cosine distance you can talk about dot product they're identical or you can talk about the arc length or whatever it is some right uh some distance so that could be that and you can be creative with distance measures so long as you're aware what high dimensional spaces do to you now what can we do having projected points into higher dimensional into embedding semantic embedding space these are called sentence embeddings or vector embeddings since many synonyms exist so long as the meaning is clear from the context we let's just call them embeddings what all things can we do from the embeddings you know if you remember when we talk about our data data generally generally, when nature, when data, there's a generative force behind data, it is relative rarity to find data uniformly spread in a vector space. So when you look up into the sky, you don't find all the stars are randomly uniformly distributed. What do you notice? You notice clusters of stars which you call galaxies. Isn't it? So clustering is natural. You look around a forest, a cluster of trees, isn't it? Clusters of houses form a residential neighborhood clusters of these concrete and glass buildings form the commercial district and they tend to form clusters isn't it and these clusters are just about everywhere human body each of the organ is clusters of tissues of a particular kind and so forth so clustering is everywhere one would imagine that if you project all these sentences into an n-dimensional vector space, you would expect clusters. So what would those clusters tell you about? They talk about points that are semantically relevant, they're similar. What do you say when many people are talking similar things? Generally similar what do you say when people when many people are talking similar things generally what do you say you're saying they're talking about the same topic isn't it that's the definition of a topic topic is conversation or things about the same about semantically similar things similar semantically similar conversations or sentences form a topic. And so, by the very nature of projecting data into an embedding space, you have the notion of topic modeling. You see how naturally it follows from that. Now, topic has many many methods. If you remember at one time, and those methods are still there, they haven't gone, we use Gensim. Gensim used algorithm called Latent Dirichlet Allocation, LDA. Now I won't go into those because this course is all about transformers, but just for you to know that any problem that transformers solve, there's a rich history in NLP of solving it by alternate methods. And so one thing that is often worth doing is whenever you solve a problem, a task using transformers, to go and check if we had solved this problem using one of the other methods how well would it do now generally transformers these days tend to outperform but not always it is always worth cross-checking because there may be scope for improvement and so forth so yes the topic modeling comes out naturally from that. And not only, and you can visualize topic modeling, but you can say, how do I visualize a 768 dimensional vector space? Or how do I visualize a 500, a very high dimension space? All we can visualize is two and three dimensional spaces. Right? So the trick is, you use dimensionality reduction techniques. That is again something you have learned in the ML 100 course, in the Introduction to Machine Learning course. There's a whole slew of techniques that takes the data represented in high dimensions and brings it down to a lower dimension. Sometimes the purpose is to find the most reasonable lower dimension in which most of the features or the substance of the data is preserved. But often it is also used for just human visualization to get a gut feeling of the data. And when you do that, when you use, there are many techniques you can use. Principal component analysis, you could use another form of things called LGA, linear discriminant analysis. There are many, many such techniques you could use to project down data. Some of them are linear. And then there are non-linear techniques like, for example, TSNE and UMAP. A UMAP at this moment is generally considered by far the best, in most situations, not always, by far the most, for visualization purposes. So when you UMAP that vector, you can get a very gut feeling of the topic modeling, right? You can see it on a, in a page. And if you remember the sentence, but the documentation gives you an illustration of that. So do that on the classic data set that everyone uses for this, which is the 20 news groups. And you will see that it does a fairly good job of it. Now you could use this for many tasks, right, so that's your sentence embedding aspect of it. Now you could use sentence embedding for search of course, k-nearest neighbor search, but we still have the problem how do you search against a billion objects? Because you're doing vector comparison. So we said that, no, we have a cheaper way of doing it because once again, in machine learning, there's a long and wonderful tradition of lots of creative methods to do approximate nearest neighbor search hands. And there are many, many methods. We in particular looked at FAS, SCAN, these two. And there are more and more coming up and going, and HSNW, hierarchical navigable, small-worlds map approaches, and so forth. But the gist of it is that these methods help you find nearest neighbors, but when you do that you lose a little bit of accuracy. What will happen is it's almost like you are, let's say that, how should I say, suppose you're looking for some, to take an example, on the beach you find some beautiful starfishes on the sand or sand dollars on the sand or something like that and you are looking to you want to pick the sand dollars so if you just one by one pick up the sand dollars you will take considerable amount of time but if you're impatient you can take a big bucket and just scoop the sand right with lots of sand dollars in it hopefully if you do that let's say and assume that the sand is rich with sand dollars you you don't just get the sand dollars you get the sand also but you don't want the sand so you need to a way to sieve out the sand does it make sense so now what can you do first is suppose I tell you a 20 cent 10 sand dollars now hand picking 10 sand dollars is easy you get exactly 10. but when you keep scoop bucket of sand out, you don't know whether it has 10 sand dollars or not. So you have a sort of a hyper parameter, how much to scoop out, how much results to bring back, because it will contain sand dollar as well as sand that you don't need. Or that's a metaphorical way of putting it. More realistically, it will contain, let's say that there are 10 real good answers there, but those answers may not show up right because some of them are the not so relevant answers you want to get the most relevant answers so to have a good recall to make sure that all the good answers are there and not missed you want to scoop out a lot of sand you want to scoop you want to retrieve a much larger result set but it is cheaper because the trade-off is even though you're scooping a bigger result set instead of just 10 results set, in it is so far so many orders of magnitude cheaper and faster as a search that that little bit of price that you pay doubling or tripling the size of the results you get or even 10 times the results that you get, is a very small price to pay. You get the whole bucket of sand, heavy bucket of sand. And you know in there are sitting your sand dollars. But you still have the problem of shifting through the sand and being left with only the sand dollars. Would you agree? So to do that, what you do is now you can say there are many things you could do one easy thing is you could say i will do the exact vector similarity measure right but when you do that and you're comparing against just 50 results right to pick 10 of them or 30 results to pick 10 of them. How do you do that? Putting 20 or 30 inferences through the gold standard, which was that put the query sentence and each of the sentences, each of the documents together through a transformer, BERT transformer is a cross encoder, I mean, then produce a measure of how similar they are. That was a gold standard. We didn't use it because that was expensive. But we have sieved down the comparison to such a small subset now that it is now cheap to do that. So we can do that as a re-ranking mechanism. Does it make sense as a final step? And when we do that, what will happen? Now our precision also goes up because if we just preserve 10 and throw out the 40, there is a very, very high likelihood that you got both high precision and high recall, right? So this is the journey that you take to create something that you would actually deploy because you got precision, you got recall, and you got precision you got recall and you got performance and scalability right and remember scalability matters because through a gpu like ultimately these transformers are i mean you can deploy it on cpus or gpus for inferences right generally you deploy it on the gpu but when you deploy it on the GPU, the GPUs these days are prohibitively expensive. And you're passing inferences through it. So what you have to do is you have to pipeline. And the faster the two things that matter, you want to make sure that the number of inferences that you pass through or you need a per user query is small if you do 1 billion inferences like where we started out per user query you're doomed right and the user experience is terrible but you have to bring it down to a very short number 50 inferences you can literally or 30 inferences to pick 10 out of them easy you can literally have a batch size of 30, right? Because they go as batches, mini batch. You can pass it as a batch of 32. So in one shot, you can do an inference. And that is what you want, isn't it? One query, one shot inference. Ideally, right? And you get the result. So those are the considerations of scalability and performance. And finally, the deployment consideration is no user these days. I mean, our attention span is such that in four seconds or more, we would have left the website. Right? That's a fact, usability fact. So people expect, people already begin to dislike a place if it if it takes more than a second so the gold standard is to come back in under 100 milliseconds right the rest of it is network latency because it has to go through application layers and so forth in fact the gold standard is you want to like in most enterprise like for example inference engines if it is mission critical you want to be under 10 milliseconds, one millisecond, you can keep on improving the thing. But for AI, 100 is supposed to be reasonable for query. So you want to come back in 100 milliseconds. So those are the practical realities. And so that was the intellectual journey that we went through. Now, on the theoretical side, we did two things. And this is I'm still summarizing what we have studied so far. We studied two research papers. One was the great classic, Attention is All You Need. But when we did that, we took the paper, that paper, as we realize, introduced the transformer architecture, which is a sequence to sequence model. It has both the encoder and the decoder. And if you remember, I said language model is you take input, you create an abstract internal representation, a hidden representation, and then you decode it, that decodes it into whatever, into French, into Hindi, or just a summarized text, into poetry, into whatever, right? The output you can then think about, right? Decode it as such. And of course, now we are entering the world of multimodal learning that makes it even more interesting, but we'll get to that. Now, that paper, if you realize, it's biased. The explanations are very biased towards fast computation. Because in those days, the biggest problem in the natural language processing world was that RNN, CELAR used, LSTMs, etc. They were not really using the parallelism that was possible from this massive GPU farms. They were basically sequential like compute models. As sequential compute models, it was this limitation. So the transform was a great breakthrough was not only that it brought the concept of attention, but from a computational perspective, now you could pass the entire sentence at one go or paragraph or whatever it is at one go. In fact, you had to, there was no way you could give it one token at a time. So which was a big win. And these days, research papers tend to be very benchmark heavy. Look, this has moved the state of the art by so many points. And so the flavor of the paper, the great paper, is very focused on the computational aspects and showing how this parallel computations does it. But in a way that paper did not take a, and it's not supposed to take a research paper, it's talking to a community of people who completely understand what attention is and all of these things are. So it just went ahead and gave only the sussing breakthrough. But for us to understand, we went back to the foundations. We understood what attention is, what self-attention is, and what multi-head attention is. And we sort of took it down and took a slower approach to sort of building up to that paper. That was one thing. The second thing we did, the second week we did is the BERT paper. BERT tends to focus on the encoder part of the attention, the full classic transformer. And it is quite interesting that just by focusing on the tr- just the encoder part of it, it manages to do quite a few tasks right for example for classification and for such things encoder is enough you just need a good hidden representation coming from the attentions and from that you have already gotten a contextual a semantic understanding of what the what the thing is and you can do your job you don't need to necessarily put it through a decoder, right? So BERT and all its derivatives are encoder part. And I mentioned in passing that the GPT and such, they focus primarily on the decoder part of it, right? Because, and the main thing, for example, of GPT, the main crux is to produce the next token. Isn't it? Remember, what does the decoder do? Takes the hidden representation and what it just produced to produce the next token based on that, based on the previous token, the next token in the hidden state. state right and so which is exactly what for example chat gpt is right well chat gpt is more than that i sure i don't want to say it is only the decoder of the transformer there's a lot more going on in these large language models today uh it isn't just that you take transformers and you almost a lot of the good things that come into it comes from reinforcement learning uh it's a topic we'll end with this this course whether it. It is one of those optional days. It goes beyond NLP. But those of you who want to understand the full architecture of these newer large language models and why they work so effectively, you have to look at the fact that reinforcement learning and human prompting is a big part of it, of the training, right? And we will get to it. In in fact today I'll show you examples which will make you realize that compared to what we have which does include reinforcement and all of this the the pure transformer of smaller size not so large trans models they are sometimes they can be impressive sometimes but hopeless sometimes right and we'll see some of those situations so we'll see the good and the limitations of it today so that go ahead yeah i moved into new territory today no questions so either it's all crystal clear or you haven't understood much and you've forgotten whatever we talked about last week which of the two yes if i have a question go ahead mostly most of it is tracking one thing that you said about retrieving a much larger result set and sieving through it right how do you retrieve a much larger result set from like billions of yeah see um that was the technology of approximate nearest neighbor search what you do is you have essentially taken billions of them and in some way in this this vector space that you have somehow you think of it as you have partitioned it into sub regions right either overlap overlapping or non-overlapping partitions there are many many algorithms to partition but somehow you have quantized it remember we talked about all of those methods quantization etc etc right so let's take an example. Suppose, I mean, just to take a colorful example, suppose you have a color and there are 32 million colors. Right? But you have one easy way that you can search through 32 million colors is, and you get a color and you want to find its nearest colors. You don't want to do comparison to 32 million colors. What you could do is you can create a vocabulary of just 256 colors, you know, that's the 8-bit color depth. Now, just 256 colors. So you have 256 buckets. And every vector, every color that you get, you associate it or you index it by one of the 256 colors. That is the key. Value is, of course, this color. So then what happens, you have now reduced the problem set that first you find which bucket to search in and then you are searching a smaller bucket isn't it so now take this idea further there are much smarter algorithms than the simple one that i mentioned these algorithms manage to subdivide the partition the feature space into regions so the only thing you do is given a query vector which of the sub regions it goes to subdivide the partition the feature space into regions so the only thing you do is given a query vector which of the sub regions it goes to which of the buckets it goes to and you search only in that bucket right and you find the k nearest neighbors only in that bucket does that answer that master once got it. Yes. Thank you so much. Same question. So when you pick up this whole piece of sand, do you pick up in different areas and get to a bucket? Or do you get the whole bucket? So Albert asked an interesting question. We use the metaphor of scooping a bucket of sand, one bucket of sand, and getting more than the ten dollar then the ten sand dollars we hope for but you'll get lots of sand to or results that you don't you want to ultimately remove but where do you where do you scoop the bucket out so that is where the a and n algorithm is the whole gist of the a and n algorithm is it tells you where to scoop from. In other words, that whole beach, it has sub-divided into regions, sub-partitioned into little regions and it says, ah, this thing, go and scoop sand from here. And when you scoop sand from here, you'll get your sand dollars that you're looking for, relevant sand dollars you're looking for. Does that answer that? Go ahead Abhijit. I was just playing around with common words like Hindi and English. So when you put something in a different context, for example, I took Rajdhani there is like restaurant also there is a train also and there is a capital also so it completely messed up my like what I was trying to search to the context so it didn't know that like restaurant part completely it didn't know and it was just talking about trains it didn't even talk about Delhi that is right it just started connecting but it was just talking about planes, it didn't even talk about Delhi. That is why. It just started connecting Delhi, but it was more talking about planes. So here what I'm trying to do is like if there is common voice within Bangladesh or Nordic state, it can go in a different region where there is no correlation at all. That is why, that is why. See that's a very good question. So I'll repeat the question that Abhijit raised, a comment that he raised. He said a very interesting thing. In the Hindi language, there is a word, rajdhani. It possibly stands for, originally stood for the word capital, like Washington DC for the United States. Then it also stands for a very popular train. I mean, one of the fastest, well, India's version of high-speed trains, which means it goes at least 40 miles an hour on average, is Rajdhani, right? So that, but I think now the Shatabdis are there, right? Which go much faster, isn't it? And is Shatabdi the state of the art or is the state of the art much faster now? When they bought it. By the way, they were going to make high speed, sorry for the digression, they were going to make this high speed trains using the Spanish technology. Has it happened or not? Yes, I think it's from Mumbai to .. OK, so those must be going like 200 miles an hour. Wow, nice. Nice. Yeah, I'm fond of it because my father worked in the Indian Railways. He was an engineer there. So anyway, trains are Rajdhani. And Rajdhani is also the name of a very popular restaurant series, restaurant chain. So when you do conversion from English to Hindi, it gets totally screwed up. Actually, do not, one of the weakness of vector search is that it is always looking for context to explain the word. When you give too few words or just one word and you search for it, actually vector search doesn't do so well. This is one of those situations where the tradeoff is that a keyword search or a linguistic search is more likely to give you results. So and again, that brings us to the side issue that sometimes what people do is they will take vector search they will take so-called sparsely the link keyword search put it together once one but one bucket of sand from here one bucket of sand from here right and then pass it to the cross encoder right and that too is done actually so but having said that my my suggestion is when you suspect that semantic search is available and it's certainly true when you're searching in google when we started our google search 20 years ago 25 years ago remember the whole mentality was keyword search but the world has evolved in the transformer world and to the extent that all searching these big mighty in the transformer world and to the extent that all searching these big mighty things are completely AI driven now, you should assume that these things are at play. So you're much better off putting a narrative there rather than keywords. And we have to sort of shift our mentality. See, the world has changed, even though it looks the same. I keep telling people that the change is so foundational happening all around us that the world has changed. We have just not caught up to it. So try that now, give it the context, say the sentence, and then you see that the translations will be much more accurate or your searches will be more accurate with these words. I just had to play around and confuse it more. Yeah. So let me see what you do answer. Yeah, no, when you try to confuse it, you deliberately give it ambiguous context. Then you're trying to literally test the limits of how much semantic extraction a transformer can do, but then that is good to do, but then you realize that you're consciously doing it. If you give it a reasonable sentence, I really loved the food at Rasdhani. Yeah, and I gave it five stars or something like that. But then, yeah. All right, so any more questions, guys? By the way, the word, the technical word for words, name, term for words that have multiple meanings is homonym. Those are homonyms. Okay, all right. Any other questions before we start with today? So the fastest train is one day by 160 kilometers per hour. 160 kilometers per hour. Okay. That's the fastest one nice that's pretty fast that's pretty is it is it the bombay pune no no one day by the time that So the Spanish thing hasn't come yet. First class Spanish. First class Spanish. That's probably in the United States. That's a nice thing. Okay. All right, guys. So let's take a, we have had a review. If you have no questions, let's take a very short five-minute break, and then we will continue with today. So, guys, I've been giving homeworks and some of you have done. Now those of you who did make some noticeable progress in your homework. Please raise your hand in your slack or in physically. Albert did. Praveen, you did, right? You did. Very good. Yeah. Dhwani, you did. And Junaid has done. So actually, Junaid just showed me his work. And I would like us to do a small show and tell before we continue to the new topic, because we'll get again lost in the new topic. The show and tell is long overdue. So, Jeanette, would you like to take over? Jeanette, can you hear me? Yeah. Let me share my screen. No, I think it's on. me share my screen so uh i guess you can all see my screen. So what I did is I downloaded all the data on PDFs from the National Institute of Health about diabetes. And I wrote some code to extract blocks of texts from those PDFs and then feed that as the sentences to the model. And it was like taking 15 minutes on my laptop to run. So I had to serialize and DC realize it to once the embeddings were created to make it faster. So now I'll ask a few questions and see what answers it gives me. So. oh okay uh okay the question is here okay so so first question was what is diabetes it gave me a very good answer and this is like 10 to 12 big PDFs. The next question is, let's say what is type 2 diabetes? And it gave me a very accurate answer. Right. And let me ask one more question. What should be the diet of a diabetic person? So it gives me the diabetes food pyramid. It divides the food into groups right that is a good answer any any more questions that we can try maybe you can ask like what's the diet of the person for inari with that just kidding Yes, so I think that's it. Does diabetes affect all ages? Okay, let me ask. Yeah. Yeah, can affect people at any age even children yeah so this is basically just a search yeah yeah this is it's semantic search yeah yeah there's a question okay there is a question What are the complications of diabetes? The most serious problems called by diabetes is heart disease. So it's kind of answering all the questions. Corpus is all the PDF files, they are available on the web on the National Institute of Health Diabetes website. But you have to do some work to extract the text from those PDFs. If you just extract line by line, it won't work as well. You kind of have to extract blocks of text and each block becomes your sentence. Did you use Tickr? I used a library called in Python, it's called MU PDF. Yeah, so there are two, Plumber and pymu PDF. So those are in my requirements. So how do you do it in Java? Some of these tasks make it faster. Typically do extraction from PDF and JSON. I can skip that. OK. So how large is the software? See, when I created the embeddings and then serialized them, that file is like 18 MB. Yeah. 18 megabytes. Thanks. Yeah. Thank you. Thank you, Javed. You're welcome. And Vani, you are ready with your demo. I can show mine. Hello, can you hear me, Asif? We can all hear you. Oh, you can hear me. Okay, thanks. I don't know if he can hear me. So we can hear you in the time you're pulling up your So can we have a look at the corpus that you created? I just want to know if the query sentences were very close to the, the responses, the blocks of text that you copied. Hello? Yeah, we can hear you. Sure. So this was just from like the last assignment where Asif said that we should do scan instead of using phase. So I have two different codes, I think. So this is one method which was using dot product. And so this does the same, has the same features as the phase one. Actually, it's a lot more simpler, I would say. The second, so should i like run the code so it's like a simple where obviously it's not within like a user interface model so you have to actually type in the query beforehand uh but it was the same thing as before where if you type in golden retriever it will give out something close to the files relevant to it. Oh, so you have to run it from the yeah after I just have to run the corpus. But meanwhile, yeah, the second one is this one has its own class and everything. So this one's a lot more easier to use with the user where, uh, you, so it asked you like, or it asked you to type in a query in this case, since the Jupiter, uh, Jupiter notebook doesn't have that like user interface where you can type in the user input. Uh, this one is simple where you change the query and it'll present this data here. It's a lot more cleaner, I think. But this one uses cosine scores rather than dot product. So that's probably the only difference. There is a widget. The way to get new data. Jupyter allows that. It does? You have to use the widgets. Okay, okay. Because I know, at least for the other stuff I've tried it works on Python very easily. I'm sorry. Yeah, but in this could be a picture. Introduce that you I would do. Okay, but yeah, and this is good. So you've got pets to work with. Yeah. Anyone else would like to I can share Can you hear me? Okay, i'm gonna share my screen. Can anyone else hear me on zoom yes okay i'm gonna share i don't know if he is gonna ask someone else to share but one second We're just seeing the Slack channel. Yeah, I think it's Asif's screen actually. Okay, I don't want to unshare his screen. I'll wait a second. Okay. i'll wait a second okay asif kashish is waiting to share results i think we cannot hear also because asif's mic is mute yeah it looks muted i can message the slack channel actually the one he's on. Okay. Repeat what I said. I don't know if anything got recorded so I'll repeat what I said. I don't know if anything got recorded. So I'll repeat what I said. See with rich variety of transformers and transformer architectures coming and many, many problems being solved by by using the transformer architecture in creative ways. One of the realizations we are coming to is that there is something quite fundamentally right about the transformers, powerful about the transformers. We were thinking of a neural network in terms of the neurons and the edge connections, because it is the weights on the edges that we update. The training process is finding the best possible weights of the parameters. And we do that through back propagation and gradient descent, some form of adaptive gradient descent. But now there is an increasing feeling that while that is true at the atomic level, it's just like, you know, a protein molecule is one analogy from biology could be a protein molecule is a fundamental thing or amino acid is is not just all the atoms it is a particular configuration of the atoms and it makes amino acid a provider functionality and so all proteins are made up of amino acids in some sense so there is a close analogy between that and what's happening in neural networks we are realizing that somehow these attention heads and these transformers are sort of like they're sort of like the amino acids that put together can create all sorts of proteins right and in the same way, by re-architecting the attention heads and how many of you put and where you put the encoders or the decoders and things like that, and putting things before it and putting things after it or chaining them together, we are creating various transformer architectures. And it's become a zoo just like there there's a large catalog of proteins we are creating a virtual zoo of transformers architectures now not only are we creating a zoo of transformer architectures there's another thing that's happening which is interesting so let's take the the one that we talked about last time. The paper BERT showed that if you train BERT on that corpus that it was trained on, it could do at least six tasks in which natural language processing tasks, classification, question answering, continuation, finding the password and so forth. And it could do and in at the time BERT was published, it achieved in 2018 or 2019, it achieved state of the art results, isn't it? But the question comes, when you train something on a generic corpus, it can answer general questions. But if you look, word is is divided into tribes in some sense there is the financial people they are the techie people right they are the medical crowd and they all have their own sort of a dialect within the larger english language they use terms in a particular way. So instead of putting a general purpose corpus of text to train this neural architecture, what if we trained a transformer, exactly the same architecture, but we trained it on specific corpora. For example, for the financial world, let's take a corpora that is only made up of things spoken in the financial world. It is not hard to gather such a corpora. And what if we do that? Now, you can do that in two ways. You could just take a massive amount of financial documents and then train that from scratch. Or alternatively, you may be better off first training it to the general corpora, corpus, and then saying, okay, now further let me train it to the financial corpus. Are we together? Fine-tune it. And then you get a fine-tuned BERT model for the financial world. Exactly. Yeah. Yeah. And then if you look at the fin something on hugging faces, there's another collection of the older ones. Today, we'll use one of the older ones. Right. But maybe actually as an exercise, why don't you make it a lab? Let's use the Bloomberg, the Bloomberg. Right. So what you see is that there are two degrees of freedom. completely new transformer architectures or you could take an existing architecture and train it further train it on specific corpora right domain specific corporate you can adapt it to a domain and both ways you get some really good results, transformers have proved so effective, like I told you, that natural language processing, which used to be RNNs, LSTMs, GRUs, nowadays are mostly transformers, transformers all the way with occasional use of LSTMs here and there and so forth. The world has changed quite radically. All these transformers are fortunately donated to the open source community with one conspicuous absence. Which big transformer is absent from the open source community? Well, GPT-4 and chat GPT and GPT 3.5. The GPT is the latest open AI GPT. They're not releasing it anymore. But by and large, the community, all these companies, they spend gazillions training these transformers. And isn't it an amazing thing that the spirit of collective research is there? And so they all release it to the open source community. Now, the biggest aggregator in the open source world of these transformers, like the watering hole, if you want to go get transformers, where do you go to? You go to Hugging Face. So Hugging Face website, and I'll share that now. Or am I sharing that? What are you seeing on my screen there? The Slack channel. Slack. That's the wrong one. So this is the website, huggingface.com. And if you come here, let me increase the size. You go to models. And I want to just walk you through Here let me increase the size. You go to models and I want to just walk you through the models. You notice first let's focus on the left and see what sort of models are there. Multi-modal learning has become dominant. The boundaries between various subfields within AI, like computer vision used to be its own world. And when people use computer vision, the quintessential example that came to everyone's mind is self-driving cars, or finding what is a, things like that. Classifying an image or things like that. Then there's the world of natural language processing, text and sound. But then we realize that NLP is beyond text. It also has to do with sound. Isn't it? So, but today those boundaries are blurring. So if you notice, I started the course by, with multimodal learning, because multimodal learning is the new frontier where the boundaries between all of these are being blurred. You can ask a question in text and get a response as a video. Or you can give a textual description and literally, I mean mean there are some of these lovely lovely examples people have created on the web they have used mid journey and a combination of a chat gpt and mid journey to literally take a textual description and produce and and of course a daily to and stable diffusion and everything and ultimately produce a whole video from the text. So imagine what transformative effect it is having on so many industries. I hope we saw that you could ask questions visually and get answers, and we'll do some of that today. So that is multimodal learning. Then there are computer vision tasks that it can do, but we are going to focus on the NLP task. Some of the basic NLP tasks that you can do is the oldest is classification. Right? Classification comes in many forms. For example, the easiest one that people look at and still an imperfect one, but the easiest one that comes to mind is sentiment analysis. Very useful in commercial world is the opinion expressed, is the comment, is the sentence of positive sentiment, neutral sentiment, or negative sentiment. Right. So that is an example of classification. You're classifying in three buckets, three labels. Another example is, for example, classifying based on what the subject is. Is it talking about science, politics? Right? Or history? What is it talking about? That's again an example of text classification so take text and classify it into a set into a target space finite target space now a token classification to determine for example one easy one is when you have a text and you break it up into words into tokens the parts of speech is it a noun is it a verb right is it an adjective is it a proper noun what is it comes into that question answering we can answer questions right then translation can you translate the text from english to let's say french or Hindi or to whichever language you want, Mandarin, right, Vietnamese, whatever, right. Or you could do another form of translation. You could say, not translation, but sort of similar to that is a neural style translation. Translate this bit of text into a sonnet right make it rhyme so there you are applying a different style to the text could we do that or write it in shakespearean language amazing thing is we can do all of that today and in the course of this workshop through the various projects we do we'll do all of it and in a way from today i would like to start the bigger journey of exploring all the things that we can do with transformers in the lab parts of it then comes summarization quite often you contain you see a long article and then you say where is the tldr right where is the summary of it and And can it produce summarizations? Now summarization can be in two forms. It could be just extracting the seminal sentences from the text, or it could genuinely be rephrasing it but summarizing it. So there are aspects of it. Conversational. Now have a conversation with the transformer as a chatbot. Could you do that? Now, adjacent to conversational is text generation. Can we have the system write new text that answers your question? Very close to question answering. Like Junaid just showed you a demonstration that on the diabetes data set that you could ask a question and it could produce an answer. But could it generate new text along with that? Now there the mileage varies actually. So don't be too impressed with it. Text to text generation, again what we just talked about, Can you do one form of text to another? Fill in the blanks. You know, the original way that BERT was tasked, mass language model. There are gaps in the text. Fill in the gaps. Right? And by the way, that is not just true of gaps. Today, you can do, you can render a person's face, for example, from a partial face. And quite often, it is very, very close to the real face. So it's a fill in the blank exercise, isn't it? A certain part of the face is occluded, is masked, or simply not visible because the picture is taken from a certain angle. And you're asking, can we now fill it in and make the full face right we can actually you can literally unmask a person right so uh well i suppose beware next time you feel like going into a bank with a mask going into a bank with a mask. So sentence similarity is something we have been doing using sentence words, you know that. But one of the interesting things is this cryptic thing called zero-shot classification. And I'll speak to that for a moment. What is zero-shot? And we will now go into the lab. Zero-shot is this. See, the biggest problem in training, let's say that you're training a transformer or AI system for text classification, right? So what do you have to do? There is this very, very labor-intensive part that you outsource to third world countries. Take the text and go label it. People are doing, not even, I mean, a lot of work. When you work as a data scientist, you'll be surprised that the first thing they'll give you, think of it as a rite of passage. You join an organization, you're thinking that you'll do some great algorithmic, you know, neural architecture, you sit in craft, and you go and sit in your office. And the very first thing they give you is, okay, why don't you sit and annotate all this text and label all this text? Right? Somebody has to do it. And it's the equivalent of you joining a team and they ask you to sharpen the pencils, right? You sit and label the data and annotate it and so forth. We all have to do it and actually there's a lot of learning. The first thing that you learn is that quality of the input data drives the quality of the precision or the goodness of the model at the end of the day. It's a worthwhile lesson to learn now one of the things people have been asking is wait a minute suppose i trained a transformer to classify into something text into some categories whatever the categories may be certain classes and it doesn't matter what the classes are. Can I, it has picked up this, it cannot possibly classify, unless it has picked up the semantics of the words, it has a semantic understanding of the text. If it does have a semantic understanding of the text, can I do this, I give it new labels. Let's say I give it labels now, and say, education, science, politics, three levels or something like that. And I basically say, is this text about education? Is it about science? More? Or is it about politics? But it has never been trained on any label data containing those three as the labels. The sentences with those as labels. Can it do that? It's sort of like a holy grail. It seems like how in the world would you do that? If you think about basic classifier training, you have to train with data, right? But you're saying that now we are not going to train it. Take a pre-trained model, which was doing something completely different. But can it solve this problem? And the magical thing is, quite often and more often than you think, it can solve the problem. Or at least it can solve the problem enough that instead of annotating it with labels, you can just review and correct the few mistakes isn't that a good thing it accelerates the process of data generation data preparation and that is when you can do it without ever showing the model or training the model on this task data data specific specific to this task, that is called zero-shot learning. Right? In NLP. Now, zero-shot learning has a bit of controversy attached to it, because people always used to say that even if you can do it, it can't be that good as actually doing the hard work of preparing label data and then passing through it. Things have improved. Today, when it came out, zero-shot learning's accuracy was sort of needed improvement. But today, as more and bigger models have come up, zero-shot learning seems to be doing quite well, acceptably well, and certainly as a preliminary step that reduces the quantity of work you have to do in creating level data by a huge amount. Now, sort of the middle ground between zero-shot learning and the traditional training of a classifier with lots of level data is to take a small collection of level data, few shots, just a few bits, and then do that. So how does zero-shot work? Can you imagine? How does it work? In the case of target variables, how does it work? Hint, think of what we have done in the last two labs sentence transformers what do they do what do sentence transformers do do semantics search and find the nearest embeddings so for example what could you do instead of treating the target space as labels, you could convert the labels into vectors, embedding vectors, isn't it? And now what you have to do is you let it classify. You just know that whatever it was trained on, where does it go, right? Where does, what it was trained with, the old, let's say the old labels for ABCD, there are many, many ways of doing it, but one way, ABCDEFG. Now you just map your ABCDEFGs to this, your vectors, your new vectors, like create their embeddings, create the embeddings of these vectors, these new labels, and basically match old labels to the new labels in a way. Am I making sense? You map it to the new labels. Now you're in a good shape. All you have to do is predict which of those, where does it go? So it's just a word mapping to see what happens. Yes, exactly. To see what. So I think that one set of things exactly yeah so you realize that at the end of the day the resolution of mysteries are very simple things once you get the concept of embedding you'll realize that vector spaces are magical things right especially with dot products but how would you do it so, say, if you had a pre-trained model? And then now you have a training model that's a command. No, you don't train it at all. You're taking that model, and you're saying, zero shot means no training. But I'm changing the target space. And what I'm going to do is I'm going to say, this is the label you have to produce. So now think internally what will happen internally the transformer will first predict into the original space from the original space labels it will map to these levels through an embedding because those labels and these levels are all projected into the embedding space and everything that we have done in the last two sessions comes into picture and you can map it. It's one way of doing it. They're mapping one language to another language. Vector embeddings, yeah. Approximately, right? You're just looking at the nearness. This label, politics, is close to which of the labels here? Right? That's all. That is it. Vector closeness, vector similarity. And so you can do zero-shot learning. Let us do these tasks. Yeah. I have one question. Not today. Don't even mention it. It's for the next class. We will do it next Saturday. One shot? One shot, few shot. No, we will do, but hold that thought. I said one question. Sorry, I'm not seeing the screen that what screen are you sharing is it the board or something now after all the conversation let's go into face library so oh i will give you guys these files but just listen to me and then immediately i'll release this file so i just realized so you have the semantic search already right let's take baby steps with hugging faces hugging face hugging face has made so what's the contribution that hugging face brings library brings it is first the aggregator of transformers pre-trained transformers. So when people talk about pre-training, remember that today we think of training as made up of two parts. You don't usually train a transformer from scratch because that would be horrendously expensive and you would need ginormous amounts of data. So what you do is you start with a transformer or a model that has been trained to something adjacent to your problem. is you start with a transformer or a model that has been trained to something adjacent to your problem. And you try to find the closest adjacency. Take that. So that model which has been trained is called pre-trained. And on Hugging Face, all those pre-trained models are published. Now your question is, how do you take one model? You can either just use it, because if you have an exact map to your task, just use it. And now the surprising thing is, you'll be in enterprise, especially in things like customer relationship management, sales, marketing, and customer service, and all of these domains, and enterprise, you know, error logs and things like that. It is surprising how often, already you're in a hugging phase you'll find something that pretty much works even without needing any further work. You just get a pre-trained model for what what you think is adjacent but it sort of works so well that you don't even bother further training it the further training and and refining it for your thing your problem domain with data label data for your things is called fine tuning are Are we together? So two steps, pre-training and fine tuning. And when you pre-train the model, just to now let's bring in some language that you find very common in the Hugging Face libraries, they will use a pre-train, right? When you say pre-train and you give the name, it is the namespace or the name by which that model is registered with hugging face are we together can you please one more so when you when you publish a model to the hugging face hub it's called the hub you will have to give it a unique name give it whatever you want to call it you want to call it the abracadabra you can call it the abracadabra and thereafter anybody who wants to use it will refer to it by abracadabra right so uh let's search whether abracadabra exists or not we'll find out so um so that's a convention but those models the presumption is they're pre-trained and they often use the word checkpoint like they ex some things that explicitly hope that you need to do further will you say that Those models, the presumption is they're pre-trained. And they often use the word checkpoint. Like some things that explicitly hope that you need to do further work. They just say that we have checkpointed this much data, you can make further progress with it. So you'll come across all of these notions of checkpointing and pre-training and so forth, but that should now be familiar to all of you. So let me take the simplest example, because think of it as sentiment analysis is the hello world of natural language processing. By the way, sentiment analysis even today doesn't quite get sarcasm, though it almost gets it now. The transformer models are getting smarter and smarter. A few years ago, the joke was, or the sarcasm was on sentiment analysis, that it fails on sarcasm, right? But now, well, all things grow, right? And this baby has grown up, all grown up now. So we'll see. see uh actually i didn't in this example i didn't do sarcasm so let's take it as a homework in the lunch try to create very sarcastic statements and see how well some of these things do so how does hugging face i'm using this as an example of hugging phrase so the value hugging phrase brings beyond just aggregating is an api that is so dead simple that for most tasks you can get done in like four or five lines of code. Imagine a vast amount of intellectual deep thinking, deep thoughts and research, mountains of research and gazillions of dollars in training these massive transformers, and you can use it just like that, with three, four lines of code. Isn't that amazing? That gives you the value of open communities. So this is it. So now we take the sentiment analysis, what you do is this line first line from transformers import pipeline so transformers is the library module in hugging phase guess what it is about it's this vast bundle of transformers right all of the transformers have been standardized into one particular way of usage standardized into one particular way of usage use the transformer library of hugging face and then it sort of has standardized on all the transformers properly or rather other standard if you create a transformer because hugging face is the standard there's a huge pressure to conform to the norms here right to do that and also it provides you a transparent layer over whether it's written in pytorch or whether it's written in tensorflow so it's like an app exchange for transformers absolutely what to what app exchanges for Salesforce or um the app store is for iPhone and and whatever except that it's free there's no charge right so now a pipeline is is quite literally what happens is when you do a pipeline is a function in the simplest form it's a function that takes the name of a task. Are we together? So more specifically, the first argument, as you know, in Python functions, you don't need to give what is the variable. But if you were to be more exact, you would have said task is equal to sentiment analysis. But you don't do that. Convention in Python is the first argument is not legal. But it is a good idea to name arguments if you want to because you're being very explicit so let us do that i added to it now this transformer we got what does it return to us it returned to us a classifier but there is a problem how did a classifier come across a transformer isn't a transformer meant for many use cases? For finding the hidden words, hidden masked words, for classifying, for completions, for whatnot. So a transformer is a building block, right? It's a building block. It is not a classifier. But what you need to do is you need to screw in a classifier at the tail end of it for it to be able to predict how do we classify typically in machine learning, it's your logistic function, softmax function or something, you need to screw in a softmax head or a logistic head at the end of it. Isn't it? So you're mapping it in that full thing of SAN. You're trying to give it an area that it has to search on, and that is the classification area. Oh no, no, forget that SAN thing, the search thing. We are not within that idiom anymore. So forget that metaphor for now. We are saying that when you do a complex task, there are many, many steps involved, right? Quite often. So it is not the work. It's just not that you throw a, the way the transformers are created, they're meant for many tasks. So they are basically incomplete objects. They called headless objects right because the purpose for which it will be used is not defined so you have to screw in so whenever you write a code whether it's a neural network or whatever remember what do you do at the end of it the last layer of a classifier is usually what a softmax or a logistic unit reviewing from your introduction to machine learning course it's all usually that right so what do you need to do transformer screener classify at the top of it the the thing so that is good now but here's the problem this transformer is expecting input as tokens who builds the? What you're sending is text. So you need a tokenizer. Right? So there are things you need to do before and after the transformer is the one but actually what happens is the tokenizer also you do with a transformer. Right? There is another transformer which is specifically trained to tokenize and detect parts of speech and do all sorts of fancy things and convert the word into a word vector embedding. There's tokenizing, embedding, and then that embedding goes into your transformer to all the attention and all of that. And out comes some logistic, you know, some energies, what I call voltages, in some sense. And those voltages must be softmaxed to produce your probabilities, which you need for the classifier. That's one way of thinking about it. So it is like a garland that you have made, which has many beads in it, right? a pipeline of more proactively it's a pipeline of many components and so what hugging faces has done it has given you two choices you can you want a bicycle i can just bring out a bicycle and give it to you or you can say i need a handlebar i need chains i need seat i frame, and then you sit and put it together. You get both the low-level approach and the high-level approach, right? So if your task is served by the high-level approach, just use it. And it is surprising, the last I checked today morning, there were at least 16 or 20 tasks that come. And in fact, 90% of your common tasks that you would use, they come already baked in. So somebody has already created the pipeline for you. Hugging Face Library has already created that pipeline of all the necessary components for you. The only thing you need to specify is the meat of the matter, which model you are using are we together and even then if you don't give it a name there'll be a default one that it will pick and just say ah this guy didn't give a name let me use this are we together but everything is pluggable you can change the tokenizer you can change the the model and you can change many many things along the pipeline. That's the nature of this. So let's start with the very basics, the hello world. We created a sentiment classifier. So using the pipeline function but the pipeline as you know underneath is that. Now if you look at this pipeline function in some sense it almost behaves like a constructor of a task like during object oriented programming you would think except that given a task argument it manages to construct a model model in the traditional machine learning sense it does it does a job now it is ready to serve inferences or do whatever you want it to do so now moving forward so and when you run it on your machine guys it will take a while because these models are oh but what's the size of some of these models big now a good sentence transformer is the distal bird so let me explain it says distal bird uncased, fine-tuned, sentiment to English. Like as cryptic words go, this takes a cake. So let's break it down now. Bird is bigger. And when you run bird, especially the bird base orbase or BERT-large, the two BERTs mentioned in the original paper, you start soon being troubled by the slowness of inferences. Or when you have a lot of inferences to make, throughput is low, and or the compute cost in GCP. So one of the things people did is they said, okay, can we take a bird, a much smaller model that is almost as good as bird, basically. So that architecture is the distal bird. And it's a boon for those of you who insist on doing machine learning on laptops, which in my mind is the definition of insanity. What is it? No, it's not done. At least connect to a workstation, a powerful workstation in your house. Use the laptop to connect to it or something. Like for example, where am I doing this? Oh, I'm using a laptop. Or am I? No, this laptop is actually connecting to... Look at this. Where is it connecting to? It connecting to the linux server there right so don't do that but anyway if you're doing it on small machines distalbert is pretty good base distalbert itself has various sizes of the model base will usually be the the default one now sometimes they also use the word small medium large right so base uncased not case sensitive training on the data fine-tuned fine-tune means not only has this model taken the normal training through all the corpus for all the you know wide variety of tasks it has been further fine-tuned with something called sst to english what could it be sentiments yeah and i suppose so i don't know exactly what is this to english is if i were to guess it is that they have only looked at english sentences sentiment sentences right uh possibly that could be it but uh enlighten me if you find the real meaning of it the rest of the phrase but that's that convention you'll see followed so now you'll see that for different models you will see these extensions used what is what is fine fine tuning is the second part so you take a transformer which is made for many tasks and then you fine tune it for your particular task and by giving it your data you give it further data for your task label data for your task and you continue the training right for the training so you know like you do you train and then you validate the models so you still need to do the same no here now what is happening is we are saying training is broken up into two parts pre-trained and you get pre-trained and then you have to. So it's somewhat like a door. You know, you go to Home Depot or Lowe's these days and you buy a door and the door has no color and you need to paint it. So think of it like that. It's more than that actually. You're actually giving it label data, You're further training it specifically to your task and in fine tuning the performance does fly up. So you have to do it. If you have data to fine tune it, do it. Good thing is it's not expensive. You can do it on reasonable hardware. So we did that. Now we can be more explicit. This is a best practice with Hugging Faces guys. It is always good to know which model you're using so you're not surprised. And when a better model comes up, you can plug in the name. So how would you know if a better model has come in? And this is part of the culture of hugging face. Hey, let's go and see how it is. Let me take something. What am I taking? Sentiment analysis here, somewhere here. Do I see a sentiment analysis? Feature. Let me. OK, we don't have to do that. We can go to sentiment analysis. sentiment right and they'll oh no it just came up with results if we just search for models filter by name or filter by task name so let's go to centi centi man well okay that's a classifier task so so it isn't specifically given. Let's filter by sentiment. Ah, look at this. We came up with 2,000 models that can do sentiments for us. Right? So why do you think there are 2,000 models? Surely they haven't invented 2,000 architectures. Because creating a new neural architecture, transformer architecture is painstaking. What they have done is they have fine tuned on specific data sets. So for example, a Twitter Roberta based sentiment, what does it, I'll tell you how to interpret it. They have taken a corpus of Twitter tweets and presumably labeled it. Then they have used the Roberta base model. Roberta is another bird variant, Roberta. And it's also defining quality is that it's relatively faster, smaller, big model. So you use that base and it has been fine-tuned for, like the fine-t tuning is done with the task of sentiment analysis. That's how to interpret it. So that was the data to find them. Yeah. Somebody sat and labeled it and then passed through it, took a Robita base and then fine tune Robita base to this. And so the model that you get it and people often use the word checkpointed or something, that model, they uploaded it here. But now suppose you want to go one step further. You want to only take Twitter tweets that talk about COVID, let us say. What would you do then? Twitter about COVID. Now you need to go get only covid specific data go label it and then further fine-tune it isn't it go through further training of this model and and you can go on and on you get the idea right yeah so it is also uh categorized to the best one in the top yeah so the sorting is most downloaded not necessarily the best so sometimes the best that one just came out for example he talked about the bloom for finance the financial yeah and we will use that actually the bloom sentiment and let me use bloom yeah see this is a bloom fine-tuning sentiment model with so many things, right? And if you look at this, it will talk about big signs on the, oh no, this is on the IBM website. There's a FinTech, this is Bloom and FinTech, yeah, you can't. So Asif, I have a question. Okay, we'll come to it. In a moment, we'll come to it. So why don't we go to a lab and you'll see this example. So you take one distilled bird, you give it a sentence. The NLP sessions have proved useful so far. I know that you all are violently disagreeing with it, but let's see. What does it say? It says that if somebody says this, the sentiment is pretty confident. It's a positive sentiment. Right? Well, you may say. Hello? Let's try a look. So I have to start from the beginning. Sorry. Look, can anyone hear me? We can hear you. Yeah. we can hear you yeah yeah I think after this not picking up the audio from this zoom let it come so one thing to be sure if you're going to do transformers. Oh, one second. Okay. So guys, two things you have to remember with transformers, good internet and good GPU. Right. So, which means that it's a good idea probably to buy Nvidia stocks, or maybe it's already going up too much. AMD, AMD is now coming up with a competition, I mean the thing which is pretty good. So guys let's look at this sentiment. It said, it still says positive. I think it's also sarcasm in the way you say it. Make it sarcastic let's try this right so by the way the same thing when I did a couple of years ago, a similar thing, it used to fail miserably. It wouldn't get it right. But now you can see that these models are evolving, isn't it? Let me say like, are you sure that these sessions? Yeah, why don't we do another one? Tell me the text. And then, guys, I apologize to those of you who are putting it on the chat. I'll come to you in a moment. What is the sentence you want to give? I was thinking, are you sure the NLP sessions have two minutes to one? Let's try this. Right, and then send him. Sure. You're questioning it's validity. I hope it's not shared quite widely. All right. Now let's take a few more. See, if you look at it, actually some of these, it's become so difficult to reach customer service for my product. Right? You're not using the word sucks or terrible or things like that. It's endless call waiting to begin with. gets that but then i put a phrase oh fraptious day kalukale right now do you notice that there is not a single word that makes sense except for oh right or day or day are the two fabulous glue clay are both cooked up words and yet it is able to give you a score of 99 positive why do you think it has done that because of the estimation sucks exclamation is that a really big assumption? Definitely. So you could. No. Because it's filling it in. Like it ignores the words that don't make sense. And then. Or day or day. No, no. So let me, let's try it. Let's try it with an example. Give me something that you think is, again, if you just did all day and see what happens. I will comment this one out. It's positive, right? But the score is different, very high confidence level. Suppose I say, see, just exclamation mark it still says positive just an exclamation mark You can see there's a bit of a limitation here. It's not, should be not positive. So what can we do with this model? You can find them. No, this is a very basic model. Distilbert is a very basic model. Replace it with, and one of the lessons we are learning with transformers are bigger is better right use one of the larger language methods and so i leave this as an exercise for lunch one of you find which of these models you like in this it's a download but fortunately we have a gigabit fiber here try it wi-fi might not help you so you might want to physically connect your laptop to this but by the way 500 mb is not a big deal you can download over the wi-fi also so but i try to create see i'm trying to create examples that will hurt let's take a fintech example you You create a text, a fintech one. This is the one, the process, FinBird. FinBird is a clue that it's a bird for finance. Typically, people when you see a slash, it means that process AI is some company or organization. organization and we use it to do a text on this the trading is very bearish today the then today the market rallied and nasdaq gained but not much i'm deliberately creating things that are a little bit ambiguous it will come out and say the first one is neutral and the second is a bit positive. It is positive. Then why the trading is very bearish today? Why is it a neutral statement? Because the bears win. The bears win. Remember one man's good news is another man's bad news in the financial market. Whenever there's a buying and selling of stock, one believes it's a good time to buy another is sure it's a good time to sell. So why don't you say good? Let's try that. So when should we fall in? Let's try that. It should be possible. We'll find out. Oh, I have to first run this. It's very simple. Just a few lines of code. That is it. That is why it's literally the hello world of NLP. So anyway, while this comes comes through let me show you the result because it has run on the other server if you give it to the normal uh sentence classifier that we just created which is not fine-tuned on financial data it certainly gets it wrong when it says it it's very bearish today it gives it a positive score it is not a positive news it's just you know winners and losers statement the other one is statement today the market rallied and nasdaq gained but not much it says it's a very negative statement would you be unhappy if you had nasdaq stocks or nasdaq index funds and market value you know it gained a little bit you wouldn't be unhappy right unless you were greedy right so uh so that's the that's the value of pre-training it so now i'm going to move a little bit faster but before i do that let me take questions from the chat um is there could you mention what are the questions actually the question will be on slack oh on slack and what are the questions actually the question will answer oh on slack and what are they okay will it ask okay actually wanted to show them all right by the way i'm not muting anybody from my side unless, you know, it's creating a negative feedback. So please unmute yourself and share your screen. Can you hear me? How does it? A pre-trained model. Yeah. So you have a pre-trained model. Yeah, so you have a pre-trained model. Remember, it can do a generic set of tasks or maybe pre-trained for your, when you give it more data, it is like running a few more epochs of training. What do you do when you train a transformer or a neural network? You take data, training data, and you run a few epochs through it but nothing prevents you from adding one more epoch at the end a few more epochs at the end with a different set of training data so what it will do it will shift the weights you know the weights are going to readjust and it will adapt to this but it will not go through radical transformation in the sense that there will be minor adaptations for this. Okay, what's the next question? Can you see my screen? Okay, let's look at, let's look at Kashi's demo. Can you speak up? Try to say because I feel that is there anybody else who is having this problem remotely? they can't speak or did i do something here can you can someone on zoom hear me oh i reduced my volume sorry hello yes yeah yes we can hear you i think it's uh asif's okay can also hear me oh okay i can't hear you're on mute now all right now we should be having both ways okay perfect okay so i ended up using the corpus and like the data set that you had given so i got it to work in streamIt. It took a lot of time. There was a lot of issues on Mac. So I did all the ones that you had shown us. So for a text to image, that can just be anything. So it generates that and I set K equal to five. So it showcases just five images, but you can always change that you're right to see more um and then even if you were to do something else it it works so that's the text to image search and then try a lion on the mountain it got lion it got mountain but the first one it got well a bit of exaggeration to call it a mountain but yes definitely okay so that was text to image and then my text file to text I ended up uploading a text file of the book the power of now just to see but I need to do some transformations to the text file because when I run some text so this book is if uh what's I'll just do something good for you something like that um because the file the text was a little weird it's not getting the complete sentences so i just need to do a little bit of data cleaning and i think it'll be able to yeah so it's getting some of the headings and it's not completing the sentence because the line is breaking in the text file. So I think I need to do some data cleaning. And I think this will work properly after I clean the file. Yes, I also think so. See, data preparation is a large part of it. When you take a book, a text version of a book, you have to know that there's a lot of things to subtract or elate from it. And much of the data preparation when you deal with the like for example uh gutenberg which has the largest collection of free books they give you the text file first like 5000 words are just the copyrights and the preface and all of that table of content you have to sort of eliminate that. Yeah. One way you could do it is, you know, it's bringing up the chapter headings. Right. It is. You could use the regular expression wherever you see everything in caps. Regex. Okay. Yeah. Okay. Good idea. I'll definitely add that right during break. So, yeah, I just have to work on the data cleaning, but I think this is working fine. Once I then I told his book isn't it? Yeah, that's right. That's right. And then for image to image, just from what we had done in class, similar to that. Oh, wait, let me do a different picture actually um okay maybe of the moon oh it comes up a lot of pictures of the moon yeah so it's from that same data set you would use with the 25k unsplash um file, really good. And yeah, this text one, I didn't use the ANN one yet, but I'll add that in. I was just trying to get Streamlit to work, so it was a little sometime. But yeah, thank you. This is very good. This is very good work. Sorry, I had muted down my voice and then I couldn't hear you. No worries, thank you. All right, any other questions guys on the- 12.25 PM. Any more questions for you? No, that's it, that is it. Okay, so what we will do now is move a little bit faster now that we know this example. Hey, I think Haripriya has a question for you. Go ahead. now that we know this example go ahead go ahead Haripriya has a question oh Haripriya are you there I think she stepped out yeah maybe she stepped out so guys you now know that we can do Yeah, maybe she's stuck. Okay. So, guys, you now know that we can do things. What can we do? Let me show you some examples that may, you may say sentence analysis, blah, but if you have sentence analysis, you can, I'll give you a homework. This is your homework. Play with the Hugging Faces textifier you re how tough will it be one word change instead of sentiment analysis your text your text will your task will say classifier right and then you would give it the labels and whatnot right so do that homework is use the hugging face text classifier. That is a little, very short homework. Do it on your own. But I wanted to show you something that is a little bit more impressive, which is this notion that you could... You're seeing my, oh my goodness. Okay. Hopefully this is better. Yeah. Yeah. So these are called the zero-shot classifiers. I explained it to you, right? Generally, you tend to take slightly bigger models to make it work. They have to have learned a lot generally. And the bigger the model, the better they tend to get. So I'm saying there are three texts that I'll give. And you have to classify the three sentences into one of the three labels, economics, politics, science. You could give more sentences, guys. If you can think of other sentences i'm really happy to add so you know you see the magical thing not only are you giving it the input you are specifying the target space you normally don't do that with a with a model classifier at inference time it It's non-standard. So you say, how in the world? You seem to be doing both training no training directly inference. That's why the word zero shot. No. But can it do that? It turns out that it does it. So it says this is a fear. I think there should have been there is a fear. But again, this is a fear that we may be heading into a global recession in 2023. So it says it's about economics and not much about science or politics. Would you agree? The next one I said, a free and fair voting and voter participation is the foundation of democracy. It says politics. Then I said heavy objects deform the space time fabric around them, leading to the gravitation force. is obviously Einstein's theory of relativity that the heavier you are the more you distort the space time around you and then things that pass by they tend to go through the bump that deformed space and it looks as though you are attracting it right you're pulling it so well science yes if you're you're attracting all of us to you yeah I can see I'm pretty heavyweight you're attracting all of us to you. Yeah, I can see I'm pretty heavyweight. Well, I'm trying to be lighter, create less deformations in space-time. All right. Another homework is try to replace it with other models, guys, with Hugging Face you have to play. People have created an ecosystem of so many models do you notice how many models are there 166 000 and every day it keeps on increasing right we'll do a few more examples guys i hope this was illustrative I hope this was illustrative. The third one is, fourth one is text generation. Let's try text generation or continuation, you can say. So what is the task? Text generation is a task. I say the happy cow jumped over the moon. And the output by default, the default model produces the output that says the happy cow jumped over the moon as we approach the place of our meeting the young beautiful woman stepped forward and asked me when what i called me to my i don't know what junk it produced okay this this seems to have produced some junk. Then what happens if you use different models, explicitly GPT-2, Distilled GPT and Facebook. Facebook opt is a more recent model. Let's give it a code to continue beyond the code. Now, anybody remembers this code? Anybody has seen this before? It's on the wall. On the wall in the back of the room. Yeah, it's in the wall in the back of the room. So you ask it to go generate, I don't know, this one is almost offensive what it produced. So this is the risk guys, you know. It is a risk with text generation. You don't know it may produce socially offensive statements. And GPT-2 says education is not the filling of a pail, but the lighting of a fire. Then it goes into that, as well as the fact that the New York Times wrote about the event. There's also an online petition of 1,000 signatures. And even if you disagree with the reporting, there's also an online petition of 1000 signatures. And even if you disagree with the reporting, please share. Is it at all relevant? It's nonsense. That was GPT-2, guys. GPT-3 was already better. And GPT-4 is what your chart GPT is, right? So how much difference has happened? Destelbert says says but you could even imagine a world for an ordinary person at least a year and a half from where our lives are not in any way threatened or destructive in that it was a good time to let us know that we are in peace even if you could understand it it doesn't make sense? It has no relationship to that. What does Facebook do, OptModel do? It somehow brings in Mark Twain, right? The first thing I learned in kindergarten is that the world is a big place. The second thing I learned was that the world is a big place. Whose quote is that? Mark Twain, possibly. It says so. No no like the first one uh it it actually belongs to a greek philosopher named plutarch okay but it is wrongly attributed to wb eats of england okay why be so why be eats of uh it's of england most people think it's aids who said it but actually it was plutarch of ancient priests who said it so what is the state of the art on these topics now now comes and this is the problem guys we are facing a dilemma these big models people are claiming you can't put it on your laptop you say well i have a big have a big laptop, a big machine. Then also they're hesitant. GPT-3 can now be accessed, I suppose it's been. But GPT-3.54, et cetera, they don't give it to you. So I'm looking at some of the state of the art models, chat GPT, cloud is from Anthropic, chat GPT is from, of course, OpenAI. Ask the same question I give them. I said generate more text to follow the sentence. This is the prompt I give. So chat GPT, which is literally GPT-4, in a sense there's more to it, but you know it's not just that. Remember now reinforcement learning has kicked in and other technologies have kicked in. It says in order to truly light a fire in a student's mind, educator must be willing to take risks and experiment with new teaching methods. This would involve incorporating... It looks far more relevant, doesn't it? And ultimately it goes on to explain, ultimately the goal of education should not be to simply fill a student's head with facts and figures, but to them become passionate engaged learners who are equipped with the skill and knowledge they need to succeed in an ever-changing world by lighting the fire of curiosity and imagination within each student educators can help them unlock their full potential and become lifelong learners who are eager to explore and discover new things. So chat GPD is just like a sentence transform. It's a pure text to text. It depends on the task. It was never trained for the task of images. It was trained only for text to text. Now I'm seeing a comment that you can train better than ChatGP. So we can give an image and then get another image. Yeah, yeah, no, see that brings in this whole question of chains of things. What you can do is ChatGPT can become just one member of a long chain of things you can do. So for example, you could be taking spoken audio, converting it to text, asking chat GPT to convert it into a sonnet. That's popcorn doing it. Creating a sonnet and have another transformer convert it into audio into a human voice of a child reciting that poem. You see how you can do that. So you can create long chains of things. But to like incorporate you have to use that you have to add the API in between so actually most of the startups that are happening around challenge is exactly that they're saying what chains can we form and do fun stuff with it right They're not used to modern. They have to. That's what Elon Musk was complaining about. Oh, me too. Now they've become greedy. They're anything but open. Right? Because Microsoft is coming. I think it's also because it's a good way to make money without having corporate standards. Because everyone's using chat, you can see. it's like one of the most popular ones like everyone uses in school college work right so it's like for them they're thinking how can i continue to make money that's why they have that whole like absolutely it's a financial model yes yes i'm sure there's also i can't keep thinking it's a bit of a microsoft thing 20 years ago they're supposedly their strategies don't know how if it is fair or not it used to be adopt extend extinguish right they would adopt a technology then create their own extension that nobody else could have and then finally just completely take over right and if needed as if it's sad that they are monetizing it uh for their own benefit and they are willing to kind of extend their model but then keep that information to themselves and not share it with the world so that nobody else is able to monetize so for the open AI they quickly realize that other people are also monetizing maybe not as big as a piece of the pie like they do but able to monetize and they don't don't want that which is ridiculous yeah see the community practices in the research output should be open applications and products built with it should be monetized but this time they have sort of crossed the line they are saying research output itself should be yeah we should monetize because so many applications can be built into it we see a possibility yeah we are not sure it's not a paper, it's like a white paper. Yeah, that's right. Let me see how it can be. It's a long bragging rights. So then, not many of you know, there's a company actually, Shalini here in our audience works there, a wonderful company called Anthropic. Some of the deepest thinkers in that are actually a few of the deepest thinkers that I know about neural networks I noticed and I was surprised that they have all moved to Anthropic. Pretty good guys. Google funded. Yes, Google funded. So they have their own chatbot. I put it to that and I would strongly suggest you guys go get an account at cloud. So here is what it said. In my view, it is, it is shorter but much more sort of relevant. Look at the continuation it says, education is not the filling of a pail with the lighting of a fire. A teacher does not simply pour information into students' minds to be stored and retrieved later. Real education sparks an inner passion for learning and curiosity of the world. That explains it in a way. So it follows the style. Say in English, there are many styles that you write paragraph. One of the styles is that you start with a sentence, the thesis thesis and the rest of the paragraph amplifies that thesis or elaborates on which is the way it took it fuels a lifelong love of discovery and blah blah i'll leave it to you folks to ponder over lunch if i have succeeded in being a skillful teacher but with that hey I said my comment about our attraction to you was not about your true mass but your the massive knowledge that you have yeah we want that to increase not decrease I know that's something and you don't worry it's all for fun okay but you know your teaching is amazing in a sense classrooms where you know they ask people to go uh just look at the powerpoints and do like a practical session but you're doing the combination, doing the thinking and the practical stuff. Great teaching. Oh, thank you. Thank you. This is my motto, actually. That's why it's there on the support vectors wall. I would love to be remembered as a person who lit a fire, who created, you know, at the end of our life, you know, we have built so many products that if you and I, as classmates now, 30 years of scientific and engineering training and work later, we have built all the great, you know, products that we wanted to build. Now is the time to build the next generation. Who will go and build the next products. I believe in that. Thank you for saying that. So guys, you see the possibilities and the limitations. Never, and by the way, this was very interesting. I had not noticed it, that the first one produced text that is actually objectionable. It's quite objectionable. It is so degrading for women. Hey, do we have this Jupyter notebook to modify? Yes, I'm giving my apologies. Like I said, let me show all of this. Within a minute of our breaking for lunch, it will all be on Slack. Thanks. I should have posted it before the class. I apologize. I didn't do that. But I'm on my last one, guys. Bear with me. But this one. Okay, not the last second last one, something called named entity recognition. See what happens is suppose you get a lot of text, and you have to say, who are all the people mentioned in the text? That's a tough problem. You have to inspect every word and say, is it a person? And then do it. So let us see what it does. That technology is called named entity recognition, person, place, location, entity, like a World Health Organization or something organization. These are the things that it extracts from text. And it has been getting surprisingly good. One of the years that I put it to in a previous project is I had to do data anonymization for very sensitive data. I had to pass people's performance reviews. You can imagine there's nothing more sensitive than performance reviews and surely as an engineer myself, I didn't want to see what the CEO wrote as performance review for the CTO who was my boss. But I still, the whole NLP, everybody's techs had to go through some NLP process, some task. So the whole thing was nuclear because if there was even the slightest leak of personally identifiable information or this sensitive information, I was like, pretty much we would have lost our jobs. Because you had to be extra, extra careful when you deal with data that is so radioactive. So getting PII out of text fields is easy. Make sure names are gone, emails are gone, this data gone. But when you talk of anonymity, you talk of things like many things. There's L diversity and what is that? K anonymity. Like are there at least K people who are exactly like this so that you don't know which of these K people it is and K should be sufficiently large. Right. So, for example, if you find that somebody doesn't have a boss, somebody doesn't have a performance review at all ever what can you compute see you right so you lose key anonymity there so things like that l diversity so those things he could enforce but then the problem was text because you're dealing with text and people tend to write people's names or peter good at this and Peter has this to do. That's a style of writing performance. If you just to emphasize that you're talk, you know who you're talking about. Yeah, you need to remove Peter from the equation. So what I did is named entity resolution to mask that. Before, it's a pre-processing step before it goes into the algorithm. So here we go. Peter Maxwell went to San Francisco. By the way, occasionally if you see an article written by Peter Maxwell, you can remember it's a code word. It's written by yours truly. It's one of the pen names I use sometimes. The other is ripe apricots. Some of you know about the ripe apricots, isn't it? So presented his paper at the Moscone Center. So look at this. What are the proper nouns here? What are the named entities? Peter Maxwell is one. San Francisco is the location. At the Moscone is it. Let's see if it got it right it says the first one it found is peter and it is pretty sure it is a person right second one maxwell also is pretty sure is a person then it says there is a word san it's a it's about location then Then Francisco is also about location. And MU, it sort of seems to have gotten wrong. It says it's a location, Oskone, because it broke Mosconi into two parts. And the Oskone part, no, San Francisco, Mosco, did I, Mosco, why did? So I don't know. Moscow. It has also marked as an organization. Ni, it has marked as an organization. So what can you infer sometimes? Because it uses word pieces. Right? Remember I talked about word pieces? Mosconi, isn't it Mosconi? Oh, that's the reason. Maybe that is the reason i got it wrong thank you musconi i stand corrected let's do it again so it'll take a little bit to happen but you can see so while this is happening let's go and look at something which i would like to end today with. So guys, today is Hugging Faces Baby Kindergarten Day. So these are simple exercises. The next time will be Hugging Faces Deep Dive Day. And then we'll be done with Hugging Faces and the last four sessions will give to fairly deep things, last four days will give to fairly deep things. Last four days will give to fairly deep things. In recently, there came a paper that completely, like two papers that in multimodal learning that were fairly transformative. One of them was Clip. You have encountered Clip. We used it in search. What it did is it could translate an image into caption, into text. And they call it zero-shot learning, but there's a caveat to it. They don't need level data simply because they scraped only those images of the web that had the caption written below it. And so they actually had level data by scraping the web. But it was zero-shot in the sense that they didn't have to throw a bunch of you know people and ask them to label it they then came but then right after that came another paper which was Dal-e I suppose it was a word game on wall-e remember wall-e movie right So it was Dal-E. And that paper starts with, and we'll discuss that paper in this course, not today perhaps, but maybe today, the architecture of that. What it does is you give it a textual prompt and it will render an image for it, however impossible that image will be. So for example, the paper asked for chair designed like an avocado. Now to my knowledge, no furniture manufacturer makes furnitures shaped like an avocado fruit, right? And yet it made perfectly good chair designs like an avocado. And we can look at the images. Likewise it created some daikon on a walk. I believe daikon in Hindi we call it the Muli right? On a walk and walking a dog. So imagine a daikon walking a dog. It perfectly nicely rendered a daikon walking a dog. You give it a, you just say a billboard, suppose you have a company, right? And you just started a company like I did, Support Vectors, but you want to create an impressive image for your website, so you have to look big. Right? So, you know, some people tell me that to be successful you have to look successful right so what you do is you you give it a prompt a high-rise glass building with the logo support vectors and lo and behold it will create a beautiful building impressive looking and put support vectors as a logo on it. This is in Delhi? In Delhi. So Delhi caught people's imaginations and a lot of people started doing it. Its open source versions came in and there was rapid progress. One of the key progress elements was stable diffusion, an approach which was actually radically different from dally then opening i pretty much adopted stable diffusion and made some small more training on it and some adaptations further work on it and they call it dally to a huge marketing school right and the stable diffusion people were really right anyway, the fact is there's a whole bunch of architectures and now there's a look into things that recent works like control net, et cetera. It makes it even more fine tuned on the prompts, respond to your prompts better. And there are entire companies like mid journey is one of them that for a paid service, you can give prompts and generate images and things came to pass that a come a one guy i think in wisconsin medicine or somewhere there there was an art competition and great artists would bring annual they would bring their big paintings and his painting was submitted and the critics absolutely raved about it. It's so great and this and that. Look how finely the details are and how well Masterstroke there is in doing this. Everyone agreed this is the best and it got the first prize. And then it turned out that it was a transformer that did it. But then starts a controversy, a big controversy in the art world. Is it legitimate or not? Is it creative work? He claims it is. Instead of being creative with the brush, which is now ancient, he had to work really, really, really hard creating the right prompts. It's not that easy to create that. We had to slog for days and days and days to create just the right prompt to create this beautiful painting to make computer produce this painting. Right? So it goes back to the debate, like if you create a picture using Photoshop, graphic, right? Is it art or not? Today, we agree that it is art is graphic art. It's not photograph, photograph, we expect some degree of it art or not? Today we agree that it is art, it's graphic art. It's not photograph. Photograph, we expect some degree of originality, even though there also a little bit of contrast and et cetera we allow for. But we don't allow for putting a sunrise behind a hill when there was no sunrise. That's not a photograph anymore. So that was graphic art we acknowledge. So there's a big debate now because all the actual artists are frightened. Because they feel that now AI is going to eat the lunch. And it is trained on their work, which is a big copyright issue. Thank you for bringing that up. The big thing is, transformers are mathematical objects they need a lot of training these things have soaked into the world work of 2000 years of art let's say and 2000 years of literature and that is why they are doing it so in a way they are sitting on the shoulders of work that's copyrighted and belongs to perhaps 100 million people. So therefore, who should they pay royalty to? To 100 million people? So what it means is that the legal framework, see, in the Western world, there has been the framework of patents and copyrights. It has been excellent. Part of the reason there is so much innovation in the West is because of the patent and the copyright laws. You have an incentive to invent, innovate, because your innovation is protected and you get rewarded for it. But if your innovations are not protected and your creative work is not protected, you lose the motivation. You can see countries in which copyrights are not respected are very uncreative countries. So it's a big debate actually. And so now with that background, let's look at an example. So there's some libraries you have to install. I gave simple prompts. One of them is, and this is literally an example that was online, image of a squirrel in Picasso style. Picasso was a great cubism, right? In a cubist style. Does it look like a squirrel in a Picasso style? Like Picasso would have done this, right? So play with it, guys. Say in the impressionistic style, right? Give something in the impressionistic style, et cetera. You'll be surprised what wonderful art exists on the web. And what they do is a lot of the people are monetizing it like this. They will create this service on the web, but it costs them because it is compute resources. What they do is they let you make the image free. They'll give you a low resolution version, but they'll say, if you need a high resolution printout right or print on a canvas we are giving you print on canvas or print on paper or print on metal and then the markup is huge you can do NFTs also and if it is also yeah that's a very interesting thought yes you could do that so the other one i said which one which i thought would be hard, I said monkey on a boat in a stormy sea. And it seems to have done that. Would you agree that this is a plausible picture of a monkey on a boat in a stormy sea? Yes, ask Mona Lisa by Mooney. No, go ahead. Can we ask you to draw Mona Lisa by Mooney, not Da Vinci? Oh, yes, yes, you could do that. You could do all of that. It would be fun to see. Yeah, go ahead. Text to image. Yeah, text to image. text image look at this code what did we do we said three lines get a model you give it a text and it produces an image all there are next two lines are meaningless because i'm just saving the image and displaying it you see how easy it is You see how easy it is? So this is on generative. Generative, yeah. Not the search, because the first one was search. Yeah, generative AI. So we are now moving into generative AI with this. Harini, go ahead. Asif? Yeah, go ahead. Yes, please go ahead, Harini. I'll ask later. Please go ahead. I think it was you, Mohsin. Okay. Go ahead. So okay so here you're importing from diffusers import diffuser uh pipeline that is also coming from hugging face it is all from hugging face okay thing is from hugging face got it so diffuser library is part of the hugging face for uh you know so people have uploaded all their libraries there right somebody has gone so far as to take really good scikit-learn models and post it to hugging faces now. There's a bit of an ethics and play here, right? AI has a huge ethical debate. You're absolutely right. Because human, like... because human like the simple classifier the simple things work through it through lunch take an hour lunch and one hour to do the lab because afternoon will be theory and I'm posting all of these notebooks before I forget I'll just slack and I'll pause the recording so welcome back guys after lunch now we are starting the second session of the saturday april 1st we are going to be, I missed a lab. So I'll do one final lab. And after that, we'll go into the bird, into the sentence bird Share. No, I'm sharing the new share. Ah, this hook is huge. Yes, much better. Hooks, are you seeing my screen? Yes. Do you want to record this? You're recording. Yes, I am recording. and I'll make sure it is. Yes, it has been recorded. All right, folks, I missed that may not have occurred to you. You can give it a picture. Let's say a picture like this. And now you can ask what is it? So then when it sees a picture like this, it will come up with and say it's a snow leopard. So it's somewhat very close to the search problem we did last time, find other pictures similar to it. But in this particular case, it comes back with the name of the object that it thinks it is with probability. So it's very, very, it's pretty clear that it's a snow leopard as far as it can make out. I suppose the snow is a giveaway. But you can do more than that. And here's, I think, where when you first see it, it is impressive. Now, the second part, you may or may not be able to run on your machine based on whether you have ubuntu or not i know that it works certainly on ubuntu and these are the instructions for installing the optical character recognition stuff in ubuntu similar instructions are available to ins for other operating systems and i'll put a link for that. But for now, just look at this picture. Do you see an invoice here? It's a scanned image of an invoice. And you could just, and pretty long invoice actually, thank you and all of that. And you ask this question, first question you ask, what is the invoice number right and it comes to the answer that it is us001 let's see if that is true yes it is it is that right here, US001. Another question, what is the invoice date? Invoice date here comes out to this, which doesn't look like a date. Now, can you guess what happened? Look at the date here. Invoice date. Yeah, the slashes got interpreted as one. Right? So there's a little imperfection here. But you could use a slightly more powerful optical character recognition thing, because it's an OCR thing it didn't get the slash right yeah you could ask what what is it built for actually it should be who is it built for who is it built for and it says it's built it sorry who is it built to it is built to John Smith it sorry who is it built to it is built to john smith right and likewise you can ask um other questions right what is the amount charged for labor and so on and so forth and it will answer i don't know like you i don't know if you guys were aware that these things can be done we are happy with it in documents we have this captured but there's a lot of discourse here okay now with this algorithm doing it all three yes no but see there is a there's a sort of an arbitrage thing right the information propagation is slow most people they don't know that the world has changed right so all the softwares are still doing pretty well Most people, they don't know that the world has changed. So all the software systems are doing pretty well. No one knows this. So this is also like using transformers? It's transformers, transformers all the way. Remember this entire course is Transformers. What exactly did you do? You landed this OCR, you downloaded this image into text. Into text. And then... On that text, how do you mean? Did you get multiple documents on that one image? Yeah, so what it does is, there are two aspects to it. One is that it's acting on the image. Like for example, this leopard, right? When you say, what is this? This is literally a transform, somewhat like clip architecture. It is basically trying to tell you what is the thing that you can most closely associate with this. So that is pure transforming. This image is a little bit more complex and we have to study it a bit. See this Tesseract Osea is an Osea library. So it is certainly doing some text extraction, but you have to know where the text extractions are we taking out the entire region but you can't do that because if you do the text extraction you don't know which value goes where right in the picture so suppose you say right the word who served the invoice who served the invoice? Who served the invoice? Let's try that and you'll see. And you notice that it isn't that easy, it isn't just OCR, because it has figured out that this is the company. So it's able to semantically place meanings to their locations in the image. So this is also doing like the it's creating the tensors also, yeah? Everything. So see guys, when you see this these are just some examples. Today were the easy examples. Next time will be more code. So this is open source, is that it? Everything, of course. I'm not teaching you anything that's closed source. There is another product where you can scan invoices and create an Excel sheet with all this information. Oh, easy. See, OCR, the standard OCR has been there for a long time with little value adds. If you know semantically this is this, this is this, this is this, you can do that. But now we are going to the next level. Because before it would like put build 2 and sit down so it will kind of do that mapping but this is going to be cool yeah you can literally just scan through like run through 200 documents and say okay what is the sum total of all the amounts wow okay all you have to do is batch it i mean mean, write that little bit. You get, it is not creating unnecessary data, it's just giving you one answer, 154 or 6, right? And then you can feed it to the next one. You can keep accumulating it. do the problem is all your examination can be the professor in the middle used to take him off a previous ta of this class somebody who was a part of the teaching style kunal lal some of you may remember so he's actually doing a startup exactly like this for a dockycombe what he does is when people come to the US immigrant, there's a lot of legal paperwork. So all of these documents exist. And so all you have to do is take pictures of all of them. They don't even have to be straight aligned or whatever, just snapshots and upload the whole directory of pictures. Don't even have to tell them this is the passport, this is the visa, this form this is that form and immediately it extracts without being told what is what and which field is where because it differs from country and so on and so forth it immediately extracts everything and creates a nice dashboard for you right for you it even knows which document it is okay so he's i mean people are actually forming companies on that now Right? A volume. It even knows which document it is. So he's, I mean, people are actually forming companies on that now. So all right, this is a big fun day. What are the things that transformers can do? But now I will change track and go into the, into understanding. So in theory, we have one sort of lecture behind. We have finished sentence BERT, semantic search, and so forth, and I mentioned that behind it are some crucial ideas. One of the big ideas is the sentence BERT idea. So I would like to do this paper for the remainder of the time in some detail, right? It's a very easy, relatively easy paper as papers go. So let me start on it. This paper came out in August 2019. So in a way, you can say that this technology has been around for almost four years. And so we'll go over it now carefully. Let me get rid of my keyboard. I will convert this screen to my writing, decrease it a little. So let's look at the first thing. It says in the abstract that Bert and Roberta, Roberta is a variant of Bert, have set a new state of the art performance on sentence pair regression tasks, like semantic textual similarity, like how similar are two sentences. You can do that. Why is it a regression task because similarity is a number when something predicts a number a model predicts a number it's a regression task isn't it by definition so uh this thing is often abbreviated as performance on, oops, sorry. What did I just do? Sorry. Yeah. And this. So this is the important thing, performance, state-of-the-art performance like this. So remember I was saying that the papers these days are very metrics-driven, state-of-the-art. It starts literally in the second line itself, the SOTA is there. So the problem with these networks is that, suppose as they say, so what he is saying is that finding this requires 50 million inferences. So if you want to find, suppose you have 10,000 statements and you want to sort sort them sort sentence pairs by similarity sentence similarities it's all right you can use word regression tasks for finding similarity and you have some way to do that you do it now how many pair comparisons will you have to do to find the five closest pairs let's say or something like that you'll have to find all combinations and sort them now Now, how many combinations there are? So that's a bit of combinatorics, right? So suppose there are n documents, right? Now, each document has to be compared to the other n minus 1 document, right? So you have to multiply it by n minus 1 and now the distance between a and b and b and a should be the same as similarity number should be the same so you don't have to do twice the comparison you just have to do half of it so this in other words this is n choose 2 now when you do 10k you realize that for N is equal to 10K, these comparisons, pairs, comparisons are 10, right? 1,000 times, well, 999, if you want to put it this way, divided by two. Isn't it? And that comes to, well, this one, forget N minus one, it's almost 10K. So 10K times 10K is 100 million. 100 million divided by two is 50 million. 100 million divided by two is 50 million. So at that time, the hardware used to be V100, was very expensive hardware, like data center class and so forth. And you would pay massive amounts to rent it or use it or buy it. On that it took 65 hours to train but now what what if you on the other hand took one query so let's say that you could want to create a search engine so you have a query sentence and you want to find the nearest match in the document so you have to now go and compare to 10 000 entries so you say well that should be fast right well it turns out that that wasn't fast either because that would take you again all things said and done uh to answer a question would take you 50 hours right pairwise comparison with work answering a single query would take 50 hours none of these are practical no. No, these are not practical. So people knew, as I said, that BERT is a great cross encoder. It can give you good answers, but it is way, way too expensive. So, and in a way, as I read through this statement, it is a review of what what you have already learned but what i've already been talking about but um we talk about it again now where did my mouse go sorry here it is now let's go here Let's go here. Also this sort of task, right? You can't use it for clustering and for things like that, because all it is doing is a pairwise comparisons and things like that. So you can't do that. And through, I mean, he's just saying, what are all the problems that you can solve and here there is such a thing in neural architectures those of you who took the deep learning workshop known called siamese network now siamese networks are exactly designed for this right what do they do so let's let's take a little primer on Siamese network. What a Siamese network would be, irrespective of what architecture you use, you pass a data through X, it produces some hidden representation of X. Right? You pass Y through it, actually is the same copy, you don't, even though I'm making it look as though it's different you basically you pass it y and what you do is h y right actually let me just say that xp i think the let me call it the positive p for a reason something reason, something, and same thing, a negative sample, and I'll explain what it means. It will create a H negative sample data. So imagine that it goes to some neural network, doesn't have to be transformers, any neural network, it will create a hidden representation and embedding out of it. Once it produces an embedding now comes what what can networks do what siamese networks can do is that you can now train a network in such a way you can create a loss function sorry this is a loss function in such a way that if p is close to x and n is unrelated to x then the distance some distance measure of x and p should be less than the distance measure from x and n would you agree isn't it so basically what we are saying is using these two, some distance, distance of hx, hp, it should be substantially less than distance from hx, h negative. That's why this is called the positive sample this is called the negative sample are we together and this can be done what you do is in this and this is often called Siamese network, these words you'll see together, Siamese network triplets and triplets to train it because how do you train these things? You give it not pair but you train it with, you take a reference image, so for example think of image, people, right? So you take a picture of a dog, a golden retriever, and then you put another picture of a golden retriever. That's a positive sample. And then you put a picture of a mountain. It's a negative picture, negative image. So you create a data set with whatever number of rows you have. Let's say 10,000 rows, 10 10 000 training data of positive negative and reference pictures the first one is called the reference image right and the point is p should be p should be close to in the embedding space would you agree that's that's the only way it will make sense that positive should be close to x negative should be far away okay and if we can do that if we can create a loss function right that leverages this and typically in this thing is the loss function is and we'll get to more precise definitions let me just call it hx hp minus the Hx H negative plus epsilon. You say that the loss between them can end the max of it, like the distances. Let's say that. Why will I say max? So let's let's parse it this way. First, let's look at this. You want this to be positive, isn't it? But what happens in the beginning when you're training, in the beginning of your training, your positive will go randomly somewhere compared, suppose x is here, p is somewhere and n is somewhere. It is easily possible that the distance to n is closer than the distance to p. So your value will be negative. But you need a positive loss value to be able to gradient descent. Otherwise, what will happen? If you try to minimize a function which is negative, it will run off to minus infinity. You don't want that. So what you do is you put a threshold at zero not only put a threshold at zero you do even better you say that if the distance so let us say that negative is greater than is less than positive distance sorry for distance positive distance is actually did i write it the other way around? The negative distance should be more. Yeah, the negative distance should be more than the positive distance. So max of that. Or positive minus. Okay, we'll see. I think I'm bumbling. I'm screwing up the formula. It's here. Let me see. I thought it was common sense. Yeah. Similarity. Oh, no, no, no. I'm using distance and they are using a similarity measure so in my in my distance term this is correct the distance of negative and positive and the converse of distance is similarity so then you would say um some people write it with distance and write it with similarity so then you would say that you can say similarity of x minus similarity to the positive sample right minus similarity and then the size this the size of this similarity and the size of negative similarity sorry negative sample right and you put a minimum threshold of epsilon that they should be at least this much apart right that is a that is the thing it's just a mathematical way uh you can twiddle around with it and you realize that yes this is what we want to this is here right so do we agree that this formula makes sense where we where are we ultimately if you have a triplet you want to make sure right that similarity right is closer this not to the positive samples yes right so and these are sentence embeddings so the distances are like this now comes the question so this explains this kind of image do you see that there are two networks this is one supposedly so in siamese network right they make it appear that there are two networks but actually there are no two networks there's a same network the way they say in the languages or in the in this paper it says you take two networks with the same shared weights shared weight means they are the same network you first pass in a then you pass in b you pass in x then you pass in the positive instance and then you pass in the negative instance right or you batch it all three and you get the results so then one now let me explain the pooling part here what is this pooling see what happens is you remember that in a sentence but you pass in the words right token t1 t2 and the embeddings ultimately what comes out output hidden one H2 HN right so they have multiple ways and then there is a special hindering for the HCLS. So the classifier, remember the initial token, special token. So some people, what they do is that they use, you need to pass in, suppose you write a classifier where the two sentences are similar or not, or something like that during the training process. You can. You need to pass in a vector, but which vector? So you can just give that special output for that, you know, the CLS token. Some people do that or some people, what they will do is they'll take the average of all the vectors, the output vectors. Right. So the average is called mean pooling. You take the mean of all the vectors the output vectors right so the average is called mean pooling you take the mean of all the vectors average generally this paper says that they experimented and they found that mean pooling works better so they use mean pooling and that vector goes into your the basic the rest of it is easy softmax classifier right you feed it into and how do you feed it into it you take this use pooled vector the so actually uh two two let's say u and v in this are the two sentence a becomes u sentence b becomes v as hidden representations after pooling average of their pooling and then what you do is you concatenate you do something funny funny. You take U, you put V, and you put U minus V, right? As three, you concatenate this three to create a three times as long vector, let's say. And then what you do is you feed it into, because then what will happen? You get the representation for sentence A, representation for sentence B, representation for sentence B, and a representation for their distance. And you feed that concatenated information into a softmax, and you say, well, classify. And by the way, usually when you do classification, you mean multi-class classification, like one of that. There's a dog, there's a cat, there is a bird, there's something else. When it is two, then it is implicit that if you have just a binary classifier, softmax could be replaced with a logistic or something like that if you want. So even though they are like showing two networks, it's like one network. It's actually one network in which you pass the data yeah you don't even pass data one after the other because you can batch you can set it in parallel you just shoot both of them through or in the triplet case you should all three of them so the problem is so what problem did it solve you still are giving it pairs no what you do is you are actually taking a training data which is made up of triplets somebody has sat down and said for I'll take this reference image and this is similar to it and this is dissimilar to it so a lot of work has gone in preparing the data but because it has gone in you don't have to do pairwise comparisons 50 million comparisons just this Samese network with this loss function is enough to train it are we together and that's what you do that is why sentence word gets trained very easily but once it is trained at inference time what will you do inference time is easy because you you can forget about all of this all you need to do is the thing that really mattered are these vectors u and v are the embeddings isn't it you have the network that produces u you have the network the same network bird plus pooling all the weights have adjusted themselves they'll produce you the u and the v those are your embedding vectors and if you want to see if two things sentences are how similar they are just go and do cosine similarity as simple as that so what do you save into your database in your index the u and v the vector database saves u and v sentence a goes in sentence b goes in u and v come out and you can send them in parallel in mini batches. Now there's a bit more to it. See, this kind of thinking is very useful when you try to do what is often called one-shot learning. So I'll explain that. See, one example that is often given is a very common example given. You have employees. For a given employee, you may have just one or two pictures in the employee database, HR database or security database, but the person walks up to a booth with a camera there. Now you need to determine, is this person, let's say that, is this person Mossman? So how would you train that? Normal classifiers would be trained by giving it lots of examples, Mossman and everything, every pairwise example it will be trained on. The trouble is you have only one picture of Mossman to compare to so you get one shot right so what you do is you train you take some images in the database and you train the siamese network you say that okay let's say that for for this image session this image is the reference image right I will take another image of Shashenda and say these are positive samples and every other image of the other employees and call it negative sample. And I will therefore create lots of triplets. All I need is two images of Shashenda and then sometimes what you do is you just do distortions or just flipping or something like that to create the other image. And sometimes what you do is you just do distortions, right? Or just flipping or something like that to create the other image. And then you pass it to this network and it gets strained. You see that, right? Yeah. And in fact, the second image may be just a distortion of the first, right? There's some weird thing of the first. So you can use Siamese because what you can do is you have the positive sample and therefore all the other n minus one employees, they can help create the negative samples. And then you can pass it to the Siamese network. And basically you can have your cake and eat it too. Like typically in a classifier, if you want to tell which animal it is or which employee it is, you would ask for lots of images of the employee, lots of images of a bird, right? But not now. That is the power of this Siamese networks. It's a very powerful architecture. I won't go much more into it. Suffice it to say that it is one of the great powerful architectures. In the deep learning course that you will do, the foundation course, we will go through systematically some of the big architectures. And Siamese architectures is one of them, along with autoencoders and GANs and attentions and all of that and CNNs and so forth. It's one of the big, big, we'll do it in considerable detail. But considering that we will do it in considerable detail, considering that we will do it in considerable detail i won't go in here so suppose you have a classifier go ahead oh yeah oh obviously but today so that brings us GAN and you'll see that when I talk about GANs in the deep learning courses, it is shocking. I'll tell you how it is. At one time, I think it was Lex Friedman who started his AI course in MIT by doing this video. He just showed that we have a special guest, right? But he couldn't be here. So we had to make a video recording to introduce this course. So there was a video of Obama welcoming everybody to the class, right? With completely his mannerisms and everything. And everybody got fooled it was so photoreal some absolutely realistic it captured his accent it captured his facial expressions and everything and the whole video was generated and then the next part he showed is how it was generated somebody is speaking it is going through style transfer, it's going through a process of Obama-izing this voice, imposing it on the picture, and creating the facial gestures that would go along with it. And for that, to train the data, you need sufficiently long video and audio footage of Obama. Obviously, Obama's permission was taken to do this. That was then. Today, we now know that the state of the art has advanced so much that with just a tiny clip of you, just one or two pictures of you and tiny clip of you we can create any fake video any fake image you're doing whatever saying whatever in your voice right yeah come again no no yeah but see the the thing is people are not realizing how far the field has gone. And it's almost like you know you have animals in the forest, and they don't know that there are highways now there this land has been in, you know, highways have been laid. And so they're suddenly caught dead in the middle of the highway with vehicles going around that's a that's what is happening to people now they're believing see disinformation is spreading so fast there used to be a greek saying that a falsehood goes around the world in the time truth ties its shoelace right that fact has always remained true why because disinformation is always sensational right so suppose i just told you let's take an example like today it's a normal day right i'm there my wife is there and today it's a normal day right i'm there my wife is there and actually today happens to be a special day my daughter is coming one of my daughters but let's say it wasn't true we were just living a normal and somebody says yes if you know right that your daughter has just come home would be would would my first reaction be to doubt it no so why would somebody say such a thing unless it were true i would immediately get up and go running home yay my daughter has come isn't it you see that so the fact that it evokes strong reactions from us is the fatal flaw in human beings all misinformation is designed in such a way that it is the is novelty factor hooks us. And so you can go on creating a lot of disinformation and disinformation is winning these days is very hard. The old, like if you go back and read the literature, the newspapers also 20, 30 years ago, they had a very measured scientific tone, right? Now you look at it and everything is trying to capture audience and attention, and even the professional newspapers and journals and newscasters, they are getting trying to be sensational. Even weather has become a big thing, like all these atmospheric rivers coming along, sometimes there's a huge storm coming and lots of damage will be there and you go out there and nothing happens right like it barely drizzles right but they'll always exaggerate on the side they will they will always you know err on the side of exaggeration rather than saying that you know there is a possibility that it may happen. A smaller possibility, more likely it will be a drizzle. They won't say that. They'll say, it can happen. Tomorrow can be a big storm. Well, anything can happen, isn't it? That's the how much and all the insurance . That's right. That's the last thing we expect. That's right. So people don't distinguish between the possible and the probable. So whenever they hear something, the question is, is it possible? And if the answer is yes, they believe it. Whereas the question they should ask is what is more probable? To think in terms of probability is not there not there in human nature whereas that is what is needed possibility for example there could be angels sitting next to me you just don't see them and they are giving me all the ideas that i'm talking about but is it probable if nothing else who knows i mean i don't think so but because there's no physical evidence of that and occam's razor principle says that it is not necessary to postulate the existence of angels around me right common sense thing is i read the paper and i'm explaining it to you right so that's how it goes i, that's the nature of disinformation. Anyway, that's a digression. How is it that since you brought the GAN, has the GAN related to the brainstorming? Let's say that GAN was invented in 2014. And actually one of the co-authors of GAN happens to be my cousin yeah along with all these big guys so 2014 and it took the world by storm after that fake deep fakes deep fakes fake images became but at that time again the first original GANs were very unstable they became far more stable with the coming of variational GANs and then the Wasserstein GANs and GANs itself have been evolving transformers right and GANs and then the Wasserstein GANs and GANs itself have been evolving. Transformers, right, and GANs. Oh, goodness. So when you come across, hold your thought, when you come across this neural architecture site and you see how powerful each of these neural architectures are in their own unique way, you'll realize how much the world has changed. So for example, if you're a developer writing code, forget about it. Like I think Shan was here today. He was saying, I didn't remember that, but he said that three or four years ago, I had said that the world will be completely different in five years. It'll be hard to recognize. AI would have eaten the world for lunch. I said it would happen in five years. Three years later, you have no doubt that AI is eating the world for lunch. So change is everywhere. So I don't know. So if you like AI, there is actually a TV show. And usually I don't watch TV show, but I like the ones that are about science and math and AI so the three TV shows are there one is Bones right it's about a it's it's sort of a fictionalized version of the life of a great forensic anthropologist who was partly, I believe, on the spectrum perhaps. So that I love, you can try it out. The other one is called numbers. You have seen big bad of numbers. Yes, absolutely. It's actually about use of machine learning quite a bit. Like each episode, you know, some crime or some mystery is solved using machine learning numbers. And the third one that I like is a person of interest. Right? So the person of interest is somebody has created an AGI. Right? And, but people don't know. So the creator goes about looking at the world and says, you know, the milkman goes doing his job and everybody is doing that. What they don't know is that the world has changed, right? The underlying world has changed. It looks the same, but it isn't the same, it has changed. So well, that's AI, guys. Anyway, now comes this one little equation that we missed. The only thing that we missed, this equation. So let's look into that what are we doing we concatenated the u v the embedding vectors and their difference together to create a long concatenated vector we want to pass it into softmax to get your voltages your probabilities right but there is it's just a thing that because right at that moment, you can put one more layer of weights to train on. Raja Ayyanar? Like that what linear transformation of this vector should you actually give to softmax instead of giving it the vector itself. Raja Ayyanar? So it's one extra bit i noticed that that's not they had put in and it must be for good reason they must have noticed uh right uh benefits from it so how do you discover that you should also multiply you know at the back of your mind that you can like you know these weight matrices right they're like peanut butter and jelly right so you can keep layering it you can just put it as sandwich between any layer that you see in your network seriously right the only question is to put it or not uh that's my intuitive picture and to some extent the dense layers feed fully connected is also like that they're like peanut butter and jelly just wherever you just layer it in and see does it get tastier or is your network doing better? But I mean obviously when you do it you can say in hindsight of course you should have done it. Why lose that degree of freedom? Because if it is not necessary it will be a diagonal matrix. It will train itself into a diagonal matrix. But of course it won't. So they must have found it useful. So this is it. The only math in this entire paper was this bit. And you know this. And the cosine part makes sense during inference time, given two vectors, similarity is cosine, of course, right? So then, of course, you have to see how well it beats the state of the art. And you can see that it does. The s-birds, like you notice that if you take, so these are various NLP tasks and corpus or data sets on which you benchmark. And so everywhere, you notice that the biggest number is notice that the biggest number is this this this this this this this this actually i should use a this is too thick a pen so i should just say this this right so you notice that how many of these are there one two three four five six seven eight nine so one two three four five so there are five other neural arc like transformer architectures there right except for one this one right in its very first paper like in its very first paper, like in its very first version, the sentence but beats those models hands down, right? It takes six of the gold medals out of the seven. And what is this STS-3? These are like a text similarity measures, tasks, and there are data sets on which you try it out. Benchmarks. So, you know, if you want to compare with somebody, you have to do apples to apples comparison. So you do it on those data sets. So isn't it impressive? It takes away. It's just like the BERT paper itself. It came and took away. So there's a bit of history to it. Like when the sentence BERT came out, it took people a little while to realize that what had happened and that what it is doing to search. First, they realized that goodness, now we can do AI-based meaningful search. Search was still possible using the cross encoder, the BERT itself, but it was horrendously slow and not feasible. still possible using the cross encoder the bird itself but it was horrendously slow and not feasible possible is not feasible but this made it feasible now you hybridize it with the approximate nearest neighbor algorithm and it becomes really possible right and then you tie it and then you can say that we can always have improve the accuracy in the final stage by bringing in a cross encode if you want to is it being used you bet I I'm under ND I can't talk about it but let me just say that some of the biggest biggest social media companies are using this while the rest of you are using Elastic Search elastic search so the difference between s but and the s the word oh but so what the thing is you take bird right see here's the thing uh this is at the beginning no no this is it see here you made a siamese network what are the things there is a there is a transformer here followed by pooling and then the rest of it which transformer you used if you use bird it is s bird if you use roberta it is s roberta you got the idea that is all it is because you still have the choice which transformer you'll use you can use anything and now people are so the latest ones actually use different they're even better ones right now if you remember that uh both a bird and those birds have been trained on certain specific tasks so when they are trained first on the analyte task then you say okay um that large small whatever the size is so if you remove this now observe if you forget about in this table, forget about the birds, S birds, sentence birds. Look at this. The first benchmark puts the universal sentence encoder as big. Isn't it? But how much is the difference? There's a 10 point difference in score. 10 point difference in score those differences don't just easily come because universal sentence encoder i remember when it would come out it was considered a big advance on on everything because if you look at the column everything is smaller than that by a margin right then if you use the you look at the next benchmark. Once again, universal sentence encoder is doing very well. And the infosense, GLOVE. So you just use GLOVE embeddings and you find similarity. What was GLOVE embeddings? I told you that before people started doing context-aware embeddings, they were doing just training a neural network, not transformers, no attention, just a deep neural network. Take a feed forward network and train it to produce embeddings, but it's a static embedding. One word will have only one embedding. So it is word meaning. So you could do king minus man, plus woman is queen, that sort of thing. So at the level of that, the trouble is words have multiple meanings. So GloVe was one of the better ones, one of the best ones for those static word embeddings. And their very nature was that they were context unaware. They were global embeddings. And so if you look at it, for this task, universal sentence encoder was actually not doing much better than just a simple sentence inference using glove right now here also this universal sentence encoder is still the state of the art for most of those tasks and this thing dethroned it sentence word dethroned it for all tasks except the sikash right so it is worth if you want to explore find out what the state is today what knowing where are we what are those numbers yeah and once again you notice that if you look at this thing, this column, sorry, this column, you can clearly see that the winners quite not trained for STS, etc, etc. Always the scores are high for these models. I won't go into the specifics of the benchmark it isn't important why doesn't people in enterprise use this see frankly right enterprise well i'm going to say something controversial let me stop recording so guys this is sentence but I was thinking that today I could do one more architecture. I would do but instead I'll plant some ideas into your head. Some thought questions. The third question is, think about the, see from words, like you think about how DALI could be done without reading the paper of DALI. How can you convert text into images? Right? So I told you that the other part of the journey is there. Let's say that image, because on the web, you can scrape captions from images. No. So you have some way of clip. Clip will convert, the clip architecture, we'll do that next time, will convert an image to text. But how will you know? So let's think through some of the steps by which you'll convert a text to an image. You can't reverse it, if you reverse it. So suppose I have a text, and all you do is find the closest caption in the embedding space. And then next to it, you find the image image you will only end up regurgitating images that exist isn't it from a repository of images so that becomes nothing but image search using voice prompt isn't it and in fact we saw that we we did that in the sentence but but now think and i invite you to think don't don't look at the daily and all of that yet invite you to think, don't look at the Dali and all of that yet. Invite you to think. Up to clip I've given you, up to sentence, but you know the embeddings, some way of creating hidden representations. Whether you use sentence, whether you use something else, doesn't matter. Some embeddings. Words can become embeddings. Images can become captions and therefore they can become embeddings. So what do you need to do now? You have a matrix. What can you do? You can project both of them, let's say in a embedding space, all the captions and so forth. So now I can take a text prompt. I can find the images that are sort of where, you know, the neighborhood in the vector space where all of those, you know, the images whose caption embedding are nearby. Isn't it? Wherever I go around it, there are some images whose embeddings are nearby. I can pick some of those images. And now what do I need to do? Merge them. Somehow, exactly. Somehow, I need to create something. Now, merge them, I can do with Photoshop, but that's not a good idea because that would be layers. So what I could do, and at this moment, think through some ideas. I'll give you a hint. Imagine you started with Gaussian noise, just noise. How would you know that from noise? So let me make it even like this. Suppose this is an image, so imagine a sphere, globe, and you know that your prom has taken you to this region of the globe, and you know that your prom has taken you to this region of the globe, on the surface of the globe. There are all of these images there, but it doesn't matter. Now imagine that noise is at the center. I mean, it's at the core of it because it's not going in any direction, let's say. And you generate an image in the beginning that is just noise. No, no. So what can you do to the noise? in the beginning that is just noise so what can you do to the noise right that will take you a little bit closer to that region so what you need to do is you just make small perturbations to the noise right and check whether you have gotten the the embedding of the noise has in any way come to that neighborhood closer to the neighborhood not to any one picture but closer to the neighborhood then you make further perturbations on it and further perturbations on it right I mean these are just ideas I mean I'll explain the stability and all these algorithms in detail but very hand waving some approaches could be, and not very efficient, that you randomly make perturbations. Right? And then you keep picking, let's say that as a selection, you pick those perturbations that took you closer, then you perturbate those perturbations. Right? Right? And so it's a process of like, that's why you use the word diffusion in these words. So you keep perturbating till you end up in the neighborhood. But remember, you don't end up on an image because other images are pulling you away. The embedding vectors of other images are pulling you away. So you will take some trajectory and land somewhere, right? Such that you are in that neighborhood. But your picture doesn't look like noise anymore. It has taken shape. You would start doing something, some line, something, or the curves will start forming. Isn't it? So you are merging parts of different neighborhood images and getting new images. No, no, no. You are taking a random image. You're not looking at this you're basically saying let's make a perturbation to this you do something right you put some random structure on it and you see have i moved closer to this neighborhood right then you make more perturbations keep making these perturbations, right? And doing this, and then you come very close to that. And after a little while, your loss function is not going down, right? Because you're basically sitting in the region, in the center of those image embeddings. Whatever this shape looks like, is there any reason to doubt that it is the composite of those things? So now we can evolve this idea. It is only in the latent embedding space. No, no, see, think about it this way. You take an image. So here is the embedding space. In this embedding space, what you do is you have a text prompt. Now, the text prompt has put you here somewhere. You know that around it, let's say that there's a dog sitting up on a table. It has put you here. You need to get there, but how do you get there? You have an image which is complete noise. So your text embedding is here. You do small perturbations of it. You make some random mutations or perturbations on this image. Look at its embedding. Its embedding is not in the center of this area. It is zigzag somewhere, right somewhere right you say did it zigzag the other way around or this way right if it this way but right it will make some random but just this way this way means may not be a start by it may go this way right but it's still closer then you make some other perturbations to it it's like oh little closer let's keep this right so what will happen you you are not you don't know where this is. All you know is wherever this is, my distance to this is getting less. You're seeing this. So you are just making perturbations to this image and asking, hey, am I getting there? Am I getting there? And suppose you were to keep doing this hand-wavingly. So this is a hand-waving argument what do you think will happen two things will happen you will have infinitely many paths to that trajectory and also when you reach that region there will be many many points many many points that have the same sort of a image that many images because you have taken many paths see every path is different and so every path puts a different structure on your image perturbation image right so what is that because an image is a composite of all the perturbations and all the perturbations are different cumulative paths are different every time you'll come up with a different image is it giving you some sense of how it works? No, from text to image, the reverse part of it. How are we generating text to image from it? So take this idea now and obviously I've given a very sort of a hand-waving. So I'll tell you something that I learned in grad school. You would read a paper and this theorem, corollary, lemma. So then I would talk to a friend and the friend would say, no, wait a minute. He would give a complete hand-waving argument in the air. And you know that is imprecise and so on and so forth but you know it has landed you in the right spot right after that refinements are there you can make it more precise then you start reading the paper it all makes sense right so you should always understand these concepts like that start with something you know intuitive feel for it let it be partially wrong then you get it but you see how you can approach this class of problems. So that makes text to image possible and you can generate infinitely many images out of a text prompt. Oh yes, yes, yes, you do. I mean gradient descent for the neural networks is a given because you create a loss function which is the distance from here to there and you gradient descent it. So you will make, but see sometimes what happens is if you want to keep randomness there, say first of all gradient stochastic gradient descent anyway has randomness, zigzag path built in, but with stable diffusion there's a little bit more technology to it there's some deliberation to it and we'll come to that yeah it's a very good deep ideas fun ideas and that brings us so you notice guys that this time when we are doing nlp for those of you i'm focusing a lot on generative ai yeah And in the deep learning course also, it will be very heavily generative AI. Because between, see, it used to be that machine learning was mostly discriminative machine learning in the sense that descriptive models are that tell cat versus dog, or inference, or how much revenue will your shop earn next year, number, prediction prediction regression problems. Those are discriminated. Discriminated means based on certain input data, what is your prediction of the label based on this. But so you find the conditional probability. Whereas the generative models from probability theory take a different point of view. They say that including the target space and the inputs, what is the probability distribution? Because if I know the full probability distribution, I can just cherry pick a point from a region and say how probable it is that this is a cat, right? So suppose the probability of it being a cat is very, very low out there, right? Or dog or whatever it is. So now what I can do, I have a full probability map. I can randomly keep sampling it. So I can now generate input data, right? Because I don't need only the data that was there. I can start generating. If I have learned the full probability distribution, hey, there are infinitely many data points I can pick out and say, hey, this is X, this is Y. So in recent years, GAN really made the deepfakes and all this there. So you could see the generative thinking taking off. And after that, recently, generative AI, I mean, it's sort of the buzzword. People don't seem to be, if you say that you do machine learning, they will glaze over it. If you say you do data science, some old stuff, right? If you say I'm doing AI, oh, pass. If you're doing generative AI, three VCs are running after you.