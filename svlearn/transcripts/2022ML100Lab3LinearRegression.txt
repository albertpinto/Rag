 All right folks, welcome to the third lab session. This is Saturday morning here in California, a beautiful and pleasant morning. So for those of you who are here in California time zone, thank you for being here despite the excellent weather outside. And for folks in India, it's pretty late. I appreciate you being awake on a Saturday night. With that, let's get started. So on Thursday, we covered linear regression. The linear is to fit a line through data. So suppose Y is the target variable, is the response variable, and X is the predictor. We took the example of an entrepreneur on the beach for whom the input variable or the factor was temperature on the beach and what the entrepreneur had to predict is how much ice cream would sell uh given that temperature and just to wet your recollection the frame to frame the problem if the entrepreneur um the entrepreneur needs to go every morning and get ice cream from the wholesaler ice cream is a perishable good if you buy too much and you can't sell it then it's a loss if you buy too little you'll have the you'll have to sit there and envy your neighbor who would have a busy time selling ice cream, the next shack. So with that framing of the problem, our goal is that the entrepreneur purchases the optimal amount of ice cream from the wholesaler. So given the temperature, the estimate of how much ice cream would sell is the response variable. X is temperature, Y is the amount of ice cream. So with that, this is a what you would call univariate regression or simple regression. Typically in high school, when you study it, people often name this as simple regression now if you generalize from that to more variates or more input variables or factors that affect the response or the output like in the case of the entrepreneur and his estimation of how much ice cream would sell another factor could be wind speed on very windy days less folks are likely to be on the beach on another factor is whether it's a work day or it is a weekend right holiday or holiday weekend or is it a work day if it is a work day parents are busy and they're unlikely to bring their children to the beach. So the number of people on the beach would be far less and consequently the sale of ice cream on the beach would be far less. So when you start factoring in, when you start incorporating, many factors or features, so we may, features. So we may, you notice that I'm using a lot of words interchangeably or synonymously, because in this field, you consider them synonymous, factors, features, input variables, variates, right? People often treat this and regressors and so forth. People often use these words in literature diversely based on which tradition they come from. Now in the machine learning tradition the word feature has stuck. We treat input variables as features. The features that the model learns from, the features of the data that the model learns from. So temperature is a feature of the external reality where you're trying to model the amount of ice cream, estimate the amount of ice cream that you would sell. So when you take many features and you still build a linear model, a line becomes a plane, you're trying to fit, suppose it's two dimensions, you take wind speed and temperature as input variable, and the response variable is an estimate of ice cream. As you can imagine, in three-dimensional space, that solution would represent a plane as a linear solution. And we went through much of that in the theory part, so we won't repeat it today we need to move a little faster on the other and so you can generalize from that from three from two covariates or two features to three features four features or n features the the what is a line becomes a plane and becomes and its generalization is a hyperplane. The distinctive feature of a hyperplane is that it is linear or to put it more sort of a different way, wherever you are in the hyperplane, the perpendicular to the hyperplane points in the same direction. In other words, or more formally, the orthonormal vector doesn't change at all, irrespective of where you are in the hyper direction. In other words, or more formally, the orthonormal vector doesn't change at all, irrespective of where you are in the hyperplane. So you can imagine a line and you can see that the perpendicular vector points in the same direction everywhere. Likewise in a plane, likewise in a hyperplane. Whereas if you take any other surface, you can see, imagine that you just got out of bed and you're coming out of your blanket and your blanket is all crumpled up. It's a surface. Then at any local region, if you treat it, it looks a little flat in the local region. Then the orthonormal vector, the perpendicular vector, would point in all sorts of directions. This place, it would point towards you. At another place, it would point another direction. this place it would point towards you, at another place it would point another direction. So surfaces in general have orthonormal vectors that keep changing from point to point. But hyperplanes are specialized surfaces where all the orthonormal vectors are pointing in the same direction. So that's another little bit more formal way of thinking of a plane. But my basic intuition of a plane is just a sheet of paper, a straight sheet of thinking of a plane, but my basic intuition of a plane is just a sheet of paper, a straight sheet of paper. So now we will start by reviewing the lab we did last week as a starter. We took a lab which was, and we took a data set which was the univariate data set. Now, just to wet your recollection, we have some imports. Let me go to those imports. So this is a little bit of stylistic element just to make the Jupyter notebook not look so boring. You can ignore it. Even if you delete it, it wouldn't make a difference. Now the imports are what are the libraries that we are going to use. We are using numpy pandas and pandas profiling, a convenient thing. If you don't have install each of these libraries by doing a pip install or you can go to the Anaconda navigator and install these libraries. Conda install a pip install will do. So the four libraries, five libraries that will sort of predominantly use are numpy, pandas, pandas profiling, scikit-learn and matplotlib and occasionally and a few other libraries which we will use at different times in the workshop, not all the time. They are the SciPy, which is the library for scientific computation. Yellowbrick is a new library that we will use today. It helps you visualize the model that you build. So it's a good library. Matplotlib is the graphing library. Seaborn, Altair, they're all competitors. They're all graphing libraries. There's certain functions here in this file, but I would say at this moment you can ignore this this till you're a little bit more conversant with all of this. Now folks, let me remind you, the things that we don't have the time to cover, but I really hope you are pursuing are picking up pandas on your own. If you are not, please reach out to the teaching faculty, the teaching assistants. They would be very happy to help you ramp up on pandas, give you a basic idea of what pandas is. They'll walk over the notebook again with you, the introduction to Pandas. You must know that. You must have a basic idea of NumPy, and you must start picking up scikit-learn. Scikit-learn, you pick along, pick as we go along. But matplotlib, which is the data visualization, very simple. But even though it is simple, dead simple once you know it, it takes time to pick up. You need practice. It takes a day to become fairly conversant with Matplotlib for plotting. So please do give time to these things. Reach out to us for help. Otherwise, you'll feel handicapped as we make progress with the lab. Last time, we went very slowly. But today, we will cover with the lab. Last time we went very slowly but today we will cover a substantive territory so we won't have the luxury to move slowly and explain the basic aspects of this library. It is implicit. We'll assume that you know these libraries, at least the basics of it. So all right, coming back to univariate data set, it is the simplest data set we can have. It is all there on the GitHub, in the support vectors GitHub. So you don't have to download it to your machine. Now, this line of code, just to remind you, is Pandas. Very simple to read a CSV data. You say read csv and it will read csv into a data frame a data frame is the simplest thing if you have a spreadsheet in your mind with a google spreadsheet or excel or whatever just one sheet of it pretty roughly speaking that is what a data frame is except that the data frame, so it represents data in a rectangular form, rows and columns. Columns are the features, rows are each row of data. So for example, each row is a datum. Datum is a singular for data. So it would, for example, in the case of this entrepreneur, it would represent one temperature reading and the associated amount of ice cream that was sold on a day when that temperature existed on the beach. So X and Y put together is one datum. One datum makes one row. And the entire data set, therefore, is a lot of rows of this datums, instances, data instances, and therefore, you have rows and columns. Columns here are X and Y. So when you load this data, this is a Pandas data frame. Now, Pandas is much more, the data frame is much more powerful than a spreadsheet. In fact, for data scientists, a data frame is a fundamental unit of work, whether you're dealing with Python, you're dealing with R, Pandas or R, statistical language, or you're dealing with Spark. dealing with Spark, many of you are data engineers, excuse me, if you're dealing with Spark, or in most of the data science languages and data engineering languages, wherever data transformations and data manipulation is involved, you will often find that this concept of a data frame keeps recurring. And it's more or less well understood in the industry that it's a rectangular form of data. But what data frames give you in Pandas is not just a spreadsheet of data. It actually comes with a lot of functions that makes it easy for you to deal with that data. So for example, when you say data.head, it shows you the first five rows of the data. And head and tail comes from the UNIX shell syntax. Head means show the early part, tail means show the last few rows. First few rows and last few. By default, it is five, but if you give an argument, let's say if you say 20, it will show you 20 rows. let's say if you say 20 it will show you 20 rows right so you have this data set as you can see there are two columns here x and y these are the two features x is the input feature y is the response variable now the rows of the data well in computer science as you know it's a convention to start counting from zero so you say zero one to three is the column index start counting from zero. So you say zero, one, two, three is the column index. These are the values. These are the last few values. If you sample random, so sample is a function that gives you randomly whatever number of data you want. So it'll pick a few random rows of data, and it will show you what it looks like. You should often use sample to get a more realistic sense of data quickly. Now, once you have the data and you do describe, so this function needs explanation. Remember, in theory, we talk about descriptive statistics, things like what are some of the descriptive statistics, mean, median, mode, standard deviation, minimum, maximum, range, quantiles, and so forth. So if you do describe, and by the way, include is optional. This is saying that include all the variables. In this particular case, it's not necessary, but I sort of have gotten into the habit of writing this sentence anyway. Transpose means rotate the table from rows to columns. So I like it neatly like this in which columns are x and y and the rows are x and y features and the columns are the statistics. If you don't do transpose then it will do the other way around. It will make x and y the columns and it will make the statistics the rows, each of the statistics the row, but it doesn't matter whichever one you play. I hope you went through this and we have done that last time. So now you may wonder, what about skew and what about kurtosis? So I leave that as an exercise to figure out how you would do that. We did that last time, if you remember. figure out how you would do that. We did that last time. If you remember, we use a hint scipy, we use scipy stat function for that purpose. But if you don't remember, look back at the labs of previous one. Now we come to a function which is like, are there any null values? One of the one of the problems with data sets always is that you will have missing values. In other words, there will be some day in which you know how much ice cream you sold, but you don't have the temperature value. You forgot to record the temperature value or you have a day in which you have a temperature, but you forgot to record the amount of ice cream sold. So you have holes in the data. These holes are called missing values. So missing values shows up in the data as null or nan, N-A-N. N-A-N is not a number whenever you see the word n a n capital n little a capital n nan uh don't don't start celebrating and thinking it's a non right it is not a number and not a number. I'm joking. It's a bad joke. All right. There's also an excellent library, a missing no, which makes it very easy to see missing numbers. Now for this data set, all of this is way, way overkill. It's a very clean data set. There are no missing values. If you search for missing numbers, what it will tell you is if there were missing values it will show you holes in the data like this thing would not be perfectly black right so uh and a missing no is a lovely library to help you analyze this now then there is so i'm just introducing you here to the various libraries. There is also a library called Pandas Profiling. So on the data, if you call profile report, what it does is it will, well, I have not done it here because it will generate a lot of output here, but I invite you to run this. When you do that, you will realize that it generates descriptive statistics, quite a bit of the descriptive statistics. Maybe I will run this. This is a short data set. Let me run it of the things. So now when I come to profile reports, see what happens. Here we go. When I run this profile report, you'll notice that it is creating a summary of the data and it will go and do a lot of things generally it goes doing exploratory data analysis on your data and it's quite nice you don't have to do it by hand because this tool does most of the exploratory data analysis on your on its own now it is still running so i will let it run for a while. Then with the data, so I want to give a pipeline. Whenever you have data, first you do descriptive statistics. You understand the data, you look at its nature. When you look at its nature, you sort of do univariate and bivariate statistics. And then after that, you should visualize the data. Next step should be visualize the data. Why? Because when you visualize the data, your eyes, as I said, are marvelous engines of learning. They'll immediately grasp the nature of the data. So it becomes, then when you do a model building, you're not flying blind. You have some idea what sort of model you'll be building. Are we together? So, all right, this is still running. I'll let it run. So how do you visualize the data? You plot the data here, scatter plot, as simple as that. You notice plt.scatter x and y, right? When you do this, you get this data. So now let's look at this data and think for a moment. Looking at this data, would you say there's a linear relationship between y and x? Would it be a reasonable hypothesis to say a linear relationship exists and therefore let's find the best linear relationship. Yeah. Yeah, you would say that, right? Now but let's prettify this data a little bit though. This data, if you were to submit it to any journal, it would get rejected, even if your research or whatever is perfect. Why? Because it has a few deficiencies. It is missing the X axis label. You don't know what the X axis is. You don't know what the Y is. There is no title to this diagram or to this plot. So one of the basic things or disciplines in this field is you should must label your axes so this is a labeled version of the axes x and y and this now i must say that when i do that you could you know you can just give x-axis label x y itself but if you notice i give it, now suddenly there's a curve ball here. Let's look at this line, scatter. This part was obvious. This is the same as this part. They're very basic. Now what happens is it is sort of part of data science, though it's not machine learning. language of its own. And just like React is a language, React and JavaScript, you use JavaScript to do stuff. And it has its own syntax and so forth. In the same way, most of these data visualization libraries, Matplotlib, they will give you the very basics easily. But if you want to create a pretty plot, you want to add good aesthetics to it, then you have to try a little bit harder. You have to read the documentation and try a little bit harder. So here what I have done is this part was the familiar. It will make the plot, put those dots. But we are seeing something more. It's alpha. Alpha is visualization speak or the design language people talk about it, artist. Alpha means transparency, it is the alpha channel, but it basically with it. Play with all of these values and see how your visualization changes. S is the size of these dots. You notice that I've made the dots a little bit bigger. I've made them nice circles. I've changed the color. I have insisted that the color should be salmon. I like the salmon color. Okay. and you should change it to whatever you like then comes the next line this next line needs some amount of interpretation you say plt.title and all you had to do is give x there and if you had just said x it would be fine but why is it that i have given this peculiar syntax so So I'll explain what this syntax is. You're in the field of data science. Now you'll be doing a lot of graphs and plots and so forth, and there'll be equations. So it turns out that to write equations or to write anything mathematical expressions, there is an old, old standard. It is called the LaTeX. So LaTeX and this is a mini language. LaTeX is a very simple language. It is again something that I encourage you to learn. One easy way for you to get started on LaTeX is to go to Overleaf.com. There's a website called overleaf.com. It's a free thing. Go open an account and practice and pick up some elementary tutorial on LaTeX. If you do so, then what happens is you will be able to produce, like for example, instead of just a boring X, you have a mathematical X with the arrow symbol. That is what this produces. This is the X with a long symbol. That is what this produces. This is the x with a long arrow. Do you see this? X with a long right arrow and y with a long right arrow. This is a mathematical notation. And also notice that the title here scatter plot of y versus x. Now y and x are in mathematical notation. The rest of the text is in standard text notation and they are bold text so uh go ahead albert so what's the s equal to 150 oh size of the size of the bubble you see the size of the dots here that's the size of the dots right so sorry as if hey in this four statements you wrote the title first and then define the arrows so does it like take all the information and then plot yes yes the order doesn't matter okay okay you can write this four statements in whichever random order you like it will still work oh thank you so the way it is is MATLAB does a, there's a notion of artists. What happens is that the artist, X label, Y label, these are all artists and title. So they all get different responsibilities. The Y label artist will go and do this, the X label artist will go and draw this part, the title artist will draw the title, and the scatter plot artist will scatter artist will do the inner part, and they will all go at it. It doesn't matter in which order you give it. So this is it. And then a little bit, Matplotlib is not the only game in town, it turns out. As I mentioned, there's a library called Seaborn. It is actually built on top of Matplotlib. And some people believe that it is prettier or easier to work with Seaborn as a higher level library on top of Matplotlib because in a couple of lines, it gets the work done. I have mixed feelings i do like as seaborn some things it does very well but it is also a small library what it can do it can do what it cannot do then you have to get back to matplotlib so it is good to know matplotlib and seaborn both the syntax is not very different. Some little change of variables and so forth. But this part is coming from matplotlib. Do you notice that how we are still using matplotlib, but this line is the cbon line. We are using the cbon library. If you do that, you will get a plot like this. Then there is Altair. Oh, did I, i uh let me oh by the way while we write it remember profile report was running so it has run and uh what is this this is a table of contents because it is producing a lot it is producing let me see Let me see. Oh, sorry. It is producing, as you can see, a variable, statistics, et cetera, et cetera. Yes. You realize that it finds correlations. So what is the correlation between X and Y, and so on and so forth min max. It draws a histogram, a toggle details. It will tell you statistics. So the same statistics that we had min max descriptive statistics. stakes. So now you notice Pandas profiling gives you all of this, a coefficient of variation, the median, so skewness, and so on and so forth. So I won't go into all of them. So let's get back here. So now you can do it using Seaborn. It turns out you can also do it using altair right let me run this guy when you do it using altair altair gives you the ability to it even gives you mouse over ability you see this a plot is not static you can do this you can move this plot around right i believe you can save this output it to png and so forth export it easily so there are many libraries in python python for data science that's a nice part of it but anyway so remember we have been through we loaded the data we do descriptive statistics then we visualize the data and now we have a sense that okay let's build a linear model around it. So how would you build a linear model? First of all, and now we're getting into the machine learning part, the model we'll build is the regression model. In regression, you have the inputs and the output. In our case, what's the input? X is the input. Y is the response variable or the output. First, we need to separate them out, right? So the data frame had both of these as two columns of the same data frame, but we need to break it out. Input needs to be its own data frame, and output needs to be basically an array of numbers. So this notation does it so you can say when you do data in quotes y single column you're saying just give me the y column of the data that will become the y and give me the x column of the data but there is observe that there are two square brackets this is panda speak for saying when you give it to me don't give it to me as just an array just a column pretend that it is a matrix it is a full data frame that happens to have one column in other words give it to me as rows and columns right as a data frame so in more a specific language this will be a data frame here for you, and this would be a series. So it's basically a column, but it has this putting as a... Wrapped up as a data frame. That is it. So double square bracket wraps it up like that. Also observe the convention that capital X, remember we talked about that, capital X is the input vector or the input matrix and Y response is always the output. Capital X is a convention you follow in all of machine learning that it will represent the input data. Now we have split the data first into output and input. The next thing we need to do is split the data further into test and train data. Why? We need to hide some data under the pillow. We don't want the model to learn from the whole data. Why is that? Because the model might memorize the data. The basic assumption is that, remember the example I gave, the children going to a meadow in which there are cows and ducks. How would you know if you show a cow that the child has already seen, that the child doesn't have photographic memory and just repeating back to you that it's a cow without understanding what a cow is one way to do that is to show the child a cow that the child has not seen or a duck that the child has not seen and asking what is this and if the child has a concept of a cow and duck it would be correctly identified right so therefore you need to hide some data under the pillow right we do that this is again is so some of these syntax are, they'll become habits for you. If you're getting introduced to now, you will see it everywhere. And you will do it practically in all your data science notebooks for all your times, right? Splitting the data. This is, and one of the nice things about pandas is and scikit-learn is that the how powerfully and briefly in one single sentence you can do this by the way when you split the data it will shuffle and and then split the data so why why what does this random state is equal to 42 me uh see which of the random rows that randomly the rows that it will pick up if you there's a random number generator if you seed it with a fixed point then every time you run the notebook you will get exactly the same results in other words the test and train data will split exactly the same way otherwise Otherwise, each time it will proceed differently and you'll get a different test train split. In production, you need that, but when you're learning, you don't want to be surprised with a different result every time, or subtly different result every time you rerun your notebook. So you tend to seed it with a random number. The conventions that you see, quite often people seated with zero. But 42 is sort of an inside joke in the data science community. If you have watched the movie or read the book, Hitchhiker's Guide to the Universe, there was a story, there was a scene there where humanity creates a computer, enormously powerful computer. They go to the computer and then they ask, what is the meaning of life? And the computer grinds on and on and on and on. And generations pass. And finally, the computer has an answer. So people go to that computer and say, what did you find? And the computer comes out with the answer 42 so that story that episode in the hitchhiker's guide to the universe has become part of the culture within the scientific community so it's very common for people to as a just for fun see the random number generator as 42 so don't put too much importance to 42 if you have your own favorite number you can put it there as a habit or just put zero basically you're choosing yes go ahead kyle yeah the the super powerful powerful computer in the story is actually the earth. That's right. It turns out. That's right. We're run by little white mice. Yes. Nice. You guys have your, remember your Hitchhiker's Guide. All right. So now comes the part. Now that we have done, so these are the preparatory steps. We separated the input and output out. We separated the data. We hit some data, test data under the pillow. We will take the train part of the data, subset of the data, and now we will build a model. It turns out that after all of those preparations, building a model is very straightforward. We first instantiate algorithm. In our case, we have learned so far only linear regression. So we instantiate linear regression. And then we say, linear regression, go fit to the data, fit to the training data. Fit is a word that basically comes from statistical language. In machine learning, you could have easily interchange the word fit with the word learn. It would have been more intuitive or aligned with the way I have been explaining things to you. So whenever you see fit, replace it in your mind with the word learn. So you're telling the model, model.learn, go learn from the data that here is the training data. Here's the input, here is the output, go learn from it. So what will it do? It will go and find the best fit line. Now, recall that the equation, this was the equation of the line we talked about. Beta naught is the intercept. Beta one is the slope. And what in the world was that epsilon? It is the irreducible error term, which you never can avoid. Now, in this, y hat is the prediction, remember? Predictions wear a hat. This is your y hat. And so let us find out what comes out with it. You can ask the model, once it has fitted the data, or learn from the data, what was the intercept? And it will say, oh, intercept was minus 0.11, close to zero. What was the slope? Oh, it turns out that the slope is about 8.3. Let's go and look at the data and see if it makes sense to us. Would we agree with it or not? Let's look at this data. For zero to 10, if it goes, the variable increases to about 80 or 80, a little bit more than 80. So it makes sense, isn't it? 80 and change divided by 10 is approximately 8.3. Isn't it? So yeah, more or less. So this machine seems to have learned well. And so you feel happy that, yes, it agrees with your sense of it. Now you need to find out how wrong the model is. You need to find out the R squared. Remember, we talked about the R squared, the coefficient of determination. Just to wet your recollection, what was the coefficient of determination it is the you take the null hypothesis which says there is no relationship between temperature and the amount of ice cream sold or between x and y in that case the best prediction of y is just the average values of y. So if you have a model and your model certainly is better, how much better it is relative to it? So you take the mean squared error or actually the sum squared error. You can take the mean squared error of the null hypothesis and compare it to the mean squared error of your model relative to the null hypothesis. So it is null hypothesis error minus your model error divided by null hypothesis error, right? So relative. That's the definition of R squared, capital R squared. definition of R squared, capital R squared. So a mean squared error is 32. What it means is that, and if you take the square root of 32, how much is it, guys? About 5.5, isn't it? Right. And so if you look at this data, does the band of value seem to be, like, if it is twice of 5.5 is 10, that is roughly the band of the data, isn't it? The level of errors you are making at any given point. So always validate the answers against your common sense. The coefficient of determination is 0.947. Is that a good number or a bad number? Remember, what is a perfect number? 1. But perfect is bad. There's something wrong there when you get it. It really should be there. But close to 1 is a good number. Close to 0 is a bad number. So this seems to be a very good number, right? We seem to have arrived at a potentially good model, isn't it? Now, I just want to, well, it so happens that many textbooks end here. They say, all right, let's move forward. But it is not actually enough. And you may still be wrong. Your model may still not be the best model. And so you need to do a further step, which unfortunately most textbooks miss. So let's go through that. And one good thing about the textbook that I prescribed for you, the statistical learning textbook, is that it pays attention to residual error. Speaking of the textbook, guys, this is chapter four, chapter three. We are in the third week, so when you go back and review, please make sure that you review the whole of chapter three. Forget about the p-values and the f-statistics and so forth. Those parts you can skip for now. We'll come to that later, but the rest of it, just use that whole chapter to review whatever we have learned. So what is the residual? Residual is the error, y minus y hat. Actually, I wrote it the other way around, y hat minus y. It doesn't matter. So if you want to see for each prediction of the test data, how wrong were we, or trained data, how wrong were we? So it shows you for different data points, for different predicted values, 0 to 20, remember our predicted values are 0 to 85 or something like that, 83 range of values. For each of the values, what is the range the different predictions made, and what is the range of error in those predictions? What is yi? The prediction minus reality, what is it? Now, when you do the experiment on the training data, you get this yellow dots. When you do it on the test data, you get this red dots. So what are we supposed to look into this? Interestingly, we are supposed to stare at this, called the residual plot. Do you see any pattern here, guys, in this plot? There is no pattern here, right? They just seem to be random, except that they seem to be clustered around zero. There are more dots closer to zero and less dots away. In other words, if you look at this on the side, the frequency plot that you have, the histogram that you have, it seems to show roughly a bell curve distribution in a very rough sense a bell curve or a normal distribution when you see that that is a measure that your model that builds gives you confidence that your model is right you would like to see that your errors are normally distributed or bell curve distributed around zero right and you want to see that there is no there is absolutely no pattern in the residuals when there is no pattern in the residual you feel happy and there is a lovely word that you use hang on guys there's a lovely word that you start using and there is a word actually it is called homoscedasticity right the the word is homo skidasticity well that's quite a big word and the first time i read this word many years ago, I was amused because it reminded me, this is a personal aside, it reminded me of this word from the kids' story Mary Poppins. That word I have noted down, that word is, there is a word that Mary Poppins, who was the governess for some children in that story, to build up their confidence, boost their confidence. She taught them this word. The word is supercalifragilisticexpialidocious. Very nice. Right, very nice word. And when I wrote it out on a page, it's literally the one whole line long. So it's a long word. And then the children memorize the word and then they feel pretty good about themselves because they can repeat it and look smart. So in the same way, when I encountered the word homoscedasticity, it reminded me of Mary Poppins' words. So it's a big word. Get used to it. What it means is homo means uniform. Skadasticity here means the errors, the variance of the errors, like are the errors spreading out or do they show a certain pattern of are they funneling out or doing something like that? They are not doing that. They are uniformly spread in the vertical direction if you look at it, but the variances are not changing. So that is homoscedasticity. So this data shows, the residuals show homoscedasticity and it has no patterns. Both these conditions are true, right? So that is the best that you can have. Now, so we are doing pretty good it seems let us visualize our model predictions over the data what we'll do is we'll take lots of points between in the range of the data and we will make a prediction we'll make a prediction plot so we create new data and we plot it out over this data when we plot it by way, this is our cooked up data. Remember the data frame? This is the NumPy speak for creating our 1000 points of data in the range of data, whatever the X range of the data is. And you create a data frame, you plot it out, and the rest of it is just matplotlib. When you plot it out and the rest of it is just matplotlib when you plot it out and observe that i have forgotten to add the title and the description and the labels so i leave that as an exercise to fill in the blanks here add the title description and labels, so would you the solid red line is the prediction, do you think that the prediction line or the or the model. Raja Ayyanar? agrees with the data is very close to the data. Raja Ayyanar? yeah it does right, so we have now multiple evidence our our square is good our residual analysis. good. Our residual analysis gives us confidence. There are no patterns in the data. There is homoscedasticity. And obviously, our errors show a normal distribution. Good. When we visualize the model, it is good. It seems to be very faithful to the data. And so when you have so much evidence, you're led to believe that you may have found an effective model, right? You may have found as effective a model as you can. Now, remember the famous truism of Box, the great data scientist, all models are wrong, but some are useful. Remember, you can always create yet another model, perhaps more complex, that agrees with the data just a tiny little bit more. But is it worth it? Probably not. If this model serves the purpose of the business, whatever real-life problem you're trying to solve, then you need to stop. Because if for no other reason, this one follows the Occam's Razor principle, it is the simplest model that you can build, isn't it? And so here we are. We have created our first successful data science notebook. We have done the first exercise, and we have built the first model. And what is more interesting is we got it right the first time, isn't it? So take a moment. We'll take a very small break of maybe three, four minutes. Savor your success. Because in the rest of your data science journey in life, it will never be as simple. It will rarely be that you'll hit upon the solution in the very first attempt, and the simplest solution will work. So we'll have to, as we encounter different data sets, you will see that you have to do many more things. But this is the foundational. This is the bare minimum you should do always. And this should also lay a pattern of what you should do. So remember, what were the steps we did? We loaded the data. We did a descriptive statistics. We checked whether there are missing values. We need some further cleanup and so forth. Then we visualized the data, we built a model, we did a model diagnostics to see how good the model is. Finally, we visualize the model, right? And that is our data science journey. As we do different exercises today, you will see that we continue with the data science journey in every problem. All right. So, guys, no more than five minutes break. This is a question asked by Siddharth that what is so special about the fact that I see no pattern? so special about the fact that I see no pattern. See, what happens is, so I'll give you a physical intuition. If your model has learned all the information part of it, soaked in the information part of it, then the error part of it will just be the irreducible error right it will be the noise left behind in every data there is signal and then there is the error part and inherently you believe that the error part will have a normal distribution errors tend to be more likely to be small than big so one of the earliest discoveries about errors was, which goes back to Gauss, is that errors are normally distributed. In fact, the history of it is that when many people were trying to make a function for the errors, so somebody made it like a sloping line, that errors decrease as you go, linearly decreases as you go far from the, as you increase the size of the error in data, when you take a lot of data or something, A quite off most people say instruments are slightly wrong or even your instrument keeps fluctuating it it will have it will show minor fluctuations a lot but it won't show major fluctuations so when you stand up on your weighing scale in the morning you notice that your weight if you stand up three times you get three different weights right but the three different values will be close to each other they wouldn't be they would certainly not be 20 or 30 pounds away from each other. Isn't it? So large errors are rare. Small errors are common. And Gauss quantified this into this bell curve function. And that's how bell curve actually was created. And it is also the word normal is used. Because it is the normal. Normally, you see errors distributed as a bell curve, right? Normal distribution. So you see that here on the right hand side and that more or less a normal distribution. And the point is that if your model, imagine that your model is soaking up the information. If all the information is soaked, only noise should be left behind. And so this is the pattern. This is how noise should look like all over the place in every region of it for small values, for big values. Everywhere, it should look the same. It should show a bell curve distribution. And its variance should be the same. There's no reason why the band of errors for one set of values should be different for the band of errors for another set of values. So does that give you intuition on why we like these residual plots which show no pattern? That's that. Now let's go to the second data set and we'll now move a little bit faster. So second data set is also the same thing. We load the data. What do we do? We sniff around the data. We look at its head and tail and sample, And then we do a descriptive statistics of it. Or we realize we look for missing values. There are no missing values. Hey, so we don't have to do any cleanup further. They can do your data profiling, which I won't do. And now comes what comes after descriptive statistics, visualization of the data. Let's go visualize the data. And when you visualize the data, you notice this. Now, when you look at this data what does it tell you is the relationship between X and Y linear it looks like a sinusoidal two bands yes it's not linear right in fact it looks somewhat like a sinusoidal distribution so let us say what happens if you blindly try to build a linear model, the same exercise that we did in the previous thing, you do it, what would happen? So I'm repeating the code that we did last time. We do the fit and you do the slope. You say intercept is this, slope is this. You project, you do a prediction. You get an R squared of 53%. Actually, 53% is not bad, right? So what is a good R squared? It depends. In marketing, in sales, in advertising, even 0.2 R squared is pretty good, right? In hard sciences, like physics, et cetera, anything less than 0.99 is people get worried about. Right. So it depends upon which domain you are, but here's 53. But how would you know? And this is a point I'm trying to emphasize. If you don't do residual analysis, you wouldn't know that your model is not the best model, that there is scope for improvement. So what you do is you do residual analysis. And now you look at this data. This is the residual plot. Do you see that this does not look as much like a normal distribution as you had hoped on the right hand side? The error plot. It doesn't sort of very vaguely like a bell curve. Not quite. Would you agree? Yes, skewed yeah it's sort of skewed and now what about the residuals do we see a pattern does our eye see a pattern in the residual yes very clearly there is a pattern if your eyes can see a pattern then you are barking up the wrong tree it's as simple as that right what does it mean you left a lot of signal on the table your model did not capture all the signal so it doesn't mean that your model is wrong there's no such thing as wrong it just means that there is a better model that you can build are we together there is scope for improvement and this is further shown when you try to visualize the model predictions over the data. Let's visualize the model itself over the data. Now see how the data looks sinusoidal. And how does your model look? Well, you took a linear regression, so it will fit the best line. I'll let you ponder over it for a moment, and you will convince yourself that this is indeed the best fit line that you could have drawn through the data so as a linear hypothesis this is as good as you can be but what can we do what did we learn in thursday when we see bends in the data what should we do day when we see bends in the data what should we do we should increase the polynomial bends plus one that is right so what we should do is we should increase the our feature space take x x square x cube etc etc you go to higher degree polynomials and do a regression in that so that is polynomial regression how do we do it? Absolutely no complication. This is it. We took, how many bends did we see here? Two bends. So I can start with a minimum polynomial degree of three, right, two plus one is three. You do that, polynomial, and all you do is you transform the data to create polynomial features, and then you do the same linear regression that you have been doing so far. All you did, though, is added. This is the only extra thing you added to the data. The input data now has become more three column data, x, x squared, x cubed. And now you get observed something very interesting look at your mean squared error 0.03 and here what was your mean squared error 0.45 significant improvement isn't it 0.45 versus 0.03 45 percent I mean, if you do it as percentages, but not, 0.45 versus 0.03. And not only that, remember the R square was 0.53. And what is the R square here? Is this far better? 0.966, 0.97, give or take. And, but, so this is encouraging, Raja Ayyanar?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam Take a moment to see this. Do you see a pattern in the residues? No. Much less. Almost no pattern in the residuals, isn't it? So this seems to be getting there. And the error distribution seems to have improved a little bit. It seems to be a little bit more centered towards the R0. Now, let us visualize a model over the data. Ah, this is what it looks. Would you think that this is a better model than the last one? Yes, it is right now. As a further bonus, I mentioned that you know that you have to do three degree polynomial or three terms are needed. But remember, I also told you that this looks this looks like an odd function. And so you can be smarter and do only odd degrees. You can do X, X square and X five. Right. So you could have actually taken polynomials of degree 5 and if you do that you will see something interesting this will improve a little bit i leave that as an exercise for you but also observe something um Asif, if you don't mind, can you repeat the last bit? Remember the even and odd part that I mentioned about functions? What did it tell us? If you review the theory section, I told you that if you see something which in your mind's eye looks to you like an odd function, f minus x is equal to minus fx that kind of a thing then you should consider only odd number polynomial so if you take x x square x cube a three three three two bands means three degree three degrees you want to take so you will be x x square x so i mean x x cube x five x so you should take polynomial of degree five it gets even better but leave that to you see now you reach a point of diminishing returns and the question that comes up is should you stop at degree three why becomes occam's razor principle you have already reached the r square of 97%. Is it really worth from a business perspective to make an even more accurate model, or do you risk overfitting? So that's a question you have to ponder over. So what did we learn from this, folks? That residual analysis matters. And you should keep trying till you get no pattern in your residuals. I'll take another data set like this. So this should be easy. I have one question. You have fit underscore transform. Yes, it will create polynomial features. Why fit is there, not only transform in the method? X is fit transform, no? Yeah, I mean, we are just transforming the X to a polynomial. That's right, that's right. So the reason is, first, it has to do with the language of the function, the polynomial features. Like, if you just fit to a data set set it will internally figure out how to do it but then if you want the output also you have to do the underscore transform also okay so fit transform puts both of them together i got it one shot gives you both these things thanks that's all it is. Now we go to univariate two. I will leave this as a homework for you. But let me quickly go through it because I want you to do it. You look at this data. This is no different. How many bends do you see? Three. So you should take a polynomial of at least degree? Four. So do this, guys, on your own. Nothing new. If you don't do it, if you just build a linear model, you will get an R squared of hopeless R squared. Do you see? Your R squared is actually worse than the null hypothesis or close to the null hypothesis. Basically, zero. Strong patterns in the data in fact the whole sinusoidal curve inverted is showing up in the data pretty bad and you know it when you visualize your model on the data it is your model is practically the null hypothesis isn't it? Right. And so you have to do. I've been mowing my backyard with my tractor. So and I have grass allergies. Wow. So, all right. So why did I take it of degree nine? You just play with it and see which works. I took it because if I need four bends and I take even powers, I take the power two, four, six, eight, things like that. So play with it yourself and see what happens. At this value, do you see a pattern in the residual? Guys, do you see a pattern in the residual? No. No. Almost not, right? Maybe a little, you notice there's more clustering here or something like that. No, your eyes can't discern any significant pattern. Lovely thing is, do you see that your error distribution shows a normal bell curve? Look at the error distribution on the right. So you seem to have hit upon the right answer. Why nine? So this is the experimental thing. I said four. Start with polynomial of degree four. See how well it works. Then keep moving onwards till you, and this is a great example for you to do your bias variance trade off. Right? Play with this. And when you visualize your model over the data, this looks good. Right? But this one, we won't release. We will leave it as a homework and we'll release this lab one week later. Likewise, let's go look at another data set. This is a bit hard. This is also your homework. I'll quickly give you a glance at this data set. Again, univariate. It looks like this. How many bends can you see? How many bends can you see? About six, if you look at the small ones. Yeah, all right. Include all the small ones. It depends on your eyes, and so I expect you to plot it out. Here I won't tell you the answer. How many bends you should see, how you would would model because this is your homework. Do it on your own. Now I hope this is beginning to get to be pattern. We are still doing univariate, but now we'll do more complicated after a little bit. Dataset 4, look at this. This looks like a necklace. Looking at this, how many bends there are? So you should be able to model it with binomial of 8 too. I leave that as an exercise for you. Again, if you don't do that, your null hypothesis will be pretty bad. If you just do a straight, not a residual analysis, it will be pretty bad. Your model will be bad. And if you do it with straightly as a not residual analysis will be pretty bad your model will be bad and if you do it with two you get it right so this is it i'll let you play with that that too is your homework so kyle we are going to release univariate which ones are we going to release univariate one and that is it. Then univariate 2, 3 and dataset 4 are homeworks. So we just give links to the CSV files? Just links to the CSV files. And just put instructions on Slack that these are homeworks. We'll release this after a few days. So guys, remember that when you have homeworks, you do it or you need help, you can always reach out to us. We'll have review sessions for the homework and we'll have clinic sessions. We'll help you do the homework. So the first time when you enter this field, even though later on all of this will look simple in the beginning, entering a field is the hardest. You're learning too many things. You're learning machine learning itself, data science, but you're also learning pandas. You're learning how to use the tool, right? You have a chainsaw and you have this. You have to carefully learn to use them. So you have to learn pandas, you have to learn Matplotlib, you have to learn NumPy, you have to learn scikit-learn and all these tools that you have to learn. So you're overwhelmed with a lot of learning in the beginning. The going gets easy because you learn the tool once, once you have learned the tool you don't have to relearn the tool. But then the rest of the workshops, ML 200, 300 etc, you will you won't be learning tools anymore uh ironically it gets easier because you'll be focusing purely on the data science we'll we'll do another one data set 5 which is a function like this do you see a function like this now this is an interesting function you will this one i want to mention and i will walk through you say this is easy how many bends do i see one bend maybe another gentle bend here two bends who knows but if you try to model it do the same thing a linear if you just make a straight line model it's hopeless you see a strong by the way in this data i forgot to add error so um it's very pronounced you don't see error band here at all now if you fit a straight line to the data this is the best straight line you can fit there'll be regions where it is vastly wrong you try to do polynomial and you can play with it you will find that you have to go all the way to degree 6 before you can do that. Now, notice that residuals in this are pretty significant, 1, 1.5, etc. If you go to degree 5, you do the residual analysis, you observe something quite peculiar. You look at this and what does it tell you? Do you see a pattern? Pretty wacky looking pattern. A pretty wacky looking pattern. And it's a very clean data, actually. There were no errors in it. Now, what is interesting is the size of the residuals is extremely small so you have two opposites the residuals are close to zero close to zero residuals means you have hit upon a good solution and still you find a lot of pattern in the data so what are these two opposites telling you can you guess guys make a guess what do these two opposites telling you? Can you guess, guys? Make a guess. What do these two opposite things tell you? One tells you that you might have a good model. Another tells you, hey, there's pattern in the data. So you're probably still barking up the wrong tree. The data is biased. It's probably a mathematical formula. The data is biased. It's probably a mathematical formula. Yeah, it is derived from a mathematical formula. I forgot to add an error term, but it is good. Actually, I preserved it like this to make a point. But so the point is, guys, that when you see a situation like this, the way to interpret it is that, OK, I acknowledge the fact that my model. okay i acknowledge the fact that my model there is a better model out there but given the fact that my model works for this data and we can validate that by looking at projecting the our predictions on the data if you project your predictions on the data how good a fit is it it's a near perfect fit. So you acknowledge that your model may be quite different from the underlying generative function, but with caution you can use it. Why do I use the word with caution? Typically what happens is when you do things like that, there will be certain regions where the model begins to fail and typically it begins to fail in the periphery. So now I will mention this. Actually, I should mention a very interesting phenomenon here. Regression. Yes. So, okay. I will talk about it because it speaks to something interesting. What happens if I deliberately, for any of the analysis, take a polynomial of a very high degree and fit it to the data? Suppose I'm able to fit it. So this one, I'll give you some hints because it's your homework. It is like this and you have taken a degree of a polynomial of a certain degree. I don't know how many degrees that we take. We took polynomial of degree six. When you take a polynomial of degree six do you still see pattern in the date in the residuals there is some residual pattern isn't it like it is not perfectly uniform would you agree guys Would you agree, guys? Yes. It's not uniform. Then you get tempted to go to a polynomial of, and this is the fit at the sixth degree, a reasonably good fit. What happens if you start going higher? Suppose you go to polynomial of degree 12. What happens is something very interesting. Your residuals are a little bit less, and your fit is better. Do you see that this is fitting better? Yeah? Right. But what you start seeing is something called a range phenomenon. There is a great mathematician called Range, a data scientist called Range. He observed that if you go to very high degree polynomial, then you start making mistakes. Your model starts making mistakes in two regions, near the ends in the periphery of the data and wherever the data in between is sparse, there's not enough data because it's almost like wherever it thins out in the region, you will see that you will start seeing oscillations, lots of oscillations, right? So that is an example of a range. To make it more pronounced, just see what it predicts beyond the range of the data do you notice how it starts just shooting up it was making prediction in the range up to 100 and now suddenly it's making predictions of 200 300. do you see this dotted line guys predictions for high values Do you see this dotted line, guys, predictions for high values? So it develops a tail. And here also, the left-hand side also, you start developing artificial tail. This oscillation, I call it the wagging tail. If you have a dog, you'll relate to it. Your model begins to develop wagging tails on the periphery. So I think we have a separate notebook for ranch phenomenon. Oh, we do have it separate. So let me I forgot that I had created a separate notebook. Let me of course, oh, yes, ranch phenomenon data. Let's do that. Let's do that let's do that actually um all right and i use data set three it seems to do this all right so our data set three is this oh yes this data set is quite interesting let's go through this data set when you look at this data set how many bends do you see three one two three right now let's see what happens if you you know that a straight line model will be hopeless so we are repeating the same analysis, doing a straight line model. And when we do it, of course, residual shows a pattern, very pronounced pattern. And your predictions are hopeless. And your R squared would be horrible. What is your R squared? 0.06. And test R square is actually below zero pretty pathetic you visualize the model this model has no fidelity to the data right it has literally tone deaf to the data well you could go and do polynomial regression that's one big trick we learned this time so what happens when you do polynomial regression? Suppose you do polynomial regression of level four. You realize that it won't work till you keep going on to higher and higher degrees. It turns out that at degree seven, it begins to work. Now you wonder where are the seven bends in the data? Maybe a little bit here, extra one, maybe a little bit here, but that still doesn't add up to seven but it turns out you have to do that and you feel pretty good about yourself because you get a 94 percent r squared now you may say well we are getting somewhere but then you look at the residual and what do you see do you see a pattern there is a pattern in the residual and the do you see? Do you see a pattern? There is a pattern in the residual and the errors don't quite look like a normally distributed error. So that is depressing because just now you saw a very high r squared but right then you get deflated or demoralized because you see this residual. So you say all right let's visualize the model on the data. When you visualize the model on the data what happens? because you see this residual. So you say, all right, let's visualize the model on the data. When you visualize the model on the data, what happens? Do you see that near the center of the data? It actually sort of agrees, but what is this problem? Do you see this oscillation here at the end? It begins to introduce artificial bends in the data. It begins to introduce artificial bends in the data. I tried this for a polynomial of degree 10 or something even higher, and you'll see even more oscillations of the data. So this is the wagging tail. And it was discovered at the beginning of the previous century in 1901. at the beginning of the previous century in 1901. And it was discovered as a fundamental limitation of polynomial regression. So when this begins to show, often it could be a sign of many things. Maybe you don't have enough data to suppress oscillations. Maybe you need to be careful not to apply high degree polynomials to the ends of the data, which is always true. It will make mistakes near the ends, right? And to me, it often is suggestive of the fact that maybe this, the right framework to look at it is not polynomials, but transcendental functions, right? And if you think about it, in hindsight, isn't this reminiscent of a bell curve? Let's look at the data in the raw form. Isn't this reminiscent of a bell curve? Isn't it? So perhaps we should have started out and used a bell curve instead to do this, except that we don't know how to fit a bell curve to this data. All we know is how to fit lines and polynomials to the data. So that brings us to a very interesting topic. If we can guess, like, for example, for dataset two, we guessed it's a sine wave, right? To this, we guess that it's a bell curve. Right to to this we guess that it's a bell curve. Is there a way that we can build a model in which we try to fit the best bell curve or the best sign wave? That is much more logical to do than going about fitting lines and polynomials to the data. It is true. You can do that. And let's take let's take that journey. Give me a moment, please. So we'll take that journey. But before we do that, I want to do one more topic. Actually, I can do it in whichever order needed let me do the power transform first another big topic so guys you notice that we are introducing a lot of topics we did linear regression polynomial regression we realize that there is wrench phenomenon right and now we are also we are going to learn about something called a power transform. Let's do the power transform. What are power transform? We will look at a data set. So to do this data set is quite interesting. You want a model to predict Y in terms of X. But when you look at Y, it looks like this. Do you see the funnel shaped? Like as X increases, the band of Y values increases. As you go from the left-hand side to the right-hand side in the x-axis, look at the y values, the spread, the variance of the y increases, right? So this data, the question is, how would you build a good regression model with this? Well, once once again we always take the linear model as a baseline straight line as a baseline if you were to do that you would get an r square actually a 0.69 69 percent not bad and for whatever reason the mean squared error is zero i don't know why i think there must be i try it out again why it came up with the mean squared error is zero. I don't know why. I think there must be a try it out again why it came up with a mean squared error zero. It shouldn't. There's a bug here. So if you do a residual plot, do you see a pattern in the residue? Yes. Yes. But this pattern is very unique. Two things things observe. You observe that the residuals, they themselves sort of, the variance of the residuals, do you see they are increasing? Right? At this value is zero, the variance is very small. Yeah. But at this value the variance is large isn't it the variance has been increasing as you move along right and this sort of pattern in which the variance increases uh for just to be funny it looks like a loudspeaker isn't it or it looks like to me like shellac home's pipe or it looks like to me, like Sherlock Holmes pipe. If you see the picture of Sherlock Holmes holding a pipe in his mouth, right? So it sort of looks like, I don't know, you can form your own intuitive picture about it, but you see the variance spread. So there is a lovely word for this. We knew that absence of pattern is homoscedasticity. When you see this particular pattern, the variance increasing, right? It is called heteroscedasticity. The opposite of homo is hetero. Hetero means varying, changing, different. Skedasticity, the variances. A heteroscedasticity, right. And if you look at the errors, one interesting sign that you will see is that the errors will not show a normal distribution. When the errors don't show a normal distribution, it's a classic telltale that you're probably barking up the wrong tree. So now you can say, why can I go and visualize? And of course, if you put a line through the data, you realize that this is deceptive, that R squared of 69% was deceptive. Because this line, even though it sort of is near the data, it completely misses the point. Isn't it? It misses the structure of the data entirely. And so we need to do better. We may try polynomial regression. Let's say polynomial regression of degree two. What does it do? Does it remove the heteroscedasticity? the heteroscedasticity? Not really. You still see the variances increase, isn't it? Variance here is low, variance here is high. It's a little bit better centered. Yes, that's true. Why? Because now at least your line will bend, right? There'll be at least one bend trying to follow this. line will bend, right? There'll be at least one bend trying to follow this. And let's see, did I visualize it? Visualize the model on the data? Yeah. So you notice that it is a little bit closer to the data. You have to ask yourself, well, is it good enough for me to make prediction? What will happen is that it depends on your real life situation, but it's not. But it turns out that when you see things like this, the thing that it should tell you is something interesting. There are more telltales. Okay. If you draw a histogram of the values, of the y values, right? And x value and the y, x is of course uniform, y value. What do you notice in the histogram of the y x is of course uniform y value what do you notice in the histogram of the y value does it look normally distributed no it looks like an exponential decay it looks like an exponential decay when such things happen perhaps a little bit of a theory is in order. So, and this is a very interesting concept guys I would like to mention. It's not in your textbook, but it is important and most textbooks miss this point, so we must do this. I can't tell you how often I have looked at other people's, you know, people come to me for help in the data science notebook. And sometimes all it takes is the solution that I'm about to tell you. And almost like magic. It feels like magic when it suddenly works. I still have to go to the end. All right, so I'll explain this to you guys. Suppose you have a relationship. Why? Can you see my writing? Transform. The topic is power transform. So let me take a color that is pronounced. Yeah, this should be good. So we are used to writing our equation like this. Beta naught plus beta one X plus beta two. Well, I don't know. You can do polynomial expansion, but for simplicity, let me just leave it like that. Plus an error term. So Y hat is like this. Right. So that's it. The real and then you have an error term. So there is an assumption of linearity. Polynomial expansion does a little bit more. But how do you know that the real relationship is not y to the power n is equal to beta naught plus beta 1x? Right. This is also possible as a basic generalization. It could also be log y is equal to the log of this beta naught plus beta 1 x, right? It could be like this, or let us say that y is equal to, it's a product, x1, x2. Let's take an example. Suppose you're predicting the price of a land. And this is the length, this is the width of the plot of land. You would agree that the price is proportional to the area, isn't it? But if you are given data like this, x1, x2, y, would you agree that it would be foolish to build a model like this? A linear model would say x1 plus x2 that wouldn't be the right approach isn't it you know if the underlying relationship is this but how can you how can you use this in this situation can you hazard a guess guys how would you apply log right on both sides to do a log on both sides so what you can do is you can do a log y then becomes proportion is depends on log x1 plus log x2 and you treat this as new variables y prime x prime x1 prime x2 prime and then you can write an equation then you can hypothesize using a linear equation that this is beta naught plus beta 1 x1 prime plus beta 2 x2 prime so you can use your linear regression technology is still for such a situation predicting the price of a piece of land when the data that you get is are the length and width of the land. Do you see that guys? So in other words, linear regression methods can be expanded in their power using certain what are called power transformations of the data. You're taking logs of the data or taking, you know, the appropriate finding the appropriate power of the data. And now the question that remains is, which of these things should we do? And do we need to do it? And what I'm saying is, whenever you see a situation like this, whenever you see in the response variable, a distribution like this, this should be a wake-up call to you. A distribution like this, this should be a wake-up call to you. It should tell you that a power transform is called for. Are we together? Now, it is a fallacy. So one thing, first, here's something. In this field, in data science, you know, in... I'll quote something from my native tongue. I don't know how many of you are Hindi speakers. There is a statement which goes like, and let's see how many of you can understand it. Neem Hakeem Khadrajan. Who got it? No clue. No clue. Neem Hakeem means a doctor, right? But immature Hakeem, half baked Hakeem, half baked physician is Khatra-e-Jaan. Khatra means danger. Jaan means life. So it's a danger Jan means life. So it's a danger to your life. And you see that. Be careful of an overzealous young physician. So it's an old wisdom. So the same thing is true for data science. It is surprising, actually surprising actually well our field has gone through a lot of hyperbole machine learning because the reality is it's a hot field everybody is getting a job in it everybody is in a hurry to pick as many things as possible so you're doing a breath first spreading thin all over the place and going deep nowhere. So the consequence of that is that people often miss the foundational ideas. And by people, I don't just mean practicing engineers and data scientists. I actually mean researchers. So I encountered, for example, a paper in medicine that was very well quoted and cited by its peers. And what the paper said in essence is that in the medical community, in natural nature, there's often log-log distribution. Whenever people do data analysis, if X and Y, they can't find a linear relationship, their next instinct is to take log of all the variables, or some of the variables, and then do analysis and log-log. Taking the log seems to be the next best thing that you can do. And I'll give you an ex reason for that especially when data has a long tail distribution it is one of the facts i didn't talk about it that you know that the average is one of the statistics but you also have the geometric mean and you have the harmonic mean did you guys learn about the concept of geometric mean, harmonic mean in class? Okay, geometric means is instead of adding the number setting the average, multiply all the numbers and take the nth square, nth root, right, raised to the power one over n, right? So that's called a geometric mean. Now one of the truisms is that quite often when data shows skew, depending upon how strongly it shows skew, you should either consider... So average will be much bigger, geometric mean will be next, and harmonic mean will be even smaller, and with the right skew distribution. And then sometimes, quite often often harmonic mean will sort of be underestimating your general intuition and geometric mean is just about trying the goldilocks and so it has become common in the medical community that whenever you see uh data with a skew which happens quite often you take the geometric mean now if you ask people why geometric mean, they'll shrug, they have no idea. But okay, then it's a very common practice to take log-log. You ask them why? They say, well, you know, it's that log-log doesn't always work. We shouldn't use it. And the person came up with counter examples of when it wouldn't work, right? And what I want to say is, if you are trained, I hope if you're trained here at Support Vectors, to you, that should be obvious. There can be any power relationship between input and response, isn't it? Log is only one of them. It could be any power relationship between input and response isn't it log is only one of them it could be any power of it or any other functional transform of it so you would have expected that you should have expected that right it is in other words what looked like a discovery in that domain to mathematicians, it is basically or should be obvious. Now, we will take this. Now, this power transform has a very interesting history. And there is a story behind it. So there were two great data scientists or statisticians. One was in US, in Wisconsin, and his name is Box, the same fellow who said all models are wrong, but some are useful. And the other is Cox, Jeffrey Cox, who was in England. As chance would have it, one married the other sister. So they were brother-in-laws. So one winter, apparently the story goes that they came, the Brit, Cox flew over to meet Box in Wisconsin and it must have been a cold winter. And maybe there's a bit of embarrassment to the story, but the way I heard it is that, and this part, I don't know how true it is, the way I heard it is that box and cogs decided to go hunting. When two mathematicians go hunting in the middle of the winter, you can rest assured that there'll be no birds that they'll bring back. Right? Bird hunting. So they must have been getting bored sitting there waiting for something to show up or a prey to show up. And then they had a bright idea. They said, what if we wrote a paper in our joint names? Wouldn't it be funny? People will talk about the box cocks. And for some reason, they found it hilarious, apparently. So they came back home and sat down in earnest to write a paper, to come up with an idea. And the idea they came up with is actually one of the big ideas. It is the power transform idea. So they created the so-called power transform. They found a method to tell you what is the right relationship between input and response. So now power transform is also there in scikit-learn very easily what we will do is we will power transform the y here and see what it does when you do the power transform of y look at the initial histogram of y and look at the y power transformed what do you say about the power transformed y? Anyone? It looks more like a belker. Yeah, it's closer to a normal distribution. It doesn't show this pronounced Q. So it is a method that you can use to tell what it is. Now, there is a mathematical expression for it. It's the lambda of the box-cox transform. One of the exercises for you to reading exercises to go read on the box-cox transform, because we are a little short of time. I won't go into that, but because it is a bit of theory. Or if you want, I can cover all the power transforms together. There are many power transforms now as a separate session, extra session on one of these evenings. So when you do the Box-Cox transform, look at the original data. The relationship between X and Y is on the left, and the relationship between X and the power transformed Y is on the right would you say that a straight line is a pretty good candidate a linear relationship now is a pretty good candidate to model this data definitely it is right and so that is a lesson to learn it's one of the things actually um uh it is i wish i could people were more aware of that this thing exists. There's this bit of thing. So what we will do is we will do that. See, your R squared has jumped to 0.85, right? 85%. There's something wrong. I don't know why my test error is showing zero. Figure out, guys. I leave it as exercise for you to figure out what screw up I did for the mean squared error to be zero oh me oh you know i know why it is i know why it is i deliberately put it to two decimal places you see 0.2 so to two decimal places it is zero i have to go to more decimal places four or five decimal places to see the real error yeah yeah i should have watched out for that 0.2 f means error up to the two decimal places to see the real error yeah i should have watched out for that 0.2 f means error up to the two decimal places value so now if you look at the residual now just compare it with the residual of before this residual shows heteroscedasticity isn't it even with the squared relationship in the raw it shows extreme heteroscedasticity but after the power transform what do you say to this first look on the right hand side does it look like a normal distribution it does right it looks like a bell curve goss would be pretty proud curve. Gauss would be pretty proud of this curve. Now, what about this? Do you think it shows any pattern? No. In fact, it's a classic example of homoscedasticity, uniform variance, as you scan your eyes from left to right. And that's the value of a power transform. You can take an intractable or hard-looking problem, but ask, is there a transformation of the data that will make it into a linear problem? Before you rush on to use some esoteric machine learning model like deep neural network or random forest or something like that boosting which are black box models that don't give you any idea what the relationship is no interpretability becomes hard at least give it a try linear methods are the most interpretable methods they are legally defensible they are free of bias or the bias becomes obvious right So always try hard to use linear methods with this, just not linear regression, linear regression with this expanded toolkit that I've taught you, polynomial expansion, log transformations, and so forth. Use all these techniques. Try your hardest to model it using linear methods. So one point that I wanted to make is log transform is not always right. This is what people keep saying. If you take the log of the data of y, what does it become, guys? Log of y, is it any better? Not really. It doesn't look like a uniform bell curve at all. So I will leave it to you as an exercise to figure out how, what the models look like and play around with it on your own. But taking a log improves a little bit, but you can go ahead with it. So when you do with the log, do you notice that the fit is not as perfect as you would have wanted, correct? So use a power transform. Don't just by default go and use a log transform. All right, that's the lesson to that. That is about power transforms let's this is a good point for us to take another five minutes break and then we will get together and deal with yet another topic in our arsenal which is the non-linear methods go ahead hey i had a question so is there any chance that you take, I mean, the question is, why do you need one equation to fit the whole curve? Can you split the curve into little sections and have different equations that combine them together? Yes. So that is called piecewise regression and splines and so forth. It's a topic I won't be covering in this workshop, but the simple answer is yes. Plain and simple answer is yes. The general observation that, for example, what is true, a model that works in one region of the data doesn't work in another region of the data. So you can partition the data into multiple regions and build different models for different regions. It's actually a common practice. One example I'll give you, if you're predicting, let's say, credit card fraud in transactions, right? So when you do fraud, you can build a fraud model, which works for small amounts of data. Right? You can predict fraud based on certain predictors. But when the values are large, you need to bring in a different analysis for that. Right? Likewise, if you're looking at, see, when people sell houses, if there is a fraud involved you know your grandfather is still alive but you create a death certificate for your grandfather and sell his house or in India so have you guys have you have you folks heard of the walking dead people so what happens is that powerful people your powerful neighbors or relatives they will go to the police station and produce your death certificate. Anytime they see that you're out of town, they'll produce a death certificate. They do it a lot to the widows and to poor people. Your maybe unscrupulous cousin or brother or somebody will do that, produce a death certificate. Now, using the debt certificate, will claim all your property, right? And then when you come back, you realize that the house is not yours and you are declared dead. You're not even entitled to the usual social security benefits, the ration card and other things in India. Apparently there are millions and millions of such people. And there's an activist who has been fighting for the rights of these people. Because when they go to the police station and they say, see, I'm alive. They say, no, we have a certificate that you are dead. And they actually throw the person out of the police station. And it's been happening by the millions. So it's a tragedy. So fraud happens at a vast scale in India when it comes to property. But it also but then also what happens is anytime you buy land in India and this is this may resonate with some of you in Bangalore in one of the coveted cities. Typically you'll buy a land from one guy. You'll try your best to make sure that the land actually belongs to that guy. But right after you bought the land and put a house on it another guy will pop up and say it is my land and a third guy will pop up and say it's my land and they will all have perfectly good legitimate papers and it becomes very difficult to ascertain whose land it really was who really is speaking the truth so there's a lot of fraud now in the us such frauds happen but on a smaller scale. Now what happens here is the fraud, the pattern of fraud for normal houses, you know middle class houses, shows a certain pattern. The kind of fraud that happens for Mac mansions, large houses, shows a different kind of pattern. So when you build a regression model trying to predict a risk score, So when you build a regression model trying to predict a risk score, Raj, common sense says that you should use entirely different methods for different parts of the feature space. Okay, understood. And following another question, so let's say we have a distribution, you created a curve, it fits properly. Does internally, does it create an equation of the curve to predict the next instance or the next outcome? Or it just tries to, I mean, how does it do it? Function, it's a function. So it actually creates a mathematical function. Yeah, you input any value, it will give you a prediction. Yeah, but when you input a value, does it try to fit the curve or does it create an equation and then... No, no, no. Internally, the best way to think of it is that it has an equation in mind. Like, for example, for linear methods and all of this polynomial, everything put together, there is genuinely a function whose coefficients you can see okay it is just plugging it into that equation and producing the value for so for double bend three bend four bend all those for all those curves and actual equation is created of that really linear models are that they are nothing but a linear equation right that is why uh build after you build the model what you're left with is just the coefficients model coefficients and why inference with those linear models are super fast because it's just a any data comes it's just a very trivial computation plug it into the equation and you have a prediction okay yeah that's there so you talked about the power transform right so does it cover all combinations if it's log maybe one use case something else maybe a sinusoidal. See power transforms a very good question that Albert asked does it cover all relationships what if they are sinusoidal structures and so forth. So the thing is it it doesn't actually. Its limitation is it will cover only so long as some log or power transform leads to a linear relationship. If that is true for the data, if that is not true for the data, even power transform will fail. So, for example, you can't somehow turn the gamma function. Remember, I showed you the curve, the one that we dealt with in data set five. That was actually generated. I mean, once you get used to the transcendentals. And guys, if there is one advice I can give you, every week, learn the transcendentals. They'll be very useful. Yeah, this happens to be the gamma function actually i generated this data simply by sampling off a gamma function a gamma transcendental function so this your power transform will not help you right but the power transform is the second thing you should try okay yeah see what happens is data is data has a generative function the point is you don't know so there is a systematic journey of trying things out and seeing what works right and the idea is and this is one of the things the reason i'm emphasizing all this is important what people did is that as methods came about boosting and random, which you will learn in the next course, ML 200, we will do all of those methods, those kernel methods and powerful methods, and then later on deep neural networks, that course that just finished. There's a temptation. Why use your brain? Just apply this model to the data. But you lose something vital. You lose interpretability. You can re-impose interpretability or try to impose, but it's never perfect. Those are black boxes. In the theory session, I warned you against the dangers of black boxes. More than that, it hasn't furthered understanding. See, when you use a black box to make prediction, all you know is that predictions come out. What you lost is interpretability or understanding. When you can explain a phenomenon using an equation that you can lay out on the table and see these are the coefficients, this is a log of this, is that, what have you learned? you learned you have advanced knowledge right you can now write in the textbook you can say the relationship between this is described by this equation you have furthered science but when you just use a black box you haven't furthered science you're basically giving it to the black box to figure it out and give a prediction and those things are fraught with danger. So we must make as much effort as possible to use linear methods before we give up and move to nonlinear methods. So, you know, there's black box methods. Are we together? Try to stay with simplicity. Remember the definition of knowledge in the scientific field is fundamentally, Occam's razor is at the heart of it. You need the simplest effective explanation. So all three words are important. The simplest of all. It must be effective. It should work. A simple explanation that doesn't work is no good. And it must explain, not just predict. Right? It must have an explanation for it. So that's the Occam's Razor principle. All right, guys, let's take another five minutes and we'll get back. Asif, I have one question. So when you said polynomial regression, right? If I apply log log then it becomes from multivariate to univariate right so why do all this hustle of bends plus one odd and even and all this stuff you apply log then and like you get it in one bucket because it becomes univariate because log transfer will say will help you get to a fraction it will tell you so suppose it's a quadratic curve you know like the parabola if you do a log transform you'll come up with a value of 1.5 so what do you deduce from that log transform won't work remember log transform is applicable to the target variable y right okay let's say that's a y equal to a x plus b x square plus c yes and we apply log on both sides then log y becomes the new variable and here and become it doesn't log a plus b becomes an expansion you'll have to expand it in a taylor series ax plus b b x square those two you have to expand in a taylor series expansion not possible otherwise okay got it let's say if a and b are one then it's possible got it if a and b are one then it's possible got it yeah so uh yeah so so the point is you have to put exponential so if y equals e to the power a x plus b then if we do a log transform then log y becomes linear and that is exactly what power transform will tell you when our transform comes up with a value saying zero, it means you take the log of y. And it's also true, for example, one over y may be a function of beta naught plus beta one x or something like that. Okay. So the negative powers of y. So power transform will take care of that log is a special case it is when the lambda is zero it is the log for lambda positive is the positive powers or other values of lambda negative powers okay so if say y is 2 to the power beta x plus beta naught then we have to do log transform but with base 2. because it will just have an underlying extra constant of natural. There will be an extra constant, log of E, log of 2 will become 1 over, is a constant that will get added. But you don't have to do basic. You can just do natural lag. Remember, any two logs, natural log base 2 and log 10. They're just related to constants. Yeah. Good questions guys, good questions. All right, so you guys have had a break, but I need a break for two minutes. I'll drink some water and be back. I'll drink some water and be back. So folks, you notice that when we do linear regression, I'm taking you into far greater than and much more extensively. These things are, you don't usually find it in one place. Very few textbooks have gone through all of it, unless they are a textbook that only deals with linear regression. Maybe they may have it. I haven't come across. You learn these things through years of experience and also perhaps because besides computer science, I have background in mathematical physics. So we use some of these techniques quite a bit. So anyway, we will now move to a new topic. And I'll ask you an interesting question. But let's start with the story. There was there this was it Bernoulli or Jacobi? Was it Bernoulli or Jacobi? Okay, I'm forgetting the French names. Is it either the Bernoulli brothers or the Jacobi brothers? Okay, one of those two. Oh yeah, Bernoulli's. So it turns out that the Bernoulli's, they come as brothers. And then there is a nephew Bernoulli also, and they were all great mathematicians. So the trouble is, people would ascribe or give credit of one's work to the other. And that would completely infuriate the brother. Right. And this mix up was well celebrated in European history apparently. So one day, one brother, he found an elegant way of solving a specific problem. It's a Bacchus-to-Crom problem. I won't go into it. It's like, if you roll a ball from a height and it has to go here, what is the fastest path to go here? It cannot, you can make a slope like this, you can make a slope like this, you can make a slope like this, various shapes, but what is the correct path, slope? It says that if the ball rolled on that slope, it would be the shortest path to go from here to there. Remember, not the shortest distance, but the shortest path. And one brother figured it out and threw a challenge to the other brother, hoping to see if the other brother could do it. So there used to be a rivalry between them. So that became actually a public problem, because the brother didn't pose it to his other brother. One Bernoulli didn't pose it to another, but made it a grand challenge. So Europe had a tradition of all these grand challenges that I talk about in mathematics. So many people gave their submissions, right? And most of them were wrong. Some of them were okay, complicated, but right, things like that. So Newton, he had a, Newton was of course a Brit, and he had an extreme disdain for the Europeans. He believed that he invented calculus. Europeans believed that Leibniz invented calculus, or discovered calculus. And there was a healthy rivalry. The Brits, in general, used to have a pretty hostile attitude towards the Europeans in those days, for whatever reason, historic or political reason. when newton saw this problem he publicly said nonsense i'm not going to waste my time on such silly things or something to that effect but then privately he couldn't resist trying to solve it so he solved it in a pretty elegant way but now the question comes how do you do it so what he did is that he solved it in a pretty elegant way. But now the question comes, how do you do it? So what he did is that he sent it anonymously or in some fake name to as a submission. So when Bernoulli saw Johan, one of the brothers saw the solution, he famously said, I can recognize the lion bites claws. Right? The sheer power, intellectual horsepower of the solution clearly points to Newton, basically. Right? So I'll use this as a story or as a pin to say, just as you can recognize a line by its clause in the same way sometimes you know if you know your transcendental functions you can ask this question when i look at the data to me this looks like a sine wave i recognize it i see its signature then why should i not try to fit a sine wave to this data or i see a bell curve why should i not try to fit a bell curve to this data good questions but then you have to go a little bit beyond linear methods and should i not try to fit a bell curve to this data good questions but then you have to go a little bit beyond linear methods and today i'm going to introduce you to that so let's take this data set once again the same thing this data set 2 which was which i believe raj you you said looks sinusoidal isn't it so now what do we do you know that a sinusoidal, isn't it? So now what do we do? You know that a sinusoidal equation, if you go back into your mind and look at the equation of a sine wave, it is this. Way back in the dim fogs of history, right? You must have done the course which taught you that the equation of a sine wave is this. It's very simple actually. A k is the wave number basically the related to the amplitude. A phi is the phase shift of the wave. A is the amplitude. There are only three degrees of freedom. So you can either increase the amplitude of the wave. you can stretch the wave or shrink the wave or you can take the entire wave as it is and translate it, the phase shift. There are only three degrees of freedom here when you're thinking sinusoidal. Does it make sense guys? Yes. Right? That is it. And so there are three parameters by which you can measure it. So now there is actually a way that you can fit a function directly to the data, which is what I have done. You hypothesize that it is a sine wave. You notice that you write it as a sine wave function, right? And then comes a little bit of a, there is only very simple, just like you have a model.fit. Here it is curve fit. This is part of the SciPy library. You can say curve fit, assign wave to the data. So let's read it out. This curve fit function is asked to curve fit a sine function. Where is the sine function defined? You just defined it here. Fit a sine function to the data. Input is x, output is y. And initial guess, you can take whatever you want your initial guess to be. So you have to make an initial guess. You're saying, let the amplitude be 1, let the wave number be 1, let the phase shift be 1. If all three are 1, use that as a starting point. And from there, start finding a better and better solution in the hypothesis space right and it will do that when you do that it will come to the conclusion that amplitude is 1.45 let's go and check is it true amplitude is 1.45 so from zero it this yes 1.45 the phase shift is just a little bit one and the wave number which is 2 pi over lambda is this right so you if you want to know the wavelength what is the wavelength here it is six right so did by the way, did I write it the opposite way? K is this and phi is this. OK. So you can plug those things into this and see whether it all makes sense or not. So this is the answer that it comes with. Now, let us compute the confidence intervals. This is, again, a little bit. You can say can say, how confident are you of your answer? It will tell you. In these things, you can't find r squared. r squared is not defined. What you can do is see the correlation between the prediction y and the actual value y hat. So the best you can do is make a correlation plot and the correlation plot should be linear. Y and Y hat should be linearly related. If you do the residual plot, would you say that the residual plot has no patterns? Yeah. So it does exhibit homoscedasticity. So it does exhibit homoscedasticity. And let's plot our model over the data. So guys, this code, read through this carefully. There is no magic here. There's absolutely no magic in this code. Maybe I'll walk through this. You first scatter plot the data itself. Then you create new data, and you take the sine wave of your newly created data and plot that. And then you plot the confidence intervals. You can see that the model pretty accurately has gotten it. Right? So what about another data set? Look at data set three. You remember data set three looked like a bell curve? Do we remember that, guys? Data set three. Where is my data set three. So you see the data set three looks like this. This should evoke a question in you, isn't it a bell curve? And if you were to think like that, you would say, let's go try and fit a bell curve to this data. When you do that, once again, the method is exactly the same. All you have to do is write a bell curve function. All you have to do is write a bell curve function. How do you write a bell curve function? Well, this is the definition of a bell curve. You have to know the mathematical expression. So let me explain what the expression for a bell curve is. It is scary. It is actually, once you get used to it, it's easy. But the first time you see the expression of a bulk curve, it might be intimidating. So y, sorry, let me take black here, y of x is equal to 1 over 2 pi 2 pi, right? Sigma, is it sigma or sigma squared? I will remember. So today my memory is over sigma squared, a half, right? So this exponential function will give you, it is symmetric because it is a square of x, makes it symmetric, left and right side have to look the same it starts at 1 because e to the 0 is 1 so this is mu and sigma is the spread of this and it tapers off to 0 right now let me see if i got my formula right off the top of my head. Oh, square root. Oh, sorry. I got the square root off. Sorry. That's what I was thinking. This is the square root. So people often write it also like this, 2 pi square root sigma, which is the same thing. This is the same thing as 1 over 2 pi square root sigma outside it, right? So this is it. So if you were to write it like this, you're just defining the function. The rest of the thing is exactly the same, what you did. You notice that this part of the code is exactly the same that you did. So this is a technique you can use. Now, according to this technique, it says that the mu, the center of the bell curve, is at 1.4. Let us go and see, is that true? The center of the bell curve is around 1.4. Is that true? This point is around 1.4? Looks like. Looks like, right? It is 0.98 approximately. So A that you have to multiply it with and mu sigma. Because the bell curve may be stretched vertically. So then once again, you make a prediction. And do you think the prediction agrees with the data? Would you agree that this curve completely beats, hands down, beats the polynomial regression? How many of you would agree with that? Right. And likewise for the gamma distribution. Now it's getting repetitive. Gamma, you again need to know how to write the gamma function. distribution. Now it's getting repetitive. Gamma, you again need to know how to write the gamma function. And when you write the gamma function and you plot the, well, the scatter plot between y and y hat, because there was no noise, you get perfect correlation. And look at the agreement, how well this regression model, this prediction model agrees with data. model, this prediction model agrees with data. So guys, what did we learn? That sometimes two things is, I'll give you two lessons guys. First is that so often I have seen that it doesn't often happen when you're looking at univariate, but even in bivariate, etc., quite often, or multivariate, you can look at the data, visualize the data. And if you are familiar with transcendentals, you can spot a friend. Or to use Bernoulli's language, you can spot the line by its claws. to use Bernoulli's language, you can spot the line by its claws. You can spot a friend of yours and say, aha, this is what it is. Let me try using this. And when you do it, it may or may not work. Your hypothesis always may be true, may not be true. The data will speak. But it is worth trying. So in other words, there are there is a world beyond interpretable models, which in some sense are even easier than linear regression or more direct than linear regression. But they need you need to be aware of that. You need to know your thing. So the other lesson that I would say, and I cast it typically jokingly, now do one thing guys, this is your home dataset is reminiscent. Now I'll ask you this question. I wrote it as a beta function, but actually this shape belongs to, or is mimicked by, or produced by many other distributions. For example, a log normal could look like this. A beta distribution could look like this. A Poisson distribution could look like this. A beta distribution could look like this. A Poisson distribution could look like this. So your homework is, the homework is for you to try out, could you please do that? Just replace this function, this function, gamma-like function, with the other functions functions a log normal uh beta distribution poison distribution and see how well they fit and please post your solutions your model pictures and your answers to the slack channel please do this and guys do this you learn a lot by doing this. Do this and see, do other distributions also fit? Right? By the way, this famous Bacchus-Strom problem, there's a lovely article that you can go read on this. There's a lot of history to it, the famous problem of the Bacchus-Strom. Go read it if you are interested in the history of this field. Go read it if you are interested in the history of this field. It's a very interesting reading, and of course it is at the foundation. One of the foundational ideas used is the calculus of variation, which for for calculus of variation is practically at the foundation of theoretical physics. Quantum field theories are all based on calculus of variations. Anyway, so I'm going to stop here, and I'm going to take the next 15 minutes to review all that we have done. Actually, we have a choice. No, I won't review. I will now do a multivariate data. So this is all good for data of one variable. In real life data, there are multiple predictors that go into it. So we will take one data set. It's a famous data set, auto mileage data set. It's an old data set. And let me give you the basis of it. The target variable is you have to predict the mileage, the miles per gallon of a car. And you're given certain features about the car, how many cylinders it has, how big those cylinders are, you know, the displacement. So what is a cylinder like? What is an engine like? Imagine a cylinder. Okay, let me, a very simplified picture of an engine like? Imagine a cylinder. Okay. Let me, a very simplified picture of an engine is, imagine that you have, this side is closed. This is fixed. And you have a piston here. a piston here. The piston can go back and forth. And what happens is that there is a little injection port here, fuel injection, a fuel injection port, and there is also a spark plug, somewhere helping it out. So what happens is that a fuel is sprayed into this. So you have atomized fuel, your gasoline fumes. Then you create a spark. That spark ignites the gasoline. The gasoline. the gasoline and what it does is it pushes the piston when it ignites, it heats up on ignition and it pushes the piston back. Are we together? And so the piston will get pushed back to let's say this location right and when it does that there's a crankshaft mechanism that converts it into rotary motion right so what is back and forth of the piston? What it does is this turns and the momentum of this turning will cause this piston, because once the fuel has been spent, there's an exhaust port and has been evicted. Now there's a vacuum. It can easily go back in. It goes back in and the process repeats itself. This is your internal combustion engine. This is an internal combustion engine. Now, in a car, actually this crankshaft, it is connected to not just one cylinder, but it works together with quite a few cylinders. So when you hear these words, V4, V6, V8, it literally means that the engines are in a row of Vs. They're like a book. If you were to imagine a V-shaped engine, it's one of the oldest engine designs and imagine that engines are laid out in a v formation the cylinders are laid out in a v formation here right and at the bottom of this is the crankshaft this crankshaft is somewhere here at the bottom so what is happening is they are all going like this right one is going in the other is coming out and so on and so forth right and so you have rotary motion so this is the classic design of a v8 engine or v engine now you have v4 which has four cylinders v6 v8 v8 is pretty much as high as we go these days v12s are relatively these days. V12s are relatively rare. And even V8s are relatively rare because they are basically gas guzzlers. So if you want to be a fuel efficient, then you would go with a V4, is it? So this is it. The predictors of miles per gallon, common sense would tell you, is how many cylinders you have. The more the cylinders you have, more the cylinders you have the more you know fuel you're injecting all the time right because you're injecting in so many more cylinders each of which is going through the ignition right how big the cylinders are right so if you cylinders are big basically you have a big engine right big displacement would happen so cylinder volume and the displacement the pushback, these are all features that will help predict. But that is on the energy production part, but then this energy that is produced the wheels that are turn is dragging a the weight of the car forward, isn't it, the vehicle or the SUV forward. So weight is another factor, isn't it, how heavy it is. of the engine design and the weight not necessarily a feature not necessarily a fundamental feature in its own right is the acceleration now the way acceleration is given in outside the normally is like meters per second square like how hard you're hitting the accelerator but in the car industry the convention of acceleration is actually the opposite. People count how many seconds did it take you to go from 0 to 60, 60 miles an hour. So the smaller the number, the more powerful the car. Right? And the bigger the number, the less powerful the car. So that would depend upon, I mean, obviously, if you want great acceleration, you want a mighty engine and a small car, which is what race cars and sports cars are. So that's what you do. So these are the features that go into it. It also, it turns out, depends upon the origin of the car, whether it's made in Europe, it's made in US, it's made in Japan, and where it is run. Actually, many of you may know that the same manufacturers, Ford or Mercedes, et cetera, will make a car that practically looks alike, but in europe will give higher mileage than it will give in u.s because europe has a much higher fuel standard they require you to do more to bring about fuel efficiency than the u.s in u.s the laws on this matter are a bit more relaxed right so there are different countries of different conventions. So the origin of the country for that matters. It's an old data set. Today, by today's standard, our mileage is much better. So now let's ponder over this data set. We load this data set, we sample over the data set. So what are the features here? Miles per gallon. This is our target variable. How many cylinders it has. Now it makes sense, guys. Right? Displacement, how far the piston is pushed back, right? Horsepower. Horsepower is of course a consequence of all of these. Weight. Weight of the car, right, or the vehicle. Acc the vehicle acceleration now remember acceleration here small is better because if it says 15.5 seconds it is much worse than 12.5 and it makes sense this is a tiny little car some of you may remember this one chevy oh chevro chevette i have no idea what that car is relative to to AMC Matador, which also, I don't know if it is a model, still in existence. So year of the car, the year the car was made. So which do you expect to have better fuel efficiency, the car of the 1970s or the car of the 2020s? 2020s? 2020s? Yes. Especially electric. That's not there yet. That's not in this data set. So, yeah. So if you look at the range of values, the year, it goes from min is 70, max is 82.'s 12 years worth of data right and there are three countries of origin so we will analyze this data now this data has problem there are some missing values they are represented in the data set as question marks, so you have to remember to clean out this data set so guys I would strongly encourage you to very, very carefully read through this lab. Right and try to do it on your own, we will give you homework on somewhat related data sets to see how well you give you some practice on doing this sort of analysis. So actually in view of that guys today I'll take an extra half hour. So you take this data and you clean it out. You just drop, I believe, data wherever you see. You just drop the data which has question marks because you have the luxury of dropping a few rows. It's okay. Also, the name, would you think that name is a predictor if you change the name of a car its mileage would increase no so name is an irrelevant feature let's drop it so let's drop name from the from our data frame it's use. Let's see if there are null values. Fortunately, there are no more null values. Null value analysis also shows there are 392 records and no null values. Information on the data. Now, something interesting. The origin is interpreted as string. One, two, three, which is good. Why do we interpret origin as string? Because even though it is placed as a number, in reality, it is a string. It stands for, if you read the documentation of the data, it will tell you that one is US, two is Europe, and three is Japan. Or I might be getting the specific order a bit off. It's categorical. So what's the benefit of that? People in databases, right, the people who do database admins, they try to save space. So they will always code it. They'll put a code, 0, 1, 2, 3, instead of putting the word America, I mean, US and Europe, et cetera. So that is also not useful. It's not terrible. No, origin is useful. It turns out that the origin of the car matters in predicting the mileage. Remember, I told you that if the safe car is run in Europe, it has a better mileage. So now, this because if car is run in europe it has a better mileage so now when you have might be that there was a missing value there might be a question mark over there and that might have caused it to get interpreted quite like but it is indeed a string value in other words should be treated as a string value because uh one two threes are not ordinal numbers. You're, Japan is not more than US. It is just a different coding and we should treat it as a categorical. Okay. So now let's look at the histogram. Once again, very simple data.hist. Do you see how easy it is to create a histogram? Nice, beautiful histogram of it. So let's ponder over this histogram a little bit. Observe that acceleration seems to have a more or less symmetric distribution, but other things have a somewhat skewed distribution. Displacement, most cars have a small displacement and then some cars have a significant displacement. Cylinders, of course, most cars are four-cylinder. Well, you can't read this tiny font perhaps, but it says four and some cars have six cylinders. Oddly enough, there used to be cars with three and five cylinders in those days i don't know how they were designed maybe horizontal displacement uh h shaped engines or something very different engine design by the way we we the v series the v4 v6 va design to my knowledge is more than 100 years old now it's still going strong and the the only thing that will probably displace it is the electric cars, other electric cars. Now, if you look at the miles per gallon, do you see that the miles per gallon looks asymmetric? It has a skew to it, a modest skew to it. Likewise, horsepower. So, and weight, they seem to have skew. So i'll leave this as an experimentation to you guys does it make sense to do appropriate power transform or not now let's make a joint scatter plot between these variables and this is where we spend a lot of time thinking so the diagonal is just the histogram, miles per gallon. So by the way, this is miles per gallon cylinder, these features. And these things are scatter plots. And the bottom part are called density plots. It will have circles. Regions of high density will have lots of circles around it. Regions of low density will have less, you know, more spaced out circles around it, right? So it gives you the data density. Where is the data most dense, okay? So these are called kernel density estimator, KDE plots. KDE is an involved topic. We'll deal with it in the ML 200, the next workshop. But here, I just think of it as density plots, two-dimensional density plots. Right? Now, observe this. Let's zoom in. When you look at the relationship between, look at this. Are you guys able to see where my mouse is? Can you see? Is it visible on zoom where my mouse is guys? Yeah. So I'm looking at the relationship between miles per gallon and cylinder. Miles per gallon is the y axis number of cylinder. Why is that number of cylinders? Is the displacement, sorry, this is displacement. Displacement, how much displacement of the cylinder is there means how big each of the cylinder says uh essentially is the x-axis so what do you see the more powerful you make an engine the bigger is the bigger the each cylinder in the engine the mileage increases or decreases that's so very unfortunate I sports cars. I wish the mileage improved. That would be a magical land isn't it? So well, unfortunately, real life, the more powerful the car, the worse the mileage. It decreases. Is the relationship like a line what does it look like to you does it look any no it looks like a DK it looks like a decay curve so keep that in mind likewise this looks like a decay curve this looks like a decay curve what about this this is a little hard to tell what the relationship is isn't it but you see this little bit of a funneling effect here. Now, this somewhat looks linear. Do you see this relationship, guys? Modestly linear. What is this? This is displacement against horsepower. The more the horsepower, the more the displacement displacement which is common sense if you increase the size of a cylinder will you get a more horsepower engine or a stronger engine or a weaker engine you would get a stronger engine isn't it more energy is there more thrust is there okay so this makes sense to us likewise so guys take this notebook and ponder over it. We are all used to cars and automobiles. So observe it, study it, study these boxes and align it with your experience. Ask, does it agree with your experience or not? to actually this analysis problem that is there in your textbook in chapter three, literally, but except that we have solved it using Python, not using our language. The textbook means scikit-learn textbook. So we go here and data has an asymmetric distribution. It turns out there are far more automobiles from US, less from Europe and Japan, that are being more Japan and Europe at the least. Now, let me introduce you to another beautiful visualization. These are called the violin and box plots. What the violin shows is the data distribution right how much data is there for each value of miles per gallon so obviously from this what is the peak most cars tend to have how much mileage here looking at this data in in u.s what's the average mileage or the the more the most 16 17 15 16. what about European cars they tend to have about 30 32 30 mileage and what about oh no these are Japanese cars what about European cars Europeans have about 25 and it also speaks to how strict the fuel standards are in different countries Japan obviously focuses a lot on fuel efficiency. The cars are really, I believe even now this is true, if you want the most fuel efficient car you can go get a Prius compact that will give you 60 plus miles a gallon or so. And or so and so likewise these are the plots and so guys familiarize yourself with these tools these plots they will help you understand the data inside is the box and the box plot it it contains the the main quantile of the data 25th to 75th quantile and then the whiskers like how far the data stretches min max values so now just writing it as a box plot right these are outliers and these whiskers are the limits beyond which you have the outliers right so this is again the data distribution i'll just shrink it a little bit. Now, when you have data like this, you will have correlation between the data. Now, that brings us to an important fact. When things are correlated, especially they have a lot of correlation. So this is the correlation between any two variables, right? We remember correlation. We can plot it out. And whenever there are strong correlations, negative or positive, then we have a problem. See, when there's a correlation between the target variable and any of the features, it's okay. It's a good thing. It means the target variable tends to vary with that input feature. But when there's a correlation between two of the feature, then it's problematic. Linear methods, they develop a peculiar problem. It is called the problem of multicollinearity. Let me summarize by first making the statement. High correlation between the features, input features, is a bad thing. Whereas high correlation between a feature and target variable is a good thing. Why is correlation between features and input features a bad thing? It leads to the problem of multicollinearity. What it means is that, look at this bowl. Remember the error surface looked like a bowl? The more two variables get correlated, the more that bowl actually begins to look like a canyon, like a Grand Canyon. So in a grand canyon, imagine like this, some parts of it. What happens, especially if you can imagine there's a valley here or a river flowing here, do you realize that it's very hard to find the minima? It could be anywhere, right? The bottom is pretty flat and it is stretching the length of the canyon. But each point represents a vastly different hypothesis. Right? So multicollinearity comes because they are all of these solutions that all lie to be the right solution. And very small changes in the data will make the model learn a completely different point as the solution. A completely different set of parameters as a solution. So it becomes unstable. Are we together? Multicollinearity leads to instability in the solution. And this problem plagues only the linear models. So you have to watch out for it. You will see that you make better models with linear methods if you find the highly collinear factors and you weed them out. Suppose three of them are highly correlated, keep one, drop the other two. So we will do that. As we move forward, we'll do that. So now let's look at first this homework, this book says, predict miles per gallon simply as a function of horsepower. So horsepower is your x-axis. Y-axis is miles per gallon. When we do that, this thing clearly is a curve. It's not a straight line. So when you have a relationship like that, and by the way, this is one of the tricks, let me sort of mention, it's one of the things a physicist in general, theoretical physicists are very attuned with. Whenever they see a distribution like this, their basic instinct is to think y is proportional to 1 over x to the n. Because all of these distributions look like this, inverse power. These are called inverse power distributions. So those things are good approximations. It doesn't mean that those are the only things that will do it. There are many functions whose distribution look like this. But of all those functions, the simplest is this, because still a polynomial function, whereas all others are transcendental functions. Am I making sense, guys? So if you hypothesize that the relationship between y and x is this, how would you linearize the problem? the problem simply by taking log of y will be proportional now to log of 1 minus n times log of x. In other words, this is 0. Log of y will depend on up to a factor n log of x. So in other words, you can build a model like this. Log y is equal to beta naught plus beta 1 log of x, right? So this is your y prime and this is your x prime. So you're saying y prime is equal to beta naught plus beta 1 x. So this is an instinct. I wish it was more emphasized that wherever you see an inverse relationship, you can try this technique. So you do that. We will do that. And when you take the log log here, what happens? Do you notice that this becomes a linear? So our hypothesis was right, isn't it? A power law seems to describe it by the way this is real data it's not something that i cooked up it's a real automobile data from long ago this relationship becomes linear and you can write a regression equation right if you make a regression equation just on input and output variable without taking a log transform, you will end up with, what happens if you don't do that? Let's first do it without the transform. You make a prediction. What is your R squared? Let me increase the font size here. Your R squared is about 56, 57%, right? But when you look at the residual what do you see do you see that the variance increases as you go from right to left yes so what is the name for this from left to right yes from left to right yes the variances increase of the residuals what is the name for this phenomenon this is called heteroskedasticity yes heteroskedasticity this is heteroskedasticity right and when you you can see that there's a little bit you can prediction error ignore that uh this one i'll skip i won't we don't have time for this um when you visualize the model prediction on the data a straight line hypothesis misses the point right it misses the main characteristic of the data which is that there is a bend in the data pronounced bend gentle sloping of the data on the other hand when you take a log transform and it's very interesting, that the book does not mention or allude to it. It basically asks you to stop here. And I think that's one limitation of this book. You take a log transform. And now you build a model. What does it do to your thing? 72%. Remember, with just one variable. Just one variable, which is is what was the variable i forgot um horsepower with just horsepower 72 percent of the explanation is there you can explain the mileage literally in terms of the horsepower right the more the horsepower the less and now when you do a it's a 72 percent is the r squared and now you look at this guys what do you notice do you see a pattern in the residuals? No. There's no pattern. And the errors have a much more bell curve distribution. So this is a much better model to make, right, clearly. And when you project your model on the data, I've done a log-log data. It seems to go as a straight line. But I invite you to do it in the original data now, transform it back to original data, and see how well the curve goes through the data, right? So you might wonder that, so this is it. What about power transform? Shouldn't we have tried power transform? We just learned about power transform, didn't we? So what happens if you do a power transform? Let's do that. You do a power transform, right? And I invite you to do that. Skip, I think, pipeline. You do a power transform. What will you do? A power transform will take you to 64%. Do you notice that? So what does it do? A power transform will take you to 64%. Do you notice that? So what does it mean? The power transform was not as effective as using our own intuition. Yes, the log-log transform beats the power transform. And this will always be true, guys, when you can understand the data. And you you know your math you will always come up with a hypothesis that will beat you know just just mechanically using a tool always right so what if we did a polynomial regression well even polynomial regression doesn't take you much better it just becomes 66 percent doesn't beat the 72 percent of the log log transform model so this is it you experiment you play around with things now that was using one variable now let's go use all the variables right so first before you do that you should ask yourself what is the null hypothesis? Dummy regression will give you the null hypothesis. So there is a dummy regressor, right? It will just pick whatever. It will pick the median of the y or something like that, mean of the y. Then the first model is you do a linear model without any data transformation. But now all the predictors are there. So you go from a naive linear model to a naive multi-feature linear model. It goes to 80%, pretty good. But when you look at the residuals, what do you see? You still see heteroscedasticity. You see the pattern. You see the funneling. Not very good. And when you, so obviously it's difficult to visualize higher dimensional data. But now what we will do is we will apply the scaling and power transform. Just for fun, we'll apply the power transform. And so there is, let me introduce you to a function called pipeline. In data science, we often talk about pipeline. Data goes through a set of transformations. Now let's remember the fact that we should have anyway always remember to scale the data. Do you remember? For linear models, it's not necessary. But for general machine learning, you should always scale the data. So here I show you that actually you can do all of that in one single line. Make pipeline. First scale the data. Do appropriate power transform. And then apply the linear regression. So these two pre-processing steps become part of the model itself. Pipeline model. These are called composite models, which is a pipeline. And these are the steps it will do. So when you do that, and when you try to make predictions with this model, you realize that it goes to 82%. Well, a little bit of improvement, right? But you still see some level of heteroscedasticity, right? So do you realize that that power transform, even for multivariate, led to some improvement, a modest improvement? What about a log transform? You do a log transform of the data, and then let's see what happens. You judiciously do log transforms of some variables. Also, you drop, the crucial point is, you drop some variables that are highly correlated with weight, right? So you say that, you know, because of high correlation, maybe they are screwing up the model. And guys, play around with it. These are ideas. Data analysis like this needs a lot of experimentation. Did I try this? Did I try that? Let me try this. And then you hit upon good models. So here we go. We do this, a log transform, and do you notice that the model jumps to 87%? And then it is rather satisfying to see that the residuals, do they show pattern? No. Now you have homoscedasticity. So you have found a good model. Actually, this analysis, you'll see many examples of this analysis on the internet. you'll see many examples of this analysis on the internet but i haven't seen carefully done analysis like this in any one of those notebooks on the web if you guys find something google it up you don't see what other people have done but in my experience we have not now comes one more concept that i want to bring about. Linear models have a problem. Problem is that outliers, there are two kinds of data that can hijack your models. One is outliers. So outliers sitting there, they will just mess up your model. They'll pull the curve towards itself, especially if you're using mean squared error. So you have to use a different loss function, error function. So the other is actually even more insidious. These are called points of high leverage. These are actually hiding in the data looking normal. But when you do your learning part, mean squared error, they have undue or overwhelming influence. And they influence the curve to look one way, the line to look one way, whereas their absence would have made the line look more representative to the data, more faithful to the data. So these are points of high leverage, high influence. And so there is a technique called the Cook's distance outlier thing. And you can see this hyphenated line, that there are a few points of high influence, but not significantly high. Those influences are, like, if you really look at it, how much influence they exert, these are still small numbers, relatively speaking. So you come to the conclusion that, yes, there are a few things, points that exert. And so it brings you to the question that maybe instead of using linear methods, you can try some other methods. But remember, in my view, you have already achieved 87% R squared in real life data where there is always a bit of confounding factors. There are things that you don't know about. This is a pretty good model that you have built. Now, one question comes, and this is a very important question. So in mileage, which is the most important factor? Should we work on the displacement of the engine? Should we work on the, you know, which factor most influences the mileage here? Is it the year that the more recent the car, the better the mileage? Is that a factor, the biggest factor? Is it the origin of the car that's the biggest factor? Which country it's made up of? Which factor is most important in the hotspot? So there is a technique. This feature importance helps you understand what your model is saying. One easy way is just to take the coefficients of your, if your data has been standardized, you can just look at the features, the coefficients of the features. features the coefficients of the features and the coefficients of the features are pretty good indication of how important it is if a coefficient is big means one unit increase in the feature will cause a big increase in the target value right and so looking at the coefficient gives you a sense that weight and horsepower and year are significant. All other things are relatively insignificant. Do you notice that? All other factors are relatively insignificant, but the top three factors are weight, horsepower, and year in that order. Well, it turns out that this works only for linear methods, linear regression. There is a more powerful feature that goes to the great mathematician and data scientist. Oh, on this machine I did not install Shapely. I apologize, I need to install Shapely. So draw this plot out and when I post it, Kyle, let's post it with the Shapely, this thing run. Okay. Yeah, I didn't, in this laptop it isn't there. And I also put instructions that we need to, people need to install the shape package. So Shapely was an amazing person. He had like, for one, he made amazing discovery. He was actually, I believe, a classmate or a friend, a collaborator of, okay, some of the greatest mathematicians at Institute of Advanced Studies. I'll take the names later. I'm too tired, I think, mentally. So he created this idea. Suppose you play a game, collaborative game. Suppose it's cricket or football a cooperative game and there's another team and that and your team won now how much of the success can be attributed to the goalkeeper to the wicket keeper to the batsman to the various batsmen etc right batsman to the various batsmen etc right how should the credit be distributed like how much was the importance of each person that's a complicated problem in real life that decides how much people are compensated when a project is done people fight over it said that i made a big bigger contribution i made a big this is there it turns out that it's a game theoretic problem for which there is a precise way to answer it. He wrote an equation which intuitively says that play the game with the player and without the player and see how much improvement there is in the game value score when the player is present and now do it for every player and then not only every player do it with all possible combinations of players three players two players ten players etc when you do that very large computation and you you can get the so-called shapely value of each of the players for that one idea shapely was given the nobel prize because it solved an outstanding problem in game theory and economics right how much does each factor matter you build a model right so today for example the great debate is how much does increasing the interest rate, how likely or how much will it actually influence inflation? People ask these questions. What would happen? One thing in debate is if you increase the minimum wage, what does it do to the economy? The big debate. Nobody seems to be agree agreeing on that right so anyway uh from that here how does that idea relate to this to to our linear methods and machine how does it relate to machine learning so the shapely is more universal what you do is you imagine that each of the factors each of of the features, are playing a cooperative game in predicting the value, in putting the output. Horsepower has some influence. Weight, displacement, they all have some influence in the mileage. So it is a cooperative game, in the game theoretic sense, between these features. So you can apply the machinery of shapely values to come up with this i wish this was working uh given time so after i finish the session i will run it right here and show you you come up with very beautiful plots um very beautiful plots here for this that again will show to you pretty much the same order of importance that weight, horsepower, years are the most important factors. Now the beautiful thing is Shapley generalizes to all machine learning models, not just linear methods. Looking at the coefficient only works for linear methods and only works if the input variables have been standardized, right? But Shapley always works. And then there is also something called partial dependency plots, which shows you some intuition of the target value, how it depends on each of the factors over the range of values of that factor, right? So partial dependence on each of the factor. Both of these plots, let's take it up next time. I'll take some time out and we will cover it but we have exceeded our allocated time for india i know it's getting very late at night so i will stop here and take questions was it fun guys today oh yeah so what if errors are correlated errors what do you mean by errors are correlated uh like the error correlation shows up as heteroscedasticity right you see a pattern there. So you're saying that the shapely solution solves everything. So just use that. Yeah, you should use shapely all year. So you don't need auto mpg lab is to be released to also one person. So Kate, one hold on and say, which is asking what is it you don't need. So you don't need other options on the table, the log and? No, no, no. This is only for feature importance. Feature importance, you have only two choices. Either you look at the coefficients for linear methods or you do Shapely. Linear methods, you can look at the coefficient if the features are scaled. But for other other things it doesn't work but shapely always works right it is only for feature importance okay please go ahead the auto mpg notebook is to be to release too also right yeah release too but please make sure if you're releasing it uh please make sure that in the version that is online and the shapely and these things have been properly run and the Shapely and these things have been properly run. I'm running the notebook now. I just installed Shap. Okay. And upload the notebook again. Hey, sorry. Asif, I had a question for this Shapely thing. It's not always possible to remove. Like, for example, you can't make the interest rate zero to see if it's effect on inflation. Sometimes you have to have a number there. How do you in that? No, no. What happens is that if you feed it into the model, by definition, you would have many values of the interest rate built in into the data set. Otherwise, the data set cannot learn. built in into the data set. Otherwise, the data set cannot learn. Right. But how do you know how much weight the interest rate has in inflation? Yeah, see, what happens is you build a model, let's say you build a linear model, right? There is a certain, what it will do is, it will first set the value of the inflation to some fixed value, let's say zero. At zero, it will say taking inflation out, now what is the feature importance of all of the models and how well all of the features and how well do they predict, I don't know, inflation. Then if you bring the interest rate back in, then how well it correlates to inflation. So it- We need to be careful that one input variable doesn't influence the other. That is always the problem with linear methods, but with Shapley, this is what you do. See, even if two variables are highly correlated, the way the thing works is, so I'll give you an example. Let us say that displacement of the cylinder and the horsepower, let's say, are highly correlated, which I believe is true here, is highly correlated. you know that one is enough in your model it will give 50 50 importance to both right instead of giving saying i think i think i understood um yeah it's not which is not a perfect thing but that's what it will do yeah you eliminate if there are co-dependent um yeah okay got it dependent. Yeah. Okay, got it. Questions? Any other questions, guys, people in India, you already asleep? How many people are still there in the audience? 17 are still here. Good. Guys, those of you who are new, if possible, please stay back for a few minutes after this. Alright guys and then if that is it please do as you notice now we are picking up speed guys and please do study these labs and we'll do the homeworks really do the homeworks and play around with it polynomial degrees play around with it play around with different functions to fit into the curve think of your own function transcendental function or whatever and say let me functions to fit into the curve. Think of your own function, transcendental function or whatever, and say, let me try to fit that to the data. Data science is all about, science is all about experimentation. You experiment with ideas and see what works. Albert? So if you have dependent features, which one do you drop? You have to experiment which one to drop a linear model which one dropping which gives you the least decrease i mean gives you the best effect all right guys so thank you for being here we can stop the recording