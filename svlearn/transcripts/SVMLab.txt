 We are live on YouTube guys, be careful what you say. And recording is about to start. I believe recording has started. All right, guys, I hope you guys are all seeing my screen. I'll increase the font size. I hope you guys are all seeing my screen. I'll increase the font size. Is this font size okay or should I increase it a bit more? I think one more. One more. All right, how about this guys? Yeah, it for me. I have a large screen. So I'll just recap about the river dataset. We went through all of it. What did we do? We did the standard imports, no changes. We loaded the data and this by now, I'm just repeating so that becomes second nature. We did some profiling of the data and there's the profiling of the data. And for some reasons that was over here. So then we built a naive logistic model, which failed. Then we did polynomial and logistic, which led to some degree of success. And finally, we did a more careful analysis. In a more careful analysis, we went to, and that more careful journey was through regression. It just illustrates that quite often when you solve a problem, you can take all sorts of indirect routes to finding the solution, building a good model. So when we did that, I remember that we had some confusion matrix and values classification. We got close to 90% accuracy, which is impressive. So that kind of accuracy, now what is ROC curve? It just gives you an idea of how good your your model is you want the line to have the top left it seems to be doing that this is the number of mistakes you are making once again the number of mistakes we are making is small so feature engineering worked then we tried decision trees you can treat to do it again just a couple of lines of code. Now there are a lot of things you can tune. Like for example, here I wanted to make a relatively simple decision tree. I said max depth is equal to five. If you don't set that depth is equal to five, it'll make a very, very complicated decision tree. Depth three, four, even, maybe. So when you do that, you realize that this is actually visibly worse than future engineering. And there are quite a few mistakes. When you do model diagnostics, you realize that you get the accuracy. Actually, yeah, you will get some accuracy and some, this is for, I think we move now to random forest. I already moved beyond decision tree. This is random forest and then, i think i have run this class in a different order so something is off anyway i'll post the rerun everything and post the latest version today what i wanted to show you is how would you apply a support vector machine classifier to this data i've applied the svm classifier to the river data and i have applied the svm classifier to the California housing diagram. Remember, river is a classifier problem. California housing is a question problem. We are trying to predict the median house value. So with that in mind, you realize that from a code perspective, it's very straightforward. You go and build a model. Once you build a model, you build a model you use it to predict once you predict you go and get the confusion matrix when you get the confusion matrix once again you create the classification report and you get about 80 percent uh spm model that is 80% active. Now when you look at this number, you realize that this is not as good as your feature engineering. It is actually worse than random forest, but better than decision tree. Once again, this illustrates the no freelance theorem. This problem was not something which brings out the strength of support vector machines you could do some other problem to bring out now i'll give you this as an exercise with the default values of svm you see that i'm using very default values for doing the sd the model that i get is not that great yes The model that I get is not that great. Yes. Yes? Somebody is typing. Yeah, there is a lot of disturbance. Please mute yourself. And maybe unmute you. Yeah. So you notice that I use the vanilla version of the SVM. SVMs actually traditionally take a lot of tuning. There are a lot of parameters to tune. Guys, somebody please stop typing or I may have to mute everyone. So if you just check if you're on mute, if you're not on mute, make yourself on mute and when you feel like speaking please unmute yourself. Okay so when we look at this classifier you notice that I have not done anything special here. There are a lot of parameters that you can play with in a support vector classifier. Remember, there was the budget, the cost. How much cost are you willing to give it? And things like that. There are many, many parameters that you can give. You also remember that you could have used a polynomial kernel because polynomial regression worked for us, right, for the river data set. So you should play with this. You should change this to polynomial. you should try out different things and try out these parameters i'll give you guys a hint svm can outperform all the other models here but it will need tv okay i leave that as an exercise for you to play with and try. I've deliberately left it in a vanilla form at which the results are good, but they could be much better. They're not overly impressive. Remember that the state of the art results you would get is close to 90%, 89, 90% accuracy. We are not getting that with SVM in the vanilla form. So this brings out the fact that complex models need a lot of tuning before they start. The hyper parameter tuning is a big deal with them. So we can do that. And in the vanilla form, it's good, but not excellent. So it's worth looking into. So one of the questions that arises, remember I talked about, so this is, by the way, a new thing I'll introduce you to. When you look at the probability or at what probability of it becoming 0 or 1 should you declare it to be either? Let's say that the probability of 1 is what? At what point should you put your probability cutoff? Say that if the probability of a point being 0 is greater than or 1 is greater than what should I declare it to be one? The positive test case. So you can do this plot. These are called precision recall curves and discrimination thresholds. So what they show you is, you see this red line? This shows how the precision varies as you increase the probability at which you put the cutoff. the probability at which you put the cutoff. And recall decreases as you increase the probability at which you put the cutoff. So obviously you want a point at least the sum total of precision and recall. The F1 value achieves a maximum. So you get a very good F1 value. So you notice that the F1 value, which is the harmonic mean of precision and recall, it peaks here, which is why internally the classifier will use that as a threshold for deciding that at this probability, we'll declare it to be one class versus the other. So it is worth plotting this out to see what the model is doing internally. And the precision recall curve is very similar to the ROC curve, which shows you the recall and precision. And a lot of people these days are beginning to prefer the precision recall curve to the ROC curve. I guess you can use both. Both of them have value. But you will see this being used more and more. Now I would like to introduce you to a library called, it is more common, it's called SHAP, S-H-A-P. It deals with the work of a Nobel laureate, Shapley. Shapley got a Nobel Prize for solving one problem. Suppose a team, let's say a cricket players, they win the match and you want to apportion some reward money based on how much each contributed to winning the match. How would you do that? If you just give everybody equal money, then the guy who didn't contribute much, he would get the same as the captain or the guy who made the most runs. So the question is, how would you distribute an amount of money or amount of rewards, whatever form of rewards, proportionate to contribution? So that problem is quite interesting and it maps to machine learning in one sense. You can ask this question, if you treat each of the features as a player, at a given point, let's say that given my weight and height and various metrics, various features, which factors are responsible, more responsible for me having or not having diabetes, let us say, or heart disease, or any one of the diseases. From a medical perspective, it would be important to know, for example, which is the highest risk factor at this moment for me? Which factor contributes most in pushing me towards diabetes? Which factor contributes most in taking me away from diabetes? That would be worth knowing. And that can be answered by using the analysis that Shapley pioneered. In Python, it exists as a library shaft. What it gives you is the contribution of each player, player being each of the features in determining the target variable value in making the prediction. So you realize that in random forest, we had something called the feature importance. I told you that take it to the grain of salt, don't trust it too much because for a variety of reasons. It's a very rough and ready measure. Shapely values are much more robust. They have more nuances. They're very in-depth and they are a part of the effort to do explainable AI or explainable machine learning these days. So at this moment, I just gave an idea of it. It says that the X2 factor is twice as important as X1 factor. You can apply to SVM because SVMs don't have a feature importance built in. And you can apply this to any kind of model. You can apply to gradient boosting, you can apply to detail networks you can apply shapely to anything you want i mean together now this is one addition to this then in the california housing also if you remember we did all sorts of things in california housing i will now give you a way of using support vector machines. Now what is the coefficient R squared in the mean squared error? 0.07. Do we remember what was the mean squared error for random forest? For random forest, our mean squared error was 0.05. So support vector machines in the vanilla form is not doing as good as this but it's almost as good the r square is 83 percent there it was 78 or 79 percent and for linear regression if you remember linear regression the mean squared error was 0.1 and the R squared was 70% so obviously much worse so let us see how we apply support vector machines by the way before I do that let me show you how we do feature importance you can do feature importance using in the case of a random forest, using the feature importance that comes built into random forest. You can also use Shapely. When you use the Shapely library, you get a feature importance, which is pretty much aligned to what the built in feature importance did. But you can do more than that. You can take a data point and you can say, what are the forces that are influencing it? And you realize that the force that is increasing the, so let's look at this data point. At this data point where the value, the house median value is 1305, remember that we have taken the natural log of it. So we have to take the entire log of this to get it in real numbers, because we took the log of it is 13. So at this point, it says, which is a random point we took in the data, it says that the thing that is decreasing the house price, the factor that is most influential in decreasing the house value is the age of the house. The older the house is, the less it will sell for, the less the value. I hope it agrees with common sense. Then on the other hand, what are the factors that are most contributing to high price of the house? Median income. In other words, affluent neighborhoods where people's incomes are high tend to have expensive homes, which is common sense because only high income people can afford expensive expensive homes what is the second most important factor the second most important factor seems ocean proximity in land right if if the house is it turns out that if it is inland it is not well this is very interesting if it is inland, it has quite a bit of effect on the house price. The next is location. So these are all locations. You notice that the three most important factors are location. All of these are location related. And so this is how you see the value of different factors contributing to a house price. Is it easy to understand? It just shows you that for houses which are high value, what are the factors that are increasing the price of the house and what are the factors that are decreasing the price of the house. So clearly the older the house, it works against the house price. Whereas median income, median income is not really a factor because it sort of goes hand in hand with people. But all the other factors seem to be location, location, location as realistic people would tell you. These are all location factors that influence the price of a house, which is common sense. So if you take this, you know, this plot is made for one data point, right? One rather expensive house. What Shapely comes out with is something actually quite, quite, there's a very rich visualization but it needs some interpretation if you take this plot and you rotate it 90 degrees and now you say can i see the contribution at every value of the house so these are low low house prices so at the low house prices, what is increasing the proximity of the house? It says that what is increasing is ocean proximity inland. So basically their houses tend to be mostly inland. And the factors that are... If you look at that house median age so low in low house prices very low house prices are very old homes does that make sense I hope it makes sense right on the other hand if you go here you fairly expensive home. Order by value. Let me take it this way. So you can you can order it by different sort of things by output value by Okay, I'll leave it as my similarity. So you look at some other point, the median income at this particular value, the income begins to matter in certain house price values, whether people there are affluent or not, and so on and so forth. So the median income becomes huge. And you notice that at high house prices, most of the factors that matter seems to be related to location factors, you know, longitude, latitude, proximity to ocean and so on and so forth. So not much in the housing price actually this plot doesn't say anything that is not obvious, but it is a plot worth knowing about. We are going to use this now for support vector machines. Support vector machines, like any complex algorithm, can be used both for regression and classification. Here we are using it for regression. When you use it for regression, I'm just using the vanilla version. I'll leave it to you to use it in a better way. Use it for, let say you know tune the parameters and get better values if you can but if you use it for regression then why did I leave it at this value because at this value it will run on your machines in 35 seconds but once you start tuning it you'll get better and better model, but the runtime of your model will also go up. It will take quite some time to build. So here we go. We get about 80%, 78 and a half percent coefficient and 0.05. As you know, the random forest did better than this, if I remember, the vanilla form of random forest did better than this. Mean remember the vanilla form of random forested better than this mean squared error was less the coefficient of determination was 83 percent and here it is what is it here 83 percent versus 78 percent so obviously, 5% less. Residuals look okay. There seems to be a fairly good correlation between prediction and reality. And oh, this needs to, we need to run it for a smaller sample. Anyway, I can run this. It may take some time to run. So, sharp takes quite some time to run. It's something to be aware of. So, guys, any questions at this moment? Sharp is a new thing I introduced introduced but forget sharp for a moment does the syntax of support vector machine look very straightforward by now the syntax of scikit-learn should be looking very familiar to all of you is it ah yes it's looking pretty familiar yeah shap, right? I just introduce you as a teaser guys. I don't expect you to understand it fully because I explained this in a different class. It is just introducing you to something that is interesting without giving too much explanation for it. Take it as a fact that it leads to, it somehow gives you more insight into what factors matter compared to just feature importance. So I won't talk about feature this. Maybe I'll talk about it in a separate session entirely devoted to it. But get used to it and then you should use it more often. But today was just a teaser or a sort of introduction. Now one of the things I haven't done is showed you the solution of this, everything that we did but we are going to do it in r now and it is written out nicely here so i'll explain we load the data so this line number this this should be easy line number four we are just the data, we are splitting the data into test and train data, training data and test data. We attach to data so that x1 and x2, we don't have to keep referring to them again. We look at the structure of the data which tells us that it has, it's made up of three attributes x1, x2, and t. We summarize the data. You notice that we are taking summary of the data. This is very much equivalent in pandas to what, guys? Can you tell me what was the pandas equivalent? How to describe? Describe. It is almost a one-to-one equivalent on almost everywhere here you call it summary there you call describe unique is literally the word if one finders column you do unique you will get essentially the same same output you do the correlation plots here is the correlation plot for you. The only three variables, x1, x2, x3, the correlation plot is not likely to be something very remarkable. Then, if you try to do pairwise plot, you will see, you'll get a plot like this. plot you will see you'll get a plot like this and now in R you notice that everything is just less code one line of code gives you somebody but that one does also gives you a core plot this that as you get used to are those of you who are who you will realize that a lot can be achieved in just one line as one functions and so like. Like for example here, pair plots, correlation, everything is just one liners in R. R is very good for most of the standard machine learning, though for the newer stuff like deep learning etc., it has become these days Python first. And R libraries are released a few months later but ultimately they catch this sort of are a few months later so they catch up the catch up after some time so if you naively use logistic regression on the river data set do you remember that it was a disaster you can see it the syntax of r is the way you write it is you're saying p depends on everything else, everything other than T. So what is everything other than T? It would be X1 and X2. So R gives you this very sort of a succinct syntax of building models. Do you notice that we don't have to do this explicit separation between the X matrix and the Y, the feature matrix and the Y column vector, input and output vectors. And now you don't need to do that, it just does it. And of course, if you have done ML 100 with me, you have done these labs. So this syntax should be familiar to you from the classifier. I'm just repeating the classifier of this river duke's car when you do that you get as you would expect no benefit do you see that the piva the the model is terrible the null deviants and the residual deviants are pretty close to each other which means that the model has not learned anything at all. It gives up, it pretty much doesn't learn. If you try to plot this model and see what is it doing, very much like in Python, it predicts everything belongs to class one. There is only sand, there is no river. And you can see it because you know, confusion matrix is supposed to be a two by two but actually one entire column is missing because none of the points they predict as a zero as the river points if you look at area under the curve this is the area under the curve pretty terrible for the naive model so from that what do conclude? You conclude that it's a bad model. It's disappointing. And what is the accuracy? About 60%, but that's the data split. Actually, it's worse than the data split because the data split was more than 60. It was two-thirds to one-third or something like that. But here, it is 58. So there are times when you can do worse than just majority class. So then we take the more thoughtful approach, the same approach that we took in Python. What do we do first? We take out the river. So how do we do that? This is a very, by the way, in R, the d-plyer gives you a beautiful vocabulary. So river, you pass it through this filtering process. You keep only the river points. t is equal to zero. You plot out this and this is R. And then here in Python, I used polynomial regression, right? Here I use both actually. First I tried to do it with the sine wave because somebody said, why not use a sine wave? Well, here we try to fit a sine wave model to it. When we fit a sine wave model to it when we fit a sine wave model the sine wave model fits pretty well and after that as you can see it fits pretty well by the way this part of the exercise the reason i'm moving a bit fast is because this is essentially a repeat of your mlm we have done all this in mlm do you agree guys do you remember us doing all this yeah yes we have done all this in England. Do you agree guys? Do you remember us doing all this? Yeah. Yes, we have done. Yes. It's not, nothing new. This code should look very, very familiar to you. It is. And so we have a pretty good model and then you could have used, by the way, in R I do both sine wave model or polynomial model, just to illustrate that there are many you know paths to building a good model and many approaches once again the residual plots look good and so forth and so this is the polynomial model fitting the data once you do it all you have to do is extract out distance from the center of the river which is the absolute value of the prediction minus x2 minus prediction distance from the center of the river. Once you do that, you can make a box plot and in the box plot you can clearly see river points are here and sand points are further off from the river. Nice. By the way, it's people these days try to get pretty with their, they use the violin part. Violin part if you understand it's good actually, because it gives you a bell curve distribution over the data. It gives you the, not bell curve, it gives you the kernel density estimation over the data, KDE. KDEs are just, think of them as basically density plots, right, of how the data is spread out. So if you look at that, the simpler version being just this box plot, clearly you don't even need to build a machine learning model once you do the feature extraction. Common sense says that at a distance x3 is equal to 50 or anyway 30, 45 something you could have made a decision boundary here isn't it guys separating the yellow from the green you can make a decision boundary here do you see do you guys agree yes that is it at a distance of about some 45 or something like that you already have a decision boundary I mean this is the beauty sometimes you know the problem simplifies so much just through feature engine that you don't have to build a model but of course we'll go ahead and build a model now when we build again a logistic model do you realize what a huge difference there is in the change in deviance residual deviance residual deviance has come much much lower yeah and you can see that the p values are good pretty good here standard error is the z value the estimate divided by standard error the ratios are also pretty good all those points that we mentioned we should observe are pretty good. And this time around, do you notice that you get a proper confusion matrix, a confusion matrix which has the two rows and two columns. Principal diagonal has most of the points, off diagonal has fewer points. If you look at the accuracy of this model, it's 91.25%. It means that the model is doing very, very well. When you take this model, it's doing very well. Again, this code is what we have been using for classifiers all along. And it's that? Good. Sorry guys, I woke up very early today. So when you look at the ROC curve, did I see this ROC curve here? Where my mouse is? Would you say that this ROC curve is really impressive, isn't it? Very much. Yeah. Just like we did it in Python, of course, is the same. And the area under the curve is 98% approximately. A UROC, again, very impressive. So you know that you have hit the nail on the head. You have a very, very good model. And that's the point of feature engineering that I wanted to show you with this. Same thing we can do with the flag data set. So I will read out actually this conclusion because I've written it out in the book form. The burden of the river data set analysis was the importance of a thoughtful methodical approach to data. We learned that considerable time and effort should be invested in future engineering. Doing so leads to learning the kinds of classification models that are radically more effective than a naive, hurried approach of blindly applying algorithms to data. There was yet another lesson in this exercise. Note how a judicious use of nonlinear regression or polynomial regression facilitated the process of feature extraction. Without it, the linear classifiers were not much use. This is a generalizable observation. Rarely do real-world problems come in such a way that it can be fruitfully attacked with just a single approach and a single algorithm within that approach. Many approaches and their algorithms collectively come together as midwives to bring forth the best model and effective predictions. The magic is in this conspiracy of many ideas. And that's something you should remember guys. Whenever you have built a model you're proud of, there's some thing that is making really good predictions. It is like an onion. If you peel the onion, you'll see layers and layers of things you have done to reach that high level of model accuracy and things like that, the goodness of the model. So that was the main lesson here. We can repeat the same thing with the flag data set. When we do that, no harm. Once again, naive approaches lead to disaster. We can use the quadratic discriminant analysis. Also, it doesn't take us anywhere. But if you do feature extraction, distance from the center of the river, very clearly in this plot, you can literally hand draw the decision boundaries in terms of distance from the river. And so you expect that if you build a classifier, it would be very accurate. And indeed, it is very accurate. You have 85% accuracy. You can see that the error rate is almost 84.3 and the, sorry, the accuracy is an error rate is 15.6, which is the opposite of accuracy, one minus accuracy. So pretty good model. This is the analytics. So pretty good model. You can build with this. And pretty much you can try any of the linear models, logistic regression, linear discriminant analysis, quadratic discriminant analysis. I guess you must be remembering these things, LDA, QDA from our last ML 100 class. You can apply any one of those and it will work. Now Hill dataset, I haven't given you the analysis and I kept it off. This is part of your homework. I will probably not show you the solution, but the basic point is that it works like a charm. Feature engineering works very well on the Hill dataset. If it is any hint to you, works very well on the hill data set. If it has any hint to you, this is how the hill data set looks if you visualize it. I'm going to release it to you guys. See guys, if you can build a regression model for it. Are we together guys. So today actually I'll teach you guys a new topic which uses the kernel. We learned about the support vector machines. There was one application of the kernel. Today I'm going to teach you another application of the kernel and this would also be a good point to teach neighborhood-based algorithms, algorithms that rely on neighborhood methods. But for that, I would switch over to the writing pad and therefore to the other machine, and I'll explain it there. But before I do that I'll take any questions anybody has any questions so will you share even the R solutions with us oh yes of course I will share of course I'll share all of these right but obviously please don't share it any further and keep it within the class okay good thing with R is I actually wrote it out. properly with explanations that Python one, I still have to add all of these explanations. As if you're you're talking about that book is this that book? This is that book exactly. Do you like it? Yeah, this is nice. It's nice. The contents on the left hand side. Yeah. And whatever you speak, right, all those nuggets of wisdoms are also there embedded. That's good. They're in the book. Yes. So guys, give me a minute. I'll go have a glass of of water come back and we'll continue from there would that be okay guys i'm feeling a bit thirsty today yes of course and stop the recording Thank you. Thank you. Takk for ating med. Thank you. Okay. one second Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Give me a moment, guys. Page eight. So, Thank you. All right, guys, can you see my screen? Yes. Yes. Perfect. So we'll start from here. This is where we stopped yesterday. We were talking about... Can we start recording? Oh, yes. My goodness. I must uh i stopped it isn't it so where's the recording control so the topic today is a new topic actually that is related. Oh, this thing came to the screen. You guys remember this discussion that We stopped yesterday by saying that support vector machines or kernels, they help you find the lighthouses that can guide you along a nonlinear decision boundary. These lighthouses are your support vectors. SVM is a clever technology, clever algorithm that can find the lighthouses that can help you navigate the decision boundary. It helps you flow a river through the data, the widest river that you have. So today's topic will be a different topic. Actually, maybe I should start with a different page. I can share all of these pages with you. Today's topic will be neighborhood methods. So neighborhood methods are quite interesting and remarkably simple. The intuition is the obvious. The intuition is you are defined by the company you keep. In every culture, people tell you that if you want to judge a person, go look at the friends of the person. And the friends give you a pretty good idea of what a person really is. The same is true in machine learning. And the intuition translates in a very obvious way. So suppose you look at the space of cows and ducks. You're looking at a space in which the features are, once again, let us say that the features are weight. This is your x1 weight and x2 is the size would you agree that most of the ducks would be um let me take a slightly literally take something like this so most of the ducks would be data points would be like this and most of the ducks would be, data points would be like this. And most of the cow data points, let us say, would be, what should I do for a cow? All colors, let's say cow. Okay, so most of the cow data points would be together. They would be big and heavy creatures. Would you agree with that intuition? Isn't it? Any feedback? Is this pretty obvious, right? In the weight and size space, the cows will be big and heavy and the ducks would be, the ducks would be rather light. This would be the ducks. This would be rather light this would be the ducks this would be the carbs that make sense guys yes yeah so now what we can do what this algorithm gives you a hint that suppose i give you a point let's take a point X without knowing any machine learning what would you say that point X you get an animal whose weight is given by the point at a the point a what would be your guess what animal is it is it a cow or a duck guys what would it be a point at a you'll find an animal which is big and heavy, would you guess it's a cow or it's a duck? Cow. Cow. Cow. Cow. Cow. Cow. Cow. Cow. Cow. Cow. Cow. Cow. Cow. Cow. Cow. Cow. Cow. Cow it is a cow. And what made you say that it is a cow? If you think a little bit about it, you would say that, you know, animals whose weights are similar, neighboring, you know, if you look at this point, and you look at mostly cows. In fact, here it seems to be completely cows, isn't it? The data points that you have, all the animals that that you have measured in the whose weight and whose size are similar to a's or close to a's they are all cows so and on the other hand if you get a point like this b a point b once again if you look at the neighborhood of b the neighborhood of B, the neighborhood of B are all ducks. Isn't it, guys? And so we come to a basic intuition. We say that if the following conditions can be fulfilled, which are no-brainer conditions, if we have a notion of distance in the feature space. Feature space. What is the feature space? Feature space X. So see X belongs to some dimension. Here it is two dimensional, but let us say d-dimensional space, right? This is your feature space, right? So by the way, this belongs to a symbol. X is an input vector. You remember that any input vector is a vector in the d-dimensional feature space. If you can define a notion of distance actually let me not use the word D Let us say p dimensions, r to the power p. So suppose you have a p dimension space, feature space into which the x vectors exist. Now, this is an implicit because when you talk of neighbors, you assume a notion of distance. You can't have a notion of distance. I mean, you can't have a notion of neighbors where you can't define distance. Now, you may wonder, is that even possible? It is possible, for example, Now, you may wonder, is that even possible? It is possible. For example, if you're talking about cats and dogs and pigeons, and let's say that one of your columns in the data is like that. It is the species of animal. Well, here we are trying to predict the species of animal I wouldn't take that let's say color color of the animal if you're looking at the color of the animal what is the distance between a blue and a green it's it's not defined unless you're looking at the wavelength of light and let's say we don't then there is no well-defined notion of green animals versus blue animals or whatever it is and so forth, or bluebirds and so forth. So not all spaces, not all feature spaces support the notion of distance. You're lucky when you have a notion of distance. That also brings up an interesting topic. What is distance? So let me pose this question. What is distance? Would somebody like to venture a guess? What is distance? What is the distance between two points? Let us say you have two points. Two points in the feature space, x, x prime, in the feature space, in the feature space, r, p, then what is a good notion of distance? What is the distance between x and x prime? Anybody would like to say? The subtraction of the X axis square plus subtraction of Y axis square, square root. Pythagorean distance. Yeah, so one easy notion of distance is you can say distance between X and X prime, right? And let me use this as a Euclidean distance. use this as a Euclidean distance. Euclidean distance is, distance between these two points is xi minus xi prime, that is component wise squared. So x1, x2, sorry, x1 So x1, x2, sorry, x1 of the first one plus x2 minus x2 prime, if it is two-dimensional space, square root. Would you agree that this is your Euclidean distance? So between two points of example if this point and this point are x and x prime. This is x x one minus Or rather x prime minus x one. And X to prime minus x2. And so here also I could write it the other way around. It doesn't matter. And so this distance from here to there by Pythagoras' theorem is the square root of this. Are we making sense, guys? So far so good, guys? Is this obvious? Yes. So this is called in more formal terms, it's called the L2 measure and I'll explain why. If you generalize this concept, you can talk of the Manhattan distance. Manhattan distance is as a taxi driver goes or as I suppose modern day one of these Uber or Lyft would go. See if you are in a point x1 sorry x x point x here and you want to go to the point x prime how would a taxi driver go the taxi driver cannot just you know jump through the buildings or drive through the buildings the taxi driver would be forced to go or drive through the buildings the taxi driver would be forced to go like this yeah isn't it there is right whereas the way a bird flies could be the way a bird flies would be like this so let me write this down. And this one would be taxi driver spot. Now, interestingly, what is the taxi driver's part? If you think about it, it is the absolute value of, what is this? This is x prime minus x, the size of the magnitude of it, right? And this is the x1 part of it. This is x prime x2 minus x2, the absolute value. So you would say that the Manhattan distance or the taxi driver distance between x prime is equal to x prime minus x absolute value plus the first component and the second component, the absolute value. Does this make sense guys? Is this obvious what I'm saying? Yes. It is obvious. So we realize that distance depends upon the situation. Now, one easy way to generalize this distances is to write this expression, which is called the Minkowski distance. The Minkowski distance between two points is defined as distance of between x and x prime of dimension, let us say dimension of measure n, m, let's say m is is this is given by this interesting expression x prime 1 minus x x absolute value to the power m plus x 2 prime minus x 2 to the power m and you can go to as many things as you want and you take the mth square root of it so this is a this is an interesting expression let us break it up and see what it is trying to tell us when m is equal to 1 then d1 of x x prime becomes becomes the the one-eighth square root of the square root is the one itself right so this just becomes to the power one here one i can i need not right isn't it guys so this is this plus x2 prime minus x2 absolute value so what does this look like it is is it the same as this guy here yeah so when m is equal to one, it is essentially Manhattan distance. On the other hand, let's see what d is equal to two becomes. Sorry, m is equal to two. The second and the word people use is the norm, the second norm. So we have x1 prime minus x1 absolute value squared plus x2 minus x prime x2, sorry, x2, sorry, minus x2 absolute value square, square root. Isn't it? The half is implicit here. What does this look like, guys? Isn't it the same as the Euclidean distance? Yeah. Euclidean distance now you realize that this is remarkable so we have generalized it you can now go to d3 d4 isn't it you can even go d infinity as Asif, can I interrupt? Asif? No, definitely. For the Euclidean distance, we are squaring it anyway. So why do we need to take absolute value? No, it is just, I'm just saying that you do it because it's a generalized, the Minkowski distance is defined like this. Yeah, that I get it. But in reality, what you do is for even powers, you don't need to take the absolute value. Got it. Got it. It's just a special case. Got it. It's just a special case. But in reality, of course, for even powers, you would realize that it's pointless to bother taking the absolute value. Negative to the even power would be positive anyway but now comes something very interesting guys can you imagine now you can have a notion of d infinity distance what would it be zero no it's very interesting actually so what will happen is suppose two points are like this right and this value so can I can I just for brevity use the word so is it one delta x1 to be X prime minus x1 prime minus x1 can I write it like this guys so this is the first delta x1 and this is delta x2 isn't it suppose by definition i write i create a new notation yes so you realize that what will happen is when i take when you take this d a large number m is equal to let's say 100 I'll just take an exam or even just forget it let's take 10 let us say that for example sake this seems to be about four times this or is it true or maybe six times this let us say that here does this look approximately true here guys in this figure? Yes. So what will happen is if you look at the m10, you're saying delta x1 to the power 10, the whole thing, 1 10th power, right? And see something beautiful that happens. Now I'm going to do a little bit of a magic here. What I'm going to do is I'm going to write it like this. Suppose I took delta x1 out, x1 to the power 10 and to the power 1 10th, which is nothing but delta. And this equation therefore becomes delta x2 over delta x1 to the power 10. Does this make sense, guys? Yeah. does this make sense guys yeah right and this one would be delta x1 times 1 plus now delta x2 is smaller than delta x1 in this case let us say 1 6th what is 1 6 to the power 10 it's a large i mean to the power of 4 itself is 1 by 1 to 9. So it kind of approaches 0. So you realize... 1 by 6. No, it can't be 1 by 6. 1 6 to the power 10 is approximately 0. Yeah. Okay. You want to try it out on a calculator? Let's do that. Will it approach to E? It will be infinite or no? We will do that. Let's do this, guys. Let us do this. We are saying, where is it? Does it have a sign? Oh, it's 6. So it's 6 times 10, right? Going in the denominator, right? The denominator is getting bigger. So 1 divided by six to the power ten divided by six is equal to point one six point one six is there a sense of power here in this this windows i'm not we x to the power no i think you have to change it to a different multiply ms what is m MS? end up not moving scientific square it two times then multiply by six again. Oh, goodness. That's okay. You have a scientific calculator in Windows doesn't exist. Oh, do it on my phone. Square it five times, sir. Yeah. Square it, yeah. Okay, I could do that. But anyway, this is close to zero. You're right. Yeah, yeah, yeah. This will be close to zero, actually. As if I got it, yeah. You guys got it, huh? Yeah. So you'll realize that this number is very close to zero. Okay, let me do that here. I opened like uh okay let me do that here i opened up the r so x is equal to 1 by 6.0 what is x this x to the power x that's 1.6 to the negative 8 i'm sorry um x to the power 10 times of 10. yeah yes this is it what is it two point one four one three four power minus zero so it's a very very small number would you agree on the other hand if it is uh by the way this is only to the power six if i would reach the power 10 10 yeah it comes to one point two one four to the power of minus eight ten to the power minus eight exactly 1.65 to the would you all agree that if I were to give you a salary 1.6 to 10 to the minus eight dollars you would be rather unimpressed right I would rather not take it yes exactly it's like that you you got nothing essentially right so that's the intuition now let's go back to that what we are trying to say is that this is approximately zero isn't it this term is approximately zero and so what happens if this term cancels out you're just left with x yeah one to the power one tenth is just one one and so this is approximately one so what is it saying by 10 already the meaning is obvious by the time you go to 100 what will happen once again this will be remain isn't it yeah and so think about it as d go you realize that d this is approximately equal to this d 100 would be even more better approximation to just delta x1 and so d infinity is essentially equal to the biggest distance you know whichever axis has the biggest distance will matter. So the difference gets amplified. Yes. So what happens is that the infinite norm just picks up whichever distance along the axis is the biggest one. All others go to zero and that is the only one that survives isn't it so what is it it is actually finding this of all the delta x1 delta x2 delta x let's say it's p dimension data then it picking what? It is just picking the max of them. Does this make sense guys? The maximum distance along the axis. d infinity is just this. It picks up the max. So this is a very powerful concept and the person who discovered it is, or to whom it is attributed, is Minkowski. Minkowski, by the way, was very influential in creating the whole theory and a bit of the mathematics that would later on go into the special theory of relativity. So well, this is it. This is the notion of distance so you say well that is uh getting pretty interesting mathematics now what is its relationship to um machine learning well go ahead right here just a tv as i've heard that minkowski is probably einstein's phd advisor I've heard that Minkowski is probably Einstein's PhD advisor. Minkowski was Einstein's PhD advisor. Maybe true actually. I'll have to look up. That's a very good question. I don't remember his PhD advisor but quite likely. Could be. Yeah. And a wonderful mathematician by the way. The Minkowski space, the four dimensional space in which special theory of LGBT lives, the space time is Minkowski. So now, all right, we will now continue from there. And we say that, can we do more generalizations? And so you can ask yourself, what is the distance between two words? You could easily ask that question, right? So there are many ways that you can come up with the notion of distance, for example, right? So some of these ways are quite interesting, actually. We will go into, when you look at machine learning you find that people define their own very carefully define the distance between two vectors the input vectors and that often is their secret sauce they will guard their definition of distance sometimes because it gives them that extra bit of performance because they did something pretty clever in doing it. So that raises the question, what is distance? What is a more broad definition of distance? And the definition of distance is the following. Broadly, a distance, a metric is, and the word people often use is metric also. Anyway, I'll use the word metric but metric is not exactly what it is a theoretical physicist like me would frown upon them. You'll say metric is more like G-mini. Forget about what that is. Distance is something that has the following properties. Distance between two points x and y and now I use the word x and y or maybe let me just use x and x. x prime. Is always greater than or equal to zero. In other words, it's a positive definite quantity. Does that make sense. Distance is always positive. Right. Yeah. Right. right makes sense the other thing that it says is that if distance X and X prime is equal to zero if distance it implies bi-directionally it implies that X is X prime they are both the same points that X is X prime. They're both the same points. Yeah. This also makes sense, right? If you don't have to go even a little bit to reach the other point, it means that that other point is where you are. Both are curves. Yeah. And the third is uh sorry and the third thing that comes in here is uh something which is also pretty obvious see if you have a suppose this is a point x and maybe actually x and this is a point x prime in whatever space would you agree that the distance distance is always the shortest path right by definition let us define is the shortest path and the formal term mathematicians uses geodesic. Now where does the word geodesic come from? Geodesic people, people like me who do differential geometry and so forth is the intuition comes from geo geo usually stands for the earth so imagine a globe in your hand right now look at this what is the shortest path from here to let's say europe london or to let's say india the shortest part you could claim is that you could burrow through the center of the earth and come out the other end. Yeah. Isn't it? Yeah. So that would be 6400 times 2000. The radius of the Earth is 6400 kilometers, so 12,000 something kilometers later you would be on the other side but you know that if you take a flight from here to uh let's say india you go more like if i'm right about 13 000 miles which is more like 20 000 kilometers isn't it so where did that extra distance go come from well it comes from the fact that you don't burrow through the center of the earth you instead fly over the surface of the earth right so when you have the earth you can go from here to here not not this part but you have to go but you have to go this part around the Earth. Isn't that obvious guys? And this part on the surface is the shortest part. You could do this, you could go the polar route, whichever way you go. Let us assume that the Earth is one big round, which is not really true, but it's sort of ellipsoidal slightly. But assuming it was a sphere, you would have to go along one of the arcs, along the surface. So that is the shortest path, geodesic, the path along the geodesic. So the word geodesic has stayed. It is used more formally in mathematics to represent the shortest distance between two points. Here, in this situation, you would agree that the shortest distance between this is this, right? This is d x x prime. But suppose you take any other point in this space that is not on this line. Let us say that you take a point. Why would you agree that or any why? Let me just take it here. I don't want it to look like a triangle. Let's say even this y or z would you agree that d x y plus d y x prime will always be greater than equal to the x x prime. So this is z and I could take another point w here, right? So if it is w, you realize that these two x to w and then w to x prime would equal this distance right in that situation but in all other situations this inequality will hold for any y value whether y is here or it is z or it is w will this thing hold yeah right so it is a case actually it's sort of related to something like schwarz sc actually case actually it's sort of related to something like Schwartz SC actually inequality but it's common sense so we say that any function that satisfies in a space that satisfies these properties is a bona fide definition of distance. You can just conjure up any kind of function that fulfills these three properties. If it fulfills these three properties for all points in the space, it is a distance measure. Are we together, guys? Yeah. That is a distance. So you say, well, that's taking a deep dive into what a distance is. Once, for once, you should know that, guys. Distances or definition of distances and similarities are at the heart of machine learning in many ways. And today we'll see what we can do with distance. See, we assume distance because we always think Euclidean space, three-dimensional space. But data exists in higher dimensional spaces. We need to have a clear definition of distance. So this is it. So I'll just summarize these three criteria. What were the other two criteria? This, two criteria this dx x prime is greater than equal to zero isn't it and a dx x prime is equal to zero implies or the mathematical word people use is i don't know if you're familiar with it i ff it's a misspelled if if it also is represented with this bijective reason sort of it means x and x prime are the same point So these are your basic notions. Anything, any function that fulfills this is a notion of plastic. So now let's come back to our cows and ducks all over again. Let me, might as well redraw the cows and ducks because they're way up there. I'll redraw this let's take this example and now i'll start introducing more animals here these are ducks right these are cows oops what just happened okay these are cows. Then let's bring in some other animals. Let's bring in, let me have some big ducks. What is close to a duck? Think of a bird that's close to a duck. Swan. Swan. Okay, let's bring in some swans. Let's give them a nice swan. Swans are elegant creatures. We better give them a nice color what color should we give swans let's try well I'll just give them pink because I can't think of color at this moment let's say that these are swans swans are bigger are they bigger they're bigger than a little bit yeah yeah. Looks like a flamingo. Pink. So some small swans can be confused for a duck, isn't it? I suppose that was the story of the ugly duckling. So here we go. And so now comes this interesting question. and what's close to a cow? Buffalo. Buffalo, okay. Buffalo, yeah. And I'll represent a buffalo with, what should we do for buffalo? Let's try this color for buffalo. Buffaloes are even bigger, isn't it? Yeah. OK, so well, they seem to be going along a diagonal. Let's take something that is, let's give these names to that, weight and size. Something that is big in size but not very heavy. Let's say giraffe. Would giraffe do or zebra do? I don't know if they're close to the weight of a car. Zebra maybe but giraffe are too big. Giraffe are too big. All right, we'll go. By the way, something about giraffe I learned just yesterday from my kids. Giraffe is the only animal that, mammal that cannot speak, that has no vocal cords. It's always silent. only animal that, mammal that cannot speak, that has no vocal cords. It's always silent. Yeah, I can imagine such a long neck, right? Yeah. So. So there we go. So let's say that we take zebra and we'll give zebra a color like this. Zebras are big, but not, are, what do we say? Well well they're not bigger than cows. We'll take giraffes. Giraffes are, let us pretend that they're not heavy. So any biologist who's here, please don't get insulted with my lack of understanding here. lack of understanding here. So we'll pretend that these are. So let's give names to these. And we'll call these are mostly ducks. And these are swans. These are cows. And these are giraffes. Are we together? So now let's ask ourselves, how can we tell whether something, using the notion of distance, how can we tell whether something is a cow duck giraffe or swan easy suppose I give you a point like this let me let me take some points a B A, B, C, D. You could say that A, B, C, and D, they are, what are they? You could identify. Let's go with A. What would you call A? One. One. And to make a decision, you just look at the neighbor, but the question is how many neighbors do you look at? You could look at any one neighbor two neighbor three neighbor in all these cases you would get a fairly good answer isn't it but suppose you look at a point this guy let me call it a prime what about a prime is it a? Is it a duck or is it a swan? It's a little hard to tell because the nearest neighbor seems to be, or especially if you, let me do this. Let me go back and bring those points. What was that dot that I did. Okay. Almost on the decision boundary. It's too close to the decision boundary. So if I take a point that we make it deliberately way confusing for you. This point a prime. What is a prime Diffic difficult to say. When you take k is equal to 1, the nearest neighbor, so k is the number 1. It is a duck. But if you take k is equal to, let us say 5. Now what happens? See, if I make a little circle around this, then what happens to A? There is most really swan. Yeah, the majority of the animals around it are swans. Do you see that guys? They're only two ducks, but in a circle around A prime, the rest of them seem to be swans, like one, two, three. Two ducks. two ducks. Two ducks. Actually, let me make it six. K is equal to six. Four swans. So now, what would you say? Which is the majority? Your intuition will go and say that this is a swamp. Right? So your answer depends on how many neighbors you look at. When there is overlap, the number of neighbors matters. Are we together? But how many neighbors is too many? Suppose you take k is equal to, at this point, k is equal to, let me take a number, 50 or 100. And to give you the situation, give you a sense of what's wrong with that, it and to give you the situation give you a sense of what's wrong with that what happens is suppose you are in a world filled with giraffes most of the data points in this is giraffes right Right. Like this. So then what will happen if you take the majority. Then what is a You would realize that the majority if K or something large, then what will this thing look like? If you take the majority, you will get something like 10 swans. Let us say that these look like about 20 points. So you will say you have 20 ducks, maybe 25 swans, maybe you have 30 cows and you have let's say 50 or 55 giraffe. Suppose this is the count. So what happens to you? You have 30 plus 80, 110. So if you look at K is equal to 100 100 what do you think you'll end up with what will be your majority imagine that you go a k is equal to 100 you will end up taking a circle like this in this big circle you have to go really far to find hundred neighbors and what will be the majority here guys giraffes giraffes exactly so you will end up concluding that a prime must be a giraffe so when you look at this what does your intuition tell you there are three different answers based on how many neighbors you take. A prime, what does it really look to you like? If you had to guess, what would you say it is? A swan? You would say either a swan or a duck, but much more likely to be a swan. You would like to go with the swan so what happened your answer went from a duck to a swan to a giraffe right so it went from a mistake to what is probably the right answer to again a mistake so this is an important lesson the k is equal to the hyper parameter hyper determines the accuracy of your predictions isn't it so you how would you do that so therefore you would you you set up the value of the hyper parameter by this means you would first divide the data into training and test and let the divide data or training data further if you want, divide training data. So you know that you have first divided data into a training or learning set. This is a test data. Now the data that you use for training you further divide. There are many methods of cross validation this this process is called cross validation i must have taught you at some point but if not i'll just at this moment give you an overly simplified version what you do is you keep some data from the learning data itself you hide it you call it the validation set. And you just give this data to train. And once you train that, you look for that model, that value of K, which gives you the best prediction, most accurate, that value of K, that gives most accurate prediction on validation set. Of course, later on on the test set. Does this make sense, guys? What you do is while learning, you keep some data aside. And you use that data to validate which value of K. You keep on increasing the value of K from 1 to 2 to 3 to 4. And so what will happen is you will get a curve. Your accuracy for values of K will go like that let's say that your accuracy is this it will go something like this you will you will get to a good accuracy number which is your best k does this make sense guys right like for example, if you take K2 high, 100 neighbors, it will become bad. This is actually your classic bias variance trade-off and I'll explain that to you in a moment. But let me bring home the intuition. See guys, suppose i want to know what your political views are i could go to one of your neighbors and ask but it's dangerous because that neighbor may happen to have a view that is exactly opposite to your view but suppose i take a few more neighbors 10 10, 15 neighbors here, you would get a pretty accurate estimate. But if I start looking at 50 or 60 million users, I might have to go all the way to Montana, isn't it, to find a neighbor there. And obviously a neighbor there, I mean, the people there may not reflect your viewpoint. Californians and people in Montana or Wyoming, they tend to have almost diametrically opposite political views. So they are not representative of you. So that is the danger of taking K too large. Danger of taking K too small is, just by accident you may get wrong answers because your nearest neighbor may be the opposite of you. Do you see the difference guys? So there is an optimal K in the range and you need to find what that K is and you do that through cross-validation. Now comes an interesting point. Suppose you have a space. Suppose data has the notion of distance. easily defined. You realize that this approach, and by the way, this approach is called K-nearest neighbor algorithm. It is based on simple notion that a thing is defined by its neighbors in the feature space whatever you think so it is a very simple algorithm you're not building a logistic model in fact there is no model building at all it's a non completely non-parametric model in other words there's no learning this algorithm is just quite it says i don't need to learn anything i just need to look at the neighbors and determine the answer. So when you look at k-nearest neighbor and you have a notion of distance obviously, then I should be able to apply that. So why is this method not universally applied? Like it's so easy, why do the rest of machine learning at all? In every situation where the notion of distance exists, let us just go use that. So I'm giving it to you guys as a puzzle. Why would you not do this? No free lunch theorem. Doesn't always help. Come again, Kate? No free lunch theorem. It's not always the best fit for the data. Not always the best fit for the data. That may be true. Yes. I mean, sometimes. Maybe to do with the shape of the data? Yeah, the type. Works well with the spherical or something. But I mean, since how the data. Little bit. Getting close. Dimension is the word word see what happens is uh exactly so let me go to the curse of dimensionality so i'll talk about because most data is high dimensional and high dimensions suffer from a curse which is sort of and high dimensions suffer from a curse which is sort of one of the worst curses you find in the harry potter novels right i remember reading the harry potter novels and there were all of these peculiar curses that could be put on you and you would be in great pain after that so it's somewhat like that one of the most painful things in this space is the curse of dimensionality it sort of punches your balloon of optimism so in this that oh goodness that's quite a word dimensionality what in the world is this but before i go to this i'll give you guys an example. I'll give you guys a illustrative example to pin the idea of care nearest neighbors into your mind. See, when care nearest neighbor, right, is based on training data, you know, you'll find neighbors in the training data. If you have a lot of neighbors, if you have a lot of training data. You know, you find neighbors in the training data. If you have a lot of neighbors, if you have a lot of training data, huge amounts of training data, then neighboring to a point, you'll find a lot of other points. In fact, if you're very lucky, you might find that the point you're predicting for is there in the training data itself. There is no learning to do. You just look up the value and return it. So which is pretty much degenerates to a dictionary situation. So suppose you know the value for every point in the feature space that could be asked as a prediction form. You just look it up in the dictionary. You're done. It's called dictionary-based learning. It's no learning at all. And it happens if you remember you all must have gone through schools and colleges the and again take this with a huge grain of salt and this is just saying something to pin the idea into your mind but somebody i think it was bill durant who once said it was a who once said that education is entirely wasted on the young. Why? Because the young people go to school and college and the last thing they want to do is learn anything. They're much more interested in having fun. Parents think they're sending their kids to college for education. The kids think they're going to college for a party, to have the best time of their life and to party around and so forth. And so the universities, they scare the kids somehow into the classes by the carrot and stick of grades and throwing them out of the college if they don't have good grades, taking away their scholarship and so forth. And so's steady. Well, obviously they are highly motivated kids. They are a minority, but most kids are highly motivated by grade and so on and so forth. Manoj Mistry, Ph.D.: So the kids don't want to study the colleges want them to study and so forth. That's how it goes. So one of the things you do when you don't want to study and you want a good grade in the exam is quite often you maintain a question bank. What sort of questions a professor is used to giving. And many professors are lazy. They'll keep repeating the same questions. They will have a set of about 30, 40 questions. And every year they'll take a sample of those questions and ask. So they'll just change the numbers and then they'll ask the same question, but the answers will be different because numbers have changed. So one of the things people often do is they don't really understand the subject. They would just study well how to solve those problems. All the steps needed to solve the problems and they would memorize it. So that when a problem comes, they would just go through the formula and solve it. Then quite often, if the numbers are slightly changed, what does it mean? The problem is not exactly what you have memorized, but it's close to it. If it is close to it, you make the little bit of a journey up to a solution that you know. So you use essentially a nearest neighbor approach to solve problems in exams because any new question you get, you try to map it to the nearest problem that you have sort of studied or memorized or if you're allowed cheat sheets you have a cheat sheet for does that resonate with anybody yes yeah yes it does right so yes that's that's your nearest neighbor essentially you use you find similar things for whom you know the answer and you don't disclose our agreement i should pose the question the other way around is there any who has not used this at some point in their life yeah right so that is that so that's the basic intuition of neos anyway i thought i'll take a digression so now let's come to this question of dimensionality and what is it and that would be something i'll talk about see dimensionality is a very interesting thing before i go to that let me bring back an intuition you know bay area is densely populated, isn't it? We live in, most of us spend our lives in these row homes, right? Track homes, in which these are cookie cutter homes. If you look at your house structure, 10 different neighbors will have exactly, or more, will have exactly the same structure of the house really is your house very unique and all these homes are very close to each other joke is that you can just if you want to exchange your book you don't have to go to the neighbor's house you can do an exchange to the window isn't it so well if you want let say, K nearest neighbor to know your views, you can even take K is equal to, be generous, let's say K is equal to 20. And 20 neighbors around you are very easy to find, right? Within a couple of two, three minutes of walk, all those neighbors are there. Is that true of us guys? Or at least within five minutes, all our neighbors are there, three, four minutes. Is that true of us guys? Or at least within five minutes? All our neighbors are there, three, four minutes. To reach a total of 20, the furthest neighbor to reach that would not take more than three, four minutes. Right? For most of us. Unless you live in a palatial place up in the mountains. Whereas, suppose you were to go to Montana or Wyoming or something like that, and you're out there in those homes, cottages. If I say, let's go find 20 neighbors, it would be quite a distance. For example, when you go to meet your neighbor there, you get into a car and you drive a substantive distance through ranches and forests and whatnot before you reach your nearest neighbor anybody who has been to those states would you agree I lived in Montana for five years you did would you agree with that yes certainly yeah I live in a university town, so I've seen places like that. One person per square mile, that's a density. That's pretty much it. So what happens guys is that if you have to go for 20 neighbors and you have to go very, very far and maybe cross a straight boundary along the way, those neighbors are not really representative of you. Right? So for example, if I want to find how much snow has fallen outside your house, and I have to take the average of the snowfall around the houses of 20 of your neighbors, you may realize that that would maybe very, very inaccurate, because snow may have fallen heavily in you and your nearest neighbor's house and your other 18 19 neighbors may be so far away that there was no snowstorm there right so it's not representative the further off you go in the in the distance the less representative they are of your local context right And so factors that are here locally in this point, you need to have locality there, locality is important. The more you lose locality, the further off you have to go, the less representative are those neighbors of your locality, your local context. And that is a point. So let's hold that point in mind. And now I'll show you something that is quite remarkable. This is something, I'll ask you guys a puzzle. Think of, don't think Bay Area. Bay Area is a terrible, terrible thing to think. Think of all the neighbors were not in Bay. Dr. G R Narsimha Rao, Or not in major cities. Dr. G R Narsimha Rao, Think of your friend friends who are not in the metropolitan areas. And when you think of them, ask yourself this question, are they close to either the sea or the Great Lakes or a state boundary either the sea or the Great Lakes or a state boundary. You'd be surprised that the majority of them or a large number of them would be living in rather along or rather near some state boundary or the sea or the Great Lakes or something. And you can do this experiment. Close your eyes, think for a moment. I'll let you guys digest that for a couple of minutes. See if that is true or not. But remember, avoid the metropolis. If you start counting only the friends in New York and New Jersey area and the ones in Los Angeles and San Francisco and Chicago area, you'll be in trouble, even though one can argue that New York once was close to water, San Francisco and LA once were close to the boundary, and so is Chicago. But think of 20 friends or 10 friends and you'll realize that quite a few of them live along state boundaries or water bodies, large water bodies. Did you get a chance to think through that? All right, I'll assume you did and or later on you can do that, but I'll tell you, I'll give you now a reason of why this sort of things happen. And I'll do that by taking a unit circle. Suppose you have a center. And you make a unit circle. Suppose your world is a unit circle. Radius is equal to one. Unit. Imagine your world is equal to one. Unit. Imagine your world is a unit circle. Now, notice this. If I ask you, let us say that your friends are uniformly distributed. uniform enough, isn't it? Let's take this. They're uniformly distributed. And I ask you this question, how far do you have to go to meet half your friends? So you say, well, that's very easy. Let us say that the distance I have to go is a distance r. Right. So, oh, sorry, something happened inside our area is proportional to pi r squared. yeah and the total number of friends are of friends is equal to pi 1 squared is pi isn't it so if you want to meet half your friends the number of friends who are within our distance that would be equal to pi r squared what proportion of all your friends are here over pi does this make sense pi pi cancel so suppose this is equal to a half right to meet to meet half your friends you have to go so now let's do this r therefore is equal to 1 over 2 square root and that is 0.707 yeah 707 approximately 71 percent r is 71 percent so let's ponder over what it means it means that this figure that i drew is not really representative 74 yeah maybe it is most of your name the do you notice something actually maybe this maybe we could do a little bit more this is more like it isn't it is this better guys yeah I will be poisoned and so what do you notice do you notice a remarkable fact half your friends seem to be living along the edge they're very they are rather close to the boundary do you notice that half of your friends are very close to the outer perimeter. Yeah. They are. That's a very interesting observation. And now observe what happens if I take the unit circle to a unit sphere. Now from here, let's generalize. Consider a unit sphere. unit sphere. Here, what is the area of friend is proportional now to the volume, and volume goes as four-third pi r cubed, right? And divided by, if you do it in the case of unit as a proportion, it would be times one cube. And so what will it be? It will be equal to R cube. So if R cube is equal to half, R is equal to the cube root of half. Right. Now, try that out, what that comes out to be. If you do that, you will realize that it is close to some 85, 84, 85%. I'm just trying to do it? Is that right? We could do that and once again we can bring up our r and say if r squared is equal to r cubed is equal to r half then 0.5 star star uh let me be approximate one third 80 right it is close to 80 percent so well uh now this disappeared uh excuse me guys Well, now this disappeared. Excuse me, guys. It seems to run away. Okay, so now you realize approximately 80%, which means that in a three-dimensional space, your other half of the neighbors are within 20% of the distance from the outer edge. Do you see that, guys? Does this make sense you know ie half you are within 20% of the edge. Now, what happens if you take r is equal to 100? Let's say 100 dimension. If r to the 100 is a half, then r is equal to 1 by 100. Let us try and see what that comes to. Actually, forget 100. Let's just go to 10 dimensions. Let's see what happens in 10 dimensions. You go there and you say that 0.5 star star 1 10th. 0.5 star star 110 guys can you guess what it will be when I hit enter 1 0 0 1 point 0 19 point 9 9 you'll be surprised it's 93 percent what does it mean most of half your neighbors are living, half your friends are living practically within sight of the border. Within the border, yeah. And if you go further to 100 dimensions, see what happens. It turns out that to meet half your neighbors, you practically have to go over the whole unit sphere or hypersphere isn't it if you take a hundred dimensional sphere do you see what i'm getting at guys so what it means is that it means something quite remarkable think about it when you go from a desk a two two dimensional circle area to a unit sphere, there is just more volume in the sphere. There's just more space there, isn't it? The intuition is it's more space. You go to four dimensions, now the points can move around in even more space. So this, if you try to uniformly distribute the point, they sort of evaporate, they sort of diffuse out from away from each other are you getting the intuition as you go from a disk to three dimensions already the points are sort of lifting up and wandering around and diffusing around you go to four dimensions the diffusion is even more the the space even if you started with a disk of really a lot of points, let's say 100 points, on a unit disk, 100 points may look a lot of points, neighbors are easy to find. But in a unit sphere, they're not so easy because now there is more space, data is a bit more sparse. By the time you get to to 10 dimensions all those points have just spread out diffused out and so your neighbors are far off isn't it yeah there are no more nearest i mean nearest neighbors are difficult to find yes and you know what you say well you know why do i need if i want to find the nearest neighbors let us say i want to find just a 10 percent of the neighbors let us see what happens if i do it with just 0.1 and in a hundred dimension i just need 10 of the neighbors you know just say very small number of neighbors okay nearest neighbors i take and do this and you realize that still it's 97% isn't it? And you can play this, it's a very surprising game, you can go and say well I'll just pick one in hundred, let me just take five to ten neighbors right and let's say that I have ten thousand data points or something like that. 10 neighbors is not a bag of five neighbors. You realize that you can play this game and it is still 95%. You can say, well, I'll take very, I have a lot of data. I really have a lot of data. I have 100,000 data points and I'll take K is equal to 10 or something like that. And so you do that and still you're going the whole distance do you see this guys I can go on making your you know one way to simulate a k is by taking a smaller and smaller fraction nonetheless here in higher dimensions you have to go very very far in the feature space to capture the neighbors that you have am i able to convey that intuition to you guys oh yes so the problem that is called the curse of dimensionality when you when you have to go far to find the neighbors those neighbors are not representative of your local context. They don't represent that point and what's happening here. And so using those far away neighbors, it's an oxymoron. Far away neighbors are not neighbors, isn't it? And so k-nearest neighbor algorithm begins to fail them. neighbor algorithm begins to fail them. So that is the curse of dimensionality. Now guys, we have today, I wanted to finish this topic of I didn't reach the kernel so far and that is in spite of the fact that we didn't take a break. What I would like to do is let me just review as in the last few minutes we review review next session, we'll take it up from here and we'll finish this, right? And we will see how kernels, applying the kernels solves this problem. But think about this, work with this and see if you got the intuition or not. So let me recap everything that we learned today from the talk. We learned that it helps to just look at the neighbors if you have a notion of distance. So if you can find neighbors, and they better be real neighbors, they shouldn't be far off neighbors. If you can, then this algorithm works like a charm, right? So think about it this way, ducks, swans, cows, a situation like this that you are seeing is practically tailor made for k and m would you agree guys they don't look forward using the k nearest neighbor algorithm so this is also short formed as k and m right k nearest little k and and then K nearest neighbor. Little k and then . So, it's tailor made for that. The only problem you have to solve is, what is the optimal K? What is the best K that I should use? And then you can go with it, find the best K through hyperparameter search, and do that to cross validation and so forth. You'll find it, and then there you go. It works very well. The places where it fails is when the dimensionality is high or you can't define the notion of distance. There is no obvious notion of distance between two points. Then this begins to fail. You can't use this. On the other hand, when you can, you should always try to use KMN. It's a very simple algorithm, very effective. As you can imagine, it's very effective. It captures the local context very well. And you can see that it works very well. Next time when we are there, we'll give some time to this. It is explainable also, right? It's very explainable. So it's a very good algorithm. You should always ask yourself, guys, that. So now we'll go back and look at a problem. In the river dataset, could i have used this yes we could we could have used reality is low the notion of distance is euclidean and it would have worked like a charm there could i have used it for the california housing data set i leave that as an exercise for you but can you guess it i leave that as an exercise for you but can you guess yeah so all right this is a 13 dimensional or 11 dimension it is a little bit bigger yeah it's a little bit bigger space so now you have to think whether it works or not and the way to make it work is actually using another trick if we could reduce the dimensionality of the problem, and of course also impose a notion of distance there, it would work. Now here is what works here. In this case there is a cheat. You know the latitude and longitude of that point, right? Geographical. So you can actually ignore a lot of the other variables and you can just ask yourself, give me another a few data points in ne'er this geographically nearby and that could be a pretty good proxy and you can see what what accuracy you get but California data set is blessed with the notion of latitude longitude you know that is the quintessence or the most obvious intuition of distance you can get from that. So you can sort of ignore a few of the other variables or judiciously pick only a couple of those other variables and work with those. But sometimes you don't get that privilege. So you can't use the notion in that case. You have to be a little bit careful. Just because dimensionality is high doesn't mean you can ignore it. So when the dimensionality is high, you can just pick a few dimensions. And use that and see if you get away with it. The other thing you can do is this big topic of dimensionality reduction. How do you reduce the dimensionality of a problem. We alluded to that. In ML 100 You remember guys, that was the last topic we covered in ML100, dimensionality reduction. Principal component. Yeah, the PCA. Principal component. And then we talked a little bit about more sophisticated methods. If you remember, I talked about kernel PCA. Kernel PCA, yes. It refers to this technology that we just learned on Monday. So we'll use that. We will use Colonel PCA to reduce the dimensionality. And then the other thing we'll learn. So if you could reduce the dimensionality, the curse of dimensionality goes away. And therefore, if you have a notion of distance, once again, you can apply K-NN. So that's what we'll do and the next thing we will learn is that actually this this business of finding k is a bit of a nuisance right so there is actually a way to be insensitive to k it is using the intuition that the nearer that a neighbor is to you, the more representative he is of you or she is of you. Right? So suppose you're welcome to take the opinion of the guy who lives in, I don't know, Nevada or Arizona, but weigh the opinion of your immediate neighbors higher, much higher than the opinion of people who are far away. If you use that intuition, then you can take pretty, you know, you can afford to go a little bit further out, but you know that their weightage, the weightage you're giving to their values is not as much. Are we getting it? So that is where we'll start with next time on Monday. So if you look at this, suppose you're trying to find the value at A prime. If you weigh the local neighbors more, but you still take 30 neighbors, you realize that a lot of giraffes will come into the picture somehow, and cows will come in. But because they are further off you weigh them down you say very little weightage to what they are saying what those data points are saying and so you still stay with the prediction that it's a swan so that is called kernel k and kernel based kernel right and so as you you notice this whole notion of kernel is getting more and more everywhere and it is there everywhere guys. I gave the talk if you remember attention is all you need, transformers and so forth. Now there's a very interesting relationship between kernels, transformers and RNNs. It's an interesting work somebody came out with and we'll talk about that. If you guys are interested I can give a talk on that but we'll Gernel's Transformers and RNX. It was an interesting work somebody came out with and we'll talk about that. If you guys are interested, I can give a talk on that. But we'll stop here today. I'll field questions now. And the other thing we learned is obviously the curse of dimensionality, right? Dimensionality, what it is. Think about it, guys, and try to develop your own intuition about the curse of dimensionality. Asif, you talked about the California data set, housing data set. You gave some examples. I don't believe we have done that as part of the classes that I took. You didn't because you didn't attend the ml-100 in the spring okay if you had mm-hmm yeah you're right when I was doing ml-100 with your batch yeah okay how do I get access to that. I'm giving you access. The notebook that I'm giving you, the California notebook has the entire analysis. You have it. So, Asif, it was also covered in the Saturday dataset class. So, I think Sandeep should have access to that. Sandeep has access to it, certainly yes he has he must be having a notebooks on it okay i think you it's it's no longer than github right uh the hundred data sets i do but everybody has their local copy now i sent out an email okay that's the bomb github gave more than two weeks notice and then shut down oh okay i'll i'll ask him. Must be helping that. Okay. Otherwise I can give it to you. I think somebody else asked it. Abhijit, you also asked for that same data, isn't it? You didn't take a back- That is true. That is true. And I'm just waiting still. You said that you'll send me the zip file, but I apologize. No, no, no. That's fine. I understand. You are really busy so but that's fine i would also need it because i didn't color and i we attended the older ml100 color you yes i'll give you the california data set for sure but not the bootcamp uh sure i understand i haven't attended the bootcamp otherwise what will happen is you won't learn anything in the bootcamp when you come to it because all the solutions Otherwise what will happen is you won't learn anything in the bootcamp when you come to it because all the solutions. I understand. I understand. Also, I said before we get into too many question answers, would you talk about the next, uh, I mean, our end of this week's your plan, I think you, you're going to cover multiple stations. So if you can just give quick idea, but what I was planning to do is some of the topics that are residual, I was hoping i would cover on this saturday see guys we are now end of fourth week you realize that we can cover only two more topics in the next two weeks so i'll give you a choice what they are i think recommender system is something that you all would want just to know how amazon makes recommendations and so forth right and so i'll be talking about the price a classic algorithm now it's called classic that won the netflix million dollar prize the famous thing price so we'll go with that algorithm right then that leaves one class i could doization, which is an important topic. You must know about regularization. Or I could pick any other topic that you guys wish. Asad, have you already covered bagging? You mentioned you're going to do the bagging. Did you do an extra class for that? I'll do it in one of the extra sessions. Okay. Bagging we did, bootstrap and add-in. you do an extra class for that boosting i'll do in one of the extra sessions okay backing with the bootstrap and i will see yeah stacking stacking i think you said you were going to talk so yeah boosting boosting and stacking yeah boosting us so asif uh so for the recommender system uh means i know they use it for items so do they also use the same similar logic for their employees or they use something else okay that's getting that's that's an off the record card yeah okay are we are we are we gonna do anomaly detection see between no anomaly detection and keep for like you know it belongs to the boot camp yeah and you have done it yeah not boot camp, you can come to the bootcamp. Yeah. So if I want to be part of deep learning, then I can pick it up there. You should pick, yeah, because we will do anomaly detection using deep learning. The anomaly detection without deep learning, we did in the previous. See there are two sets of boot camps one is all of the tabular data where deep learning helps but actually the rest of machine learning is much more effective and then there are the images and audio and textual data where this works very well deep learning works very well it turns out that deep learning in one area works very well for learning works very well it turns out that deep learning in one area works very well for tabular data which is anomaly detection that's they're called that's deep anomaly detectors is there a recording for anomaly detection by you for non not a deep learning the regular yeah see a lot of people were quietly making recordings when i was giving the talk but at that time i was still creating the infrastructure for recording okay so my i did not make an official recording but i'm pretty sure that i knew that the students were recording it on their cell phones any books books you can recommend? Oh, there are outliers. Literally, if you look at the word outliers, there's one, Charu Agarwal, who has written a fairly comprehensive survey of all the outlier detection methods. Title is called Outliers. And there are many other things. See, I can't think of any obvious other textbooks. Usually, if you go to scholar.google.com, you will see that a lot of people have written a lot of tutorials and surveys on outlier and anomaly detection techniques. Anomaly detection is one of those techniques guys that if you become good at only that, you can make a killing for the rest of your life you essentially have a meal yeah the frauds the network all kinds of frauds exactly and it is big business you get bought because today fact of life is that all the crooks are winning yeah it's always a catch-up game and yeah you're right you must abhijit you must be in a networking industry you and i are thinking same yeah yeah actually with all this walking from home right the whole pattern changed so you know they are taking advantage in all the yeah these guys are taking advantage of that for denial of service attacks thanks i will look into it that book here and probably uh schedule 10 15 minutes of separate session if needed yeah let me just say one thing one second i may say one thing guys see i'm preparing you for the boot camp okay in a sense because when the boot camp starts, it will be more intense. I will be giving you guys these two lectures a week and a whole day. Then it will become not one day lecture, one day lab. It will become two day, but you learn a lot. Those of those people who attended the last bootcamp of, I hope they all would agree that you learn almost 10 times what you learn in the workshop. You're talking about the hundred data sets. Yeah. The hundred data sets bootcamp. Yeah. Yeah. Yeah. You agree with that? Yeah. We did. Yeah. Yeah. In the back room you have those presentations and all going on yeah so in the boot camp you learn the whole life cycle of a data scientist how you do an analysis how you argue your business case how you present it right and you learn all sorts of very powerful techniques but it is hard work you have to commit to it are we are going to do it remotely, the boot camp or? The only difference will be that, see at this moment when you do your homework, you do it, you don't do it. I don't really cross check on you. I have given a reward, which is that people who do all the labs and do their project, I'll be their reference for jobs but there is no thing. But in a boot camp you would form large teams of six. I mean Sandeep will tell you that right, large teams and there is no getting away from it because every few hours you'll have to present your progress to everybody else. We have at least three presentations a day so each team has to present your progress to everybody else. Yes. We'll have, we have at least three presentations a day. So each team has to present their work at least twice a day. What I'm saying is, are we going to do in person or is it going to be more? No, no, no, not in person. I have no intention of infecting you all. Yeah, this thing is picking up again which means we have to wait for this coronavirus thing to blow over and then we have to go during the boot camp no we'll do it online guys in two weeks your workshop finishes which means i should start the registration for the boot camp now you guys are ready for it i'll do the boot camp and in parallel I'll also start the, I'm about to start the, by the way, yes, I should start the registration. Prachi, could you please take registrations for the math behind data science or have you already taken some? Do you happen to be here, Prachi? I don't know if you're here. are you do you happen to be here plachi i don't know if you're here or uh dennis and jenny's do you happen to know if you ever took the registrations for the math behind data science so guys here's the thing at this moment right you you're once you finish ml 200 those of you who haven't done ml 100 i would by now you should get the feeling that you probably want to go back and do ML 100. On the other hand, if you feel that you can pick it up on your own, then you don't have to go do it. Read it from your book and pick it up. The road forward is to two journeys. One is through engineering math, which will once and forever make you comfortable with the math behind machine learning. So typically, when you read a research paper or a textbook on good textbook on machine learning you quickly realize that there's a lot of mathematics involved. That math is not hard. You have done it at some part in your life. It is linear algebra, calculus, and probability. Three things. Unfortunately those things you have washed away with all the showers you have taken. So we just need to refresh our minds on that. Also, one of the things is the education, if your education system has been defective. And I must say globally, the one subject that is poorly taught is mathematics. Most people, they just regurgitate what they learn from their teachers or from the books or something. Math is actually the most beautiful subject. And it's really worth knowing it in the right way. And if you learn it the right way, it feels very easy and intuitive. Some of you have taken my, I noticed the people here, some of you have taken my .. Kala, have you taken it? KALA SINHAIRI- my, I noticed the people here, some of you have taken my my... Kala, have you taken it? No, I haven't. You have? Okay. And... How many weeks it is, Kasey? It will be the same six weeks. And it's like two classes? Two classes a week. So will you be able to do the boot camp and the math? So you are going to interface them or what? How are we going to do? Yeah, so there is a, you know, I'll tell you a joke. It's not a joke. In big data, in distributed systems, there is a gap theorem, right? Which, which says that between consistency, availability, and partitioning tolerance, You can create a system that will survive if the network partitions. Actually, it's not partitions, it's a modern interpretation, it's latency, if the latency is too high. Then if the latency is too high in a system, you'll either have to choose between consistency or availability. You can't have all of them. If you want to have all of them, then you don't have partition tolerance. You better have a single database, like Oracle database, one machine. Asif, so what is Spanner? Spanner is a very interesting thing. It's great that you bring up. They do very fast propagation of updates intelligently, consistently. So that's worth of a talk itself. It's a beautiful paper and an amazing implementation in the Google. But see, and it really, Spanner does, is the closest thing to violating CAP essentially but yes yeah you can pick only two so in the same way I would say that when you are doing this subject right machine learning and you're learning when you do the bootcamp and you do a workshop a bootcamp and a workshop together Abhijit you'll have to pick up between machine learning, your job, social life and sleep. You can pick only three, not four. No, no, no, that's fine. But will the classes be interposed? That's the thing, the math and the bootcamp. Yeah, yeah, of course it will be interposed. Interposed. And that's because you are the only lecture so yes i'll be the only lecture it will be intense or the other thing we could do is i can slow roll the math i can make it once a week so that it goes on for the usual three months i like to do the math once actually at least I like to refresh. The one that usually goes house people love it. So I dare you remember the math class did you like it? Anybody here who took my math class? Yeah, I think that's it was I think it's one of the best. That's the best class. Okay, so what I need to get a little bit more organized i need to create a registration and some notifications and all that i'll do that so if are you not thinking deep learning bootcamp or no boot deep learning bootcamp is about to start in two three weeks get ready for it guys but you are saying deep learning and machine learning and math three no no no no no machine learning at this moment i'm not the other boot camp other but no you're not good okay i'm only offering the deep learning bootcamp the deep learning will be a series of boot camps yeah to be intense deep learning has become a vast field right so i cannot teach it in a standard work workshop setting and do justice to it you won't learn enough the only way to do that is to give two theory sessions a week and a whole day to labs got it so we'll do a lot of it to get you prachi just messaged me she has been taking names for the uh engineer math oh she has been taking okay so guys if you want to enroll for all of this we can agree on the dates and convenient dates but the bootcamp deep learning bootcamp is starting i'm just waiting for you guys i wanted it to be a natural transition for you guys. If you are taking the deep learning bootcamp, please register right away. That is one class that may fill up too quickly. Well, these days we live in the COVID world, who knows, maybe it won't fill up. But that usually is very highly anticipated. Send your names to Prachi quickly. And when I send you the registration form do fill it up because it will be first come in case it is it becomes too popular. It will be first come first serve. What is that it is called teach me a math or engineering math? What is it called? Yeah, it is the the math 100 course the Math of Data Science a Math behind Data Science is the actual name Math behind Data Science more colloquially we just call it the Engineering Math class because uh pretty much what you did in your college as and did you guys ever take engineering math in college and yeah crazy yeah irwin crazy yes exactly but this time i'm changing the textbook oh you're changing it to what i'm changing it to a book uh which is a little smaller but it is much more focused on just the math behind data science okay yeah that will matter yeah the engineering mathematics math was a lot of math but then it actually was talking about other things like physical models and those kinds of things erwin kreisig is a classic like all all of us who went through IITs. And I think all engineering colleges use that. In the US, they used to use that. In India, they used to use that. I don't think the author is alive anymore, because you don't see any new editions. Yeah. I think 10 was the last one, I believe. Yeah. We learned from a new book. All right, guys. Somebody had a question that I interrupted. Sonal, how are you liking it so far? Are you getting anything out of it? You're very quiet. Oh, it's a little bit hard for me. I'm sorry. I didn't hear that. It's a bit hard. It's a bit hard. Reach out to me if you need help. Okay. Remind me which grade are you in? I'm going to 11th. Oh yes it would be hard for you. You're getting a very early start onto all of this. Kamya, how about you? Let me ask all the young ones. Kamya, are you hearing me? You're on mute. I think she has stepped out. She is on mute. Aritya, you also have committed to the subject, you have a PhD in quantitative field, but machine learning is new to you, isn't it? Yes. Are you finding it manageable? I understand the theoretical aspects of it, but implementing it on the code that I think I have some challenge. Libraries and code you become familiar with. You notice that now that I go through the solution of support vectors and learn it first, it all begins to look very familiar, isn't it? Yes, yes, that is true. Coding is the easy part in machine learning. It's not the hard part that will easily come let me mostly mostly we are facing problem with this persons is it very library library versions oh yeah yeah see here's the thing guys software engineering you know they they are like every thing has every field has some nuisance software engineering is always the library versions yeah yeah yeah in the in the world of windows it used to be called the dll hell yeah yeah library versions is always a big big pain and installing the library and one library won't work on this machine yeah exactly that's what we are facing with the pandas i made it work but it is not giving the exactly the same output what got from you so i think still there is some it is working but it's probably not the latest i believe you sit with me uh would you take some time i think i think that is the one uh what's her name who and i was talking because she's in the same team i think that is the one, what's her name? Bhuvana was talking because she's in the same team. I think that's the help. Yeah, we will sit with you maybe tomorrow. How about you? Are you making, are you able to follow through all this? Yes, as if like, it's better now. I think I need to practice more. And the theoretical part, like I'm pretty much good and I went back to ML under and I was checking out few things in our so now it's a bit better but I still have some questions to ask you yeah and I'll talk to you about that those things definitely guys remember you can reach out prachi is there she has been helping a lot of people dennis is here and glennis too is here you can reach out to any of them and they'll be very happy to help you right these are the tas for the clouds keep reaching out to them if you see i'm busy reach out to them but then if you feel you still need me, definitely reach out to me. That's that. Who else should we ask? Harini, how about you? You also have come, you haven't taken ML100. How are you finding it? Also for same here, the theory I'm able to understand with a bit of reading and all that. But implementing it practically i'm having issues like i have sent you my r code i think that looks really i don't know what you would make out of it but yeah i'm stuck at places so even though i'm going back and reading to find the code i'm not able to implement it and you know into what we are doing to implement it and you know into what we are doing okay we'll sit together and say i haven't i will speak to them okay okay yeah i'm still stuck in the river data set so that puts me a lot behind and the first is the hardest harini yeah wait for the first after that it becomes sort of repetitive okay so i'm stuck at the model building stage again so anyway i've sent you my code but whenever you have time or i'll reach out to prachi and see if she can yes yes i'll try to reach out to you but in the meanwhile don't wait on me i stay very busy okay 30 years yeah otherwise yeah a lot of things i'm understanding very well so that's good excellent yeah as if i take this opportunity i think sandeep had a question about his project i don't know if you can take the time to see he he did something and he sent a question to you he's not here right now at the moment yeah oh he's there no he's not here um but i i just people are asking for help you just said you will yes yeah certainly i'll look at his uh i'll check my email actually i've been extremely busy i haven't checked my i recognize that yeah i realized that so i just just thought I'll remind you. He just sent something, some bipartite something. He was telling me about it. Well, he already made the bipartite trip. Okay, that is good. I know what you're talking about. Yeah, so he did something because he talked to you and you gave him some idea, he worked on it, and then he got something and looks like he stuck or something, I don't know I'm not sure I can reach out to him yeah please if you could take some time and just give me 10 minutes maybe definitely alright guys if you don't have any other questions i'll close this meeting and we should be see you then on saturday and sunday thank you thank you we are having us this week we are having a seminar it is the seminar that we are the support vectors is hosting is by a very illustrious person a person who is often the advisor to the Prime Minister of India. He was a person who graduated from IIT, like us, one of the top rank holders, then gave up his engineering thing. He said it's all useless, it doesn't benefit mankind at all, all that people do is work for countries. He went off to the hinterland to the villages to see how real people are living their lives and what their real problems are. In his words, spent a lot of time studying their life and then he has given his life to creating engineering solutions which are specifically meant for them that big companies wouldn't care about or so forth. And he has been a serial entrepreneur, created a lot of things and he has made a lot of difference to things. He's now an honorary professor at IIT as well as an entrepreneur. And almost every prime minister of India in the last few sequences, I've reached out to him for advice. So he'll be the speaker on this saturday's talk he will talk about ai and what it means again to the topic in hand to people how can you bring it to help people oh that'll be awesome what's his name well i i don't know what his full name is. He's my friend's brother. And I only know him as Kannan, which may be his nickname, so I shouldn't embarrass him in public. What time, Asif? Sunday noon. Okay. Okay. Oh, nice. Thank you. Are we going to do the starter code for the project on saturday yes we'll take time uh we'll do that actually guys i have to just uh unfortunately i'm really crowded on time one way or the other i'll find time and i do want to get you guys started on the project maybe if needed we may not have a python session this way python we are essentially done guys or do you guys need more sessions anybody has a feedback do you guys you have told us what our libraries will be using massive and pandas was little bit you went in detail right okay so i don't know whether you want to continue that but you said you would parallelly do R the other day. Yeah, I wanted to start on R now. So maybe I'll give a week's gap, use this week for the projects and after that get into R. Okay, thank you. Giving background and that's sort of overdue. Asif, you have sessions on both the day, Saturday and Sunday? Yes, I do have sessions on both the day. Saturday I have, there are two things. Saturday noon is a quiz. You guys know, right, that there's a quiz every Saturday noon? Yes. Are you recording, Asif, those sessions? Yeah, yeah, I'm recording. In fact, Prachi has that she will be okay okay but this week topic for quiz as if random forests last time it was decision tree it is random forest I might also include support vector machines right so two topics it may be a longer quiz. Come to it guys, it's interview preparation. You know, it just make and validates whether your foundations are as good as you think they are. Decision tree, I was very pleasantly surprised. Most of you got most of the questions right. So that's good. It means you are reviewing from the book also. And I hope we get similar results for random forest in SVMs. Let's stop with that, guys. We'll have a pleasant evening. And I can smell it. Thank you, Asif. I better run. Thank you. All right. Thank you. Good night. Sure. Bye. Bye. smell it better thank you all right thank you good night sure good night thank you