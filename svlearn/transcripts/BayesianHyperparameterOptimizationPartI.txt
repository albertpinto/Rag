 So the topic today is automated machine learning. This is by far the most important topic in recent times. If there is one thing that will completely change the landscape, I think in the coming years of how we do machine learning, it would be this topic. So I'll talk a little bit about that. I won't cover all of automated machine learning, though I would cover two of its biggest pieces. Now, along with that, I'll also cover a research paper, which is on COVID. Now, along with that, I'll also cover a research paper which is on COVID. Now, actually, I'd like to start with that, with that research paper and sharing that. So, by the way, guys, are you OK for extending this thing up to 10, 10 p.m.? Or does anybody have a hard cut off yeah that's fine that's fine okay so that gives us some reading space so guys this is uh nature nature as you know is perhaps the most prestigious scientific journal or you get your articles in Nature if people believe that you have a true breakthrough. So this is one of them. We live in COVID times and all effort these days is devoted to COVID. All kinds of researchers are trying to contribute. So one of the things people asked is, can AI look at people's chest x-ray and detect COVID? Now that's a provocative question. If you remember one of the, your second project, which you may or may not have the time for, is to find, to look at chest x-rays and detect lung cancer. And nowadays there has been a tremendous amount of success. People have been able to look at the structure, the MRI and the X-ray of the human heart. And in some cases they're able to, the AI is able to spot anomalies in the heart that were so subtle in the heart muscles, cardiac muscles and the circulatory system that were so subtle in the heart muscles, cardiac muscles, and the circulatory system that were so subtle that it missed even the most experienced cardiologist. And so these are all little bits, remarkable insights. However, the fact has remained that the medical community to a large extent has remained sort of on the fence. They're skeptical whether it works or not. So we are in a transition time. AI has made a lot of progress, but still it has, in some areas, it has made fundamental breakthroughs, like for example, in the protein folding news that I talked about last time, which is truly a remarkable breakthrough for AI to do. And again, a tremendous and unexpected achievement for transformers, attentions and so forth. But there are also areas in which AI has been trying for a long time and has not been able to make a dent. Part of the reason it has not been able to make a dent is. When you try to create deep neural networks, quite often you don't know how the network should look. There are too many variables. Should I make it like, you know, should I make it out of 10 layers, 20 layers, 100 layers, 50 layers? How many layers do I need to put? What should I put there? How many layers of corn? How many filters? How many max pooling? What are the residual short circuit connections. You see that we get into this world in which there's a combinatorial explosion of possibilities. How many you know nodes per layer and so on and so forth. None of it is clear when you hit data. So we tend to take an existing architecture, apply it to that data, and from there start evolving, you know, hacking from there. We do this, we do that, and we see if that works. So a lot of deep learning after you picked up the theory and you start working on some real-world problem, it becomes hacking day after day, experimenting and seeing which experiments work, which hyperparameter combinations or which neural architecture is giving you good results. It's a lot of trial and error. So the question has been that is there a systematic way we can go about it? Because this trial and errors are extremely expensive. So think about it. So to simplify it, actually, I'll step out of neural networks. So think of a basic thing. Let's say you think of a random forest and you want to use random forest to classify potentially whether somebody has COVID or not. Now let us say that random forest had reasonable accuracy and you're just trying to improve it. There are too many hyper parameters, the number of trees to grow, the depth of the trees isn't it, then so on and so forth. And so with all those hyper parameters even which which one to use, Gini and this and that, entropy, there are many many hyper parameters even which which one to use jimmy and this and that entropy there are many many hyper parameters so for each and even and then of course the learning rate now which of these hyperparam and the regularization parameters and so forth so what in any algorithm you have that now what happens is that well in trees of course the number of trees is the regularization forest is the regularizing now with all of these it's hard to know what values to take another example is the support vector machines support vector machines are extremely effective but you need to specify for example which kernel would you use would you use a linear kernel or would you use a polynomial kernel would you use a tanh kernel would you use a linear kernel or would you use a polynomial kernel would you use a tanh kernel would you use a gaussian rbf kernel let us say that you decided on a particular kernel polynomial which degree uh rbf kernel well what is your cost function what is your gamma right and whichever one you care of course the cost function is across all of them how how big is your budget if you remember the theory of support vector machines the most crucial thing is what is your budget for mistakes the c right and gamma of course intuitively speaks to how far uh one particular lighthouse is spreading its light you know that's the intuition i remember giving you guys so with all of these you have a lot of hyper parameter training a support vector machine on a reasonably reasonable size data and a few hundred features or or even if a few dozen features can often take half an hour one hour now suppose you take um 10 values of the cost and 10 values of gamma. You're already looking at 100 potential experiments. You're searching through that. And then 100 times half an hour is 50 hours. You're looking at a couple of days for the run time to find to tune that machine now this becomes even more problematic when you talk in terms of a deep neural network or even before we go to detail network see as i said there is a no freelance theorem you don't know whether support vector machine is the best algorithm for that problem no freelance theorem says that you can't tell till you have engaged with the data you would never know. So the algorithm that you pick, even if you're doing a classification, the algorithm that you pick itself is a hyperparameter. And you have to search the space of algorithms to find which is the best one. And what is the tuning parameters for that? Give different values to the tuning parameters for that. So by the time you have zeroed into this you realize that the number of combinations is there's a combinatorial explosion of how many experiments you'll have to do how many models you'll have to train before you come to know or you pick the best of them are we together guys know or you pick the best of them are we together guys yes right so that is the crux of the problem and when you come to neural neural networks there is an additional complexity the neural architecture itself is fluid how many layers do we use con? If you use CONV, how many filters in each layer and so on and so forth. There are many, many hyper parameters of things. The architecture of the neural network itself is one hyper parameter of sorts, isn't it? So people have asked, how do we solve this problem in a systematic way? And so that will be the topic of today. And we'll do that. And so we'll go, that will be the topic of today and we'll do that. But before we do that, I wanted you to see this paper. This is a very nice paper actually. This paper asked this question that if you, can you detect COVID from x-rays? Now, the problem that this paper comes up with is that we don't have enough data. We can create neural architectures, but we still need data. So this project is divided into two parts. One is the COVID net, the neural network architecture, and the other one is the COVID-X, sort of the data behind covet and what they are pleading and they have been pleading now for many months is for governments and organizations to contribute data data is still very sparse and deep neural network as you know needs a lot of data well even with the data that they have, actually this model is remarkably successful. It hasn't beaten the state of the art, which is of course the reverse, what is it? PCR, reverse transcriptase, PCR, is that the right word? So which is obviously a good test, but not good enough it seems because it also has an error rate. But it's better than what AI has achieved so far. Now, one would like to believe that once more data becomes available, this architecture would succeed. If it does succeed, it would be a breakthrough because it means that you can get results for COVID testing literally in a couple of seconds rather than because that's what it takes to do x-ray or maybe half a minute rather than days which is which is what it takes today to get an accurate reading now this is reasonably good it's not bad but it's not the state of the art right now what is interesting about this paper is something that i would like to show you guys they go and i would give this paper as a reading guide to all of you this came as a pre-print a long time ago and i think i've mentioned this to you in the previous classes or in the intro itself so what you're looking at is a human, this particular thing is the x-ray of the lungs, and you're trying to determine whether somebody has COVID or not. And if you look at it, the top one is a person who is not infected with a lung problem or with COVID. The bottom one is a person who is infected with COVID. So qualitatively, they look different, though it's a little hard to tell very tangibly that which part of the x-ray will you zero in to be a litmus test. There is no very clear path. It's complicated. The COVID is sort of visible in the overall picture but not necessarily too much in any of the specific parts, I suppose. So these people then say let's apply a congulation neural link because it's very good at finding those hidden features that may form a good representation for the disease and when they that, they actually get very good results, greater than 80% sensitivity and so on and so forth, like this paper talks about it. Now, I won't go into all that. It's a very interesting paper. I won't go into the rest of it. As you can see, it also talks about whether you have pneumonia and so forth. But what I will do is I will look at this, the COVID net architecture. So I'll just read what is written at the bottom of COVID net architecture. High architectural diversity and selective long range connectivity can be observed as it is tailored for COVID-19 case detection from CXR images. The heavy use of a projection expansion projection design pattern in the COVID-net architecture can also be observed which provides enhanced representational capacities which maintains computational efficiency. Now it's a lot of words but what it is saying is that Now it's a lot of words, but what it is saying is that, do you see how many layers there are? There are many, many layers and these layers have a lot of filters. But what is interesting is, do you notice that there are these residual connections all over the place? The whole bottom, you can see how many residual connections there are. More than that, you also see some longer, you notice that there are some cross architectures. From here, you're split into two parts, to this one and to this one, right? And there are a lot of crossover kind of a thing, and they feed also into the main pipeline. And then there are some long-term things. For example, output of this goes to this which feeds many layers later, right? And so this whole business of residual and hybridizing the architecture is such that the first question that it works is, how in the world did somebody come up with it? Would that be a fair question to ask right like for example you wouldn't if you were sitting down with pie torch or with keras or whatever you wouldn't think normally or immediately of creating an architecture like this so how do people discover such uh interesting and complicated architectures would you like to take a guess architectures. Would you like to take a guess? AutoML. Okay, say that a bit louder please. AutoML. AutoML, yes indeed. But what does that mean? Like how would we do that? Neural architecture search, like you define range of parameters and then select the best one. That is more like hyper parameter optimization, but yes, close. See guys, what happens is that think about this, this neural, if you were doing it by hand, you would be putting all sorts of layers together right and you would be doing trial and error and seeing what works isn't it so why not do a different thing the whole idea of neural architecture searches like why not create a neural uh network whose job is in in some sense this is very hand-waving argument whose job is in some sense, this is very hand-waving argument, whose job is to think through the right architecture, put together the right architecture. And that is what we do. And this paper talks quite a bit about that, how they come upon all of this. Obviously, there's a lot of hand thinking also along with using AutoML and so forth. But the reason I pointed this out to you is for two reasons. One is that today then the problems that are unsolved are all hard problems, right? And solving them is And solving them is quite often not so much as just putting a few lines of PyTorch or Keras together, but it is thinking deeply. You have to go to the next level, which you don't get to simply by doing what we have learned to do so far. We need to learn something else. And that is the future. And that's something else that we'll talk about is this big world, something called a Bayesian optimization. So today I'm going to introduce you to Bayesian optimization. And people often say the Bayesian optimization applied to hyperparameters is by Bayesian hyperparameter optimization and the same Bayesian optimization thinking applied to neural architecture search, figuring out the best architecture of things. So what is Bayesian? We'll talk about, well, there was a guy named Bayes, a British guy who came up with the so-called Bayes rule. So it is, and you have all been taught, I mean those of you who have been with me for long enough have been through that. Those of you who did engineering math of course went through fairly in-depth biation treatment. How many of you went through that? Yeah, Praveen I remember you were in the biation, you know this machine, the math of machine learning, isn't it? Yes. Anil, you too, isn't it? And Nisarg? Yes. Yeah. So, and Pradeep, you too. You guys are there, right? And Kate. So, yeah, so quite a few of you. Subin, were you there in the math class? No. Were you there in the math class? No, I wasn't. OK, so that pretty much sums it. A few of the people here are very familiar with Bayesian. So what I will do for the benefit of the rest of you, I will assume that you don't know Bayesian. And those of you who do know Bayesian, you can go have a cup of tea or coffee while I'm talking about the elementary concepts of Bayesian learning. But before we do that, now I'll go back to basics and see how a typical blog, what is it that they tell you to data into the training and the test, learning and the test. So the test data you hide under the pillow, you forget about it completely. You don't even know about it. Then what you do is within the training data set or the learning data set, you further divide it into two parts. The actual training part with which you'll train the model and the validation part or the validation set, right? And there are all of these things like a holdout sets and the cross or the K fold and so on and so forth. But I'll gloss over those aspects for a moment. And I'll just say data is now further subdivided into two parts. The one that you actually use to do, for example, if you're doing neural networks, you do your mini batches with, and to train the model in which you do the back propagation. And then the validation set in which you deliberately freeze the weights, you don't get the back propagation happen. You just do the forward part of the journey to see what is the validation loss coming through are we together you keep making prediction on the validation set and this gives you some sense right now why is that relevant because then you can jiggle around with multiple learning parameters so let's take some basic learning parameters it could be the learning rate. It's a hyperparameter that you typically think when you're doing neural networks. And you can think of many, many such hyperparameters that you do for support vector machines as the C, the cost, the budget. It could be if you're using RBF kernel is gamma. If you don't know which kernel you're using, then you have to play across linear, a polynomial, RBF, tan, maybe even the sinusoidal and so on and so forth kernels. You have to play with all of those to see which one works. Now you do it through the cross-validation stage. You keep on churning to see which value of the, which value gives you the best cross validation results. Right. So with that little bit of a background there. Now let me start putting it to the ground. Are you guys still seeing my screen? Yes, so it's completely black right now. Oh yeah, it is black. Right. So the basic idea that you see people mention in books, and the oldest idea is to do grid search. Let us say that you have two hyperparameters. I will call them, let me just take an example. Let's take a regularization parameter and a learning rate parameter. So learning rate, learning rate alpha is one and a regularization parameter or weight decay parameter regularization. Let's say that you assume that you're going to do, let's say L2 or something. So let's say that this is your regularization parameter right now comes the problem you have a model let me just take it doesn't i'll be sort of silent on what model you have some model this model is if you think about it it is the loss that you will get is in some sense the at the end of the training the predictions that you get or the errors that you will get is in some sense the at the end of the training the predictions that you get or the errors that you get at the end of it is not just a function of the actual parameters of the model you know the actual weights and biases of the model but it is also a function of the learning rate and the regularization parameter, right? For a given input, would you agree? Yes. So these are your parameters, your rates and biases, but these are your hyper parameters. Now, in effect, you can back propagate the gradient with respect to loss. So, and this is the crux of the issue. See, you can back propagate the gradient with respect to loss. So and this is the crux of the issue. See, you can do this. Great. Let me just put it this way. The gradient with respect to the loss. You have this. And once you have this, you can do next is equal to W minus alpha grad L. is equal to W minus alpha grad L. So you had a gradient descent step. Now you also have backprop, which will take you layer by layer backwards. And so you can do it. But now for a moment, think about the learning rate. You realize that it is not feasible for you to do this. Or rather hard for you to do this. How do you take the gradient of the loss function with respect to the learning rate? It's very hard. Are we getting that, guys? Not only that, especially if the learning rate is adaptive and in the process of learning or tuning the neural network during one cycle of training or a couple of epochs of training, the learning rate changes and so forth. In the beginning, it's something else. Later, it's something else. And so on and so forth. It's much harder to tune and know how should you vary learning rate is therefore a function essentially of some time if you're thinking as a time variable as the learning proceeds and it's very hard to know what this is you can't do gradient descent you can't do back prop of all of these parameters so what and so which is why what people do or have always done, basic grid search says that suppose you have values like this, a learning rate is 0.0001, possible value 0.001, 0.0.01, 0.11. I'm just taking an example. You can take a range of learning rates, right? Or you can do in terms of log of it. You can say log is equal to 3, 4, 3, 2, 1, 0. You could write it like that. So you have a scale, sort of a log-in scale, for the learning rate. Then you have, let's say, your regularization parameter, lambda. And now you can again take values from 0.00, like quite literally 0.001, 0.01 and the same game, 0.01, you can do this. Now, when you do something like this, what you're doing is if you really think about it, you're doing this, you're creating a grid. Let's say that this is your alpha and this axis is your lambda. So for different, and again, I'm taking a log minus log alpha scale. What you're doing is, and then likewise for the lambda, maybe minus log lambda, lambda scale, you're creating, there are all of these possibilities. Let's say I took five values and five values here. So you create a grid of values possible, right? It's a Cartesian product. So it is a Cartesian product. Let me just call it alpha A and this is B. you're basically looking at a cross b values every tuple that belongs to the set am i making sense guys okay and so for each of these what will you do you'll train it you will do apply look for loss look for loss on the cross validation set are we together guys and then of course you will remember that for this you'll create your tuple. Let's say that certain value of alpha i lambda i and what was the last i. You will save these values and when you do that you will realize that you have come up with a huge table. you have come up with a huge table. How many elements there would be? 5 times 5 is 25 experiments. And after 25 experiments, you would pick the best value. But even if you pick the best value, there is still a problem here. And the problem is quite subtle, actually. It is this. Suppose here and the problem is quite subtle actually it is this you suppose this is the best value you can zoom in and you realize the best value is not necessarily here it is somewhere in the region you can zoom in and be more specific for example how do you know that if it is 0.01 it is 0.011 0.0 0.09 is it 0.05 or 0.013 or 0.02 0.03 like you don't know which of these values was really there do you see that because you you you just look at a log linear scale and the next thing you looked at is either one-tenth or ten times bigger than this. Are we getting it? So you have to go down to smaller and smaller resolutions. And when you do that, your number of experiments begins to increase or explode. Are we getting the point, guys? are explored. Are we getting the point guys? And so this has been the problem. This becomes particularly bad with neural networks. Why? Because training, let's say a simple linear regression may take a couple of seconds at most on fairly good data set sizes. Training a neural network, you guys have been doing it. Do you notice that by the time you run through five, 10 epochs of training, usually half an hour has passed or one hour has passed? Right? So a few, and if it is a non-trivial problem, even on good hardware, quite a few hours pass by. And so you are looking at running this long experiments to find the best possible value. So grid search begins to look rather untenable and it becomes particularly bad when you have situations like suppose I have a hyper parameter H1, H2, H3, H4, H, let me just say H10. And let's say that each of the hyperparameter you try out with how many values, 10 values, right? So you realize that you are doing, what are you doing? 10 to the power 10 experiments. Because there would be these many combinations these many combinations of hyper parameters even if you take if you search around only 10 values for each of the hyper parameter does that make sense guys right am i are we getting the point it's a pretty straightforward point of combinatorial explosion that if you have 10 values for 10 hyperparameters, you're looking at basically 10 trillion experiments. Is it billion or trillion? Billion. 10 billion experiments. Any questions, guys? So one thing is, what is the way out of this? How do we not get lost in so many experiments? So before we do that, let's observe one more thing in this. Suppose that some of the hyperparameters are useless. Let's say that in this particular case, I'm just for the sake of argument saying that learning rate doesn't matter, right? So this doesn't matter. Do you realize that you did 25 experiments, but in reality, they all map to just five of those experiments, isn't it? Of five different values of the lambda. Do you see that, guys? If your loss function is insensitive to the learning rate, hypothetically, and it's only sensitive to the regularization parameter, then you only tried five regularization parameters even though you did 25 experiments. And that was stupendously wasteful. only tried five regularization parameters even though you did 25 experiments. And that was that was stupendously wasteful. Would you agree with that guys? Anybody still there? Yes. Yeah, you would agree with that, right? You said it's quite inefficient, but that's part of the limitation of grid search. So then somebody woke up and said, well, there's a better idea. Let's do randomized search. And if you notice, whenever we were doing hyperparameter tuning in the normal way in the workshop, still you came to this particular session. I would always recommend that you should not use grid search, but you should do randomized search. So in particular, when you would do, Harini, if you remember, where is Harini? Is she still there? I'm there, sir. Yes. You remember I told you don't use grid search for cross-selecting. Use randomized search. Yes. So what is randomized search and why should we use it? To just make a picture smaller, suppose I take the same problem. So now I'll turn the table and I say that suppose the hyperparameters H1 and H2, right? And once again, this was your, let's say, alpha and this is your lambda for the sake of argument. So suppose you have to take 25 points, but don't take a 5 by 5 grid. Instead, randomly put these somewhere. 25 points different I don't know these are not 25 points, but whatever they are just leave it at that right. So now let's think about it, if you put 25 points do one lovely and randomly one lovely thing that will happen is very high likelihood there are 25 alpha values you are trying out the h right and there will be 25 lambda values you'll be trying out do you see that even though they're only 25 points you end up effectively testing across to a much larger space of values for the hyperparameter by randomizing it. And so let us say that the good values was around this. Now it's all around this or whatever it is. You can quickly zoom in because you have taken all of this. You can quickly zoom in and tell which local region of the hyperparameter space you should care about. And this is called the search space. In this literature, it's the search space because this is what you're searching for the values of the hyperparameter. So far, so so good guys. Now this has one more advantage. Let us say that one of the hyperparameters is absolutely useless. Let's say that Lambda is useless. Right? Or maybe it was the learning rate. Let me make it the other way around. I think I just said that Alpha is here. What is it? Learning rate is useless. Doesn't matter and Lambda matters right it doesn't matter what it is so then what happens suppose alpha doesn't matter it's still you have or i mean sorry okay which which one doesn't matter let's say lambda doesn't matter you still have how many values of alpha quite a few values of alpha or the other way around if alpha doesn't matter then you have 25 values of lambda that you have tried right and so you have a better or more efficient utilization of the search space right so that is a randomized search but even though it is a much more efficient use of the search space, nonetheless, it remains as a fact that you are searching a pretty large search space for the best hyper parameter and that search space, if you have 10 hyper parameters, which is very common, which is very common you are searching in a ginormous features i mean hyper parameter search space so at this moment i want i wish i can get some visual cues on how much we are understanding go ahead yeah in this randomized search right a worst case scenario it will be as bad as the grid search in the worst case scenario yes but the other point though i wanted to make is if we go with a randomized search it's possible that certain combination which are important you might just lose those right now say that again i missed the tail part of what you said so you know in randomized search you you're really you're really choosing random points yes so and then you're deciding you know whether one hyper parameter is relevant or not in that randomization you may lose out a combination of the parameters which are more important and might make the other the second parameter relevant. Now give me an example. Let's say that it is some value of regularization and some value of alpha learning rate. As an example, what am I going to miss out? No, I'm saying that you pick a combination of those two, right? So it's a combination of those two. You take a random alpha and a random lambda. And you imagine the model and you find the loss, the best. those two right so it's a combination of those you take a random alpha and a random lambda you imagine the model and you find the loss the best yeah i'm just making sort of a theoretical argument here that you're picking the random combinations of values of those two parameters and then you may conclude that one of them is not relevant anymore but while picking those randomized pairs you may not pick the one that would actually make a difference no i don't get it pick the one means one of what the right hyper parameter or the right value of the hyper there are only two hyper parameters right and then there are combinations let's say there are 50 combinations and And then you pick. Use the words, don't use pronouns. Use the words. Suppose learning rate and regularization are the two parameters. Alpha and lambda are parameters. Okay, alpha and lambda. And in your example, you had some 50 odd combinations, right? And in randomized case, we may let's say choose i don't know whether it's a it's a number of combinations that we pick but let's say we pick 20 out of those and based on the observation we decide that the learning rate doesn't matter but what i'm saying theoretically maybe the remaining 30 combinations would have actually mattered. See, when you're doing 50 experiments, you will do all 50 experiments. You'll take all 50 random points. But what you're saying is, if I'm getting it right, what you're saying is it may not matter around a certain value of lambda but you may have missed the fact that there is another region or region of values of lambda where lambda matters a lot and you didn't pick it right yeah that is true that is always a risk with randomized search which is another reason in fact very good point you made which is another reason that doing either grid search or randomized search is not a very good idea because you know you're looking in a very high dimensional space and at most how many experiments will you do 100 100 is nothing right in a like you need to do a billion million or billion experiments to even have a reasonable coverage of your feature space. I mean, sort of hyperparameter space, search space. And nobody can do that many number of experiments. No team has the luxury of using hardware for that long, right? And sometimes people do, if a model is very, very important, they spend a long time searching for it. So the problem that happened in the industry was exactly this. These models have become huge. And the hyperparameter search space is huge. And so when you go and try to tune these models, it would burn through a tremendous amount of computational resources. So much of it that it was besides being taking a long time, it was an ecological disaster to be able to, you know, repeatedly do this and burn so much of resources for it, right? So what do we do about it? That's a question. To see what we have to do about it, let us think about it in this see you you come up with all possible pairs or random values or combinations of let's say h1 h2 right a two hyper parameter when you run an experiment you have either chosen the points at random these points or you have picked it on a grid but when you run the experiment let us say you can run it in parallel, like across a hundred machines, but more likely you'll run it on one of your machines, your workstation, let us say, and you will do an experiment, save the result, do the next experiment, save the result, and so on and so forth, right? The thing that happens is each experiment is independent. It has not learned from other experiments that have already happened. You see that, right? Because at the very end, you'll see the results, the loss coming from all of those experiments, and then you'll decide which is the best. And then you go down, zoom in there and do more experiments. So the way to do this is illustrate this is, suppose you have parameters, I'll just use abstract words h1 h2 and obviously the hyper parameters in real life are many many but we'll deal with two dimensions because we have a two-dimensional surface let us say that the loss okay or maybe even simpler let's say that there's only one hyper parameter right let me call this hyper parameter x. Whether we don't confuse it with data. Should I use x? Okay, let me use h. Except because I've been saying h. Suppose this is your loss. So the best loss that you get after you've tuned the hyper parameter on the entire validation set and this is your hyper parameter so the l and h are beginning to look alike so i'll make it like this and imagine that there's only one hyper parameter see when you do grid search you would i mean sorry grid search you would take equally spaced values and search for that but suppose you took random values and did that so let's say that the first experiment produced a value like this the second experiment produced a value like this the third produced a value like this loss right the fourth produced a value like this then a value like this and a value like this and a value like this so now tell me one thing if i ask you to do the next experiment and i say you have a choice of picking the hyperparameter value where would you pick that value from where would you like to do the next experiment which value of h big value small value Smaller or in the middle? And why would you pick the small value? It's correct, but why would you do that? Because there is still a gap where we haven't covered that search area. Yes, look at the loss. The losses seem to be lesser with smaller values. Exactly. So you would suspect that this seems to be a more, so you would guess, based on this data, right, based on observation, you would say that loss is likely to be lower where? Lower in the circle. So if it is likely to be, it's more likely to be lower than in this other, that's a rectangular region. This seems to follow after doing a few experiments. You can never be sure, but the observation of training with a few combinations, a few points in the search space, of training with a few combinations, a few points in the search space, takes you to the conclusion that this is probably true, isn't it? So it makes common sense that you do the rest of the experiments here, the next random points you pick from here, rather than from all over the feature space randomly, right? Or do a grid search. Does that make sense, guys? I hope I'm saying something that completely agrees with your intuition. Yes. Right? And to do that, now let's put it in formal words. See, what have you done? You have learned from observation, isn't it? You have learned from data. In the beginning, you did not know what the hyperparameter values were. So let us say that you had a, it is called the prior belief could be that loss probability of loss or how much the loss would be loss to be minimum anywhere. So you're taking a uniform prior, means any value of H is fine. It could be, you don't know. So you say that this is your prior belief, you're coming with no information, but gradually as data accumulates, you now have a posterior belief. What is your posterior belief here? Posterior belief is a posterior belief. to always after seeing the observations. After seeing the observations. So look at this discussion that we just did, and we are just now translating it into a more formal language. What would you say is your posterior belief? Right? Where is the best value of the loss function likely to be? Clustered for lower hyper- Exactly, this region, right? Posterior belief would say that the, so this region, you would say now, if I were to draw a distribution on the values of H in the beginning, let's say prior, give a uniform, let's say that a probability, let me just say probability of loss, the best value of the loss to me, then it was in the beginning it is uniform. This is your prior. And then your posterior, would you agree that it is sort of seems to be more belief around smaller values, right? You believe more strongly or you say that there's a higher probability that the best hyperparameters values are somewhere here in this region. Your posterior belief, right? Are we together? Does this make sense, guys, in this view of this picture? Yes. Common sense. And so what has happened is what we have gone through is the classic Bayesian journey. You can Bayesian say that there is always uncertainty. So you could be very wrong. Your prior belief is anybody's guess you may come with a uni absolutely no belief at all right which is a uniform prior or you might have had some inkling for example you could have had an inkling let me take another a prior you could have had an inkling that the values are somewhere here. Another prior. So what are you saying that I believe that at this, I initially believed that the loss would be minimum at that red point of the hypothesis space and around that place. So I put a bell curve distribution saying my belief is strongest at that point, that red point here and around it. So you may start with the Gaussian prior. This is your Gaussian prior. Prior. But then your posterior would do this. So what is happening is your curve is, as it is seeing data point by point, as it is seeing data, your belief is adapting. later, your belief is adapting. Your belief of where, which part of the parameter space the best values of loss will be, the best course will be, it is shifting. It is not giving up on anything. So for example, the belief that the best parameter is here at this point, way up north. Let me, I don't know, can you guys see this point? Let me call it B. Do you see where I put B guys? Yeah, this. See, it may still be there if you look at it. My yellow posterior has not given it a probability zero that the best the best lost it may be there it may just so be that b and so actually b would be uh let me say b right b here it should be here the loss it is possible that the loss here is the best. That you say, it is the best, who knows? But it seems unlikely, right? Your observation seems to imply that it is unlikely to be there. The best losses are found with small values of H. So we deal with uncertainty, we are never too sure. Right, for whatever reason, there may be this tiny island of stability around B where the loss function achieves its absolute best goal. Am I making sense, guys? So it is a question of belief. Nowhere does the value go to zero. You should make it zero unless you have a prior reason to. But generally you shift your beliefs. You initially, let's say that you either start with uniform, the blue belief everywhere, it is equally likely to be a good, the best hyperparameter value. Or you could start with the red, which says, hey, so the blue is called a non-informative prior can you guess why do we call it a non-informative prior you don't learn anything no we don't know anything it is a true acknowledgement that we are absolutely clueless at this moment right of picking a red curve and saying that i believe it's somewhere around this what you're doing at that particular moment is you are saying you know what some experience tells me that the best value could be around this region are we together? And then what happens is you data comes in and the data will push this. So the blue curve, for example, will start changing itself from being flat. It will become a little like this, then it will become a little like this, and then it will become a little like this and it will become the blue the yellow curve like this likewise the red curve will start shifting it started out like this the next thing it will do is it will become like this then it will become like this and then it will become like this and it will finally start shifting and then it will begin to look like the red the the again the yellow curve the yellow curve, the posterior curve. So it doesn't matter what prior you start with. The beautiful thing about a Bayesian learning is, you start with a belief, you see a data point, you adapt, you come up with a posterior belief, right? And your posterior belief has learned from experience, learned from data, learned from data. Right? That is Bayesian learning. And this is a classic situation. What we are seeing is, and if you think about it for a moment, you will realize that, my goodness, it makes total sense and randomize and grid search absolutely don't make sense from this perspective. What we are saying is, suppose you did a random experiment, like two, three data points you got. You may start with some hypothesis, whatever your prior hypothesis is. Let each data point, and this is where the bias rule comes in, it says that a probability of a, let me just put it this way, probability, see of a evidence given a model. So suppose you have a hyper parameter H, right? And you have, or let's say that some value of the, okay, let me change the terminology to what you'll find people use this is your think of it as your loss function people often call it in this space as the objective function think of it as your mean squared error or cross across entropy or whatever this right now the probability of a loss given x oh sorry i I should write it, probability of loss and x is the hyperparameter. Now, this looks very confusing because x we typically use for data. And we use L for loss and the objective function, but I'll just stick to the language that people use now this times the probability of x itself of the the just the distribution of the hyper parameter is equal to which basically is the joint probability of x and y which is also equal to probability of x given y probability probability of y, the last function of Euler's unit without y, right? And so you can go about saying y over x is proportional to, this is sort of, you don't know it and you probably don't care too much about knowing about it, is proportional to probability of x given y, probability of y, right right so let us think through this equation what it is saying what is the probability of a loss function this is your prior what are you saying the probability of seeing a certain amount of loss is something like blue is a certain amount of probabilities py right is something like blue is a certain amount of probabilities py right this is your prior what is this given a prior you see an evidence it turns out that you said like loss is same everywhere but you happen to notice that at two points one point loss is high another point loss is low now do you still believe in that straight line or would you like to tilt the straight line in such a way that it agrees with the data? This is your data, right? Likely, it's called the likelihood. It basically says that if you believe in this, this is how the loss likelihood of having the best loss prior is distributed. How likely is it to produce this evidence? How likely is it that this value of the hyperparameter produced this amount of loss? This particular combination of hyperparameter produced this loss. And now that becomes your likelihood function. And using that, now you can come up with the probability of loss given certain values of the hyperparameter. This is bias theorem, right? And this is called the posterior. And again, I'm rushing ahead quite a bit with the mathematics, but many of you, just take it like this if this math looks confusing just think that prior belief learned like posterior comes posterior is equal to prior plus learning from data that is the gist of variation learning data that is a gesture by Asian learning you see a little bit of data and you learn so now how do we bring that down to our case see what has happened is suppose you pick some random points in the beginning right and what happens is that you start doing certain values of you know loss starts accumulating, you may start with some prior. Let's say this prior, believe that the best value is somewhere around here. And then what happens is maybe it is not here. So let's say that the best value is lurking somewhere here. Then what will happen is when you multiply this prior, and this is the mathematics of it, with this evidence that say that you have seen only five pieces of evidence it is already tilting you towards something like this are we together so what is it suggesting if this is your new y given x posterior it has taken you towards this line it means that you should search for hyper parameters in this region isn't it much more likely to pick hyper parameters and the next hyper parameter you should test out for your experiment from this region does that make sense yes yeah that is it so that is the whole point. Now, why is this relevant? Because the hyper parameter space is huge. And what you're doing is you're doing just a few initial experiments and each time you do an experiment, your prior changes, you know, your belief changes of where the best scores are hiding, right? And so you search that part of the search space of the hyperparameter space, where you believe more and more strongly, you believe that the best values of that objective function or the score or the loss is sitting there, right? So this is Bayesian reasoning. In the beginning, it feels non-intuitive because Bayesians don't come up with an answer. They say that we'll just tell you where it is more likely to be. So they go from one probability distribution to a better probability distribution. But why do we care about this? The reason we care is that a Bayesian reasoning is a very efficient means to quickly get to the solution. So, you know, just like gradient descent and backcrop is very efficient for parameter learning, weights and biases learning, Bayesian optimization has the same role to play when it comes to, you you know trying to guess or identify which part of the search space for hyperparameter space is likely to be to have the best hyperparameters or the lowest loss or the best course in your model so what does it mean in real terms? It means that it is a guided way to do the least number of experiments and quickly zoom into that region of the hyperparameter space where the best values are likely to be. Do you follow the reasoning, guys? Now, it is important you follow it because the literature on this is very mathematical and hard to read in some sense. So I wanted to make sure that at least you get the intuition right here. it because the literature on this is very mathematical and hard to read in some sense so i wanted to make sure that at least you get the intuition right here are we getting the intuition right yes yeah good so kate you have taken my math class so uh nice to see that you are getting the intuition and how about people who haven't taken that is it beginning is it making some sense how do you ask one question here go ahead you talked about the point of uh point b point b was in the scheme of things was outlier right right? Somewhere. I forgot. Whereas you're, you're biasing things that taking it towards the left side, the B7 on the right side. And that's the point with biasing learning and all machine learning. See, here's the thing, you go with the data. The limitation with all of this is, see, you can do only limited number of experiments. So if there is a very peculiar anomalous optimal point sitting in a little island of the feature space that you haven't explored, you won't get to it. So in other words, if you're looking at what would be a good analogy here, imagine that you're in high mountains. In US it would be Lake Tahoe. At a pretty high altitude, there is this tiny little, I mean obviously Tahoe is not small, but you have this lake, right, which is very low compared to the mountains around it. around it. So you're not likely to easily discover that unless you wander into that region. Now imagine, Tahoe is of course a huge lake, but imagine a very tiny pond, a very deep tiny pond in the middle of the mountains. You are not even going there to search for it. But that's an acceptable loss because what you want to, you have only a budget of so many experiments. Suppose I tell you don't do more than 10 experiments then what is the best way to do the experiments got it so given the resource limitation yes the best possible alternatives yeah and if you know biation reasoning then this is an absolute no-brainer because you would realize that a biation way is the shortest path in any space, in hypothesis space to find the best hypothesis. Okay, and we went through that process in the math class. I have a quick question. Go ahead. So you changed the distribution, how does that happen actually? I thought everything- How does that happen? All right everything happened all right let's take a let's take an example right uh let me throw one more question so that you're addressing both together now there's an implicit assumption here that the underlying distribution follows the continuous function that is that is a given isn't it? So think about it, the learning rate. If you change the learning rate by minuscule amounts, do you really expect your objective function or the best loss for those two different values, cross-validation losses to be radically different? You don't expect them to be. Oh, that part I would agree, Asif. But we are kind of saying these are all hyper parameters yes and when we look at learning rate as an example of a hyper parameter the intuitive senses it is continuous yeah can we say that for all the hyper parameters no we cannot we cannot say that and that brings up a very interesting issue so some things are not continuous for example which algorithm to pick is not a hyperparameter. Yeah. And so I'm motivating it with continuous because continuous, the intuition becomes clear. Now there are techniques that you apply. And so then it gets into some very interesting territory like bandits and so forth, learning, which I haven't taught you. But so, okay, see into this game, right? Some very interesting things come and uh before you know it even reinforcement learning has walked in so we won't get there to have the intuition simple just think of continuous parameters for a while okay any other questions guys uh answer the other question is um let's say there's one parameter we are doing a log search binary search uh underlying assumption on a binary search is like uh the distribution remains the same but you then move your search uh criteria uh by dividing the data you do a tree decision and then move uh move your search and uh in the bayesian can you give us an intuition i mean the whole search space is much faster than the log search right see here is the thing given so let me summarize it this way the what grid search and randomized search suffer from when they're searching for the best type of parameters is that you do an experiment and it doesn't contribute to the next experiment where you should do the next experiment isn't it yes that is the main problem now by shen almost by constructions is saying what is the maximum learning or information you can derive from the experiments that you have done and the results you see from them. Right? Because that is the evidence. And you basically say that, see, I have this belief and I'll give you an illustration. OK, let me, why don't I illustrate it with an example, because some of you said, what is Bayesian learning? So, Balaji, hold that thought in your mind and see if I can. Let us say that you fall asleep in a plane and the plane doesn't land where you think it should land. Right? Or in humans there's something very interesting. If you go into a witness protection, apparently, I'm told, I don't know how true it is, you don't know till you are in the flight where you're going to land. Because to protect your identity, because to save you, the information is given to the least number of people. And even you are not given information because then you'll start preparing. If they tell you you're going to the desert, you'll start buying all sorts of equipment and people can guess. Or if they say they're sending you to Seattle, they'll start buying umbrella right so so so i've heard i don't know how true it is but so they tell you on the plane now let's say that they they give a location right and it is let's say the location name is some very generic name georgetown right now georget Georgetown can be anywhere. I think the whole Western Hemisphere is dotted with Georgetown. In India, it would be Rampura. I'm sure there is a Rampura all over the Northern Belt. So now the question is, you're trying to guess what the weather there is. Let's take two extremes. You say, is it a desert or is it basically rainy all the time? Right? So you have a choice now. Let's say you landed there and you make a guess. What guess you made is, and I'll make it discrete so that it's easy. So you say that a probability of rain, or no actually continuous, probability of rain if it is a desert, desert would be zero, right? Let me just say the rainiest place the rainiest place it will tend to one right and by the way then you realize that between this there's a whole range of values possible infinitely many values possible right so for example if you end up in I don't know a Seattle it would be it would be it would be somewhere like this the probability of rain on a given day is uh would this be right 80 percent i don't know right and suppose you land in california the probability of rain would be something like 10 and let's say somewhere in here there is arizona the probability of rain would be something like 10%. And let's say somewhere in here, there is Arizona. It would be, let's say that this is 15%, 0.01. Maybe, actually not Arizona, Colorado, let's say, or something like that, okay, Arizona, would be this and complete desert would be uh what is it the death valley it would be zero something close to zero but you don't know what is happening right so i'm just taking an example that it could be any one of these places and infinitely many in between so you start with a prior belief actually let me do one thing this p y the prior belief right i tend to write it not with a p but to make my meaning clear i use this pi because pi stands pi reminds me of prior can i use pi just don't take it to be 3.1416 all right this is just belief before i know anything prior belief are we together guys probably now what happens is you go to sleep and wake up in the morning and let us say that it's raining Now ask this question. We will take these cases like 1, 2, 3, 4, 5 and we are trying to decide between these 5 cases. So let me go again here. Now what is the likelihood of rain happening if your prior belief is correct. So if your belief, if your prior belief that it's a desert is actually true, what is the probability of having seen rain? What would be an answer for that? Zero. It would be actually zero. Right. And what would be the probability of rain here? One percent chance of rain and the first thing you notice is that there is a rain. Well, it would just sort of very, it rains. So what is likely, how likely is it that this would happen? So, you know, you can go here and this, if it says that this would happen so you know you can go here and this if it says that this would be one right very high likely likelihood that it is raining right and you can work out all the other in between stages and then what happens is that if you multiply your prior with with your posterior, you would realize that the problem, that at this moment, the, where you have landed at, this is multiplied by this is one, you gradually, you will realize that it rains, rains, rains, rains, right? Now, let us say that the first day it rained. Second day, no rain. So what is the likelihood of that for number five? In the rainiest place, you didn't see a rain. What's the likelihood of that? If the prior probability is one, what is the likelihood of seeing that evidence? Zero. Zero, right? So you realize that what happens is that when you multiply your prior with your likelihood functions, reality begins to come out. And here I've taken extreme cases where some things go to zero, but what will happen is this completely got eliminated. This got eliminated, this got eliminated, right? Let me just say that this was 0.001. Certain things, they just begin to get eliminated and the probability of certain other things will start increasing, isn't it? At this moment, you had a uniform probability. The prior was each of these is equally likely, right? Desert is equally likely. It had a one-fifth the probability one right arizona all of them had one-fifth probability because you don't know you could be in any one of those five cities or five places but the more you see the evidence for example one day it rained and one day it didn't already the desert and the rainiest places are out. But their probability needs to shift somewhere, right? So what will happen? The probability of these things, half and half, so these things would go up. Right? And now let's say that for three days in a row it rained. Now what happens? You will realize that three days in a row it rained, the probability, some of this thing that it doesn't rain much will go down, this will go down, and this will start climbing up. Right? And so what are you noticing? You are going towards a... Do you see this guys? If I were to make a continuous thing here. Yes. You're getting it. So that is the magic guys. That's the most amazing thing about Biosciences Learning. In fact, it's the founding. Prior is the, prior is set distribution, right? Yeah, one fifth, one fifth. See in the prior, suppose you didn't know anything about the, where you have, I mean, where you have landed. You could have landed in five places, desert, Arizona, I mean, Arizona, I mean, desert, think of Death Valley, Arizona, California, Seattle, and the rainiest place in the world. So in the beginning, if you don't know anything, I take you out of the airport and put you into a hotel and you can't see out. You could be in any one of the five places, isn't it? Would you agree? So how would you distribute your probabilities one fifth one fifth one fifth one fifth right so I'll give you one intuition guys that I use when thinking by Asian I think of probabilities as sad and I think of one unit of sand one bucket of sand right and each of these situations you need to pour sand there based on how much you believe that is true so out of these five possible scenarios you have one bucket of sand is there any reason in the beginning without seeing data for you to put more sand in one place or than the other probably not right so that is a non-informative prior you put equal amounts of sand in one place than the other? Probably not, right? So that is a non informative prior. You put equal amounts of sand in each of the four, four or five possibilities. You are betting equally there on all five. Does that make sense guys? But next morning what happens is, for good grief, it rained. The moment it rained, your desert is out of the window, right? It doesn't look like Death Valley anymore. Isn't it? So you will take that sand and put it somewhere. You will shift it to the other fold. Isn't it? Then what happens? Then the subsequent, the second day, it turns out the first day it rained. The second day it turns out it did not rain rain that screws up the other end of the spectrum also it is not the rainiest place in the world you see that right and so you take the sand out of there also and you take it out and so what happens sand gradually keeps moving out of what is not true or not likely to be true to where it is data is doing what forcing you to move the sand out of out of the unlikely things and move it to more likely things you know things that are more likely to produce that evidence that oh goodness one day it rained one day it didn't rain and then two days afterwards it again rained right and so what it will do you have three days of rain one day of no rain and gradually things are now edging more towards a rather rainy place but not necessarily the rainiest place and more and more data keeps coming and what will happen is you your sand hill will become more precise it will become sharper higher am i making sense guys yeah right another way to think is suppose you have a very strong belief you saw the airplane heading west and so in the beginning you said you know what i'm not going to distribute it equally on california i'm going to place a bigger bet right and then uh in seattle in and rainy and this place and this place like this, different amounts of sand you did. Or maybe you happen to look outside the airplane window and you saw you were heading towards the west, right? Something like this. Now what happens? California, it rains only 15% of the time, but your evidence seems to show you that it's 75% of the time it's raining so far. So what happens? You begin to now move towards seattle your belief begins to drift towards seattle right so this sand hill begins to get smaller right and this one begins to rise up and this of course all this sand moves this way and all the sand moves this way you see how it goes so what it means is that with enough data, with sufficiently large amount of data, you will hit upon the right answer. Doesn't matter where you start from, which prior belief you start from. Asif, the confusion was the wording posterior likelihood and the prior. Can you rephrase it in this context? Yes. So a prior belief is that which you believe before morning happened and you saw whether it rained or not. It's your belief of where you are. Are you in Death Valley? Are you in Arizona, California, Seattle, or the rainiest place, one of the five places. So what is your belief? How strongly you believe? So you have a unit belief, right? You need to divide one bucket of sand across these five scenarios. How would you break it up? And it's completely up to you. You may equally divide it, or you may believe, you may have a hunch that, you know what, it is california uh frankly most people who go into it say they would love to come to california isn't it so well okay there you go so it doesn't matter you distribute your belief whichever way you like you put your spread out your sand whichever way you like that is your prior Then next day it rained. Likelihood is the probability of this hypothesis producing this data, this event. So if your hypothesis is pure desert, what is the probability that it would have produced rain? Balaji, what would be the probability that you are seeing rain if you are in a desert? Zero. Now what just happened? So likelihood is the probability of the evidence being there given the value of the pariah right so that immediately you say the likelihood is zero so the posterior is proportional to likelihood times prior so that one thing goes out of the window because what initially you started with let's say equal prior one-fifth one-fifth times zero is zero so your posterior belief that you're in the desert has immediately died is it possible slowly becomes sharp and sharp. Yes, exactly. And in fact, you know, Bayesian learning is the heart of science. The way we see it is that it doesn't really matter what your prior is, what your initial belief is. For example, you may say, you know what, the sun is going around the earth right and let's let's be even more particular all the planets are going around earth not just the sun right but then you see some evidence so long as you're willing to adjust between possibilities there is a there is an alternate hypothesis that the sun that the earth goes around the Sun is there in your mind gradually data will do what it will start shifting the evidence will start shifting and favoring the hypothesis that says that the earth goes around the Sun isn't it do you see that right and so soon you will come to the conclusion that all the planets are going around the Sun. So long as you are willing to admit that as a possible hypothesis, it is there within your horizon of possibilities. Asif sir? Yes. I found this biocene to get the understanding intuition easier when combined it with the concept of entropy so i think it is little difficult to uh for every for everyone it will be a little different how they get the intuition but uh initial understanding it's a it takes time to build that's it no you said that you found it easier to get it. With the concept of entropy. How would you argue with that? Would you like to explain to people? That will be fun. I mean, the basic understanding is that when we know this some information, then entropy basically reduces the posterior entropy. From that, we have some evidence. We will always have more order, less entropy. I a lesson that's one way of looking at it it has more it is more information rich than the prior yeah and you have explained it i think a few weeks back uh it get by cn the gets it you know that's right I remember. Somebody else was asking a question. Asif, so this prior belief and we move towards the posterior or the posterior gets better and better. The data that we use, is it that the subset of the data gets used when the, in the iterations that we move or is it the entire data set? Sanjay, you're missing the point. Remember you have taken the data, you have a training, remember that you have data into three parts. Let's go back to data. So I guess this gets confusing. By the way, we are still not done with the, the whole theory has some very interesting things, surrogates and whatnot. We'll come to that, but let get this so you have data training data validation data and this is the test data test data you take and hide under the pillow yeah you don't get to see it now what happens is you build a model m with a hyper parameter value h right now what will happen for this for each value of h the model will produce a certain value of the loss yeah right the best loss on the validation data set it will predict. So you keep the validation data the same, but now you're generating new data for one H1, L1, H2, L2. Do you notice that? You're creating new hyper parameter search data. You're doing this experiment again and again, these trials, and for each value of the hyperparameter, you're finding a certain value of the loss, validation loss. Sanjay, are you getting that? Yeah, I guess my question was on the input data. No, input data, you don't care. Whatever training data you started out with you freeze it you use that again and again you you when you're when you're going through these iterations right you're using the entire input data again and again no okay look at this picture you're not you're removing the test data out i got this if you just mean this yes yeah yeah you're using that again and again the same you're not wearing the data in this process right and so the question is guys see what happens is that this the loss the amount of the objective function think of it this way this loss or this uh the the target variable this is your y y is in reality some unknown function of x where x is the hyperparameter. Or let me be more particular, it is actually the x vector because there are multiple hyperparameters when you're searching for a point in the hyperparameter space. But let me keep it simple to one dimension. You know that somewhere in there and this point is has a problem you cannot it's not that easy because what if this function is like this and you need to find this point isn't it this is your y the minimum of the loss is somewhere here and in the height hyperparameter space, x is equal to the hyperparameter space. Right. So how do you systematically find that? That is the problem we are solving. That is the problem of model tuning. Now, oh goodness, we're getting close to 10. So how we do that is actually very interesting. I unfortunately lost an hour, otherwise we would have finished it. We need some time to finish it. Maybe in the lab time, I'll add some. Do you guys have another few minutes? Can I take another half an hour? Yeah. Okay. So the way you do that is, see this is unknown, right? And it's a function. It may take whatever value it does take. You, first of all, you make it into a minimization problem, right? So suppose you want to find the most accurate model. You wouldn't take, you want more high accuracy then you take the negative of accuracy so you try to minimize the negative of accuracy so you always take a minimization problem so what you're seeing is that what is the art min value of x for the function fx right for the function fx, right? y is equal to fx. What is the best hyperparameter, so x star, the best hyperparameter for which this objective function, oh that's what I wanted to do, this x star is equal to this. This is the problem that we are solving, but how do we solve that? That is where some will now, I've given you the physical intuition that we will be using Bayesian optimization to do it. Now, how do we do it? So there are many ways that people have developed. I will tell you the very popular and very good way. And it uses something called a tree structured parsn estimator pretty good word big word trees parsn it is typically written as tpe tree structured parsn estimate sometimes people use this you could use whatever you want and a gaussian processes are also very common now i won't talk about gaussian processes because i haven't taught you Gaussian processes yet. It's a very biased thing to do. Even in the engineering math, I didn't cover Gaussian processes. It's a little bit interesting, but you never seem to have enough time to get around to it. So we'll cover the PPE, the tree structured parser estimator, or just tree parser estimator. But before I go there, so let me give you the steps. First thing to realize is, you don't know why is the effects unknown. And you cannot analytically find it, why? Remember, you can't do gradients. Some of the parameters you can't do the gradient of, especially if they're categorically at last, you can't do gradients and so the parameters you can't do the gradient of especially if they're categorically are lost you can't do gradients and so forth so we need to do this so what you do instead is you create a surrogate function surrogate so gate right and function which answers a different question you you come up with a distribution of Y given X which value of the hyper parameter makes at that moment what is that what do you think Y will be you know high value of Y I mean sort of optimal value of Y would be at which place given this x what is the likelihood or the probability so let me put it more precisely probability that at hyper parameter hyper parameter parameter x you will see a score value y specific value y for y will take on that particular value right y could be the loss for example mean squared error that you would see 100 as a mean square data so this is a probability distribution now the question is given this probability distribution what you do is you come up with something so actually we are running out of time it's better not to rush to it guys is it okay if i stop here because this surrogate functions and uh and and the things that we're going to talk about this tree the tree parsim estimator they're a big topic we need to give it an hour or so to do it properly i think it's better we do it next time and i use this time to review what we have talked about so far would that be okay guys i think that's better i said because some of us yeah so let's do that let's see what we learned so far and i guys i apologize for the abbreviated time so i said that the traditional the the foot so let us see in the order of goodness most people don't do cross validation search they apply a machine learning algorithm scikit-learn or something, just out of the box, right? And even for which algorithm to use randomly, they'll pick one. That is as bad as it can be, right? Because evidence shows that sometimes proper hyper parameter tuning or choosing the right algorithm can give you orders of magnitude better performance, huge performance. So you should do that. Now, when you have to tune the hyperparameters, you identify the hyperparameter, the old method used to be, and by the way, you'll still see it in interview questions and books and so on and so forth. Grid search. Grid search has been sort of the staple diet in the industry for many years to do hyperparameter tuning. Grid search has a limitation, which I mentioned to you, many limitations. First of all, it doesn't efficiently search the space, hyperparameter space. Secondly, if there is a useless hyperparameter to which the objective function is not sensitive to loss, is not sensitive to, then basically you do 25 experiments but only five are valuable. So not very useful. The other problem with this is of course the combinatorial explosion. Now the combinatorial explosion is terrible. Even if you just pick 10 values for 10 parameters to do that, you're looking at 10 billion experiments, which is not really feasible. Right? So what do you do then? And people have been doing that. So what people do is they are more parsimonious. They keep hoping that they will tune only two, three parameters and they take five values of that even then they're doing 125 experiments and say five times five times five right so it gets slow the second a little bit better is you use you suppose your budget is to do 25 experiments instead of picking them on a grid, pick them randomly. It's a more efficient utilization of the search space, especially if some hyperparameters don't matter at all. You're more likely to get to the close, in the neighborhood of the right, where the objective function achieves its optimal value, you'll get closer to that more likely because you have more points along this every hypothesis or every hyper parameter axis then we learned about the prior belief a prior belief could be loss is likely you see you know prior belief is where do you think is the optimal value right put a probability value all of it coming to one probability distribution means Put a probability value, all of it coming to one. Productive distribution means you have one bucket of sand. You can spread it everywhere, but it must all add up to one bucket of sand that you're spreading over the features, over the hypothesis space, right? Putting it in a very intuitive manner. That's it. Exactly one bucket of sand, no more, no less. You need to spread it over so that the amount of is some probability density and all those densities then add up to one. So that's your probability distribution. But you can start with prior belief. You may say, I don't know, I'll spread the sand equally over the whole hypothesis space, over the whole hyperparameter space. If it is like, you know, we took this example, that discrete example of rain, you see, I don't know which of the five places I've landed in. Or you could have a hunch. It is perfectly okay to bring in hunch. It's just being subjective is actually good in this space of Aishan learning. It means that you brought some prior which is informative. Something tells you that, or your past experience tells you that you need to pour more sand on one of the scenarios than on the other, right? Or you need to pour more sand on certain regions of the hyperparameter space, which you believe is more likely to be fruitful and have the best answer, then you put another. And then you do the experiments. You do this, you train the model again and again and again. And at each time, you still make your table of X and Y, you know, given a hypothesis, how much is the loss? You do that. But the beautiful thing is that for every after every experiment you learn from it right and you keep on learning so you learn from every data point that happens and you come up with a better posterior means you move the sand around right to uh agree with the to make the evidence more likely and then what happens is suppose you have five points you have tested out and your likelihood is whatever it is loss function is whatever it is you look at the likelihood what probability distribution could have produced that likelihood it becomes sharper and sharper and your probability distribution function will start or the surrogate function it will start accumulating at the right place near the optimal value of the hyperparameter. So to do so is the Bayesian optimization using Bayesian hyperparameter optimization. So next time guys, what we'll do is we'll do a lab where you'll see these things in a very real way. But when you see this, unfortunately those libraries have become good. They become a few liners and you don't see what is happening under the covers. You just see the power of them working and doing it. Now, but then we will take out some time aside to finish the theory of this, especially at least one of them. We won't do Boston processes, but I would like to do at least the three parsing estimator algorithm and explain it in detail it's worth learning it once at least but the but leaving the theory aside the big message these days is guys all models come with hyper parameters right and go shut your books that tell you to do random so i mean grid search they are hopeless even the randomized search I mean grid search, they're hopeless. Even the randomized search, no, right? Always use bias and optimization because it is the most efficient means to finding the best hyperparameters, right? Now you may say that, you know, when you do this update of your posterior, you're doing computations. That computation cost doesn't matter. In grid search, you don't do anything. You just randomly go from i mean you just go from one point in the grid to another in randomized search you go to random points there's no computation but in this there is a computation the computation of finding the posterior right and belief posterior probability distribution or surrogate. And then choosing the next point to look into. But what happens is that that search of a posterior and then choosing a hyperparameter point based on that probability distribution or sampling from that probability distribution takes a couple of seconds. Whereas typical machine learning algorithms especially the hard ones, they can go on for otherwise hours and days. So that cost is nothing compared to the inefficiency of not using Bayesian optimization. So that's the big message here guys. So this is the concept of Bayesian optimization. So that's the big message here guys and so this is the concept of Bayesian optimization and Bayesian hyperparameter tuning. I would say once you do the lab make it a core part of your arsenal. Always do that. Now you may wonder that you see you if I don't know if you guys ever wondered you realize that I did not ever formally teach you grid search Raja Ayyanar?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro as a best practice you must always do it right and now i'm saying the reason partly is i was waiting for the day i would introduce you to biation optimization and more than that now i would like to exhort that once you learn the biation optimization always always keep it as a part of your definition of done or getting your modeling done to always use bias and optimization to find the best values of the hyper parameter just tuning the hyper parameter can take your accuracy from 10 percent to 80 percent sometimes it's like tremendous right is there is there any limitation um reason um on using bias in uh machine learning basically limitation is see compared to not using any hyper parameter tuning at all right biation is expensive but compared to grid search random say it's cheapest it's the cheapest way to find the solution the theory people find hard, right? Unless you have a bias in that, you are getting it because you have come to the math class. When I go into the TPE and so forth, and these algorithms, it gets a little bit even more involved in Gaussian processes and so forth. So it's more the scale factor more than that, I don't know, like I can't think of any limitation. Actually the tragedy is it's so poorly understood. Most books, at least the ones that I saw, the typical books, they don't have a chapter on a Bayesian optimization. Those of you who have taken different courses on neural networks before me, coming here did you encounter biotion optimization by should have a parameter tuning anyone that instead of randomized search grid search you should use a biation hyperparameter tuning did anyone if you go through that no i don't most books most people don't even know. It's like, you know, people are digging ditches when a big mighty river is flowing next to you. Does the size of data number of, the size of data make any difference, sir? The bigger the data, the harder the training, the more you should use Bayesian. Because now each cycle through the data takes a long time right one epoch is going to take a long time isn't it all the more reason that you limit the number of experiments you have to do to or trials you have to do to find the best hyper parameter see there's no reason i mean it's very hard pressed to find a reason why you wouldn't use this except for simplicity. Sometimes you're pretty sure hyperparameters don't matter. Pick any you like, right? If you're pretty sure of that, then of course don't bother. But if you're going to do hyperparameter search, for heaven's sake, by Asian often by Asian hyper parameter optimization so so far we have not used in any of the problems we solve lab lab right but now that I've introduced it now here after issue right so it's like ensemble remember before I introduced ensembles, it is like one algorithm. But once we did ensembles, I said that you should always create ensembles. Isn't it? It always outperforms. So it's like that. I'm introducing you to another concept which you should always use. And also, I mean, frankly, it will make you stand out because most people are, their learning is not that deep. They are not well-versed with all of these things. The more you use it, the better your results compared to others and the more you stand out. the more you stand up i i was uh i i was reading one article today and there it was for with the forecast they always use ensemble method oh of course thousands of models go into it all right thousands of completely different models go into it in one of the mathematics basically uh the bbc mathematics lecture they was talking about this oh that one that series you have to watch that series that is a really good nice i'll watch it i think you're beginning to love mathematics now pradeep a little bit nice all right guys i'll end this i'll definitely end recording we can continue