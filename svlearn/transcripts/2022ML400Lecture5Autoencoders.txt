 All right. I will refresh it at some point. I'll review this again in an extra session. So now observe something quite interesting. If you look at this point, so look at this. This and this. What about this point a p what happened to p a p did not get located it just got stretched by a factor let me call lambda 1 likewise this shrunk to this point. And this is also Q variation, Q prime, got shrunk to this. Actually, let me use a P prime and Q, Q prime. And let me just assume that on the other axis it is implicit isn't it guys so q shrunk by a factor of let me say a q went to a q prime by a factor of lambda 2 and p went to p prime by a factor of lambda 1 lambda 1 1 is greater than 1, right? So it got stretched and lambda 2 got shrunk. But do you notice that these two points, P and Q are unique and they're mirror symmetric parts. They're unique in the sense that they did not get rotated, right? It's just a very simple geometric fact. All other points, like look at A, it got rotated also besides being stretched. Right? So the direction in which this P and Q are, these are called the principal directions these are principal direction and what happens is that you say that if you take this matrix M and it is just a fact of linear algebra which I'll state without without any kind of a proof, is that if you write it as a vector v, which is made up of this vector, this direction, let me call it v1, and this direction, let me call it v2 vector directions. V1, V2, if you take this matrix, which is made up of these two directions, times a diagonal matrix times V transpose. So this is a bit of linear algebra, which you don't need to, if you don't want to remember, doesn't matter. But it's quite magical, actually, that it happens. That if you look at the V1 vector v2 vector put it next to that you get this matrix and d is the diagonal which is just made up of lambda 1 lambda 2 0 0 very magically every matrix can be decomposed just like a prime any number can be decomposed in its prime factors right so 21 you say 7 times 3, right? In the same way, a matrix like this can be decomposed into V made up of so-called eigenvectors. These are called eigenvectors. Eigenvectors, E-I-G-E-N, eigenvectors. And these are called eigenvalues. and eigenvectors and these are called eigenvalues and it's a beautiful fact of linear algebra that it happens and so what the matrix m will do you can figure out by just finding out the eigenvectors and the eigenvalues you know that it will turn a circle into, it will squish it and prolong it into an ellipse whose principal directions are the eigenvectors given by the eigenvectors. And the amount that it will be stretched in those directions is given by the eigenvalues. Now, what has that to do with anything? Well, it turns out that if you look at the data, remember I said, it turns out that if you look at the data remember i said it turns out that this data is you can shrink wrap in an ellipsoid so you can ask which what m what m right transform this a unit circle to this unit circle to this. Unit circle to this. Right? There must be an M. And once you figure that out, right, you can find the Y2. And that M happens to be, believe it or not, the covariance matrix. it or not, the covariance matrix, if you take the covariance x1, x1, which is the sigma standard deviation or the sigma squared, sigma squared x1, sigma squared x22 and then covariance of or it is called sigma xy sigma x1 x2 covariance i'm not going to re-explain it is basically how much data varies along x1 and x2 so it is this matrix beautifully and so in other, the covariance of the data tells you what that matrix would be, whose decomposition would produce the shape. And therefore, what you can do is you can say that if lambda 1 is much, much greater than lambda 2, you can throw it away and you have dimensionality reduction because you can just look at the line. You can assume that all the data falls along the first principal axis. So you can throw away some of the lambdas, some of the directions, or eigenvectors, and you get dimensionality reduction. Now, what I did is obviously a pretty brisk-paced recap of what eigenvalue decomposition and principal component analysis is. So that will help us solve, going back to our situation, where are we? That will help us in situation number A, isn't it? We can use, when data looks like A, we can solve using principal component analysis, right? But what if data looks like c how do you dimensionality reduce you know that there is a lower dimensional surface a line a green curve on which data sits but how do you represent it with a single dimension how do you discover that right you need some non-linear deformation to get there and that is where neural networks come in the moment i use reformulate it as a non-linear deformation immediately your mind should like that you should start thinking activation functions isn't it so let's do that suppose i have data. Let's say that you have data x1 to xn. So it is represented by all of these neurons. Suppose I do this. So imagine that I want to represent it in two dimensions for the time being. Two dimensions because we can visualize two dimensions so i can do my fully connected network the usual fully connected network this this this this this this right now suppose all i have is now you know that these things will have a weights, W, I, J, W, I, J, right? This particular layer, for reasons that will become apparent, we call it the hidden layer. You project from the input layer to this particular or latent layer, right? Somehow, suppose you do this. Now, you say that is very easy. I can take a matrix, the x input vector, and multiply it by a w vector, x, and I multiply it by w, whose shape would be n cross 2 right if i take a weight matrix n cross 2 then what will it do it will convert it into a two dimension two dimensional output right that is what i'm doing all of these weights you would agree how many weights do I have? I have a weight for n weights, W1, 1, W1, 2, W1, 3, all the way to W1n, and then W2, 1, W2, 2, all to W2n. This is the set of all the weights, isn't it? One set of weights associated with the first neuron, another set of weights associated with the second output neuron, right? So if you multiply this and this representation, the output here, let me just call it Z1, Z2. And so you write it typically in the literature, you write it as a z1 z2 vector right so what you have taken is you have taken a x vector and passed it through a box of weights and you have produced a z vector right now w and you can even add a bias term you remember that we tend to add a bias term. You remember that we tend to add a bias term, bias B1, B2. Now, what is this? Is this a linear transform? Is it an affine transform? Or is it a deformation so far? It's just a linear transform, right? Now the whole question is, how do we relate it to what we have been doing? So this part is, for reasons that will become apparent to you, is called the encoder. Now, what do you do? You say, all right, let me take this structure. So here you have n, here let's say you have 2. This is called the encoder. and i'm not writing the edges right and then you do the opposite of this you decode it you multiply it by a 2 cross n which is now which is if this is the weight matrix matrix w right then you do w transpose w transpose will be 2 cross n so it will again zoom out this to n neurons so pay attention you can do that you can take n multiplied by w to make it down to 2 right z and then z what happens is think about this if i take x and multiply it by w what do i get z i'm excluding the bias term forget the bias so i get z and then i take z and multiply z by w transpose and multiply z by w transpose. Right? This is, what is this? This is, let me call it x prime. x prime is z times w transpose, right? Which is the same as x times w, w transpose, x prime is. I reconstruct the values back right you say well as if i have one doubt shouldn't it be w inverse then only you can reconstruct it to x right no look at the shape of w w is yeah so what happens is in these kinds of things you don't want to inverse it but just look at the shape of w if you do w inverse you get identity you don't want to do the identity you deliberately don't want to do the identity transform because w w inverse if you do that will be equal to identity and so x prime will be exactly equal to x the original x isn't it yeah but we don't want to do that we deliberately you realize that if i multiply it by w transpose which is 2 cross n then i'm not taking the inverse the the weights are doing something to it magically x prime prime is X times WW transpose. Now, WW transpose is not an identity. It is making some change to X. And we are saying that the weight should be such that it could somehow be able to reconstruct the X prime should be of the same size or the same shape as X. All right? Yeah. of the same size or the same shape as x all right yeah how comes the question what possible use could this be right what could what would be the purpose be the purpose of this whole thing, this whole mischief. What could we be doing? It turns out that it has a lot of applications, beautiful applications. And in fact, what I just did, if you don't activate, you don't do any deformation, you actually have done, which is the principal component analysis actually becomes a special case of this business. How? Suppose we say that x prime, right, should be approximately the same as x for given w. And w shape you decided. It is n cross, let's let's say 2 then what have you done suppose this could work w w transpose and this is a two-dimensional matrix z1 z2 and this is x goes in x prime comes out and so what would be your loss function this is your output x prime is the output your loss you can say x minus x prime mean squared error so you know 1 over n over all i values i suppose you do this as a loss it is is called reconstruction loss. Reconstruction loss. You say, well, good grief. I want to recover. The weight should be adjusted in such a way that ww transpose, which is roughly speaking like a colloquial way of saying is that a weight squared right is such so in other words loss is equal to 1 over n so i mean i can forget about the 1 over n's and so forth why my arrays are not working okay hang on let me write it out in this x x prime is equal to 1 over n and if you see me omitting 1 over n's and sigmas, please forgive me. I'm trying to focus on the concept. So suppose xi went in minus what was this? It is xi w, w transpose square. Isn't it, guys? And I'm basically saying how do I make sure that this is practically the same or another way of saying it is 1 minus ww transpose is essentially xi right this is our loss how does this how does ww transpose look like an identity like basically do nothing right but if i can discover such a w what have i done for all the days so suppose i have a data set if i can do this w i have just found a way to create a low dimensional representation of x w, isn't it? This is essentially my dimensionality reduction. Are we guys together? That's all I'm doing. are we are we guys together that's all i'm doing because if i take a n cross two matrix and i can do this well i've done it to two dimensions let's generalize if i did it to if w was if shape of w is dimensionality of w is n cross m where where m is less than n, right? Then what have you done? If you could successfully find such a w, you have found a way to reduce the dimensionality of x down to m, right? The lower dimensional m representation. So you can say, well, this looks suspiciously like a principal component analysis. That's what it was doing. It was also a linear. Now look at W. W is a linear transform, isn't it? It's a linear transform. It's just a matrix. So we have found a matrix that will reduce this. And so the lesson here is that in the language of deep neural networks, effectively, that encoder part is nothing but principal component analysis. A principal component analysis is nothing but a linear encoding to a lower dimension. Isn't it? Because remember, we haven't applied a distortion function yet. Right? But that gives us a wonderful idea what is the idea what do we do when suppose we get the data like our case c case a case b is hopeless case a worked with just w itself you know that a simple w won't do a simple linear matrix multiplication won't do for getting to c but what will do can you make a guess that's right all we need to do is throw in a bit of deformation. And what will bring us the deformation? The activation function. And so we say, hey, this whole thing is beginning to now sink. This is beginning to look rather lovely because if I have to deal with case C, all I need to do is say Z is equal to x times activation of w right or sorry uh this is a x w just activated right and of course i forgot the bias term but who cares um x w do you see that if i just apply a distortion function my hope is that the line will get bent now into a shape and hopefully that shape is the shape of the data because and now through back propagation we can learn all that am i making sense guys so you have generalized what would be a linear transform, which would be principal component analysis to a more powerful way of finding non-linear lower dimensional surfaces to represent the data. Right? Are we together guys? Are we seeing the point? Please speak up those of you who are on remote. Speak up. is it clear yeah i'm saying that a was this look let me let us reconstruct the situation go back up let's go back up where where are we uh i went too far down look at this situation this case we can represent simply as z is equal to x dot w isn't it multiply x with this right so if you were to make a encoder layer what would you do input is x1 x2 output is just one z right this is your neural network and all you're doing is finding the weights w1 w2 so asif are you pointing at a or b i am pointing to a at this moment i'm trying to solve the a situation would you agree that this is what it is every point goes to a just a point z this is z is equal to wherever location on the line, isn't it? Am I making sense, guys? Your line is represented by one dimension, right? The only thing that matters is what is the distance from the origin, that is Z, of a given point, isn't it? Every point I can represent it as a distance from the origin along this line. Right. So every two dimensional point, like every point in the plane, which has X1, X2 coordinates, has a Z coordinate, which is the distance from the origin along the line. Are these two statements equivalent, guys? Yes. That's it. So in this particular case i can do this and now you generalize it to a higher dimension suppose the data was not in two dimensions but in n dimension and you wanted to reduce it to m dimensions you could do with a weight matrix a linear problem is solved but what about this problem how do we solve this but what about this problem how do we solve this you realize that just doing x dot w x dot w won't do x dot w will state produce some straight line isn't it you need to bend that line right so you need to bring in some levels of distortions the activation function and by now you must have realized that maybe just one layer of activation may not be enough. You may have to put more layers in between, right? So now we can generalize this and say, okay, in this encoder part, this is our encoder part, what we can have are multiple layers, right? what we can have are multiple layers, right? At each layer, there will be activation, sigma two, sigma three, activation to finally get the Z, the Z representation. Am I making sense, guys? And once again, the opposite of that, traditionally, you just invert this. You have exactly the same weights. If the weights here are W, then the weights here are w transpose, even in this, even though you are using activation functions, and you say that x goes in, x prime comes out, and you want to, you want x prime to be approximately x, right? And to do that, what is that neural network? What are the weights and biases that i need to have that will recapture this now this is all wonderful but let's take some practical examples and see what all things you can do activation functions are matrices what's that like patient functions are also matrices no no no activation functions are sigmoids. Remember, sigmoid, tanh, you know, the logistic function, logistic, tanh, etc. Or ReLU, ReLU family, any one of those things. But anything that will bring in a deformation, right? That's what it will do. So it is not, so the, see the thing is, most of neural network comes down to two things here. First is you need weights to, this will give you an affine transform. What will it do? It will rotate, it will do some rotation and stretching. Like multiply anything with any vector with the matrix it will rotate it will prolong stretch right or shrink or whatever but that's not enough because it looks like an ellipsoid you need to bend that ellipse ellipse shape you need to bend it somehow right and so for that you need an activation function and all of machine learning ultimately boils down i mean all of deep neural network, right? The core idea is this, between bending, nonlinear deformation, and the affine transform, you can basically shape it to anything you want. And there's the universal approximator theorem, right? That's pretty much all the game is. So you're saying, all right, suppose my data is along this curve, right? Along a curve. So how do I deform it into a curve, a lower dimensional representation, which is along the curve? Well, all right, you just do this. Now let's look at some practical examples. This is all very nice, but a lot of theory, but what are the things we can use it for so here are the things you can use it for one is literally you have data and you come up with a lower representation lower dimensional representations are always useful right so quite often if they are ridic like you know for example they are useless features they will get eliminated on their own right and if they are composites of features like principal component analysis kind of thing what matters is a combination of factors those latent representation will create that so one example that you do is that this book by the way this is chapter seven chapter seven of your book please review chapter seven of the book now this book is rather sparse in auto encoders it doesn't go too deep but we will go pretty deep with auto encoders and it will do some extra sessions over sundays etc because auto encoders are a very powerful neural architecture. We will do much more than your inside deep learning book test. So suppose you have the handwriting digits, handwriting digits database. It's a database of images, 28 pixels by 28 pixels, black and white, Black and white. Black and white. Was it? No, no, no, no, no, no. Chapter seven of Inside Deep Learning. Inside Deep Learning. Remember, we are not referring to ISLR at all in this course. Right? So suppose I have two. Now two can be represented by different people in different ways. Like this. What other ways can we write two? Like this. You can see that these are all twos, isn't it? But visually, they look different. They have different styles. So you would want to know, is there a canonical representation, a standard tool, that all of these things essentially sort of map to, more or less? It would be nice to find that, right? And so what you can do is, you take this vector, it is what, a 768 dimensional vector 28 by 28 no it seven no i'm getting it wrong it can't be it can't end in eight eight times it's 64 six carry and then uh 784 784 that's right 784. So it is 784 dimensional vector. This is a huge, huge, very long vector. And if we could reduce it down to, let's say two dimensions, two or four, you know, a small number of dimensions, even 10 dimension, wouldn't that be lovely to get a representation in that? You can do that and put lots of layers in between. And this is an exercise for you. Play around with it. And then you try to reconstruct the same image. Now, what have you done? Suppose you could do it in two dimensions. You could look into that picture in two dimensions, how the two looks, and see, right, what point it maps to. And when you do that, you will notice that all the twos are together. All the threes are together all the uh they separate out they don't quite separate out very well but they separate out reasonably and it's quite interesting these things you should experiment and um we will do a lab on it and we'll see this in a very critical way what happens is some things are problematic four is problematic can you tell that this is a nine or a four it gets hard to tell right this is also four right but things like seven one right they are easy to tell eight is of course pretty easy two three right five so you can you can play around two, three, right? Five. So you can play around with it and create lower dimensional. What is the value of it? Suppose you're doing, I mean, you don't do it using this. Suppose you're transmitting data. Instead of sending a 784 dimensional vector, which will take a lot of space, what if you could send a Z vector, right? what if you could send a z vector? What if you could send a z vector, the latent representation of it, which is just two, three dimension? Can you imagine how much saving you have on bandwidth? So you have a much compressed representation. This is a form of data compression. Do you see that? You can achieve that. That is one use of it. There is actually another use of it suppose i give you the parts a is this b is now it's more as if uh the x dash is it in a lower dimension is it no it's in the original dimension x dim x prime is equal to dim x which is where is the dimensionality reduction happening z z z dimensionality of z is much less than dimensionality of x right input okay so this could be let's say a two-dimensional right for example this is 784 dimension dimension a image a basic digits image 28 pixel by 28 pixel is 784 dimensions and you're creating a z of just two dimensions for example do you see what is the point then of uh prime? Oh, but how would you discover the Z? The point is in machine learning, you need a loss function to optimize on. So you need to do this and do gradient descent on this. You need to have your weights W. W is argument. So it's basically, it is the argument of some loss function isn't it you will have to do argmin w in other words you will have to do the gradient descent right minus alpha gradient of the weight of the loss x x prime so unless you that is why this is called the reconstruction law see remember you cannot discover w especially with the activation function in between right unless you have a loss function but once you have learned it then you can throw away the decoder and keep only the encoder. Then you can just keep this. And this is your dimensionality reduction. Isn't it? X comes in, Z comes out. Do you see the point, guys? Right? At this point, we don't have any labels. There are no labels. You don't need labels. But now there's some very interesting things you can do so i just said that this is producing a lower dimension but there is more you can do with it if you keep both the parts here is what you can do suppose i give you let's say that i give you um person's face and which which was supposed to be like this right but what i do is i i can train my machine the network that if i block this hide one part of it can it reconstruct the hidden part from the data that it has learned from what if it could pass this data into your auto encoder and what came out was the whole picture in other words it has learned to fill in the blanks. You can train autoencoders can be trained to do that. But why is this useful? Imagine a picture in which some part of the images coffee on it. You spill coffee on it. And what it does is it knows how faces look like, so it can reconstruct the missing information. Wouldn't that be lovely? And you realize that this can do that. You can train it to do that, the autoencoder to do that. You could do other things. For example, suppose I take this picture and for whatever reason, the faces got mixed up. So what happened is the eyes came here and the nose somehow ended up here. Can we train this to look at this picture and say, hey, the parts are in the wrong place. The parts are in the wrong place. What it should be like is obviously, it could move the tiles around and reconstruct the face properly from the pieces that would that would be a good puzzle solver for you isn't it do you see that guys right so this too solve the puzzle and it's amazing that auto encoder if you really think a little bit about it you can train the autoencoder to do this. Right? Because you create a loss. How would you, so let's think through this. How would you train to do this? Fill in the blanks. Let's think through this. what you would do is you would take the input data and you would you would create x tilde mask some parts you mask some of the parts and then what you would do is you would pass it through the whole autoencoder to produce x prime but when you do the loss you don't do the loss loss with respect to x prime you still do the loss with respect to x and the original x and x prime this is what comes here even though x what you fed to the auto encoder is some parts hidden but you are forcing it to learn by looking at a loss with respect to the original so what will it learn to do it to learn to fill it in the right way do you see this guys isn't it isn't it beautiful that you can train an auto encoder to do this you see the value guys so you can use it to fill in the blanks you can also do the same thing you can take x and you can just you know jumble up the parts of x feed it in but the reconstruction loss you still use the original image right original signal to do the reconstruction another part that you can do with this is the so-called denoising and denoising autoencoder what if we did this we We took x, we added, add some noise to it. So it is like this. It goes to, and seven is sort of visible in there somewhere, slightly. But what you do is you pass it through this to produce X prime and you force it to produce X prime like this. X prime, right? This is your X tilde. Now, if you could train your autoencoder to do that what have you just done you have created a denoising autoencoder you have created a machine that can remove noise from data isn't it and isn't that lovely that you can do it because it is all coming back to the same fact that your loss function is against the original not against x tilde but against the original image original data right so you can denoise your thing. Now, there is more to it, like how do you denoise and what are the things you do? There's a bit of theory to it. And those theory, we will build up slowly as time permits. Now, I noticed that it is 8.30. We have been talking for an hour and a half. Let's take a 10 minute break 10 15 minutes break and then we'll continue but ponder over it guys and this is do you see how powerful this idea is what used to be essentially if you think about it this is a generalization of a principal component analysis isn't it you took multiplying by w which is a a fine transform and dimensionality reduction, just the magic of activation function remaps it to a deformed surface. And now there's so much magic that comes out of it. So much that you can do with this reconstruction loss function. This is often called self-supervised learning. Can you guess why it is called self-supervised learning can you guess why it is called self-supervised learning well the loss function is there we are doing gradient descent against the loss function which is like supervised learning except that the input and the output are exactly the same the label and the input are the same isn't it that's why auto encoders are called the building auto encoders is called self-supervised learning so ponder over it guys before we take a, remember it is nothing but a fine transformation and sigma. But the main thing is you created a bottleneck. Right. And so I'll leave you with a puzzle. What would happen if you did not create a bottleneck? If your hidden layer was the same dimensionality as your input and output then what would happen would would that lead to something good uh i leave you with that thought and we'll meet in 10 15 minutes right suppose w is n cross 3 then what will be the dimensionality of w transpose what will it be a three cross n three cross n now think about it suppose i have a x one x vector belonging to rn i take x and multiplied by w what do i get um n cross sorry n cross n into n cross 3 then it's n cross 3 no it if i take n cross uh if i take n times n cross 3 what like 1 cross n. 1 cross 3 then. Yes. So you will end up with 1 cross 3, basically. Yeah. So you get this. Right? Now, it will be z1, z2, z3. Now, tell me, if I multiply this z with w transpose what will i get 1 cross 3 times 3 cross n what will i get 1 cross 1 1 cross n which is what the shape of this is your x prime what is the shape of x prime the same as x yeah that is the point so you have ended up creating this shape in one you went from n n to 3 and here you went from 3 back to n okay sorry let me rephrase my question my question was even w inverse does the same right no the point is w inverse is useless no because every possible w would be the solution there is no unique w needed what we are deliberately trying to find is a a set of weights right such that ww transpose tends to one right with the transpose if you take ww inverse you realize that every w every w will work so there is no learning to do there is no unique w that you need to find because every w will solve your problem okay here we would be looking for w transpose where it is basically equal to w inverse correct that is right that is what you are trying to do you are saying find a w transfer that is effectively an inverse of the w and that will give you a unique w isn't it unique oh yeah that will give us and the point of learning is to find a unique solution but if you take the inverse you will never because every single w will work so you can't learn now the loss function will be zero so imagine what will be the loss function if i take x multiplied by w multiplied by w inverse what will i get this will disappear so x prime will be exactly equal to this so loss will be zero if loss is already zero there's nothing to learn right yeah that's the point so do we add um activation functions when we encode and decode as well that is activation function so now this was a simple case of a so this was the pca actually this is almost the same as pca almost the same the difference is there is a rotational element it turns out that it is pca up to a rotational factor but um with the moment you apply activation game becomes very interesting you take sigma x w x w and then you are saying this produce this produces a what sigma x w is your z and x prime is sigma of what is it z w transpose okay reconstruct that so we add the activation at the end of the um no both sides the activation is so this is a simple case of a single single layer with no other hidden layers but in reality what do you do kyle see think about what you do is at each layer this is the input right at each hidden layer let's say h1 h2 layer so let's layer all the way to the latent layer there'll be sigma sigma all over the place everywhere you'll be activating it right because each neuron w dot input right w plus b activated right okay each neuron is doing the activation. That is basic neural network. Every neuron does this activation. Are you getting that? But if you look at the end result, so let's say that I have one layer, then another layer, and finally this hidden layer. This will do its activation. This will do its activation. And finally the end result is you'll produce Z. OK. So in the end, we want the representation in Z to be as distinct as possible for different labels. Right. That is right. And one of the one of the things that you learn is. And anyway, let's have a break. I also need to drink some water. After that, I'll show you how it works. Let's start. So we'll do it all over again, guys, and I'll do it more slowly. See, if you take a matrix, W, n cross pick a number let's say two say right then what happens is you take a vector and or vector and or you take a vector which is x1 xn right you multiplied by w what do you get you get a z which belongs to so let's say x belongs to x belongs to n dimensional space you multiplied by a matrix like this so guys here's the thing let me make it very real suppose you have a matrix x1 x2 x x3 i multiplied by right this is your vector. Let me just write this vector. Suppose you have this vector, and I multiply it by x1 times, actually, take this transpose. I keep forgetting the transpose here. So you take this, so it becomes like this. Hang on, let me put it, I shouldn't write it like this because this is amazing. Let me write it like this. When you take the vector x1, x2, x3, right, which is a 3 cross 1 vector, you take its transpose, x transposes, it's transpose. X transposes. And I keep forgetting to transpose the X. Just assume that I do that. X1, X2, X3. This is your weight. Now multiply it by some number. Something like W345. right what do you get and i multiplied by three and then one two i don't know six something like this what do you get guys from this what will you get you'll get a vector of what will you get you'll get 3x1 plus 4x2 plus 5x3 and the second one you will get is, you will get x1 plus 2x2 plus 3 or plus 6x3. Isn't it? This is how many dimensions? This is z1, z2. This is basically... 1 cross 2. Yeah. This is basically a vector of two dimensions, isn't it? Because I can represent it as a point in two dimensions space, z1, z2, right? So you have taken a point which was there in three dimensions. X belonged to three dimensions. And you have using this W, you have mapped it to a Z, which belongs to how many dimensions? Two dimensions. Do you see? Right? Now this is it. So this is the point of a linear transform into a dimensionality reduction. If you can find such a z, now the thing is, how do I find a good w so that information is preserved? What you can do is you can say, well, you know what? I will take z, which belongs to R2, and multiply it by w transpose. What will I get? I will get an X prime. Now let's see what will the transpose do. So suppose I have Z1, Z2 here. Again, it's actually the transpose of Z, I'll write it as Z. And I multiplied by transpose of this vector. What is the transpose of this? transpose of this, guys. It is 3, 4, 5, 1, 2, 6, isn't it? W transpose. Do you agree, guys? Now, let's multiply it out. This will be Z1, Z1, 3Z1 plus Z2, right? The first one. Then it will be 4z1 plus 2z2. Then the third one will be 5z1 plus 6z2. What is the dimensionality of this vector guys x prime belongs to it's a three-dimensional vector it has three components isn't it guys do you see that if i multiply this with this i get a three-dimensional vector so what has happened i have taken an x multiplied by w x x belongs to R3, to produce z belonging to R2. And I've taken z multiplied by w transpose belonging to R2 to get x prime belonging to R3. Now you say, well, that's a pretty exercise, but what am I learning? Now you put a constraint. You say, well, that's a pretty exercise, but what am I learning? Now you put a constraint. You say, let the loss function be x minus x prime. x should basically be x prime, the same. I want to get back my x. If I want to get back my x, the difference between x and x prime, so 1 over n and sigma and i. So those things are assumed. But basically, x and x prime must look alike. If I put this as my loss function, and I minimize this loss, what am I doing? Let's think through what am I doing. I am basically saying that 1 over n i xi minus xi trans times w times w transpose squared right i'm trying to minimize this loss minimize this loss isn't it so what will it do it will force me to find a unique w this will This will be w, such that, so this I can write it as xi identity minus w, w transpose, right? Square, right? xi, xi square basically this thing square but such that l is minimized right you find a unique w such that w w transpose looks like one right approximately or is like one do you agree so far so good guys kyle are you following me so far yes right that is it so this is it turns out that in the language that we are using this is your principal component analysis effectively up to up to a small modification. But let's say, logically, this is your principle, because you did only linear transforms. W does what? So suppose I look at z belonging to R2, x belonging to R3. If I can solve this unique W, W, then XW is a dimensionality. Z is equal to XW is a dimensionality reduction, isn't it? And by the way, I'm screwing up my X transpose W, et cetera, et cetera. It is a dimensionality reduction. Would you agree? Because I went from three dimensions to two dimensions. Kyle? Yes. And why did I have to do the decoder part? Simply because I needed to create a loss function that I can minimize. But ultimately, I've come up with a dimensionality reduction. Now, this is a linear thing this is essentially equivalent to to pca principal components analysis so another way that we can say is so we can say say that PCA is the decoder, the encoder part of the encoder part, this part of the autoencoder encoder without activation. Remember, we did not apply any activation. Or another way to say that the activation is the identity function. It doesn't do anything. Right? Would you agree? All of you? Guys, stop me if this is not clear. Stop me. I want to go slowly and I'll keep repeating till we all get it. R. Vijay Mohanaraman, Ph.D.: So so far, so good right. R. Vijay Mohanaraman, Ph.D.: Now, the. R. Vijay Mohanaraman, Ph.D.: We take the internet covariance matrix and then. R. Vijay Mohanaraman, Ph.D.: We choose choose the first few of the eigenvectors right the first three eigenvectors that explain which is what is that how does it relate to w so the idea is that ultimately once you find the eigenvectors and the x can be represented as phi 1 eigenvector 1 plus phi 2 eigenvector v2, right? So this phi matrix, right? So there is a transformation from x to this, isn't it? Something converts when you go to your ellipsoid and you have v1 direction, v2 to direction and every point X, Y. R. Vijay Mohanaraman, Ph.D.: Every point let's say. R. Vijay Mohanaraman, Ph.D.: X one X two goes to what. R. Vijay Mohanaraman, Ph.D.: It gets projected into a new vector space made up of the B1 B2 axis. vector space made up of the v1 v2 axis isn't it right and if i go v2 let's say for example or even if i don't throw what is it there is some matrix m that rotates it into a point that can be in the v1 v2 space v1 v2 direction v1 v2 are just a new coordinate system isn't it it's just a rotated coordinate system would you agree yes and so there's a matrix that takes this and does this this right and that is your weight matrix okay right in this new language that's what it is right and those are all linear transformations of the data okay that is not so easy to find the same like when when it was linear we could just use the covariance matrix but at this point since it's there are non-linearities we cannot use the covariance no i did not even do the activation i'm saying activation is just unit function right okay so it is related to the covariance matrix these weights are related to covariance matrix right okay eigenvalue decomposition but there's a little bit of math connecting the dots i'm deliberately not there because already people uh you know you guys are having okay relating to it let's not get into the math. But basically, the two are equivalent. I have reformulated it into a more intuitive language. This is far more intuitive. X vector goes in, Z vector comes out, isn't it? So simple, is find the best the the best w such that x x prime you minimize this right which is the same as saying ww transpose is almost like one and when you think about it ww transpose is a what it's a square matrix isn't it symmetric symmetric Symmetric. Symmetric, yeah. And you're basically trying to make it look diagonal to the first approximation. So we leave this for that. But so far, are you realizing that we have done a linear reduction of the data to a lower dimensional space with a w matrix to make it non-linear all we need to do is activate this if i active apply an activation function what will it do okay it will still so for example i let's look at this game i had a three-dimensional vector x1 x2 x3 multiplied by a weight matrix this weight was again 3 cross 2 what did i get i got a z1 z2 right this is what i got isn't it guys now suppose I activate this activation is just applying activation to this it will still remain a two-dimensional isn't it do you agree guys if you activate X dot W which size is yes right two-dimensional activation is not going to change the dimensionality of it it is just going to deform or create a non-linear deformation to it right so if you if the curve the data is like this and you can't fit it to a straight line all you need to do is you need a deformation from here to there, right? Or a chain of deformations, not just one. So you need many hidden layers in between that will create you the deformation, right? That is all. That is what an autoencoder is. Now, the encoder part does the dimensionality reduction. Theoder part you need for example to do the reconstructive loss the loss between x and x prime to minimize so that you can discover the w's w transform so you can discover the w we don't do when we in the decoder part we don't do like sigma inverse or something of course no no you always activate there is no such thing as sigma inverse you activate you do z you keep on activating each layer forward remember in this is what your feed forward network at each layer you activate you never worse activate you always activate so if it is relu at each layer you keep on applying your relu's okay it's as simple as that amrit is it clear to you yeah yeah i'm following them nice Yeah, yeah, I'm following on. Nice. One question I had actually is if you scroll a bit up. Tell me when to stop. Right there, right there. Could you once again go over this W times W transpose when you're minimizing a loss function yes how did you like if you look at the loss function from right above where you just yeah look at this how'd you get to that the loss function this is your standard msc loss by the way one more thing i glossed over just for simplicity typically you regularize it a little bit and i'll talk to you about it in a moment uh there's no regularization term whenever i talk of msc loss i forget the regularization term but of course you know they're regular but okay so from here how did i come here yeah yeah okay so it is quite straightforward let's look at this what is from z is equal to x dot w is it right and x prime is what is it z dot w transpose you agree yeah and so if you if you plug in your z here x prime is x w w transpose so isn't w times w transpose going to be one no no no why is that it says well let's take a matrix let me take some w 2 1 4 6 let's try yeah it's not the inverse why would it be equal to so let's actually make sure that it's not two times two is four five isn't it two times four is eight right eight plus 6 is 14, right? Do you see that it's not working out to 1? Yeah. 6 is 14. And 4 times 4 is 16 times 36, right? 46, 52. This is not equal to 1. Not equal to 1, right? Not equal to 1. So, ww transpose is never equal to one not equal to one right not equal to so ww transpose is never equal to one ww inverse would be equal to one by definition yeah definition of inverse isn't it and so if your loss function is the msc loss x prime x x prime x minus x prime squared right then you realize that you're doing x minus x w w transpose squared isn't it up to all of this half one over n and all of that and and the summation now does this thing make sense yeah good so, there's something here. What I did was pretty basic matrix multiplication. But it also is true that most of us have forgotten our matrices. Right. So while the matrix movie was fun, doing matrices, reviewing matrices is even more fun because much of neural networks is nothing but linear algebra, matrix multiplication followed by activation. That's all it is, right? And a loss function that you minimize too. I mean, I'm caricaturing it, I'm oversimplifying it, right? But you realize that you take the input, you multiply it by some matrix and you activate it. You get the output of a layer. Then that output goes into the next layer. Once again, it gets multiplied by some other matrix. It gets activated. We get the results from the second layer and it keeps going forward. That's your neural networks. The at least the feed forward networks. Does it make sense guys yeah so anybody else manish are you following so far manish jinn i'm following it yeah yeah generally following it yeah yeah generally engine is not responding but i'm hoping is following okay guys so this is it this is the auto encoder now comes an interesting question why did we create i'll ask you if anybody has a question someone hurry yeah i have one question as if so where do you build the model so on z after reduction you build the model and then you get back or you build the model on x but that's where i was confused completely okay uh let me answer this question see the purpose of this in this particular case is dimensional to come up with a look so what is the goal create a lower dimensional representation of x right so z is your goal yeah finding z is a goal right so good z for all x so you're going from x to z this is your goal and this is your fx yeah this is your goal and the way you achieve the goal is you you do the opposite also you this is your z hidden in there it is called the latent representation latent representation right and the way you do that is you do you go from x to z and then you undo that by doing x prime right and then you need a loss function to learn how to find z right because in machine learning you always need a loss function and so you can't get a loss function directly here how would you know the way to the auto encoder technology says that one way you can do it is reconstruction loss that is why it's called the reconstruction loss that is why it is called the reconstruction loss does that answer your question you minimize this minimize this reconstruction loss as if i got that part my question was okay let's say the c graph was there right using your dimensional rejection we get it to a so on a then we build the model is what i'm thinking that means in this case x was c z was a so on a you build the model then you do this decoder stuff and then you get the output back that's what i was thinking i want to know whether i was correct i was wrong yeah see what happens is that let me go back to your c that you are referring to um where am i uh yes let's look at. See what happens is using an activation function etc. C gets changed into a problem like A. You have solved the problem, because now in A, you can represent the two dimensional point, a point which has two dimensions x1 x2 to a point in a single dimension z z is simply the distance from the origin along that line isn't it in this case so you have achieved your goal that is it so here the goal is not to make a prediction model here the goal is just to come up with a good representation a lower dimensional representation that's all okay uh got it no no the goal was just converting that c to you got it that is it that is it that is one way to look at it right uh anyone else guys needs a more explanation for this so in a way b doesn't get solved right what doesn't get solved the b yeah it would be solved i was giving you that's a very good question you asked in that question see what i was trying to tell you is that solvable problems can be solved right so a is solvable because it's linear z the c is you know that there is a relationship between x1 and x2 it just happens to be non-linear right you can solve it but in real life you'll also encounter cases like b that is why it is called the null hypothesis right null hypothesis means there is genuinely no relationship between x1 and x2 and so you can never build a model to predict one from the other look at it for any value of x1 there are infinitely many values of x2 right right so this is a this you always have to know that sometimes you may try to train but the data rules the data is saying there is no relationship so don't try to build a model don't try to learn from this right relationship. So don't try to build a model. Don't try to learn from this. Right? So you will never be able to remove, reduce the dimensionality here because you cannot. The points are occupying the entire feature two dimensional space. Right? Do we generally use this for classification? Or is it even useful for regression? See why would you reduce the dimensionality is the question. You reduce the dimensionality because the purpose is when you reduce dimensionality, you don't look at it as that. You say you have found a deeper, the latent representation of lower dimension is somehow a deeper representation of reality. Okay. somehow a deeper representation of reality okay so let's take an example in medicine there are many genes for example in the human body right those genes turn on and off they get expressed or non-expressed for certain diseases right so suppose you say that the level of some activation, let's say that some problem or disease is a function of which genes got activated and expressed to what extent. You can do that, but then now you're talking of a genome that's huge. Tens of thousands of dimensions or hundreds of thousands, I don't know, as a biologist will correct me, I believe it's in the hundreds of thousands of dimensions or hundreds of thousands of I don't know, as a biologist will correct me, I believe it's in the hundreds of thousands of genes. Right? Right. Now, suppose you could do dimensionality reduction, and for that disease, you could tell that actually, if you look at this cohort of all these genes, what matters is a combination of this gene and that gene expressing. Right? What have you done some combination what you come up with a combination of some genes is one factor some other combination of genes is another factor right and then that simply don't matter but some one particular combination is one real factor another particular combination is another real factor so what has happened is you have actually discovered an underlying reality that these genes come together to sing their own song and these genes come together to sing their own song. And the whole problem that it sees is these two things expressed. And you just need to know how much of this is happening and how much of this is happening right so now you have a deeper reality and this happens all the time what people do in medical is they come up with some interpretation of what this combination means what this lower dimensional means z1 z2 what do they mean okay finding the principal components is the main idea component or it's non-linear uh encoders just wants a non-linear way of deform principle components in some sense okay right but those things the domain experts the medical people they will find some meaning to it and it will become a part of their knowledge base right right and they will start talking in terms of Z1, Z2. It will mean something. They'll give it some real domain specific names. That's what happens. Asif, just a theoretical question. Just the example that you gave about in the medical field, find a domain if there are certain genes that are causing it. Now, how do you determine what are all the factors that may be contributing to a certain disease? It's possible that, yeah, somebody may have come up with just a few genes that are being considered analysis, but how do you determine the other external factors some like the pool of factors that may be responsible for determining the output this is very good question so this brings us to the question of let us call it a classification problem what you want to do so suppose you have let's say 100 000 genes let us say that there are some uh 100,000 genes. Let us say that there are some, so suppose you are God, you're in God mode, you happen to know that there are four factors that are responsible. Each factor is a conspiracy of some specific genes, right? And obviously, in each of the factors, some genes may be contributing to all the four factors now the question is how would you know that what people do is that they take a healthy population and they take a sick population and they look at their gene expressions are we together and then they do a two-step process. They say, for example, what is the dimensionality reduction we can do, right? Such that, so suppose I have a Z1, Z2, Z4, right? That I can write a classifier with this and still be able to classify between healthy and sick. Using just these four factors. So in other words, what I'm saying is take this big genome, reduce it to Z1, Z2, Z3, Z4. And this is, by the way, I'm writing it as a triangle, but this triangle is almost like this huge reduction because you're going from 100,000 dimensions or 10,000 dimensions down to just some few dimensions. Now, and then you're writing a classifier that is able to successfully classify. What happens in reality to your question, Manish, is that many genes don't matter at all to that disease, right? Only some genes matter. So here's the thing. Sometimes, if you're lucky, only one gene is responsible for a disease, right? So one single gene mutation will lead to a disease. And you can quickly identify it between the sick and the not so sick. But most diseases like diabetes, etc. The genetic component is polygenetic. In other words, it's the conspiracy of many genes causing that to happen. And so what will happen is when you do dimensionality reduction, your lower dimensional representations, each of the factors will be a conspiracy of many genes. And when you discover those, it's quite a landmark thing, because now you have found out that small subset of genes that really is affecting you. It is called the load factor of the gene. Some genes will have a huge load factor in contributing to that fact, in contributing to this. Are we getting it? And it's a way of throwing away all the genes that you can safely ignore. Now that brings us to a very interesting breakthrough. If you're in this gene thing, you probably know about the Human Genome Project, right? That mapped the human into the genes. Now the big discovery the big thing that has come out of mit recently is the same product is that using all sorts of differential tests and if you read that you'll see a lot of data science sitting there they have figured out the function of every single gene now they know what every single gene does in the human body right isn't that amazing and that is going to lead to a whole new genre of medicines going forward right genetic medicines and see all of these things auto encode okay i'll give you a thing kyle could you stop the recording please