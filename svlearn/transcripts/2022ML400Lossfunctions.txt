 All right folks, yesterday evening, for those of you who attended the extra session, we talked about, we just reviewed the loss functions in machine learning. What it means, why it is relevant, and this is fundamentals. It is something that we are supposed to know, but occasionally a review always helps. So as I was doing this loss function stuff, I realized something. We did the, oh, hang on, let me find my writing pen, yeah. So we did the loss function for regression, which was straightforward, mean absolute error and mean squared error again i mentioned that the gauss-markov theorem the blue theorem says that the best linear unbiased estimator is the squared which is why it is much more common we talked about the trade-offs between the MAE and MSE, which is that MAE handles mean absolute error, handles outliers better, but it's sort of unstable because it only looks at the signature. It's the sign of the gradient, not the actual value of the gradient. So irrespective of whether the gradient is big or small, the size of the gradient descent step remains the same, which leads to instability because you keep hopping around the minima. MSE is better because it also looks at the size of the gradient, right? So, or the error. And so if the errors are small, then you take smaller steps. Then we talk about the Hubble laws, amongst other things, which is taking the best of both the worlds, saying that, you know, if the errors are small, look at the mean squared error. If the error is big, if you're looking at outliers, then revert to mean absolute error for those terms and then take them together. So Hubble loss is pretty common. Then there are other loss functions, Hinge and this and that, we won't go into. Now, then I talked about regularization. Once again, a review of things we did in ML100, the first course, and most of you coming to deep learning should have done that in some machine learning course if you haven't taken ML100. Namely, you have L1 regularization, L2 regularization, Lasso, Ridge, and then combination of the two, which is elastic net. So that's that. And that regularization is true whether it's classification or regression. So that's a very short intro to regularization. Today permitting we will do more on regularization now then we went to classification and there we talked about the the this we went a little bit theoretical or probability theory we talked about what is likelihood likelihood is a short form of likelihood that the evidence likelihood of a hypothesis producing the evidence but that's the full form of it so likelihood is not probability likelihood is the probability that a given hypothesis, if true, will produce the evidence or the data that you see. So what is the likelihood of the data given a specific hypothesis? And you pick that hypothesis which gives you the greatest likelihood of the present evidence. So for example, to trivialize it, let us say that you land in the middle of the night, you and your friend land in a city and you don't know which city it is. And let's say that you are quarantined because of COVID in a box and you have a little window you can look out for a few days. So both of you argue one says it must be it must be a desert, we must have landed in Phoenix. Another says no, no, no, we it must be Seattle, right or any place. Now, which of these two things is true. The next few days, you notice that it rains first day, second day, third day, not the fourth day fifth sixth seventh day not again so it rains let's say seven days out of ten right or eight days out of ten now which of the two hypothesis looks more likely are you in a desert or are you in a um are you in uh in seattle or are you in a rainy place? Now you can say either is possible, it is true. In desert also it rains, but so eight days of rain is possible in a desert too. The probability is much lower. So you can say in common sense language, it is far less likely that you are in Phoenix, the hypothesis that you are in Phoenix than the hypothesis that you are in Seattle if than the hypothesis that you are in Seattle, if you observe eight days of rain out of 10. Does that make sense guys, the intuition? And that is the intuition of a likelihood function. Likelihood is the likelihood of a hypothesis producing the evidence that you actually see. Evidence being eight days out of 10 it rained. So you would say, oh man, quite likely we are in Seattle. But it is still possible, you might be in Phoenix, but maximum likelihood is that you are in Seattle, right? So that is what maximum likelihood means. Now, there's a little bit of a maximum, the probability stuff that these are joint probabilities. Now from the joint probabilities, we are joint probabilities. Now from the joint probabilities, we are seeing that hypothesis, which is likely to produce, most likely to produce this data is the correct hypothesis in terms of a probability distribution. Now that tends to get a little bit technical into probability theory. So, and from there, I mentioned the fact that when you take the product of these probabilities, computationally, it becomes a bit intractable. Why? Because all these probabilities are fractional. You multiply a hundred data sets, hundred points in the data set, and you multiply the joint. So you're multiplying a hundred numbers, each of them's between zero and one, and then you get a number that is so vanishingly small, your computer will have a hard time dealing with it numerically. So what you do is, fortunately, you notice that you can take the log of it. Log will make it into an additive problem for multiplication. Why is log still safe to take? The technical reason is log functions are monotonically, monotonic functions, increasing functions of x, right? And therefore, it's legitimate to use log. So if you're saying I'm going to maximize the likelihood, it is as good as saying I can maximize the log likelihood. Because log likelihood, the computer can handle much better with this numerical libraries. But then you realize that in our machine learning world, most of the code is designed to minimize a function, not maximize it. So how do we convert a max likelihood to a minimization problem? You can say, well, if I'm, instead of maximizing a log likelihood, I can minimize the negative log likelihood. And therefore, the famous term negative log likelihood is there. And that is a derivation of the... And basically, out of that comes the log functions for binary classifier and multiclass classifier with minor variations. I also talked about the fact that as we go in, and just giving you a little bit of a preview into the future that from example next week we'll encounter many more loss functions for example autoencoders they use a measure called KL divergence right how much one distribution your probability reality differs from your guesses measured by KL divergence between the property between the reality distribution and the guess distribution. We'll talk more about it. But yesterday, and throughout this thing, I had this uncomfortable feeling when I went back home, and I was having dinner, and I felt that maybe this was all way too abstract. The regression problem is straightforward. We all understand the absolute error or error squared being added up. But for the classifier, all of this business of likelihood functions and negative log likelihood is getting a little bit, or perhaps a bit too abstract to reason through. So I thought about it and I asked, can I explain it in a simpler way? And so today, if you will permit, I'll take the first few minutes to explaining the loss function for deep neural networks and for all classifiers in a simpler way, in a more intuitive way. I'll just look at the results and try to give you an intuition about it. Would that help? Yes. Okay. It's a fun idea. Okay. So what I'll do is I'll create a, this is the thing we have. See, I will create, so let me put the words on the table. I'll say, what is probability? What is surprise, what relative entropy. That is scale, divergence, etc. These are the terms. So it turns out that this is actually a beautiful intellectual journey. Let's go through it. So how should we do it? Let us say that you live in a place you live in a place which has no rainfall at all ever. Like you live in some Sahara desert. Well, please, anyone of you geographically enlightened, is it true that some parts of Sahara never gets rain? Maybe it gets, I don't know. But given my ignorance, I assume that it doesn't get rain. So let us say that you're in such a part of the desert. So what is the probability of rain out there? Probability of rain is zero, isn't it? And so probability of not not raining. Under one. Equal to one. So now what does probability really mean? So there are two definitions of probability actually. It turns out that this word probability that we all use so commonly, when you try to be precise, then you realize that in the people who think about probability day and night, there are two schools of thought. One school of thought says probability is the frequency for long running trials, means just keep on every single day, peep out of the window, is it raining or not? Keep a track of it. Then take sufficiently sufficiently long runs let's say a thousand days or whatever sufficiently large number of days and then look at the proportion of times it did rain and whatever that was that is the probability of rain right so the law of large number kicks in so for example or another way to put it is if you toss a coin it may be heads it may be tails you may even get a sequence of heads but in a long running trial with hundreds of uh coin tosses on average how many heads would you see with a fair coin about half of them right roughly maybe give or take a little bit of plus minus but the more trials you do let's say you do a million trials you'll start seeing that approximately half a million trials you'll start seeing that approximately half a million will be heads half a million will be tails so that is a frequentialist probability definition there is which is pretty good there is a problem with this definition. Suppose I were to ask you, what is the probability that the earth will get destroyed if we continue to do the human activities that we are doing, you know, have long factories burning coal and having driving gas guzzling cars and factories and so on and so forth. So how likely is it that it's going to destroy the earth? When you say you're asking still about probability, want to estimate so for example if the probability that all of our mischief is not going to make much of a difference to the earth it will it has a huge resilience and the probability is one in 10 billion or something like that right one in one in one trillion and then you would say no you know we need to have these factories to keep people well fed we need to put use these fertilizers the human population is huge so we need, we need to have these factories to keep people well fed. We need to use these fertilizers. The human population is huge. So we need to do all of these things that we are doing. And it is fairly justified. On the other, the probability that our human activity is imminently going to destroy the Earth, let's say that it is something like 90% probability. That should stop us dead in our tracks and say, good grief, we need to do something about it. Otherwise we will lose the only home we have. Do you see that? So the probability has very, the value of the probability has very real consequences, especially about policymaking at a large scale level what should we do as a humanity or where should we focus our interests between keeping between easily feeding people and transporting people over long distances and you know entertaining them with lots of toys on the one hand, as opposed to stopping dead in our tracks, taking some drastic measures and even austerity measures to fix the situation before it's too late. These are huge policy decisions. And one has to choose one versus the other based on what the probability value is. Except that one can argue if you were a frequentialist you would say now wait a minute where is the evidence that the probability that will destroy the earth is very very high right because in their thing if you look at the frequentialist perspective one could argue that the only evidence that you can have is have a sort of a world, hypothetical world, in which you start, you make, let's say, a million copies of the same Earth. And people continue to do exactly what they're doing. And see out of the million copies, then how many copies of the Earth's evolution over time did the Earth destroy itself? I mean, did we destroy the Earth or make it absolutely toxic for our habitation given the human activity? And suppose that number turns out to be out of the million Earth's copies of the Earth, let's say two or three times we destroy the Earth, we'll say, well, there's a risk. Maybe we need to do some mild action to prevent it. On the other hand, if it turns out that most of the time the earth gets destroyed in those simulations, in those realities, then you say, well, we need to take drastic action because that is the frequentious definition of probability, the proportion of the time that the earth got into trouble. Are you so far with me, guys? By this definition of probability, one would have to do something like that. Speak up, guys. I can't see you. So please speak up. Does this look reasonable? Simulations. Yeah. The trouble is, is simulations of course you can argue that's not data that is pretending based on some hypothesis so you need to make a million copies of the earth and actually see time evolution of it then only you can do it but but there's a problem with it the problem is we don't have a million copies of the earth, right? And there's no photocopier for earth with all its humanity and all its life forms in existence at this moment, right? So we can't do the frequency as probability. And yet it is of paramount importance. It is relevant to speak of given human activity, what is the probability that we'll be in trouble? Isn't it? So we need an alternate definition of probability. The alternate definition of probability is the probability definition that comes from another form of thinking called the Bayesian probability. probability. And it comes from an interesting reasoning. People said that suppose we want to quantify the degree of uncertainty, right? How much we know and how much we don't know. Then our degree of belief, you can say the opposite of that will be the degree of belief in something so how strongly should i believe purely from rational data that the earth will destroy itself if we do what we do we are not looking at a frequentialist perspective because we don't get unfortunately the choice of doing a of doing a 1 million time evolutions of the earth. Right? We have one earth and one arrow of time. And so there'll be one destiny that we'll all get to. And we need to have a certain degree of belief that it is either being harmless, whatever we are doing, or it is going to destroy the earth, one of these two, isn't it? So what is your degree of belief that we are heading into trouble? When you look at the degree of belief and you churn out the mathematics, and that mathematics, of course, as you know, we cover in this, that's at the heart of machine learning, actually, a bias shins. We haven't done the bias perspective, but increasingly we know that the bias is the right perspective. Machine learning is all bias in thinking in many ways. But even though I've not reformulated much of what we have been talking about, I've been trying to talk in simpler terms, but if you really think about it, you never know when you say that a picture is a cow or a duck. It's a belief. The model believes to a certain degree that it's a cow and to a certain other degree that it's a duck. You're dealing with beliefs. Those beliefs, they follow the same laws of probability. And as people notice, as mathematicians notice, that they follow the same rules of probability, which the frequentialists use. And so it comes up with that. If you really think about it, what your machine does, it sees one example of a cow, a picture of a cow, which is very different from all other pictures of a cow. Nonetheless, it predicts a probability. It couldn't have predicted the probability in a frequentialist sense because it's not getting gazillions of such experiments. It doesn't even make sense. But it comes out with a certain belief that it's a cow or a certain belief that it's a duck. So that is Bayesian probability. So keep that thing in mind. So that is Bayesian probability. So keep that thing in mind. Now, given that fact, we will talk of a new concept now, given the fact that we could talk about probability without necessarily doing long experiments, but this is an aside, by the way, I wanted to tell you that there are two different definitions of probability. But now I'll talk about something called surprise. So let us say that the probability of something happening is 100%, right? For example, what is guaranteed to happen? I think there's a quote in US that death and taxes are the only inevitabilities. Isn't it? So, okay, let's say that sun rises from the east. Every single day sun rises from the east. So what is your measure of probability? If the probability is one, P of the sun from east is equal to one, then tomorrow you see the sun rise from the east. How surprised are you? Not at all. Not at all. Surprise is zero. Not at all. Not at all. Surprise is zero. On the other hand, suppose all of a sudden, so suppose the probability of sun. So imagine a contrafactual world, sun from the east was 90 percent 90 percent then every morning when you wake up and you see the sun either in the east or the west you will be a little bit surprised isn't it if you see it in the east you would be less surprised if you see it in the west you would be a little bit more surprised is that correct yes 10 10 right so sun comes out in the east you say oh yeah it tends to come out of the east sun today comes out in the west you get more surprised or and it's related to history west you get more surprised or and it's related to history most of the time the sun is bright and shining and then you're not surprised but one fine day you wake up and you see that the sun is all red right because it's a solar eclipse let's say or something like that right what does it do to you? We are hugely surprised unless you have been listening to the news, or unless you are familiar with astronomy, but you can imagine a couple of hundred years ago, a few hundred years ago, when most people didn't have access to that knowledge, and much of this knowledge was not disseminated. Suddenly to find that the Sun is not what it's supposed to be, is an element of huge surprise, isn't it? And you know what it did? It caused most people to immediately start praying or sacrificing animals. If that didn't suffice, then taking a few of their fellow mates and sacrificing them and all sorts of things used to happen. Those are all expressions of surprise and fear. So when a rare event happens, you have far greater surprise. So you see that the higher the probability of something happening, the smaller the surprise, isn't it? And the rarer an event is likely to happen, when it happens happens you have a far higher degree of surprise so far so good guys right yeah so you could say to the first approach let's try to quantify surprise you could say hey you know what surprise seems to be inversely proportional to the probability of something happening. Does this make sense? Probability of an event. Right? Let's see if this works. When the probability is high, let's say the probability is 1. Let's say for p is equal to one or maybe 0.99. Nine, nine, nine, nine, nine. What will be the surprise if you use this equation? It will be forgettable. Let me just make it an equality here. It is equal to one over practically one, one, right? So your surprise is one, but actually you were not surprised at all, were you? You would somehow want it to be zero, but okay, zero would be better, better. We want an expression that makes it zero. On the other hand, if the surprise is very small, if on the other hand if the surprise is very small if for sorry if the event is very rare now what is the surprise surprise is equal to one over zero point zero zero zero zero zero zero i don't know how many zeros I put one. It is basically it tends to infinity, isn't it? You're infinitely surprised. All of a sudden, in the medieval ages, the sun doesn't look like the sun. It has sort of vanished and gone dark. Right? Are we together guys? Yes. Right. And we are enormously surprised. Well, there is a problem. This all looks nice. But wouldn't it be nice if this expression did not evaluate to one but zero? So how do you take an interval which goes from one to infinity and actually somehow transform it to zero to infinity. What could we do? Activation. No, no. All you have to do is take the log. Log of this, yeah. If you take the log of this interval, it will go for... What is the log of one? Zero. Log of infinity is infinity, right? So this is good. So we now learn that, okay, what we should do is, this is a good starting place to define surprise but it would be better if we define surprise as equal to log of one over p let me let me put it x x being whatever event that you're measuring, head, tails, whatever, sun, not sun, sun from east, sun from west, log. What do you say? This is lovely, isn't it? And you can use whatever base of log you take. This is a measure of surprise. Now, in different scientific communities, it is a tradition to take this log with different bases. Our physicists, they take the natural log which is log to the base e right computer scientists and information theorists. And this is just a convention. It doesn't matter which log you take. Theorists take log log to the base 2. Why? Because it's very computer friendly. For no other reason. Simply because doing logs with base 2 is computationally far cheaper. Are we together? Right, that is it. But remember that at the end of it, doesn't matter which base you take the log in, we come to an element of surprise, right? And now I'm going to use this surprise to explain to you the cross entropy term. Let us say that something there were Y1. Why there were data, there are ten pieces of five pieces of data, three, why four? These are cows. And we are looking at probability of a cow, right? They are genuinely cows. First, third, and fourth instance of the data are cows. And Y2, Y5 are ducks, right? So their values are actually zero. So far, so good, guys. And we will take cow as the positive case so let us say that for the first positive case you realize that the data said y1 hat is 0.95 y3 hat is 0.1 and y4 hat is 0. what should we say zero zero one right let me make it zero zero zero one practically zero y hat right now let's just look at the cows you know that y1 was a cow so when you evaluate the result how surprised are you if the machine makes a prediction it says my belief is this is a cow, 95% probability that it's a cow. Are you tremendously surprised? No. So this looks right. You look at this and you say, huh, isn't it? This doesn't look right. So you are surprised. And what about this? That's when you get get shocked you say my model is probably wrong does the intuition make sense to you guys sorry as if i lost you a little bit here so y1 the one on the left hand side the y1 y2 y3 y4 so those are like the uh these are actual data you get five pieces of data you get cow you get duck you get cow cow duck right and we are looking at the probability of a cow right the machine is predicting that something is a cow. The convert that it's a duck. So just look at the cows. Suppose you were looking at how wrong you are or how's, see surprise is often is, if you think about it, a measure of how wrong your machine is. If your machine is fine, you already know it's a cow. And if the machine says cow, you'll say, that's fine. But if it says it's a duck, you'll say, that's fine. But if it says it's a duck, you say, wait, something is not okay, isn't it? It says it's more likely to be a duck, you'd be surprised. So look at this. So suppose I look at this log of, now let's look at this log of, would you say that if I were to add all the surprises, add cow surprises, it would be log 1 over y1 hat plus log 1 over y2 hat plus log 1 over y3 hat. Would you agree? No, no, no. Y three, y four. Sorry. Y three, y four. These four data points. This is how, this is your total surprise at seeing the predictions. Only with respect to cows. Does it make sense, guys? Yes. Now we'll come to ducks also right and now let's look at ducks. The ducks are zero so now let's look at ducks. I'm sorry what is y1 hat mean? A prediction that it's a cow for the first data point. This is so this is y12, this is the reality. Y5 is the reality. And y hat is what? Y hats are the predictions, right? Y hat is a 0.95 prediction, 0. No, actually I didn't mention it here. 0.1. Oh, that's what the machine tell us uh the change i've seen that the y y three is ahead that is okay hearts are always predictions in our world in machine learning world whenever you see a variable wearing a hat it is saying i am the prediction of a model i see okay thanks that's a basic convention we follow now let's say ducks so anybody would like to suggest a number y2 of a duck let's say that your machine says remember that we are talking about the probability of a cow right so suppose y2 is equal to um take a number 0.25 right so it is saying that for the second data point which is a duck the probability that it's a cow is 25 so what is the probability that it's a duck y hat 2 this is the probability that it's a duck right 0.75 1 minus Raja Ayyanar?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro What is the probability that it's a duck? 1 minus 0.25. 0.75. And so how surprised are you that the machine says it's a duck when it is a duck? Right? So once again, the surprise for the second data point is 1 over log of 1 over what? Probability of a duck. Right? 1 over log of 1 over what? Probability of a duck, right? Probability of a duck is y2 hat, right? And so it will be log 1 over 0.75. So in other words, 1 minus y2. Likewise, the surprise for the fifth element, so now let me put it 0.25, which means probability of a duck is one minus 0.75. One minus y hat two. Surprise that it's a fifth one is also a duck because that in reality was a duck. Let's say that, give me a number guys. What's the probability that it's a cow? Throw a number. 0.3. 0.3 0.3 so now surprise once again y 5 is 0.3 that is probability that it's a cow so what is the probability that it's a duck is it's a duck is 0.7 right so what is the surprise here log of 0.7 right so if you look at all the surprises like how surprised you were to see that it's a cow or a duck you would you would add up all the surprises isn't it surprise surprise over all the i-th elements zero to five would you agree this is a total surprise that you get machine is making some predictions if it made perfect prediction every cow it said is a cow. Sure. Every duck it said probability of a cow is zero. But there would be no surprise. Would you agree with that? Right? Yes. In other words, probability of a duck is also one. Priority of cow is one when it's a cow. Probability of duck is one when it's a duck. There is no surprise. Your machine is working perfectly. But the surprise is the element to which the machine is imperfect. Your model is imperfect. Would you agree? And that is why this is your loss function. This is literally the loss function for a classifier is equal to this. Now, let me make it in a more precise way. So this was, if you remember, what were we doing? Let us put it in this form. We break it up into two positive cases, i belonging to i is equal to 1. How many cows were there? 1. Which were the cows? 1, 3, 4. which were the cows? 1, 3, 4. For 1, 3, 4, what did we do? We did log 1 over y hat i. Because these were cows. And we took the cow as 1, like basically 1. It was a cow. This was the surprise, guys. Do you remember? Here, just a moment ago we did. This is the surprise guys do you remember here just a moment ago we did this is the surprise this is equal to summation over i is equal to one three four log one over y i do you see i'm just writing this thing in a more succinct notation, it is this and what is the surprise that you get when you say it is equal to a duck what would the ducks J is equal to two and five isn't it. R. Vijay Mohanaraman, Ph.D.: But when it was a duck the probability of a duck is one minus y hat, so it was log of. R. Vijay Mohanaraman, Ph.D.: One minus white to write and one minus So it was log of 1 minus y2, right? And 1 minus y5 log yj, y hat j prediction, right? And this is it. Now, one of the interesting things is how do you make it one? Probability, the label is zero. Label for this yj is zero because it's a duck, right? We said that one is cow, is duck the reality so how do you make you don't want to multiply it by zero you want to multiply it by one minus zero right so this is your cow and by the way this is just putting it all in the same notation don't worry about it if you work out this equation log of this and log of this this becomes this is just your y i and this is 1 minus y j in other words 1 my this is your this is your duck sorry 0 is the this is the probability or the duck the reality of the duck and so this equation becomes summation y i like when it is a cow y i is one of course so this is by the way forget the y it is just a fact right one over y i hat plus i is equal to cow cows belongs to cows and j belongs to ducks whatever data points are ducks 1 minus y j times log 1 j hat yes would you agree so far guys are you following the chain of reasoning why did we multiply y i with log one over y i because that is the surprise this is the surprise we are adding the surprises this is the actual surprise and why did i manage y i because y i is equal to one it is just a mathematical convention to do that right ignore this this is all one uh i'll tell you why it's a little bit but okay so here's the thing i'll remove this now you agree that i'm just adding up the surprises right and i think the one log of one over one minus yj, right? One minus yj. Excellent. Thank you for pointing that out. One minus yj. So would you agree the total surprise or how wrong our model is, which is the loss function, would be this equation right this is for cows this is for ducks right how much surprise you get from cows and how much of cow predictions and how much surprise you get from duck predictions now log is so you know that log of 1 over x is equal to minus log x. You know this convention, right? Because logs are additive. Right. Let me break it up into smaller parts, just in case you have forgotten our log. No, this is good. Log of 1 minus log of x. This is 0. So it is log x. So now let's put that there total surprise which is a loss function summation over minus log y i for cows i are cows right all the cows and cow data points and minus and again minus summation here of j is equal to ducks for ducks and that would be log 1 minus y hat j now does this look lovely to you easy right this is the loss function yeah this is the intuition this is the one intuition you can carry that your loss function is actually a measure of if you knew the reality because because you do know the reality, you have the labels. So how surprised are you that the model or unpleasantly surprised are you that the model is making the predictions that it is making? Right? So we want to minimize our surprise. You want to minimize your unpleasant surprise. Are there any pleasant surprises? The negative of this is the pleasant surprise, I guess. You want to maximize that if you want. I see. How come in the last equation you remove the 1 over y hat? Yeah, because this is it. Let's work out the details. Let's make a box. you remove the one over y hat. Yeah, because this is it. Let's work out the details. Let's make a box. Log one minus y hat is equal to- You move the, okay, okay, got it now. So that's the one there, okay. Yeah, this is it. But guys carry this intuition because some of this is not there in books right at least you don't easily find this sort of intuitive explanation and books is there in the mathematical community it's sort of it is there in the air people talk about it like this but not so often uh in the in the books and it's a good way of looking at it. So remember that, so I didn't get to entropy, which we will get to a little bit later. But think of entropy as the average degree of surprise at seeing all the pieces of evidence. But we will leave that aside for today. Now, if you understand this, it is just a mathematical convenience for reasons that we will get into when we talk about entropy later on. Mathematical convenience, which makes no difference to this equation. If I multiply it by the labels. So, you know, when it is actually y i so let's look at that total surprise loss function for classifier and in this case binary classifier loss function for a binary classifier is equal to minus i is equal to curves belongs to curves and see look at this term y i hat you know what is the y value here label if we give label one to curves zero to duck right what is the label here suppose i it by y i does it make any difference because y i here is one guys true right it's such a silly silly thing i'm saying i'm multiplying it by one because i know y i one y j is equal to 0 because all the j's deal with ducks are we together the first what is it the first the how many of them are cows um let's go back the first the third4 are all 1, and y2, y5 are 0, right? The labels are 0. So I can put a 1 here, and in the same way, I can just multiply j is equal to dux. And if I do 1 minus yj, which is the real, what does this become? We know that yj is zero this also becomes one this is also one and this is log one minus yj hat right so when i write it like this you it becomes the more conventional form that you find in books and you always worry that what is this whole mysterious equation like so i'll write it like this last function the way you see it in books is minus i belonging to case one let's say cows y i log y i hat right minus summation j is equal to that's they won't talk of cows and ducks i'm sure they talk of more formal language but cows and ducks are more fun to talk about one minus y j log one minus y hat j so when you see this equation in your mind here here is how I would do it. I would think about it. I would say, oh goodness, this is too complicated for me to understand. So I would, in my mind, translate it and say, hey, wait a minute. First thing I notice, this is one for cows and this is one for ducks, isn't it? So I would just go and erase those equations. And then in my mind, I would say, okay, knowing that this part is only about cows and this part is only about ducks, it is minus, it is basically log y i hat minus over cows and j is equal to ducks log 1 minus y j hat. I would write it like this. And the moment I write it like this, the next step I would do is I would say this is, well, you know what, this is summation over log 1 minus yi hat plus i is equal to curves, i is equal to ducks, log one minus yj hat. I would realize that this is the same thing. And then I would say that, aha, this is nothing but total surprises about cow predictions. Predictions are models beliefs, models beliefs models beliefs hence plus surprises about ducks duck predictions Doug's predictions models, again, models Doug beliefs. Would you agree? And so it is just total surprise. This is the way in your mind, if you notice that when I summarize it, let me decrease the, I tend to write in big forms, but in one page, I hope you can see it. So the formal log function, this log equation that you see in books, and you always wonder where did it come from? Yesterday I went through a formal derivation of it, but of course it was our optional session today i thought i'll explain in a way we'll use it because classification is the most common thing you do in deep learning right so i hope that i have interpreted this equation to nothing but a set of total surprises the loss function is just a measure of total surprise is it making sense guys yes right so that is the intuition that is the real intuition that you should carry in your mind any questions before we uh pause here is it clear guys i don't know uh it's it's i was just trying to make something that was opaque a little bit more uh intuitive pay a little bit more intuitive is this clear this was very good this is really good yeah this is the loss yeah it's simple because both are negative you can just in your mind multiply it both sides by negative one and it's the same yes all right guys so this i i had a key question All right guys, so this I had a key question. Go ahead. Why J hat is the machine's prediction about a dot or about the output is a dot or the output is a count. Why J hat? What is the? So that okay, so let me frame the problem. Here is the data. Here is data, x i goes in, the data, whatever input vector goes out, the machine will come out with y i hat, right? This is by definition, this was something I said, probability that X refers to Xi refers to a cow. Right? We also happen to know the reality, which could be like, the model doesn't know. But we know whether it's a cow or a duck. Because we have the answer. Isn't it? And so suppose it was a cow and the machine comes and tells you, absolutely sure probability 0.99999. But it's a cow, would you be surprised? You would say your machine is doing well isn't it your model is doing well but if your model comes out and says you know that it was a cow but it says that the probability that it's a cow is 0.01 now you would say now wait a minute that's not right you're surprised isn't it means you need to further train the model yeah i, I want to know why j hat is the probability of a star or the probability of a cow. Why j hat? What that means? Oh, why j hat is also the probability of a cow, but where you happen to know that the object is actually a dog. Oh, okay. I got it. Just two different indexes to distinguish yeah it's still probability of a cow when xi xj refers to to a duck in other words yj is equal to zero okay okay thank you that's what we are saying. So is the intuition clear, guys? This is our intuition. So carry this, this is very important. The intuition for regression is clear, you know, you take the error terms, you take his absolute value, or you square it and you add it up, all the error terms either. Classifier is a little bit harder to get because it gets into a more theoretical argument, maximum likelihood estimation, the log of it, negative log. If you're trained in it, after a little while it begins to look easy, but in the beginning it doesn't. But I thought I'll start with the answer itself and put some intuition behind it. So this is the intuition, just a measure of surprise. All right guys, so now we will take what time is it 8 18 we have been talking for an hour let's take a very quick five minutes break and no more guys and then our next topic will be momentum momentum in gradient descent next quantum ingredient descent. Next. And we'll talk about that in exactly five minutes, guys. My clock says it's 8.18. Should we meet at no later than 8.25 Pacific time? Let's take a five minutes and eat. So we move now to the new topic of momentum in gradient descent. So let us recap the equation of gradient descent. We said that the new value of the parameter, and we have used multiple conventions. We have used the convention beta, beta tilde I used to say for next, next value of the parameter, parameters, beta being a vector is equal to beta current value of beta minus alpha times gradient of the loss with respect to beta right implicit that is with respect to beta this is the original equation that we did in ML100 we are familiar with. When we deal with deep learning literature, you find that people use a whole variety of conventions. So most of the time we are talking of a weight, right? So you can say that in our notation, I've been using this notation that if you look at the weight vector, its next value, I'm using tilde for next, is equal to the value of the weight vector minus alpha times the gradient, obviously with respect to weight of the loss right written out in the specific terms i j let's look at the specific address of that of the layer l so suppose i have in the layer l l layer just the the ith node and then in the L minus 1 layer, there is a jth node. So the weight associated with this edge is WijL. This is just to recap, right? So the next value of this is equal to WIJL minus alpha. And then the gradient, when you do it, it becomes a partial derivative, of course, because you are being very specific. WIJL, right? This is laying it out in a specific pieces so far so good guys okay so this is the convention that i've been following everywhere uh in deep learning course fundamentals course i've replaced beta with uh beta with this and here also, any particular beta i is equal to tilde is beta i minus alpha dL d beta i, in specific component wise. That is what it is. So far, so good, guys. This is, I hope, a recap. Furthermore, now, for the purpose of to align with the convention given in this book. So i'll say to align with the which we are using for review. Review. What are we doing? We are replacing W with theta. We are saying that this w vector, it goes to theta as a vector, assumed it's a vector, and my tilde goes to t plus 1. You're seeing that in the t plus 1 iteration, right, the theta t, theta, now does he use subscript or superscript affigate? Maybe t plus one is equal to, this is essentially in my language, w tilde, right? Is the previous value of theta in the previous step minus eta. So this in my language would be, this part would be W. And this would be, what is this eta? And gradient of, so let me just write it. This book writes it as gradient, it writes it as a G vector, G, and for some reason makes it into a superscript right so the convention is slightly uh inconsistent but for whatever it is this is what the book follows so now what is the eater in our language guys what is the eta in our language what we what what are we calling it alpha learning rate. This used to be the learning rate, alpha. And what is this guy, this beast, this gt? What is this? This is basically our gradient with respect to theta t of the loss, isn't it? Like in our language, this is what it is, the gradient of this part. If you replace your W with theta. So, so far, are we together, guys? Is this anything looking mysterious? It's a little bit of jargon, you know. Nothing has changed. The intuition is the same. It's just that different people will use different notations. Why am I confusing you with new notation? I don't want to, but I'm just making you familiar with the notations of a book that we'll use for review. So far, so good? Are we together, guys? Are we together guys? So we will start with this as a gradient descent equation. So here the t refers to the iteration. tth iteration. It's sort of like time. It's yeah, t. And that's how people people think that's a very good word you use. Think of it as time. Time moving in discrete jumps. First step, second step, third step, fourth step of gradient descent. What is each step? One mini batch, right? When when the data one update to the to the parameter value is one step. Right? So the t-th iteration, and think of t as time, if that helps you. Right? So the gradient descent equation is, now guys, if all of this is getting too theoretical, I promise you this is our last pure theory thing. After that, we'll use all of this knowledge for practical architectures from next week. So according to this, it is theta t plus 1 is equal to, I'll just repeat it here, theta t minus eta gt. Obviously, this vector is assumed. And i will just deliberately write it in our notation just for whatever it's worth this is equal to the previous iteration of this minus alpha gradient of the loss right these two are equivalent this is the way to think about it equivalent These two are equivalent. This is the way to think about it. Equivalent. But different symbolic conventions. Now we realize that this is the basic gradient descent equation. When we do, we said that the learning rate shouldn't stay the same as you go from epoch to epoch so suppose you decide to train the data for 100 epochs what should you do we saw in the loss function visualization which i can bring back again let me do that it should be sitting somewhere here let's go do you notice that we'll again keep referring back to this picture in the beginning let us say that you are up here what do you want to do you want your learning rate to be small or big when can you guys see my mouse yes okay all right so in the beginning it can be bigger you want to you want it to be big you want to take rather large leaps because you want to fall off these little crevices quickly. Local minima. Closer and closer to the minima. As the epochs evolve, you want to slow down. So at the beginning of each epoch, you need to decide what your learning rate should be for this entire epoch, right? And that is what we did when we did the learning rate scheduler. The learning rate scheduler to recap. The learning rate schedule determines how, what learning rate, learning rate you will take, you will consider for each epoch. And this is very important. You change the learning rate after going through an entire epoch of data. So it is course grade. You don't keep changing it in the middle of the epoch itself. You do that. And so this equation becomes your fundamental equation. t plus 1 got modified to t minus this part got replaced with the learning rate scheduler right which is a function of right i will just mark it it's a function of t and the current value of theta or theta t and and which epoch you are in and so on and so forth that times the gradient t so we did not touch the gradient itself we had all sorts of ways to do this we had the cosine annealing we had the exponential decay we had the stepwise we had the stepwise decay and stepwise yeah well let me just call it stepwise decay for better words and then there is also one that is sensitive to the plateau plateau of loss function now this i didn't get time to cover and i don't think i'll get time to cover today so how about this we we make this a reading assignment for you guys and we do it at some point in the lab kyle did we create a lab using the plateau also no just with the stepwise and exponential just the stepwise and how about cosine yeah it is working okay but plateau is not there see if we can add it also so guys your assignment is the plateau rate scheduler learning rate scheduler on your own and if it looks confusing, then I'll cover it in one of the Sunday sessions. All right, guys. So now one thing is there. Eta, learning rate became this. But we did not play around with the gradient. Now we are going to play around with the gradient. We are saying that this equation, the next step is we are going to play around with theta t minus, and whatever your learning rate is, I don't care. Maybe you leave it as a constant eta. So for now, for the sake of this argument, to keep both of these things separate, I'll just keep it as a term and this becomes some function of t right and theta t of course of theta and t the current value and d so we have replaced the gradient with a much more sophisticated version of the gradient we are saying we won't literally use the gradient but we'll use some function of the gradient, right? Now, why in the world would you do that? The reasoning is very much like what we talked about before. See, if you don't have a momentum, a velocity, what happens is, did you remember that we used to get, if you come down and you hit this point, any one of these local plateaus or local crevices, local minimas, you will get stuck there, right? Because if you try to move away from there, the gradient will point back at it, will take you back. Are you seeing that? So in other words, suppose you are in a situation like this you are here right you are here what is the gradient here zero isn't it gradient is zero would you agree or let me just say GT but to to use the convention notation of this gt is zero at this point right and so you have been coming down here you're at this point your gradient is zero so what will happen learning will stop no matter what your learning rate is your learning will stop here because whatever you multiply your whatever your eta is you're multiplying it by zero right so the next the next value of the parameters would be the same as the previous value of the parameters would you agree guys are you seeing this it's an obvious statement i'm making yes bounce out of local minima. Yeah, one of the downsides of remote class is I can't see your faces. So I can't see whether you're understanding or not. But yeah, let us assume that you are. So speak up, guys. If you don't understand, speak up. I'll assume that silence means you're understanding it. Now, what you want to do is you want to somehow have a modified gradient G, which will make you go past it. Something that will make you go past it. Right. And go past it enough to jump beyond that, jump beyond the local, make you take a step to here. Something like this. Are we together guys? So you realize that the local gradient is not enough. What do you need? We can take the velocity or the momentum we have developed coming down. And what we do is we say g is in some logical sense momentum plus the gradient plus gt how about that logically if we use it that it is the sum of momentum and gt then what will happen we will move past this minima, local minima. Am I making sense, guys? Right? The same is true for the plateau. Suppose I have, you would just stop here, gradient is zero. But if you are on the roll, you will roll past this. but if you have if you are on the roll you will roll past this but when gt like gt equal to zero you multiply by zero you always get zero so how is momentum going to help you no because your total g is the past momentum plus gt you're saying when i do the gradient descent i will use g capital right? So even though this goes to zero, but because this is greater than zero, I will still move, I will still update my parameters. Are we agreeing, Manish? So let's say that by the time you reach this place, your momentum was 10, right? So what is your G? It will be 10 plus and the actual gradient would be zero. And so your update step would be what? T plus one would be your previous value of T minus eta times G, which is equal to 10. So you will still end up updating your parameters. Manish, you got that? OK. Manish you got that? Okay yeah. That is the whole point. What you don't want to do is stop at every time the gradient vanishes you don't want to stop. You want to have from somewhere borrow from your history from your recollection you have to borrow a direction and say I'll continue to go in that direction and what is the preferred direction here the preferred direction here is follow this curve right continue moving this or follow this don't stop here keep moving here right so you don't just bring this g you need to have an intelligent way of having some g some momentum momentum, some velocity. So that is why these approaches are called momentum based methods. Right? So where did, where is the momentum going to come from? The momentum can be like, we can take the simplest form of momentum. The couple of methods we'll talk about, one is the stochastic gradient design, SGD momentum. for historic reasons is called sgd momentum because this is stochastic remember stochastic stochastic gradient descent the stochastic gradient descent used to be that you learn, what is your batch size in stochastic gradient descent? One. One, right? Yeah. Mini batch size. So what would happen? It would zigzag its way to the minima. And then even when it reached the minima, it will keep zigzagging its way around it. We saw that. So this momentum term was initially created and it's really useful to fix stochastic gradient descent, but it doesn't apply only to stochastic gradient descent. It applies to mini-batch gradient descent also. So keep that in mind that the name is a bit of a misnomer. So what do we do? What we do is we say that we create a definition we say that we create a term called velocity or the momentum velocity momentum here so remember if the mass of a particle is one what is the momentum is the same as velocity right so that's the way people write it so So vt plus 1 is whatever it is, whatever your previous velocity was multiplied by a fractional amount. This is anything between 0 and 1, right? Plus your learning rate times gt. So now look at this term. This is effectively your new g, t plus 1. What you're doing is you're saying take the g, take the learning rate, but add a little bit from the momentum from the past also to it. This is exactly what we did here. So you can take mu is equal to, for example, 0 point. Depends upon how much you want to consider momentum, how strongly you want to consider momentum. Now let me show you in very practical terms. Remember that, so look at this thing. We will do it visually. I will take this. We will do gradient descent. But as we do it, no, close this one. Settings. This is the momentum, very high momentum. See, this is your degree of momentum, right? Think of this as the mu here. Do you want it close to zero? So when your momentum is very low like 0.01 what happens let's see i will start here you'll get stuck did you notice that we got stuck here guys well it's then it gets stuck at the next place you never know whether it will ever reach the minima or not it sort of is bouncing around and going somewhere else right so what is happening if you look at it if you look very carefully it it is got caught up in this local minima here do you see these guys and it's a significant minima and the next thing you know is it will get caught up in this minima and this is a pretty deep minima hard to get out of caught up in this minima and this is a pretty deep minima hard to get out of right so now i will start from the same place here and what i will do is i will take a bigger momentum let's say momentum is uh how much should we take let's say i don't know point 50 half the momentum let's say now what happens i'll start at the same place is it generally between zero and one yeah zero and one that mu so now you notice this is a pretty big valley here but you see that this was pretty deep half so you're taking the consideration of momentum 50 percent is coming from the gradient 50 percent is coming from the momentum what happens doesn't help from the momentum. What happens? It doesn't help. Remember, I told you that you'll get stuck in this deeper valley. You're stuck in this big valley. That much momentum is not enough to get you out. Do you see this, guys? Just looking at this, you know that you really need a strong amount of momentum. So let's go and do that. Let's go and do that.'s go and do that again and this is what we are doing we are learning by playing with it let's do this let's take a bigger momentum we take a really big momentum right right and then what do we do Let's see what we can do. We again go here and try clicking. This is really actually a hard direction because there are so many traps along the way. And you realize something happened. What happened? Let's go and see what happened. The momentum caused it to bounce around quite a bit. You see that it did get caught up in the local minima, but after a little while, the sheer momentum took it across. Where did the momentum come from? From falling down this very, very steep mountain, isn't it? And so that momentum carried it forward. It made it knock off very fast here. And then finally it came here. Look at this. So do you see the value of adding a momentum term to the gradient descent? Guys? Oh, definitely. Okay. Oh, definitely. This is it. This is the basic intuition. So the rest is just algebraic equation. So what we are saying is this is our new GT. So when we take the step T plus one, it is equal to whatever the D value was there minus a, well, this funny thing, which is a mu times the previous t minus eta gt. So you're saying that add a little contribution, a little contribution of pass velocity, right? And this past velocity, so v2, you can keep doing this. Now you will realize that what it means is that the past gradients that it has encountered, if you look at vt plus 2, you realize that this will become mu vt plus 1 plus eta gt plus 1 and now what was if you look at this you substitute the value here it is mu times mu vt plus eta gt right plus eta g t plus one so one of the effects you see that the prior two steps before momentum is getting mu squared and this is getting the learning rate is being again reduced by mu eta so this is just algebraic ways to say that the momentum that you encountered many steps ago the gradients that you encountered many steps ago, the gradients that you encountered many steps ago, their influence is gradually fading. You are much more impacted by the gradients that you have been encountering in recent steps. Right? That's, that's, yeah, go ahead, Albert. So here you can see visualization of that problem. Yeah. So if you have a low momentum and you just let it go and you don't reach them. Yeah. So how do you solve that problem? Your accuracy won't be good. And so you will see, see, this is it in deep learning. The, the, the, the, one of the biggest problem is given the non-convex loss surface of the complicated loss surface, you should never trust your first run of the data. You should always try out different values of the hyperparameter. So you have to do quite a few runs. You have to go all the way from zero to one in the moment. You have to play. So now that brings up a next question. Well, you know, it's computationally very expensive. Some of these learning things run for days. So you don't get the choice to keep on running with different hyperparameter values. So that brings us a very interesting question. How do you find the best value of momentum? The best, all of these are hyperparameters of the model. How do you select those? And that brings us to the whole world of automated machine learning. And that automated machine learning is a big topic, big topic. Today is the most important topic because there are so many hyperparameters in this deep learning world. Right? So suppose there are 10 hyperparameters each hyper parameter you take a 10 possible values, let's say momentum, the mu, you take 10 possible values, 0.1, 0.2, 0.3, 0.4, 0.5. But then you realize that you're multiplying by nine other factors which have 10 values. So 10 to the power 10 is 10 trillion. And each time you train the model, suppose it takes you one hour to train the model, you don't have 10 trillion hours. Even if you were infinitely rich with infinitely huge amount of hardware and patience, your lifetime is not 10 trillion hours. So the fundamental question is that these models that take so long to train and have such high number of hyperparameters, how would you find the optimal hyperparameters? And the search for that hyperparameters is variously called optimal. So that is a problem. And you can't do it by gradient descent because there is no gradient there for hyperparameters. For parameters, there is, but not for hyperparameters. For parameters there is, but not for hyperparameters. So you get into this very fascinating, did I talk about Bayesian today? Yes. So it is the Bayesian reasoning or Bayesian mathematics that comes in. And the process is Bayesian inference and Bayesian optimization. Those things kick in and they give you an elegant way, very elegant way to quickly find, zone in and find the best combination of hyperparameters, right? In very few steps. And so it's just marvelous to do that. Now, it is worth mentioning. Do you remember guys that when we did the basic machine learning course, ML 100, I talked about, people often talked about grid search, but I told you that grid search is actually a terrible idea at least in a very bare minimum use randomized search which is better for hyper parameters but now i'm saying that even that ultimately has a combinatorial explosion of possibilities when you have very high number of hyper parameters so it doesn't work actually in practice for the deep deep situations, deep neural networks. What works is this Bayesian optimization. Bayesian optimization is a core area of research. Somebody once told me that if you look at the total amount of money that the different companies are investing in this topic, the sum total is more than a billion dollars just to make progress in this area of hyperparameter tuning and biation optimization and so forth at this moment right so there's a huge huge investment i don't know how true that statement is but there's a but i do know qualitatively that there's a massive massive investment in figuring out the best hyperparameters and the Bayesian optimization problem. And this whole thing of learning hyperparameters, see hyperparameter is not just the gradient, your learning rate or momentum and so forth, that the learning rate, the schedule, there's more than that. In a neural network, how many layers should you have in the neural network? That's a hyperparameter, isn't it? To to solve a problem how many nodes should be there in each layer that is a hyper parameter which activation function you should use is a hyper parameter isn't it and now i just introduced one more i'm saying that well you know the mu is your hyper parameter and so on and so forth even which momentum methods we should use because now i'm going to introduce you to two more momentum methods so that is a hyper parameter and we can go on and so how do you find the best hyper parameters that gets interesting and we'll do that in the as we continue the journey over the months and into the various courses. This topic of automated machine learning is so important and so absolutely hot today. It is not something we can do as an aside in one lecture. It is something we should talk about in depth properly in the course of two, three weeks in a subsequent course. So we won't do that. So this is it. Now, this is the SGD momentum method. It has a hyperparameter mu. What is its effect? The effect of this is quite interesting. Let us say that you take some one particular parameter, theta j, the jth parameter, which in my language is W-I-J of layer L, right? Suppose the value of this is continuously moving in this direction, right? It's pulling you forward, forward. It's gradient, the gradient of loss with respect to theta, sorry, let me put it this way. Like every time you take a theta j step, theta j t plus one, right, is equal to theta j in this language of t minus this term, the step term, the gradient term, gradient plus momentum term what happens is that if all the time let's say that it is increasing in value right what do you learn from that during the learning process it is moving every step it's moving in the right direction isn't it if on the other hand this thing was continuously going up going down going up going down going up going down going up going down what do you conclude from this your learning rate is too large you could you have many things but generally basically right you don't want to take this this zigzag too seriously because averaged out it is going nowhere. Isn't it? This theta J is basically wiggling around and staying the same, right? And there are various causes like you said, learning rate and this or that, but suppose other things, learning rate is fixed in a epoch. And as you do the steps, the hundreds of steps that you do, remember that the two for loops are epochs and mini batch step, mini batch step. And remember, T refers to this step. For, for, mini batch step, you do the learning step. The gradient descent step is the last thing here. Remember the four points and gradient descent is like the last part, you take the step. You compute the forward pass, you compute the loss, you compute back propagate, and then you take the step. So when you take the step and you notice that something is behaving like this, what do you want to do? Suppose you learn within this APOP, your learning rate is fixed whatever the learning rate is even if you're using a rate scheduler but what you want to do is for this theta i in which all the changes are in the one direction you probably want to take bigger steps but for this one where it is just wiggling back and forth. You don't want to take much. You don't want to do this like a roller coaster ride. You want to more or less take small steps. You just maybe a little bit wiggle is okay. Keep it more or less the same for the jth parameter. Does that make sense, guys? Right. So one parameter continuously shifts forward. You want to pick up momentum in that direction and go faster. Another parameter is just wiggling around. You don't need to pay too much attention to the wiggle. You want to just dampen it down. Isn't it? And that is it because we want to make it converge then it has already reached the minima and just wanted to converge is that so for that one yes quite likely for the j parameter if it is just wiggling around it probably is just oscillating or around or is unstable or has reached the minimum many one of the many reasons but you realize that something that just keeps doing that what can you conclude you might as well take the average value and sit there right you don't want to listen to the um wiggles the gradients too much for that thing so in other words how much you live listen to the gradient if you if it is proportional to the momentum for the i th one uh let's look at this for the ith one you every time you're moving in the same direction so what happens all the gradients are nudging you to have higher and higher momentum isn't it picking up velocity when you add the term on the other hand here what happens all the gradients are cancelling each other out one gradient cancels the previous gradient cancels the previous that that one cancels the previous one that one cancels the previous gradient cancels the previous that that one cancels the previous one that one cancels the previous one so what is that what does the momentum term be momentum will be approximately zero isn't it would you agree guys that the momentum term well let me use the v term velocity term will be basically zero in this yeah right and the only thing you're doing is you're looking if your learning rate is sufficient as well you're just listening to the gradient up down a little bit up down a little bit you're wiggling around but in this case where this uh this particular parameter which is benefiting and going in steadily in the same direction you're taking bigger and bigger steps because you have momentum steadily in the same direction you're taking bigger and bigger steps because you have momentum there for the same fixed learning rate you're taking bigger steps so the lesson to learn from this and i would like to show it to you there's a beautiful diagram in your book which i would like to use now let me bring up this application what is it called? Cruz. C-R-U-Z. C-R-U-Z. Why am I not? Oh, C, no, not Cazor. C-Z-U-R. Strange name for the, yes your scanner there it is uh bear with me guys for a moment and i want to show this in a beautiful picture in your book this is on page 173 of your book and i would like to show this and i would like to show this a new version blah blah blah we'll do that later scan oh we are in why are we scanning badge color visual visual better okay right so Okay, guys. So look at this thing. If all of your changes for one particular parameter, one particular weight, they are all... Are you able to see my mouse, guys? look at this folks are you able to see my screen yes and the wiggles all the error are steadily moving in the right direction what do you want to do you want to take you want to amplify the steps to do you want to take you want to amplify the steps right on the other hand if your gradient is you notice that continuously oscillating yeah so what do you do it is very inconsistent so what do you do you just say ah forget about it we want to suppress these oscillations we don't want to listen to it too much that is the main point of a gradient based method and so what happens is when you do let me clear this when you actually apply momentum where am i how do i clear this? Yeah. So look at this. Here, without momentum, you notice that in the good situation, in the good situation, also, we are taking small steps. In the bad situation, also, we're just bouncing around. We don't want to bounce around. And here, we don't want to bounce around. And here, we want to take bigger steps. That is what momentum helps you do. What happens now is, now look at this. I'll zoom in a little bit more. Do you see that here, you are, each step, I don't know if you can make out, each of the arrows are becoming longer. Each of the steps are becoming bigger, a little bit bigger. Yeah. And here, it is, let it keep oscillating, but at the end of all of those oscillations, it just keeps moving back and forth and getting there. Right? It's a complex oscillations. They do you no harm basically, right? And this part I wanted to show you, it has a limitation though. When you use this gradient descent method, what happens is, see, you added this term and this term you remember these two terms we were adding mu times the previous momentum plus eta the learning rate times the gradient right this is the equation that we wrote is it making sense guys we just now wrote this equation where is it yeah yeah this is in the book here it is so this is what we wrote so now it has a problem one problem that it has is that imagine that you're moving very fast here here momentum tends to take over because Because you have a strong momentum, the gradient doesn't have that much influence. So here, where is the minima? If you're here, the minima is in this direction, isn't it? See, look at this. This blue arrow from here, it is pointing in this direction, isn't it? Let me, I hope I can draw it here. Let me use a color that will, will be visible over that. Maybe this color will be visible, right? So at this point, right, where is the minima pointing to? This is the big island of minima. So the gradient is pointing in this direction, right? Which is this guy. Gradient is pointing in this direction which direction has momentum momentum is in let me take yellow all right yellow yellow or maybe i'll take blue the momentum is in this direction isn't it guys you see you have been coming along this line so momentum is still along this direction right your gradient is still along this direction, right? Your gradient is pointing in this direction. Momentum is along the blue direction. So the additive effect of these two will be in which direction will you go? You will end up going in the direction marked here, namely you'll end up going here because that is the end result of these two momentums, right? Well, this software is not terribly good, it's smudgy, but bear with me. Does that makeums, right? Well, this software is not terribly good. It's smudgy, but bear with me. Does that make sense, guys? If you look at the effect, the momentum vector overwhelms the gradient vector. Are you seeing that, guys? Right? It reminds me of the circus riders who ride in a cage and they keep, they keep- Oh, yeah. yeah but driving in circles their momentum overrides yes they don't go crash on their heads yes so now the problem with this is when you let the momentum take over and you have a very shallow or very steep like valley right if you think about it like this let's go back to this picture see when it is look at this this is a fairly steep valley right minimize here so and some of these other visualizations they have even more steep this is a pretty good uh nice, big minima. But in some cases, the valleys are very steep. So, for example, let's let me find in the surface a rather steep valley. Oh, yes. Here it is. Look at this. Suppose this was the global minima. Suppose this is what it was. You realize that you could easily miss it on your journey home, right? You could go past it in some other direction, right? Without veering towards this, which is the point that this book is making, that if you don't pay attention, you'll go here. And the next time you'll go here. So you have literally blown past that very steep little valley there. So there is a trick you can use, which is actually a very popular form of momentum. What it does is, and the argument is pretty elegant. It says that don't take a step like this, take the step in two forms, take a two-step process. First, what you do is you let the momentum take you somewhere and then compute the gradient from there. See here, at this point, let me say, right at this point itself, we are computing both the momentum and the gradient. Isn't it, guys? Yes. But suppose we did this, suppose we did the opposite. Now pay attention to what I'm saying. Suppose here's a minima. So right or suppose at this you are here and let me mark it in color, bigger, fatter. No, this is fat enough. Why is this not very bright white? Completely white. Okay, let's see. Suppose your momentum suppose your momentum has been carrying you here so what will happen is if you listen to the momentum term it will take you here right now imagine this imagine the surface. So you listen to the momentum. Let's say that the momentum is carrying you along this direction, along this direction, right? It takes you here and you let this little intermediate step take place. You reach from X to X prime, this location, right? Let me just say theta because that's the convention your book uses, theta to theta t prime, actually. This really makes up the t's, and I'm trying to be consistent with it, which is a little bit of inconsistent notation, but that's okay. You go here, and here you compute the gradient, and the gradient will now point which way? This way, isn't it? And then you take the gradient and the gradient will now point which way this way isn't it and then you take the gradient learning step here so this becomes your t plus one so you go from theta t to theta a t prime to the momentum you're following the momentum for part one and from that location wherever you end up with now you take a gradient descent step at a the gradient right gradient t step so this becomes your t plus one right so what happens is that you will get a much stronger gradient if you wear off in the wrong direction right that is what uh direction. Right? That is what this thing is saying. Now, let me bring this part forward. Yeah, I want to see both of them side by side. Okay, now look at this. Here, let me remove all of the markings and let me mark this to line with this line. I am trying, this software is not the best. Line with. How do I line with, make it very small? Okay. Let's see if this works. I'll move this diagram for you guys. So here, at this point, momentum was carrying it in this direction. Momentum. Gradient is taking you this, and you eventually end up going this way, as this picture shows. But in this, what you do is, let's say that the momentum, it is still taking you this way, right? But what happens? Look here carefully. You are in a much steeper gradient situation. At this point, do you notice that the gradient is much steeper? Right? I don't know if you can make out. It's a much steeper gradient. So what will happen is the gradient will push you much more strongly inwards. And so the end result of this two thing will be this final motion, which we should give in some color, some lovely color. Which color should we choose? Let's say, I'm going to be exhausting the colors. Let's try this. And so you would agree that these two lead to a movement in this direction. And this is nice. So you first took a step, followed the momentum, you went far off, then you computed the gradient, which takes you steeply back in. And so your net motion is in this direction, vt plus 1. But you actually took two steps and so you don't fly off the hand you're doing here in the left one you're just sort of flying off but here you have a much stronger pullback and so you converge to your solution much faster and that is why this form of finding the gradients is, this is a very popular and for the longest time it was the standard. And this is the Nesterov momentum method. And let me say, let me, so this is right, this, this pages around 170, you must, that is Nesterov is written like this, N-E-S-T-E-R-O-F. And ROV, this used to be the most popular method for a very, very long time and its variance. Well, state of the art keeps moving. And now we have one more, many more methods. So the new family or the king of the hill is something called ADAM, adaptive momentum. Right. And adaptive has more little detail. We will go into that. What I will do is because we need to cover more territory. You got the main conceptual idea. Now think people are doing optimization after optimization. So this book author says that they use Adam as the default because it's the state of the art. Well, the state of the art has moved even further. Now people use Adam W. Keep making improvements, but conceptually is the same. I will discuss about Adam in perhaps an extra session. I would like to use the next hour to talk about normalization, a new topic called normalization. Are we done with momentum? This is the main idea of momentum, that when you do gradient descent, not only you go along the gradient, but you also consider the momentum from the past. That is the core lesson to learn. And when you do that you you don't get easily trapped into local minimas you don't shallow minimas you move forward in the right direction so far so good guys that's the main lesson so actually you're referring to the same yellow jacket book or you're going to another book this book is for those of you who missed it, this is this book. It has just come out, guys. It got published a couple of weeks ago. It got published after we started the workshop. And so highly encourage you folks to get a copy of this book. Oh, by the way, Kate, did you look at the bulk discount thing for the whole class? Oh, I need to make that call. It's the poll worker has been taking my time. I'll see if I can call during break tomorrow. Sure. Yeah. No, no, don't do that. You're busy with the poll, right? We can do it. We can certainly do it. Okay. It's on my list. Yeah. no hurry. So, so that is it guys, if you're saying we are this whole course seems to be about gradient descent. Right. In some sense, we have spent a lot of time on gradient descent, for a reason. See, neural network surfaces, large surfaces as you saw, are highly complicated non-convex surfaces, isn't it? They don't look anything like my nice, nice, beautiful convex bowl, right? With one single minima. Instead, they look like what they look like, namely they look something like this, oh boy. It is both fascinating beautiful extremely frustrating so for the for many years people said that neural networks are a hopeless cause you cannot do learning using a non-convex learning non-convex loss surface and there was a whole winter neural networks deep neural networks, deep neural networks have gone, neural networks have gone through many winters. There is interest, then it dies off, then again it surges, some fundamental problem gets solved, it comes back with the surgeons, people start using it, and then again they get stuck and the whole field dies off, and then again there's a resurgence. So we are literally in the next resurgence of neural network theory. The deep learning is most of the breakthroughs have started since representation learning work of Jeffrey Hinton. So Jeffrey Hinton is one of those people in this field who's very, very influential. He's one person who stuck through all the winters and believed in neural networks when people were abandoning hope in neural networks. So one of the reasons people did, there are many reasons, but some of them were legitimate. We didn't know how to solve problems. Neural networks weren't getting us anywhere because some of the core pieces we had not discovered, people had not discovered. So it wasn't quite working in as many situations that people would have. But it was never not working. It was always working very well for some situations. So one of the things people theorized is was that because the lost landscape is so complicated and non-convex, therefore you can't really do neural networks. It's a hopeless cause. Well, people learn to all of these techniques that I taught here, the momentum, Well, people learn to all of these techniques that I taught you, the momentum, the simulated annealing, the learning rate, the cosines annealing, the learning rate scheduler. You realize, and the next one, I'm going to teach you one word, the normalization and the regularization and all of those things. Each one of them, small as they are, they have made a profound difference. They have made neural networks work, even though these ideas that you're being introduced to, these concepts, they look small or they look straightforward. Historically, they are of tremendous relevance. This is what has made this very complicated, made it possible for us to learn in this very complicated last surface. made it possible for us to learn in this very complicated loss surface. So people now, the Jeffrey Hinton and people, they joke, they say that machine learning experts were suffering from something they call convexivitis. You know, the illness called conjunctivitis of the eyes. So they joke that people say that you must have a convex loss surface. They suffer from convexivitis they only only can work with convex surfaces but deep learning does just fine with highly non-convex surfaces so guys if you have been here with me trust me you understand more about neural networks than most people practicing in this field there is a a joke that Andrew Ng used to make long ago at Stanford. He would give a course in machine learning and when people submitted the first homework and did it successfully, he would tell the Stanford students that congratulations, now you're machine learning experts. So they would all get surprised because they have just picked up basics, linear regression and so forth. And then he would give the second part of his statement. He would say, it is because most people who claim to be machine learning experts in Silicon Valley actually know less than that you know. And in the same spirit, I can genuinely tell you, if you have followed me up to here, without exaggeration, I can tell you that your understanding is already deeper than most people in this field who have not bothered to understand this field deeply. They are all code junkies. They'll pick up code, copy code from here, from there, pick up libraries, and try to get the work done. And sometimes it works, sometimes it doesn't. But when you really understand the subject deeply, you have a mastery, you have much more confidence that you can solve any problem that this neural networks can solve. You'll get there in a short time. So that is a very important level of confidence to have. So we are going to end with the gradient descent thing now. We will talk now of a much simpler topic. And once again again I'll give you no more than a five minute break. We will now talk about normalization for the next 40 minutes. So let's recap a basic fact. In machine learning So let's recap a basic fact. In machine learning, I'll write a statement and tell me if this makes sense. standardize the data or normalize the data. So this is a claim which you have to tell me why is this true? Who would like to speak? I'll give an example. So suppose you are looking at elephants. Like the mascot of support vectors. And you are measuring amongst other things the temperature of the elephant, which is your x1, the temperature of the elephant which is your x1 the weight of the elephant at the end. Let's say this. And let us say you want to look at all of these things for whatever reason and build a model you're trying to predict all things considered if your y is healthy or not or maybe let's take even something different i don't know why you would consider temperature let's make it a regression problem. How much food to give this elephant? We're trying to predict how much food to give this elephant. Temperature might not be what I'm putting, throwing in temperature for that. So now you realize that temperature will be somewhere in the range of what? The variation of temperature will be from 90, I don't know, 95 degrees Fahrenheit. If elephant temperatures are anywhere like human temperatures, I have no idea. Or 97 degrees to 103 degrees. Is this the range? Delta is about six degrees fahrenheit it's certainly true for human beings right variation of data is within a very small range what about the weight let's say that a baby elephant i have no idea how much does a baby elephant weigh but let us postulate that the baby elephant weighs one thousand pounds and a grown-up elephant weighs pounds and a grown up elephant weighs 6,000 pounds. Any biologists, if you feel like correcting me, please do. But give or take 1,000 pounds here or there. Let's say that this is right. Diameter of the trunk, let us say if you measure in centimeters, it may go from, let's say, 10 centimeters to 20 centimeters right so what happens is that when you do the prediction of how much food it is you know that there will be weights associated w1 w2 w3 even if you are making linear model right what you will find is that weight is in big numbers, right? So your data has a wide variation of weights, but diameter and temperatures are very small variations. So your loss landscape, the loss, the curve, it becomes highly irregular. It becomes very ellipsoidal. Do you notice that it is not like nice round valley it's it's not like a nice bowl even for linear regression nice round bowl no it becomes more like a very very pinched a very sort of almost like a bowl that is more like this, pink stuff, right, on its edge. That is what this surface that you see, that I drew, looks like. You know, this is like in one direction it's very, very narrow. In one direction it looks very narrow, in another direction it looks white, right. So the cross section here is much less than the cross section in this direction given any contour line major and minor axes are widely different in such a situation a gradient descent becomes problematic right you it's a very ill-behaved learning situation so that's a technical reason but forget the technical reason just looking at it you can tell that you know what one pound change in weight will hardly make a difference but one degree temperature difference is quite a lot isn't it and you can tell about humans when your temperature goes from 98 to 99 98.7 to 99.7 what happens you know you are sick isn't it but if your weight changes by one pound most of the time you don't know it till you stand on the scale and you say oops isn't it so that's how it goes so you you want all things to be on the same scale So that's how it goes. So you want all things to be on the same scale. And so you normalize it. Typical standardization, many ways of doing it. The most popular is you replace each variable xi with the zi. Xi minus the average, right? Any data, suppose it's one dimensional data point by sigma. Right? So if you, this is called z value. And we have talked about it considerably in the previous courses. When you standardize it, most of the values will be between three and three, non outliers will be here. will be here will fall between minus three and three right generally so it is in a much narrower range and all the data points will get scaled to the same minus three to three range this is one way of doing it another way is you can scale everything to zero and one interval so there are many different scalars the in scikit-learn, if you remember, there are many different scalars. You can look it up where I put X, where X could be, there are many different things. There are min-max, min-max scalar, standard scalar, standard scalar being this guy, which is the most popular scalar in machine learning. So what you do is you take a neural net. Let's say that you have a neural net with many layers, one layer, two layer, three layer, and so forth. And final last layer producing the output. This produces your logits. But this is your logits. Let's see. But this is your X vector. So X1, Xn goes in here. You know that input now in machine learning says, input you should normalize to, you should normalize. So we realize that this is what you should do. You should normalize the input. Now what goes in? A batch of data goes in. How would you be able to find the mu and sigma? You can't find that from one data set. But fortunately, because you sent a mini batch of data, you can compute its mu and sigma. And then do it. You can pass it on. So that is why it's called batch normalization you compute your see when you compute your mu and sigma how did you compute it you need enough data to compute where did the data come from if you computed it across the batch this is called batch normalization now comes the interesting part. This data is normalized. That is fine. But this is producing its activations of the layer one. Layer one. That is going into the second layer. But the same argument applies to the second layer. If these values are not properly normalized, if they are all over the place, one value has a huge variation, another value has a much smaller variation, what will happen? This will not converge properly, the second layer. Would you agree that the second layer will have the same problem that the first layer would have had? Same machine learning problem, that if the data is not properly normalized, it will be bad. Folks, are you getting the idea? It's very simple. I'm saying that as far as the second layer is concerned, it doesn't care. It doesn't know that it is the second layer. All it knows is it got an input and it's producing an output. It is an independent machine learning unit. Am I making sense? Right. So as a machine learning unit, it would love to have normalized data and you'd be pretty annoyed if the data is not normalized. And so it is a good idea to take the output and not just feed it to the next layer, but to normalize it and then feed it to the next layer. So that brings us to this very simple thing. What happens is so far people used to this. And let me use the similar picture to this i typically go left to right actually let me first do it my way so x goes in first layer what what happens in the first layer is first you do the z uh let me say z of the layer one z of the first layer and followed by activation of the first layer isn't it and that is produced isn't it guys so what happens is you do the output output of the first layer is equal to the sigma of the of the z1 which is equal to sigma of x dot w plus bias i hope i'm making sense guys this is what comes out here yes this is yes, it's fine. This is it. And now, what we said is, don't put pass in x, remember to normalize it. Normalize the first layer. Don't take the input and just shove it in. Instead, just say into this machine into the hopper, you say, No, first, I need to normalize it. And then the machine needs to get it right and the hopper will then emit out some output and then you pass it to the next layer and once again remember to normalize n2 z2 followed by activation two sigma sigma two which will produce the activation two which is basically sigma of this and so so on and so forth so what happens is the sequence of activities normalize then linear transform linear linear linear part or people often call it the affine transform that That is, z is w dot x plus b. And here, you do the x minus sigma over mu. Sorry, x minus mu over sigma. Now, what is mu, by the way, guys? I forgot to mention, describe to you what mu and sigma are. Anybody knows what those mus and sigmas are? That's like a bell curve. It's the... For any data set... Mean and standard deviation. Mu is a word for mean and sigma is standard. Sorry, I should have mentioned that standard deviation. Mu is literally xi summation over that over n, right? And sigma is much more complicated. Sigma squared is each xi minus mu squared summation divided by n, right? Actually, let me just put minus one for reasons that those of you who are into statistics would know. It is the fact that samples can be biased. And so to get unbiased estimation of sigma, it is always a convention to just cheat a little bit, subtract one from it, and then you get much better values but anyway we'll forget the n minus one just say averaged average of the squares is a pretty good way of looking at especially for large numbers because when you're talking of thousand and thousand nine ninety nine dividing they give you more or less the same results so normalize linear transform activate this is the deformation stage. Nonlinear deformation. So A is equal to sigma of Z. These are the three steps. Step 1, step two, step three. And so each layer, we do the three steps. We must remember to normalize. Now, while intuitively it makes sense to normalize linear transform, what happens is sometimes, quite often, people swap these two. And it doesn't make a difference. Whether you do the linear transform and then normalize. So in other words, you normalize the z's or you normalize this. Does not make a difference. But if you do it more conventionally, makes no difference. Did I write the word conventionally correct? Conventionally, sorry. Conventionally, more conventionally, you find people write this, but I find this more intuitive. And so does the author actually, very nice. So what you do is you put linear transform followed by normalization, batch batch and then normalization followed by activation. Right. So it is like this. Z goes to then you go to normalize and then you do the activation. activation right so this is each layer right each layer it makes no difference a layer which way you do it so guys i hope once you remember the fact that normalization is a good idea you would agree that this is a no-brainer we should normalize in each layer before feeding it to the next layer we should normalize in each layer before feeding it to the next layer. Would you agree? Yes. It is, but you know, sometimes obvious ideas take a while to be discovered. And it's one of those ideas that made a huge difference. Now comes a very peculiar situation. Why it makes a difference, we don't quite understand. I sort of led you to believe that there is a good reason to do it. And it is a good reason. Data in each layer should be normalized. Normalization is a very good idea for the reasons that we mentioned. But the argument is not watertight. People have asked, why does normalization, and this author calls it the pixie dust, and actually that's the way it was when normalization came out. Suddenly all the models you normalize, they would give you a better accuracy. You add the normalization step, they give you a better accuracy, better predictive power regression or whatever it is so actually we mathematically rigorously we don't know why it works because we can show that normalization plus the linear step is nothing but another linear step right they're both affine transformations the word i once told you about a fine transformation right you rotate and shift and when you do this normalization plus the rotate and shift of the w it is just another w w prime on the data so why it works the only non-linearity comes from the activation so why it works in improving all the models is a little bit of a mystery but there are two variations of the normalization one is normalization of how do you compute your mu sigma so it needs a data it needs a data set right data it needs a data set right remember these are statistic over data so what data when the data is the mini batch itself which makes logical sense, then it is called batch normalization. Works very well with our dense layers or fully connected layers, fully connected neural networks or layers layers and also works with a architecture which you will learn in the in the coming weeks it's called con confnets convolational networks which we'll learn about soon. It doesn't work so well with recurrent, not so well with recurrent. Now, what exactly is that recurrent neural net architecture? We'll learn about it in the coming weeks, but suppose when the data A, B, when instead you normalize over in a different way, what you do is when the data is the activations or outputs of a layer, it is called layer normalization. Now, what do we mean by that? See, suppose you have each of this is a1, a, n. These are the activations from the, given a layer, l, suppose these are the activations coming out of it. So suppose you compute mu mean as A1 to AN over N. So what are you averaging over? Not on the mini batch. You're averaging over the features itself. The features produced by this layer. And likewise for the sigma from this, that is called layer normalization. Not so commonly used for most situations, batch normalization is very popular, but for certain architectures like recurrent neural network, et cetera, the better choice or the choice that works is the layer normalization. But that is the idea of normalization. Straightforward idea. I hope after the long journey of gradient descent, you would agree that this was common sense almost, in hindsight, except that many people, like this author, beautifully calls it the pixie dust. Normalization is one of those lovely things. It's like pixie dust. You sprinkle all over your model and suddenly your model glows. It does better. What a common sense idea. A common sense idea whose reason why it works, why this works as pixie dust. We don't quite understand, but it does. Heuristically, the idea is clear. We should always give normalized data to each layer. Heuristically, the idea is clear. We should always give normalized data to each layer. But harder to prove rigorously that that's the reason. So guys, do you understand normalization? I hope this is a simple idea. Going back to something we know, all we are saying is in neural networks, remember to normalize before each layer any questions guys anybody who didn't understand wants me to repeat something guys there's dead silence from which i surmise everybody is asleep by now no made sense made sense good Guys, there's dead silence from which I surmise everybody is asleep by now. No, made sense. Made sense. Good. Anyone else still awake? Yeah. We can count on KL. Kate, is there anyone, besides the DAAs, is there anyone still awake? Yes. All right guys, so I'll end today, like last 15 minutes we keep for QA, so we are on time. It is 9 45, actually 9 50, just give the last 10 minutes to QA. So please ask any question you want on what we learned today. We learned three things. We learned normalization, we learned momentum, and we learned a sort of an intuitive reasoning of behind the loss function for the classifier, the cross-entropy loss for the classifier. So classifier loss is just a measure of unpleasant surprise, right? How unpleasantly did your model surprise when you know the answers?