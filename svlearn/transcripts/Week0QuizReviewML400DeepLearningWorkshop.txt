 All right guys, so we have gathered here to do the quiz first. There are two quizzes that we are going to review, quiz zero and quiz 1. I noticed that most of you have done quite well in the quiz. It's a reality check. It means that if you have done the quiz and you are scored highly, you have understood the material. As usual, every week I'll release the release quizzes. Those quizzes will get progressively more technical. At the moment, we have just entered deep learning, so the quiz pertains to the moment we have just entered deep learning so the quiz pertains to the material we have done. As we move forward you'll notice that the quizzes not only pertain to the material of the last week as a review, they will also contain material from the weeks before that and you have to be mentally prepared that it is a comprehensive quiz and so you'll keep on getting review on all the material that has been covered in the earlier lectures of the earlier sessions the quizzes will also cover the lab as you may have noticed that this quiz that I gave quiz one quite a bit of it pertained to the lab and you would have done it if you have reviewed the lab. Fortunately, the performance in the quiz is quite good. So I'll keep the reviews rather short because it seems it's somewhat redundant. I'll explain through the solutions and I'll pick up somebody's solution to explain through. We'll do the quizzes in the first half hour, after that we'll take a lunch break. After the lunch break, about half an hour lunch break, after that we'll start the Python session. And after the Python session, I'll open up the flow to any one-on-one help you need, a clinic. So you can individually step back and say, I need some time and can you please help me with this? We'll do it in a slightly open forum because quite often the problems that you are having are also the problems that others are having and some people stay back just to see how those problems are solved. So let's do that. So with those words I will actually switch over to a different screen, to the screen where we are talking about. Once again, to recapitulate, this is our course webpage. If one of you have not attended this, I would suggest you do that. There will be updates on this. Whenever I update it, I'll try to remember and put a note in Slack. If you have not joined Slack, it is high time you did it. Otherwise, you'll start falling behind. It is possible now that as Slack becomes a primary means of announcements, I may not get time to send emails. So, Asif, I've got a new computer. And so the Slack channel is what, supportvectors.com? No, supportvectors-aILab.slack.com. Okay. Maybe that is a very good point. I should mention it perhaps somewhere, right in here. Why not add it? Let me just add it. While we are at it before I forget add a resource So this channel is a private channel. You won't be able to see when you just register. You have to send me an email to register. Thank you, Asif. So you haven't Albert registered, I guess. I have registered. I have access to this page. You do. I'm good. There I'm good. So, all right guys, we will go now to the quizzes. Here is the quiz. I will just stop sharing for a moment because this is a privacy uh individual people score i will take okay i'll share again now where do we go as if we could include the font size yes i will do that bear with me better Bear with me. Better? Is this legible now? Yeah, thanks. Yes, Asif. Yeah, it's good. So guys, the questions were as follows. Consider a set, alpha, beta. Let us now look at a data set where there are n predictors, each of which is a numerical predictor. There is a target variable, take, taken, there's actually, there's a misspelling here. There's a target variable that takes value within the data within the set G which of the following mappings represents a model that can predict a value in G based on the predictors so this this was perhaps the question in which many of you stumbled so I want to give you a tip see math is written in a slightly formal language but the way to make it real always before you solve a problem make it real. So you notice that G is a set of alpha beta. Try to relate it to what we learned. We talked about cows and ducks. So alpha could be cow and beta could be duck. Then it talks about n predictors. Well if you're trying to find out whether something is a cow or a duck we can think of all sorts of numerical predictors like measurable numericals are measurables things that are amounts so you can think of things like size weight you know and so forth so those are numerical predictors so now if you ask this question if you you know if you bring it down to earth this question becomes there is a mapping a mapping from these values height and weight to either a cup either a cow or duck it's a function it's a it's something that's trying to determine based on the height and weight whether something is a cow or duck. So is it a classifier or is it a regressor? It's a classifier because what it is is based on the target variable. The target here is categorical. And this notation I wanted to introduce to quite a few textbooks you'll see will write it as this. If the target space is made up of this, something like this, and G is often used. Some people use C with a cut in it. I don't like that because the C with a cut in it, blackboard C is more reserved for complex variables. So G. So what are we mapping to? We are going from, let's say in the case of height I mean, a weight and size, R squared. Weight is numerical, size is numerical. So a real value would be R squared. And from there, this function is mapping you into a cow or a duck. So this is the appropriate answer. And that is how you should reason with math. Math is always a generalization of some specific example. So the way to think about any of the mathematical things is to always think of a specific example that fits this generalization. And when you can think of a specific example that fits the generalization, it becomes a whole lot easier to reason about it. And this sort of tip is a very basic tip. I learned it actually when I was doing my doctoral work. Many of you know that after, between my undergraduate in engineering and my graduate school in engineering, I took out five years to do a doctoral work in mathematical physics, theoretical high energy physics. So what used to happen is it was the end. It was in the twilight zone between a lot of physics and a lot of mathematics. So you would go and read a book and the book would start out very, very abstractly. Sajeevan G. And I would be just puzzling over what it meant. It took me a long time to figure out that one easy way you can always understand the mathematics is just try to take a real world situation real world example and see if it fits into that mathematics and if it fits into the mathematics now you have human intuition you have intuition that you bring from your experience and it makes understanding the mathematical notation much easier. So that advice, if you can remember, it will always help you. Machine learning is again a twilight zone between many fields. It is partly computer science, partly statistics, and partly mathematics. So one of the things that has been happening in this field is a lot of the things that I used to do in the early 90s and used to be called theoretical physics. They have made their way into this field of machine learning or artificial intelligence, especially artificial intelligence. We will talk a lot about energy energies and we'll talk about Gibbs functions and Boltzmann machines and things like that pretty much the architecture and vocabulary of theoretical physics seems to have come in you know the Ising model and Markov chains and so forth and all of those things deeply imbue this field so because many people contribute to this field you see a variety of languages, dialects, in a sense, within the field. The programmer will start out with code and will start, will sort of stay pretty much close to code. And when you look at the mathematics is sort of very sparingly mentioned, then you will have the theoretician will come, the statistician will come and talk all about tests, statistical tests and different statistical learning and so on and so forth. It's getting very mathsy enough. And then people who have deep theoretical background, they will come in and they will talk about quite a bit of, you know, they will come in with a lot of mathematical notation. Now why do they come in with a lot of mathematical notation. Now, why do they come in with mathematical notation? Are they just obnoxious people who are trying to intimidate you? It's not like that. For them, that mathematics is the most natural and most obvious way of explaining things. They write it like that because they say, this is the simplest way that one can explain something. So it's just a dialect, just a cultural difference. Once you get used to the mathematical notation and in this course, if you notice the textbook that I have prescribed does not spare the mathematical notation, it does use the mathematical notation. And the reason for that is on Mondays, I want to make sure you become good, you get a very good theoretical foundation. On Wednesdays, you get a very good programming and practical foundation so you are strong on both the legs so we are going to use that textbook with notation but it does notation once you get familiar with it you will agree that actually it's says things much more clearly than is said with long paragraphs of English text. So we will get used to the notation. Anyway, this is that. This thing is nothing but this, and then obviously these are different permutations to confuse you in case you were guessing. Now, which of these are classification tasks? Let us go one by one. Given geospatial data on housing, discover the neighborhood clusters of houses. When you look for clusters in data, you're not predicting anything at all. It is pattern recognition. You fly on a balloon over San Francisco and you notice that all the financial district is clustered together in the tip of the peninsula, and you happen to notice that then there's a whole cluster of trees that is the San Francisco or the Golden Gate Park, right in the city. And then you see a cluster of residential houses a little bit down the peninsula. You recognize a pattern to the layout of the city. And that pattern recognition is not classification. Classification is all about predicting whether something is this or that. So obviously the first one doesn't fall into that category. What about the second one? Given the credit history of a person, predict whether or not the person will default on a loan. So what is the target variable? Who would like to articulate what the target variable is? What the G is? Default or not. To default or not. So this is a crucial question. When you apply for a mortgage, the only question in the bank's mind is, will you default on the loan or not? So that is a classification task. Next, given some historical weather data for Fremont, predict the noontime temperature tomorrow in the city. So what is the input data? It's the weather data for Fremont. If you had to predict the temperature at noon tomorrow, what do you think that is? What is the target variable here? Numerical. It is? Numerical. A little bit louder please. It's a numerical value. It is a numerical variable. Excellent. And so this is a problem in what kind of a machine learning algorithm is it regression regression excellent so this is a regression problem and not a classification problem given the credit history of a person predict the credit score for the person so when you get your credit score from one of these rating agencies, is that an identification or is it a quantity? It's an amount, it's a number. It's a number. It's a number. So it's not a classification problem for the same reason. Now the last one, given the polling data, predict which of the two leading US presidential candidates will win the upcoming election. Classify. yes or no? Yes. You have to pick one or the other, and that is classification. It is the classification problem that all the pollsters are busy with today, making predictions. So, deep learning refers to data scientists learning deep data science deeply. I believe all of you got this right. Now deep learning is about is a buzzword that is often associated with the application of deep neural networks to problems. It's one particular algorithm family of algorithms within machine learning. Question four, consider a data set where there are n predictors, Question four, consider a data set where there are n predictors, each of which is a numerical predictor. There is a target variable, which is also numerical, real valued. Which of these is the correct mapping from the input feature space to the output feature space? So guys, if there are n predictors, each of them real valued, the space would be a cartesian r to the n space isn't it so think of it as a two variables let's say height and weight of a person and you're predicting the person's age now we have made it a little bit more real right map it to a real world problem. You're looking at a person's height, you're looking at the person's weight, and you are trying to predict what is the age of that person that looks like him. Now you can imagine that the input space is a Cartesian space, right? X-axis could be height, the Y-axis could be weight, and you're looking at a simple cartesian coordinate space and the target space is just a line you know what is the age age goes from zero to whatever maybe 100 or whatever right 150 the human age so you're going from input to an output the input space is now we generalize from there if they are not two predictors but n predictors the input spaces are to we generalize from there. If they are not two predictors, but n predictors, the input spaces are to the n and the output spaces just r, it's just number. When you have just r, there is a word that we use in machine learning and AI. People use the word scalar. It is a word that is often sort of mispronounced. Where is my scale here? Okay, so imagine that you're holding a scale in your hand. What does a scale do? A scale helps you measure something, right? How big is it? How tall is it? Or something like that. So from that comes the word scalar with an A at the end, S-C-A-L-A-R, scalar. Mathematicians often use the word scalar for a measurable quantity, something that can be measured with just one number. If you can measure something with just one number, it is a scalar. So what is an example of not a scalar? The example of not a scalar could be, given a car on the highway, what is its coordinates? Well, now to give the coordinates, you need two variables. You need the latitude and the longitude, isn't it? Now to describe the car fully, you might even say, I don't just need the latitude, longitude. I also need the velocity. And so you can say velocity will have components, velocity along the latitude, velocity along the longitude. And you can go even further. You can say, what is the acceleration along the latitude direction and the longitude direction so now you have six variables describing the cars coordinates or behavior you know so then you have a six dimensional space and so you generalize from there here we have our n to R because the target variable is now in predict in regression that what is the target space is it one dimensional two dimensional three dimensional what is it say that again I think it's one dimensional you're right in regression the target space is always one dimension. Now, sometimes people do multi regressions in which they will have They will predict two or three things at the same time. Those are sort of the exceptions. Usually your target space is simply one dimensional and that is something to Input space can be complicated. You can go from any dimensional space where the target space is always one-dimensional. Again, for those of you who have done ML 100, 200, this should be hopefully very easy. Now, which of these are examples of supervised learning? Given some geospatial data on housing, discover the neighborhood of clusters, neighborhood clusters of houses. That is, again, as we talked, pattern recognition, that is not supervised learning. What is supervised learning? Supervised learning is a learning in which a machine learns to look at input data and predict value in a target space right so it is usually classification or regression if it is not classification or regression it is not supervised learning this is again a review of the vocabulary and this word reminds me of something sometimes Sometimes I have conversations with businessmen and they ask me, so how are you doing this? I says, we do this. And they ask, is it supervised learning or not? And I say, yes, it is supervised learning. So they said, no, actually we want more advanced AI that can solve this problem without supervision in the AI learns on its own. So that is a misunderstanding, of course, from the businessman. They have no clue about the word supervised and unsupervised words. What do they mean within the AI field? We have a special meaning for it. And it has nothing to do with whether or not you're hanging around supervising the learning process while the machine is trying to learn so given the credit history of a person where it should be a person predict the credit score of the for the person we are predicting a score a number it certainly is a supervised learning the second also is super So the rest of it is simple. You just have to look at the target variable. Is there a target variable or not? If it is, then it is supervised learning. If it is not, you just discovered the clusters. You didn't predict anything, which is true in the first case, then it is not supervised learning. Next question. Imagine that you have built an AI model that can distinguish between a cow and a duck. The inputs to your model are the weight of the animal and the size, volume. Whether the animal, and the third predictor is whether the animal has a beak and whether or not has, the animal has wings. Which of the following statements are true? Now, the first is the AI model is a classifier because the output of the model is an identification of the animal as a cow or duck. I hope by now we all agree that the target space is cow or duck, and therefore it's a categorical problem. The second statement, the AI model is a regressor because regressors have the output as categorical variables. That of course would be folly. The AI model is a cluster because cows and ducks tend to roam around in clusters. Again, now, AI model is a regressor because two of the features, weight and size are numerical, real value. And this is something to remember. Whether it's a class classifier or a regressor depends completely on the target space it has nothing to do with the input space input space always will become one way or the other a cartesian space r to the n even if you have categoricals in the input space like whether or not the cow has duck, a cow has a, whether the animal has a beak or not, has feathers or not, you will treat it as a zero, you'll treat it as a Boolean, and ultimately you'll have to treat it as a Cartesian space. So, well, we'll deal with that later. So the only thing that makes sense here is the first one. This was a one that some of you got wrong. Asif, a little theoretical question there. Yes. If you're faced with a problem where in your input, they're all categorical variables, would you ever be able to predict a continuous variable in the output. Yes, yes, you very well can actually. Okay. So I'll give you an example. Suppose your input is one of the 10 species of animals. Right? And your output is the median lifetime of that animal. Okay. yeah. Next question, imagine a young entrepreneur who opens an ice cream shop on the beach of her hometown. Now she needs to estimate how much ice cream will sell on a given day so she put so she can purchase an appropriate amount from the wholesaler if she purchases too much it will go to waste if she purchases too little she will lose the business so it is important that she builds a model that can predict the amount of ice cream sale on a given day she observes that the contributory factors for the amount of ice cream sold on a given day on the beach are temperature, day temperature on the beach, whether or not it is a weekend or holiday, wind speed, or whether it is sunny, overcast, or raining, whether or not the competing ice cream shop is open. Which of the following statements are true? Her model is a regressor because the prediction is the amount of ice cream sold likely to be sold. That is true because what is the target variable? Amount. The moment you see the word amount, it should give you a clear indication or a clear hint that you are most likely looking at a regressor. The target space is an amount. So in other words, it's a scalar, it's a measurable. The rest of the answers are not true. Her model is both a classifier. Other than that, see once you get this thing clear in your mind that it is the target space that determines whether you're doing classification or regression, then you won't get it wrong. And so this question your mind that it is the target space that determines whether you're doing classification or regression then you won't get it wrong and so this question was sort of a review for most of you i hope imagine that you have built a model that underfits the data likely because your model is simpler than the underlying ground truth which of the following is true about the errors in the prediction? Now, those people who are entering the field of AI for the first time, you wouldn't be expected to get this one right. This has to do with some prior background in machine learning. We will cover this material in the coming lectures, so don't despair. We will review this. So what happens is when models have, you know, there is a ground truth. When you get data, the problem with machine learning or the central core problem with machine learning is you don't know how the data got created. So you make all sorts of hypothesis about what is the relationship? What is the function that maps from the input space to the output space. It is some function that is unknown, but you know that behind the data is some generative force, some causation, and that causation is some mapping, some function of the inputs to the output. but that function remains hidden from you. You cannot find that. So what do you do? The point with machine, can you discover that function? Well, there's a famous statement by Box and Box was a great scientist in this field. He said something very interesting. He said that all, and I would paraphrase it and say machine learning models, all models are wrong. All models are wrong, but some are useful. So what did he mean? How can wrong models be useful? Would anyone like to elaborate or take a guess on that? How can you have, how can you say that something is wrong and it is still useful? It's more near to reality. It's not accurate but we can get like 90% or 80% times right. Yes, it's an approximation of the reality. That's the main point. So it approximates whatever the ground truth was, which you can never know, but it approximates the ground truth with reasonable fidelity. Are we getting that? So maybe I should illustrate this, because this bias variance, some of you don't know know I'll illustrate this by making a drawing here. So, you know, when you guys see my writing screen. Are you guys seeing my writing screen here? Yes. Okay. Let me... Oh, sorry. What just happened? I think I just closed it. One second. And I was trying to maximize it and it's much better now. So suppose the ground truth is like this. You have data and the data comes. This is your x axis and this is your y y the y is the target variable target and predictor and you can like i said you should it's always helpful to give some physical meaning to it so let's say that based on height this is the height you're trying to determine the age of a student. You're in a school, you look at the height and it's somewhat of a predictor of age. Now, what may happen is that the relationship may be, and obviously I'm not a medical or a biologist, so I'm quite likely going to draw something that in de facto reality may be completely wrong so forgive me for that but let us say that the relationship is like this that children are born all of them at when y is zero you nonetheless have a certain height have a certain height and and your height asymptotically goes like this and then it plateaus off at some age we don't grow anymore so let us say that this is the relationship of height to age now what you can do let's see what are the sort of models you can build you You can build a model which is like this. Let me call it model one. And one way to think of model is models are hypothesis. This is your first hypothesis. It could be something like this. It could be something like, let me take another hypothesis. It could be something like this. Oh, no, actually, let me take this out, draw it better, a quadratic hypothesis. a not quadratic third degree hypothesis so let us say that is your hypothesis is something like this let me call it hyper this is three and it bends back up and so forth and that's your next hypothesis. And then, fortunately, somebody comes up with a hypothesis, which is like this. Another hypothesis is like this. So let me call this hypothesis This should have been two, hypothesis two. This is hypothesis three. So do you notice guys that each of these makes some mistakes, right? Some places it gets right, some places it gets wrong. And depending upon what you're trying to do, so suppose the only thing we are looking at is children and children up to high school. So your data range that you really care about is only up to here. Let's say, no, I can't do this knowledge not shrink to all right I'll just use this box huge suppose you're only concerned with ages 0 till this age schoolchildren you don't care any about making predictions beyond this box this age, school children. You don't care any about making predictions beyond this box. So when you look at this data, clearly you can see that all of these answers are partly wrong. Isn't it? None of them could come up with the yellow line. And the yellow line is sort of more like a log curve, but it didn't, none of these could come up with it. But you also see that each of them got it partially right. Each of these hypotheses got it somewhat right. And that is what is meant by Box's famous statement that all models are wrong. The ground truth, you'll never know. You start with data. When you start with data, no angel is coming and whispering the ground truth to you. Because if an angel came and whispered to you the ground truth function, there is no need for machine learning. But what we do is we try to create functions we try to come upon a function so suppose let ground truth be y is equal to some g of x what you do is you come up with y hat is equal to some f of x where to some f of x where you hope that f x is approximately g of x at least in the domain of applicability. At least in the domain of applicability. Now what in the world do I mean by this fancy statement domain of applicability? I just used a bit of a mathematical jargon but looking at this problem can you trace it back to the example of the reality and tell me what is my domain of domain of applicability here? School age? School age, yes. This from here to here is my domain of applicability. Within this region I want a model that works and you can see the different hypotheses. They work to different degrees, isn't it? Now comes the interesting thought. They all are making interesting thought they all are making mistakes they all are making errors and the errors that remain even when you fit the best possible line the best possible a quadratic curve a parabola or the best possible let's say third degree thing something like a sine wave three bends to the data there will always be errors. By the way, I talked about the bends. It is one fact that I sort of mentioned in the previous lecture, which I just mentioned. Here is an intuition in machine learning that a lot of people actually don't have. See, when you see data, suppose you see data like this. Suppose you see data like this. Ask yourself, how many bends do you see? In this data, how many bends do you have? Zero. Zero, because this can be approximated like this. Now if you take another curve, something like this. curve something like this. Data always comes with noise. Why does it come with noise? The process of, see you notice that data never is clean. It always has noise in it, right? All the points don't fall in space. So we will address that question in a moment. This kind of a curve, if you approximate, it is this. How many bends does it have? One bend, right? You have bent a straight line once here. So bend is equal to one, right? And now I'll tell you something which is actually very elementary and extremely useful when you're doing data analysis with people uh it turns out for some reason i'm not aware of it so suppose i take a curve like this sorry noise here How many bends do you see? Here and here. These are the two bends, right? And here the bend is at this point, two two bends and so I can keep moving forward like I can have a situation with three bends and four bends and so on and so forth now it is a very interesting thing that if the equation if you write it as a polynomial equation whenever you have this and you try to do polynomial approximation approximation approximation again that's a hypothesis right you're trying to say i see this data with noise and i'm going to approximate it with a polynomial here's the intuition you can use. A straight line is beta naught plus beta one x. So what is the highest degree of the polynomial? It's a polynomial in degree. A line is a polynomial of degree? One. Great, it's of degree equal to one. What about this? This can be written written you can convince yourself from your high school algebra that you used to write it as you wouldn't use beta you would use abc x square are we together and here so degree is two two and you, and the only task you're left with is finding the values of beta naught, beta one, beta two, the best fit values. And that's a problem of machine learning. And here it would be y is equal to beta naught plus beta one x plus beta two x squared plus a beta three x cubed. So degree is equal to three. So what do we learn from that you look at the data so from that we learn let's let K bends bends we need a polynomial of degree how much degree so for zero bends you needed one for one bench you needed two degrees for two degrees you needed a three degree polynomial so for if you have K bends you observe in the data when you eyeball it what is the polynomial that you should write a plus plus 1. k plus 1, k plus 1. So in other words, your equation should be y is equal to beta naught plus beta 1 x plus beta 2 x squared plus beta n xn, right? This has to be your equation, right? And then if you want to, equation right and then if you want to okay so we have this bank now it raises an interesting question the question that it raises is that if i can if i can approximate data so just think in terms of one dimension. One dimension makes machine learning very easy to understand in some way, and then we take our intuition to higher dimensions. Then you can take it to two dimensions. So a curve in one dimension becomes a surface in two dimension, and then in three you know if you have what is a three-dimensional surface it's sort of a hyper surface it will exist in four dimensions and so forth so uh the the the the best intuition of course is we all like to think in terms of one dimension life is easy up to two dimensions like when your surface or your hypothesis curve surface is two dimensional we can visualize it beyond that you just in your mind generalize it so i have a joke in my field which used to be theoretical physics there used to be theories like there are still super string theories and so forth and they talk of 10 plus 1 dimensions so one of the questions that comes is how do people think in terms of 10 plus 1 dimensions. So one of the questions that comes is how do people think in terms of 10 plus 1 dimension? So I'll tell you my little trick. Whenever I would think of a problem, I would think in terms of two and three dimensions, three dimensional space, two dimensional surfaces. And then you develop your intuition, you solve your problem with that, like the way we are doing with cows and ducks. But then when you speak aloud, you say, as you can see in arbitrary dimensional space, this hyper surface, you see how it is, because you're confident you can generalize, you write as though it is in a generalized higher dimensional space, but you develop your intuition always in low dimensional space. Anyway, that's sort of of a joke so now why did i bring that here let's go back to our situation suppose your data is like this and i'll give you an approximation suppose the data is like uh how should i put it this i'll deliberately take a two dimensional surface. This is again a review. There's a few who have been with me in the past would remember these arguments. So you would imagine that the ground truth should be this. This is your ground truth. Truth that you don't know. That you don't know. And suppose, see, when you are dealing with data in one dimension, the best thing you can do is just go and visualize it. It will give you a hint on what to do. Even with two dimensions, you can visualize and see what is going on right i mean this is two dimensional data and you're searching for a one dimensional curve or three dimensional data and you're looking for a two dimensional surface it helps to just visualize and get a clue but let us say that you can't uh your this data is actually very high dimensional and complex and you can't do this. And this is how the data scatters in this space. So then you can build once again multiple hypotheses. Let's do that. You can have a hypothesis that is that it's a straight line. So you could do if you if you try to fit a straight line to the data, actually you won't do very well. The best fit straight line. So you could do, if you try to fit a straight line to the data, actually you won't do very well. The best fit straight line would be something like this. This is the line hypothesis. Oh goodness, this is a multicolored pencil I seem to have picked up. Okay. This is the line hypothesis. Why? Because for half the time, this curve seems to be going up, for the other half it's going down. If you were to actually do a linear regression model, you can verify that this horizontal line is what it will come to. If you do a quadratic equation model, then a quadratic equation model would actually fit the data like this. This is your quadratic model. And suppose you did a third degree model. So let me take another colored ink here. Suppose you did a third degree model. What you might end up with is something peculiar, actually. It needs one bend here, but let me do, maybe it will end up with something like this. And if you make, let me take a model that is really high dimensional. Suppose you took something like a fifth or 10th degree model, you'll end up with model like this. I'm just sort of conveying the concept here. So of all of these models, if you start with the first dimensional, the hypothesis model, what would you say? You would say that it is very inflexible. It doesn't have any bends in it, isn't it? Inflexible oversimplified model of data. What does this model say that the output doesn't depend on the input at all? Something like that. It's a linear model in this particular case, a linear model. You would agree that it is simpler than the ground truth. When you have a model that is simpler than the ground truth, the errors that you have are high bias errors. Are we together? You said that the model is biased. Now, without going into the definition of bias and variance, the physical intuition is if your hypothesis is simpler than the ground truth, you're suffering from high bias errors. On the other hand, when you look at the 10th degree polynomial and you are building an overtly complex model, your hypothesis is overtly complex. The ground truth is actually simpler than that. So what you have is a model that sort of shows high variance. Right, it shows a different kind of errors. It has high variance. So I'll mention this as a fact, and I'll post these notes. Overtly simple models will have high bias errors. And overly complicated, overly complicated, not overly, overly, excessively, let me just use the word excessively. That would be better. Excessively simple the word excessively, that would be better. Excessively simple models and excessively complex models will exhibit high variance errors, variance errors. And these are also called overfitting errors. High variance errors. And this is underfitting. And these words, obviously, those of you who come from a prior background with me know, but at this moment I'm just giving a taste of it and of these words under fitting errors are we together guys huh so that is the yes go ahead what happens if it's a circle a quadratic curve can describe a circle there is only one uniform bend. It just keeps bending over and never stops. Is it only one bend? Yes, it's only one bend. Think about it. You take a line and you bend it, but you don't just stop bending. You continue the same amount of bend all over. You'll come back to yourself. A quadratic curve will do it because one bend means quadratic. And as you know, a circle is described by a quadratic equation. Yes. That's it. So the bend doesn't change. It doesn't become straightened out and then become something else. Thank you. Thank you. Yeah, that's one way to look at it but good that you asked that question so guys do we do we get the intuition so when you make overtly complicated model it oscillates you have a lot of oscillations and you have overfitting problems your high variance problems when you make overtly simple model hypothesis compared to the ground truth, which remains unknown, you are making simpler hypothesis and that simpler, too simpler hypothesis and that gets you into trouble. Now, with that preliminary there, what happens is, suppose you take the degree of the polynomial, the complexity here, degree of the polynomial is a good proxy for it let's say k degree of the polynomial is going like this what happens is that there are two kinds of errors the bias error let's say the bias error will keep going down so this is the bias error bias errors and the variance errors is it tends to go up in the beginning very low variance and then it tends to go up like this so this is variance and this is a famous curve that you find mentioned everywhere bias variance so that the total error that you see in the model is the compromise between these two. It's somewhere like this. If you add up a bias plus variance, the total error, e total. Actually, it isn't just bias plus variance. There is also a part which you can't help, which is the epsilon. Does anybody remember what we used to call this? What is the other mysterious thing there in the equation? Irreducible errors. Excellent. It is the irreducible error. Where does the irreducible error come from? Where does the irreducible error come from? The factors that are not included in the model, like the features. That is right. It has two principal causes. Instrumentation. Roughly speaking, you can put them under two buckets. But they are either instrumentation error. No instrument is perfect. So if you measure the temperature, you get different thermometers. If you are really measuring, being very precise, it will give you different values. If you take a very sensitive thermometer, and one of those laser thermometers, you try to measure temperature of something, it may literally give you different values from second to second. And so there are instrumentation errors. Then along with instrumentation error, I'll also include the human error, some typing errors people make or people do all sorts of things. Then the other error is more genuine. It comes from that which you don't know. So when you say that the amount of ice cream sold on the beach, you try to parameterize it with or model it with the temperature on the beach, the wind speed. Let's say you take only these two things. Then there are factors that affect it, but they're there. didn't take for example day of the week whether it's a weekend or a weekday holiday or not right so what will happen is if you're building a model just with temperature and wind speed for a given temperature and wind speed all of these two being constant you would still see a variation of values by values amount of ice cream sold values. And that will represent whether it's a holiday or not, or weekend or not. It may represent whether or not it is going to rain, right? And other factors, right? There are many, many factors that may play. So you always have the irreducible error that sort of gives you a clue that there are things you don't know about the model. So the bigger the irreducible error, the more you should acknowledge that your model is the best you can do, but there is a lot you don't know. Irreducible errors, yes, so that's one way to look at the irreducible errors, I'll get together. Once you have taken care of the bias way and straight off, in other words, you try to build a hypothesis that is pretty much at the level or close to the ground truth, then you have the least error except the irreducible error stays. So with that as a background, I will come back now to our notes. It's a pretty long preamble. One question to ask is, well, let's take one dimensional case. Isn't polynomial regression enough? If we can describe every data distribution with a polynomial, then why do we need the rest of machine learning? It's a puzzle. Who would like to answer that? For very high dimensions? Forget high dimensions. See, you can generalize from single to high dimension. The same argument applies. So stay with one dimension and say that suppose all data was one dimensional like one target variable and one input variable yet why is it that we need other algorithms at all we can just start with polynomial regression and be done with it but classification clustering those cannot be. Well, let's only talk about regression. Let's keep classification and clustering aside. Probably less explainable. When the data is discrete, discrete like 0, 1, 1.5, not connected. Okay, the problem and remove the non uh complex the the irrelevant complexity history the quantity is a real number and the output is a real number there could be non-for non-polynomial problems excellent could you elaborate on it a little bit more uh you cannot uh not represent by a polynomial equations. What are those? What are those distributions called transcendental functions? So guys, there is a there's a very interesting history to this regression. To the best of our knowledge, it started with a mathematician called Carl Frederick Goss. Who was a child prodigy. He was the son of a very poor miner and his father believed that life is hard and you should spend your time working in the coal mines. This boy was obviously built of a different fiber. He used to like numbers and things like that. And then he was, fortunately, a teacher who really liked him, went to universities and said, there is a child you need to take over and he came. So anyway, that was Carl Frederick Gauss. And there's a very interesting history to how regression came about. I'll just sort of tell you in a minute. What happened is that there was some planetoid or something in Europe that was supposed to come into the sky that was behind the sun and it would show up in the sky after some years. What you had to tell is where in the sky to look for it and when, which day, what time should you look for it, when will it appear. So science used to be patronized in Europe through these competitions, just as we have Kegel competitions. So a lot of people put in their submissions of where and when the planet, I believe it's a planetoid that would show up. Then Gauss also made his submission. And when Gauss made his submission, his submission was practically when Gauss made his submission, his submission was practically on the dot and everybody else was wildly off, completely wildly off and all the other great mathematicians were off. So then obviously, Gauss actually wrote a paper about it in 1793. When he did that, Gauss used to be a guy who used to hide some of his best tricks. So this principle of least square, which is the foundation of regression, the mathematics of regression, he was at a quay about it. He didn't reveal it quite clearly. He sort of alluded to something and so forth. Then in 1805, a mathematician named Legendre, he literally wrote the landmark paper whose title was Li-square principle, a method of Li-squares to solve problems. When you solve the problem using method of Li-squares, you're essentially doing a regression or in the language of machine learning, you say it's the last function. So when Legendre came out with it, then came Gauss and he says, no, wait a minute, I discovered this first. And Legendre said, well, I didn't read your notes. I read it independently and I did it. So that battle raged for many, many years. Very recently, a great sociologist of science and statistician, Stigler, he resolved the debate ultimately by finding that the method of least squares was discovered actually independently by three people. Gauss was probably the first, then came Legendre, and after Legendre and independent of him was yet another mathematician in the US who discovered it. So anyway, that was that. When the method of least square was discovered, linear regression, what you would today call linear regression, then people generalized it to polynomial regression. That seems amazing. You remember those were the days without computers. So all of these computations had to be done by hand. Imagine, right? You would gather people, mathematicians would sit and tediously do iteration after iteration to solve the problem 200 years ago to 300 years ago. So then, 300 years ago, so then they were doing polynomial regression with quite a bit of success, except that they realized that polynomial regression does not solve a large number of problems. It solves some, but not a large number of problems. And the reason for that is that there are inherently functions that are called transcendental functions, whose very definition is that if you try to write them as a polynomial expansion, and there's a word you may hear that any function can be written in polynomial basis, it's called the Taylor expansion. If you do that, a Taylor expansion comes from calculus. Some of you may vaguely remember it. If you don't, it doesn't matter. But just take it that any function can be expanded into a polynomial equation. But when you try to do it with these so-called transcendental functions, the most easy example of a transcendental function is a sine wave. Sine or a cosine, right? Think of sine wave. Then what happens is your polynomial will end up with infinitely many terms. So it will take an infinite degree polynomial to represent your function uh now i told you that infinite degree polynomial means there's a lot of bends in it right and in the sine wave of course you have a lot of bands now in many functions you can't quite see the bend that the bend or the slope is changing but it is changing all the time right so the bends are moving around basically in the slope so it turns out that those transcendental functions are everywhere, the bell curve, the sine wave, and so forth. In fact, nature seems to be treating, just as we use the English alphabet as a basic building block for words and sentences. One of the most remarkable things about nature is for nature is written in the alphabet of transcendental functions. Just amazing. You look at an atom, the orbital structure are actually the harmonics, the modes of spherical vessel functions, right? You beat a drum, those are Legendre polynomials. You see, you throw a stone in the river or in the pond, the ripples will form a sine wave-like behavior, but the amplitudes will decay like Legendre's and so forth. So all around you, you know, people's heights, if you put together, they will show a bell curve distribution. Many things show a log normal distribution. So there are all of these distributions that are everywhere. And one of the things that I tell people in almost every workshop I do is that, if you can, make friends with transcendental functions. Take every month, pick one transcendental function and become familiar with it and see where you use it. These things are just magical beasts. For example, one transcendental function, some of them, one of them is called Weibull. Weibull is the foundation of reliability engineering, Weibull and Gompertz and now so forth. But one function Weibull that was discovered by literally a guy named Weibull, many years ago he submitted it to the Royal Mathematical Society saying, I humbly submit this function, which I hope is of some use. It turns out that all of, much of reliability engineering uses variable functions. The reason that you can travel today from here to Europe or India or China or Korea or anywhere, and most of your worry is about whether you'll get a middle seat or a good comfortable aisle and window seat. You don't worry about the airplane falling from the sky. And yet an airplane contains millions of parts. It still reliably reaches the other end year after year. In fact, it is the most reliable mode of transportation. It is all because things are steady. There's a lot of reliability engineering that goes there. And behind a lot of that mathematics, goes there. And behind a lot of that mathematics, behind quantum physics, behind all of nature are transcendental functions. So you would expect that when you look at data, you should suspect that transcendental functions are there somewhere in the background. One of the things you can do as a data scientist is you can often walk into a situation. If you know your transcendentals, you can spot it. And when you spot it, you can say, why not approximate? Why not model it using this transcendental functions? Now, when you walk into a crowd where people have just picked up from a programming book, machine learning and AI from a programming book doing R and Python and I don't know something. You will literally see them think of you as a magician because they have been struggling with the problem for quite some time. You walk in and in the blink of an eye, you make a hypothesis and that solves the problem. When it happens in those situations, when when it does happen those are really moments to change you just feel great and because you can walk in and solve a problem so know your transcendental sense so that is one thing i would say now what i would like to do is finish this particular quiz uh give me one second okay now yes uh i i had a very, it's a very interesting discussion what we had. I had a very different question along the lines of the course that you just mentioned that, you know, any of the models is pretty, probably pretty close to the ground truth, may not be accurate. It takes me to the question actually I had this mind I hold it for a long time. If we have AI now and we don't know any of the physical laws, would it be able to derive all the physical laws, all the theoretical physicists who are sitting and getting the equation using the, without much data like Einstein, would we be able to? Is a very good question you asked. What you're basically saying that just given data, the old way used to be scientists would study data, build hypothesis, they would write equations. And then from those equations, they would, those became the effective models to make predictions. So if you really think of the law of gravitation, for example, what is it? It is an effective model based on the data. Kepler's law. Kepler was, I would often think of him as the original data scientist. He inherited the data from his great teacher, Tachy Brahe, the astronomer. This was the day before, so this answers to your question. Tachy Brahe was an astronomer before telescopes came about. He would look at the sky with just sextants and so forth and actually now we don't talk about that but the bit of a history is that Kepler he had an amazing life but actually at night he used to be a bit lazy. So and stars come out at night especially at winter nights. Winter nights in Europe are cold. And Kepler used to drag his feet as a student of, as an apprentice with Tako Brahe. Tako would always tell him, you know, you need to be a little bit more willing to go out at night in winter. So anyway, Kepler helped Tycho Brahe and inherited all his data. Today, then he studied that data and he studied and studied and built models. He was your original data scientist. And he came up with the so-called Kepler's laws of motion, which he said that sun is at the center. It is at one of the focal points of an ellipsoid. And all orbits of planets are elliptical orbits. And ellipses have, of course, two focal points. Sun is at one of the focal points. And the area that when you connect to the planet and to the sun, the line, the area that that line sweeps per unit time remains invariant. Those are the two Kepler's laws of motion. And how did he come up with? By studying data. He wrote down this equation. How would we do it today? If you make a black box model, like a DPI model, will it be able to predict that? Yes, it will be able to predict that. And this is the problem we are facing today has it advanced science at all has it advanced your understanding or given you a clue on the causative forces it has not done that you need to ask that ai to explain itself right and then you may have some understanding of what is really going on.: Explanation is lacking. And this is an important point because in the hubris and excitement these days around AI, people are actually jumping around, especially in Silicon Valley, making bizarre statements that science, the old science is over, the only science that matters now is AI. I don't think that is true. Just be warned that AI can do a lot of things. It is a tool to assist you in science, but it does not replace science. Because science is about understanding the causations. Where is understanding in all this? But it's a tool. It's a very valuable tool. All right guys, so this is it. Now we'll quickly run through it because I know lunch is waiting for all of you, it's close to one o'clock. So we'll take the second quiz after the lunch guys. Imagine you have built a model that underfits the data, likely because your model is simpler than the underlying ground truth. Which of the following is true about the errors in the prediction. So the models will exhibit high bias errors, as we just discussed. Question number nine, imagine you have built a model that has significantly overfit the data. Which of these would you likely see? High variance errors. Which of these are regression tasks? This is very easy. I'll just leave it to you guys. I think most of you got it right. If it has to do with numerical target variable, it's a regression task. And that folks ends our first quiz. Let's take a lunch break. Let's regroup at two. We'll start with the second quiz and then we'll go to the Python. Those of you who are interested in learning Python. Same length. I will end with one statement and then we'll go to the Python. Those of you who are interested in learning Python. I will end with one statement, guys. See, the world has heard of Kepler, right? But Kepler stood on the, his data came from his teacher and his master, Takobrahe, right? Kepler was a predecessor of Takobrahe. Out of astronomy, most people have not heard of Tachybrae. And this is true in artificial intelligence, and I feel it's very, very true. The majority of the grunt work is in gathering data, cleaning data, preparing data that often occupies 90% of your time. That is the hidden work that people don't see. What we see in research papers, what we see everywhere talked about is finally the last 10% of the work, the AI algorithms and so forth. You guys are learning the AI algorithm so it is all right, it's good to be excited about it, but don't forget the hard work that goes before you can feed your data into the AI AI algorithm still a very laborious and manual task and it is as worthy of recognition as Taka bra he was along with Kepler with those words I'll end today's first quiz Let's take a break for half an hour and we'll regroup. Asit, since you finished, I was about to say something else because the discussion about transcendental equations came up. And it's very interesting learning that nature represents itself as a transcendental equation. Is it just because of those transcendentals couldn't be expressed in, could not be modeled by these ai models that you know we take that as the physical representation and then when we know like for example the weibel equation or something it's coming from the least uh i mean the weakest link failure model so we take that as the basis and then we apply on top of it collect the data fit to the Weibull equation that is the data science part that is data science you do feedback in fact reliability engineering machine learning aspect is fitting it to these models Weibull, Gompers and so forth so the Weibull is something that a model would not have given right so we see the thing is you take a sufficiently complex neural network it will internally model it as variable it will it will learn the variable but it won't be able to tell you that because the way it has learned is so very complex it won't spit spit out the Weibull equation for you. In other words, it doesn't produce a closed form analytical equation. It just does what Weibull will do. It will make predictions accurately. Oh, so you want to know the physical reason why it aren't? Yes, you want to know that is the problem. And that is the problem with AI at this moment, which is why if we could have it explain to us what it did, it will actually help us further science. I think there's a whole book by Weibull actually, I was reading that because I'm from the reliability engineering field and it's a Bible for us. Oh, excellent. Sorry, I can't from the screen see who you are could you name yourself Balaji you have never come to my office have you at support vectors yes I have come once and I started so you probably haven't seen I have a shelf full of Webel and Gompers I think the shelf behind you showed me, on the right, I don't know, one of the places I saw that book. Yes. I'm very, very fond of Weibull. It has a lot of applicability in survival analysis, cancer and so forth, and many places. You call those models accelerated failure time models. You're probably aware of that. Yes, yes. Yes, excellent. Very interesting. Thanks for the explanations today, Asim. Yes, you're welcome. See, I'll just, since you asked this question, see one of the most amazing things, and Einstein used to say this, that the most incomprehensible thing in the universe is that we can actually comprehend it. We can actually understand it with mathematics, right? With all these transcendental functions and so forth. Now, why in the world can we describe the universe so accurately with such uncanny accuracy, with simple transcendental functions, all of these equations and laws of physics, and, you know, they are very simple and yet they can describe the universe. It's the most amazing thing. And the same applies to Vaidhyanathan Ramamurthy, AI when you come here, you'll see that that that AI architecture, the kind of mathematics that goes into it is very trivial some amount of linear algebra, a little bit of probability theory. Right. And what that's about it. And what? That's about it. Some vector calculus, some basic calculus, linear algebra and probability theory. That's all that goes into it. And yet it is able to do such wonderful things. That's what we looked at. It can translate languages. It can, you know, recognize pictures. It can do all sorts of amazing things today. So that's the amazing thing with science and knowledge. All right, guys, I'll stop this session. I'll stop the recording and stop live streaming. Hy subscribe cho knh La La School  khng b l nhng video hp dn you