 All right guys, so if you visit the YouTube channels or the support vectors channel on YouTube you will see the live stream which is live now so you can watch it from the comfort of your living room couch watching TV with that then I'll start the recording now give me a second I'll share the screen. All right, folks. So last time we learned about clustering. Clustering comes under the class that is called or the type of algorithms that are called unsupervised learning. In other words, it is more about pattern recognition rather than predicting something. So there is no training data in which you say okay these are examples of cows and these are examples of ducks and generalize you instead get just data no labels and you're just supposed to find some interesting patterns in those in that data so just as a quick review suppose data comes to you like this and it may so be that they're about cows and that's but you're not told that and you have to just observe that actually data seems to be clustered into these two points now we learned about three clustering algorithms k-means clustering hierarchical clustering and density-based clustering the k-means clustering was the simplest one we saw how quickly it converges. You pick two, any two, like suppose you are doing k is equal to two. You want to find two clusters. You just pick two random points, declare them to be the centroids of the cluster, and then you assign all the points to the nearest cluster. Then you, the third step is you find the centroid of those clusters and then you repeat that process two and three and surprisingly in a very quick number of steps if your data truly has good clusters you will converge. K-means clustering is very popular. It is often used at huge industrial scale. For example, people apply to huge social graphs and things like that and big data scales. So it is perhaps the most widely used clustering algorithm and given its simplicity. It has a couple of limitations. One is you have to tell how many clusters there are. You have to guess it. And if you don't know how many there are, what you have to do is you have to tell how many clusters there are. You have to guess it. And if you don't know how many there are, what you have to do is you have to repeatedly do the algorithm for different number of clusters that you may guess, and then you look for the elbow in that. So it can become computationally intensive. Now there are a couple of other limitations. Sometimes you may accidentally start with really bad initialization points so which is why they always suggest don't run k-means clustering only once even for when you know that you need three clusters run it a few times and pick the best one the best one and there is a measure the measure is within cluster some square distance wss is within cluster some square distance, WSS. That's a measure of coherence, cohesion rather, how good the cluster is. So that is a recap of what we did. Then we had the agglomerative clustering, which is computationally much more aggressive. It starts with the assumption that each point is its own cluster. And from there onwards, it starts rolling up, it starts agglomerating or accreting those points into little clusters, into bigger clusters, and so forth. So you can decide in one spell stroke, you get all the values. You want two clusters, three clusters, four clusters, whatever number of clusters you want. You can just cut through this so-called dendrogram and you will have your clusters. Very good visualization. People often use it in many, many fields, especially in the biological fields. You see it used extensively, this kind of clustering. Now in this clustering, one of the questions that happens is we have a definition of distance between two points, but how do you define distance between two clusters? Well, it turns out that it depends upon the situation. There are many, many definitions possible. You could take the shortest distance between two clusters, you can take the largest distance, you can take the distance between the centroid of the two clusters, or you can take the largest distance, you can take the distance between the centroid of the two clusters, or you can take the average of all the pairwise distances and so on and so forth. You can come up with more and more definitions. Each of those definitions of distance between clusters is called a linkage function. It gives you, and you need a linkage function it gives you and you need a linkage function using different linkage function leads to slightly different clusters and you have to sort of try them out and see which one you like best then comes another clustering which is actually quite effective Now the clustering like K-means and hierarchical, they suffer from a limitation that your clusters must be round, you know, they must be not round but roundish. They must be sort of, they must have sort of a convex hull around them. When you sort of globular clusters, there must be. And if they are not, as you will see in this lab, they soon get into trouble. So there are many other kinds of clustering approaches. One of them, very powerful ones, is the density based clustering. It basically makes a very simple hypothesis that clusters are dense regions, whereas irrespective of their shape and right. So let's go search for dense regions in the feature space and wherever the density maximizes, that's your center of the cluster. In some sense, so that's the two sort of approach. Well, two sort of approach we take. One of them is the DB scan, which is based on the argument of reachability. You create a certain radius and you say that if within this epsilon radius of a point, if I can find n neighbors, let's say 10 neighbors, then I'll declare it to be an interior point. Otherwise, I'll declare it to be an exterior point, I mean, an outlier, right? Now, think about it. Does it agree with common intuition? Let's say that you look in Bangalore and let's take a neighbourhood like Kormangala. It's pretty densely packed. So wherever you're standing, would it be reasonable to say that within a 10 minutes walk, you will meet at least 10 people? On a day? You would probably, you'd probably meet many more people. So, therefore, it is the interior of Bandalore. Whereas, let's say that you go onto the Mysore Road to some, you know, two, three hours away, and then way out in the, under a tree you're sitting in a rural area, then if you walk for 10 minutes in any direction, it is quite possible that you may not meet too many neighbors and therefore that's a classic definition of an outlier. If you were to build your house there, most of your friends would say, oh, it is so far out from the city, isn't it? City being the cluster, population cluster. Do you see how well it sort of agrees with human intuition? So using that approach, you can now have two points you can reach. So you take a point, you create the notion of directly reachable. Two points are directly reachable if and only if one is in the epsilon neighborhood of the other i apologize for this the two points are reachable at all if you can reach either directly or through intermediate hops you you have somebody in between who is reachable, greatly reachable, and from that another person is greatly reachable and so on and so forth. And ultimately you reach the other point. So the way this algorithm works is that you first randomly pick a point, keep on sampling the data, till you hit upon a point which is an interior point. Now once you hit upon a point which is an interior point now once you hit upon that point which is an interior point you start finding all its readable points so suppose you started the red one and you start searching for its reachable you'll soon end up finding the red cluster and if you pick a yellow one here and you will end up at all the points will end up picking up all the points from you. Yeah, so you'll end up picking the yellow cluster. So that's the thing and whatever remains becomes the outliers. So density based clustering has actually one benefit, which is very, very useful. One of the most significant useful one of the most significant one of the most useful tools of applications of machine learning is in finding outliers outliers are anomalies anomalies are important because if you want to do a fraud detection it is anomalies in some feature space that that behavior of, let's say, credit card use or whatever, you can tell that this is fraudulent because it stands apart or anomalous from other observations. So. Many think network intrusion, medical, for example, if somebody's biological or biometrics are completely off, you should be able to tell. So now, while dbScan is quite good, unfortunately, it suffers from two limitations. One limitation is it's very sensitive to the hyperparameters, namely your epsilon radius and how many neighbors you must have within that radius. your epsilon radius and how many neighbors you must have within that radius. So because it's so sensitive and if you play around with those values you can end up with entirely different clustering. So it was deemed as a limitation of dvScan. In practical terms it's not that big a limitation actually because soon you come to know your data and you soon come to have a sense of the definition of your epsilon. Sometimes it's hard, but quite often you are able to do that. So dbScan actually is very widely used and very popular. Nonetheless, in principle there's a limitation, but the most significant limitation of dbScan is that it doesn't scale very well with large datasets. So given very large data set, it runs terribly slow. There are some enhancements to dbScan, for example, the optics and so forth. We won't go into it. The next sort of a station in our journey, if you say, the next algorithm that we covered the last time was Denclue. Denclue take some more hill climbing approach if you remember if you think of each point as a little lamp then what you do is you start from any point in the feature space and you go in the direction in which if you take that step, the light for you increases the most at your point. So you take the path of the greatest increase in light, if you want to think of it like that. And so it's a hill climbing approach, gradient descent, and it will take you to the brightest points in the cluster. The brightest points, the points at which the lights are really shining is sort of like a pole star there guiding you forward attracting these things are called attractors so you have found the attractors just like kevin's clustering has centroid and then clue the equivalent of that is the attractors now attractors are not necessarily the centroids, they can be different. They're just the brightest point in some sense, the point where the gradient is the maximum. I mean, so the density is the maximum. So that is that. Now, so that was a summary of what we did the last time. So how do you find clusters in Dentloo? After a little while, you make a circle and you say that the density must be at least this much, otherwise it's not part of the cluster. Otherwise it's too far off and the rest of them are outliers. So that is density-based clustering. Any questions before we do a lab on this guys one small doubt so this dental uh we are saying like we are not going with that like uh density or something like uh where we have centroid kind of thing right so uh light or gradient whatever we are trying to get that so that is something like again it will be lighting more where we have density more right so yeah yeah it is a density argument not a centroid argument see the centroid may not be the point of your greatest you know the point at which it achieves a maximum of the field actually see it is called the think of it as a sort of a light field so you're finding that point where the light is the maximum okay okay so and paradoxically that may not be either the how should i say it may not be the densest point or it may not be the centroid but generally is very close to that so that's okay thanks so there's a bit of mathematics there's a people who are into calculus or mathematical thing they will talk of a great, they'll talk of a field, a potential field. Right. And they are looking at the point at which the field achieves a maximum. So that's how people think about it. It's a bit mathematical. I didn't introduce the mathematics here, but probably not worth it. But the mathematics doesn't add any more to the meaning. The most intuitive way is to think each point is a little lamp and it's radiating light. And so you're moving in the direction of the greatest light. See, I don't know if it is true in India still, but at one time when I was there young, and India was mostly rural and the cities were far off. So when the trains in the night, they would be approaching a city, you could literally see the glow of the city in the air from far off, like an hour away you could see, or 45 minutes or something away, you could see a little glow in the sky and the glow became brighter and brighter and brighter and you got the sense that your train is heading straight towards the glow, right? And the city is your cluster. Whenever I think of density-based clustering it reminds me of that. Alright, so with that being true, let's do a lab and see taken a data set which I created to illustrate the point. So imagine that you, now let us look at this. By now this must be looking very familiar to you, isn't it? Imports, NumPy, Pandas from the previous lab workbook. So we are reading a data set called, let me make the screen a little bit bigger. Is this more readable now? Yes, sir. Yeah, so what are we doing here? We are reading this smiley.csv and we look at it and we notice that there is X and Y, right? And by the way, the P here you can ignore. What I have done is I've given a hint that it is the name of the cluster from one to seven. But the data is actually just x and y remember it's unsupervised learning there's no label. So when you look at the data only at the x and y you have well i seem to have 30 94 points if you look at the ground truth what does the data look when you visualize it so what does it look like this is what it is yes smiley isn't Yes. So I literally sat down and created it. Now, if you look at this data, you see that there are outliers. There is eyebrow. The eyebrow cluster, does it look roundish? It is not roundish, isn't it guys yeah it is a concave kind of a shape crescent shape yeah but the eyes are globular they're roundish now the nose looks a little bit triangular but still convex more or less but the smile what about the smile itself the lips it is also crescent shaped right it is not at all convex so it is not at all roundish by any means, globular by any means. So we suspect that k-means may not work, and let us go find out what happens. What we do is we will run k-means for many values of k-means. And the inner code is very simple. Do you see this? k means fit. And by the code is very simple do you see this k means fit and by the way they use the word predict but predict just simply means you're telling what is the label associated with it will just go and apply a label to the cluster but in any case we plot it this is a little bit of a code and what am i doing i'm taking up to 16 clusters right and then you come to this picture which just for fun I will say we'll look at it piece by piece so k is equal to let's say k is equal to one when you look at it maybe I can make it even more can Can I make it even more? No, this is about as good. So when you look at the first picture, it says that everything k is equal to means all the data points are one cluster. But that doesn't give you any information. What happens when you try to find two clusters? Do you see? It is like below the nose, a little bit of the nose, below the eyes it's all yellow, is like below the nose a little bit of the nose below the eyes it's all yellow and above the eyes above like you know eyes and above are another cluster it have colored the clusters differently so would you think this is good clustering yes better than first one it's probably better than first one but it doesn't you know because you you're not completely yeah it is not capturing the clustering then you go to k is equal to three well now one eye and eyebrow are one cluster another eye and eyebrow another cluster the nose and the lips are a cluster and if you really think about it this is the best you can do are a cluster. And if you really think about it, this is the best you can do. Isn't it? So this at least is reasonable. Now you go to K is equal to four. What happens? Things begin to get a little bit more interesting. Now each of the eye and eyebrow are a cluster. Nose seems to have merged with the half the lips and a tiny bit of the nose and the other half of the lips are together, another cluster. So it does seem wrong. So k is equal to four is also not perfect. You go to k is equal to five and what do you notice? Now things get more interesting. Eye and eyebrow are one cluster. This left and right are different. Nose is a cluster in its own right. The lips have broken down into two clusters. And as you go further and further, you start noticing this pattern that the lip now has become three clusters for k is equal to six. For k is equal to seven, the game becomes more interesting. Now if you look at the left eyebrow and the left eye, they are coming across as two different clusters. Do you see that guys? They're being represented by different colors. But on this side it's not. But if you go to this one, k is equal to 8. Now, each of the eyebrows is a separate cluster. Each of the eyes is a separate cluster. Nose is a separate cluster. So except for the lips, it looks perfect now, isn't it? But the lips is a disaster because the lips has been split into three clusters, which it is not. It is just one cluster so think about this if the lips were not there this clustering would be just right at k is equal to 8 you would be pretty happy wouldn't you yes now let's go to k is equal to 9. well now things begin to get bad half the eyebrow this eyebrow has split into two clusters lips of course are two clusters here and k is equal to 10 gets preposterous now even the new nose has split up into two clusters right and k is equal to 11 gets worse and worse. So you expect that somewhere around K is equal to, maybe K is equal to eight is about somewhat acceptable in this list, right? So now let's draw the scree plot, you know that the Elbow plot and see where it takes us. So remember there were two possible answers, K is equal to eight we liked, and then the other one we liked was k is equal to three do you think that this is at least his eyes are separate clusters nose and the nose and the lips are one big cluster isn't it maybe this was somewhat reasonable uh four is not good. This is not good. Let's see what the scree plot says. So now I'll just shrink all of them so you can see all of them together. Yeah. I think a little bit more. Now observe this part. Look at this picture here and look at this. Where do you see the elbow, if at all? Between 2 and 4, sir. Yeah, between 2 and 4, that is 3, right? At 3 there is a possible elbow elbow but not a very clear one there are actually no clear elbows in this when the elbows show they really show if it was your arm you wouldn't want your arm to be bent like this isn't it there is no clear elbow to this arm and when you get this smooth sort of a thing with not so clear elbow, it should be a hint to you that maybe K means is not doing so well. Are we together? So it's sort of a smooth, you have maybe a little bit of elbow at three, which makes you think that K is equal to three was somewhat reasonable. K is equal to eight was somewhat reasonable, but it is here, no clear elbow. Five K is equal to 8 was somewhat reasonable, but it is here. No clear elbow. 5 K is equal to 5 was this. The lip was split into half. Not a good idea either. So this is a clear sign. When you don't find an elbow clearly, you should actually assume that something is wrong with the data. With your algorithm, your algorithm isn't quite working, K-means. Now, this is a point, actually, I don't usually see mentioned in books. So I thought I'll tell you that this is from practical experience. This is how I usually get a sense that something is suspicious and the algorithm isn't working out. So now let us see what happens if we try a different clustering algorithm. How does dbScan do? Where is dbScan? Okay, ctrl shift alt alt yes db scan back oh now db scan is there we go dbScanNetbook. And so. So now it's the same data. Let me increase the font. I'm not doing anything new. I just picked it up. The same data I read XY. The ground truth. This was the ground truth. But now look at the code. The code is very simple. I just fit one line of code will do it. It's the beauty of the libraries. These libraries, machine learning libraries have become very, very powerful these days, most of the work you do with just one line of code or two lines of code. So it is not a very programming heavy field. It is like, and all this code you soon become familiar with and you start doing it easily. So when you look at this and then the rest of it is just visualization. Visualization code paradoxically is more complicated. Just showing it on a screen. You have to know a little bit of Python to properly do it. But if you look at dbscan, the real code is only this one line. The dbscan algorithm, let me highlight this. Just this one line, do you see that? The first line, that is all. The rest of it is just, you know, trying to visualize the data. It turns out it's a little bit more complicated. So now look at the clusters it has found. It has found six clusters. Do those clusters look right? Yes, sir. Yes, they do look right. And that is the power of dbScan. It is a little unfortunate for density based clustering. It's a little bit unfortunate that this clustering is usually you don't find it covered in textbooks because in practice, methods like this are very, very effective. And that's what you use a lot. In fact, my go to methods has often been I'll try k-means just to see, but the moment I noticed that there's no clear elbow, then I will, and even if there is, I will always cross validate. I'll try two, three different clusters, different algorithms. And in the algorithms that I try, k-means will be there, but db's density-based scanning would always be there. Because it just sort of works and when it works it works like a charm you saw how hard it was for k means to make head and tail out of it now what are the other benefits you have of of of this is it has also found the anomalies you know these pimples these outliers or anomalies on the face it has it has been successfully able to identify those also all the outliers and well that's your lab on clustering what i will do is let me post both of these files on your should i post it on the slack channel guys files on your should i post it on the slack channel guys i'll post all of them on the slack channel and once i actually let me do it right now otherwise i may forget uh give me a second uh this is it uh uh and where's the slack channel here it is give me a second i'm doing it so you guys will see it in your message board in a second son what about the hierarchy cluster it is not necessary for the lab here no you can do that so i'll leave that that was the point uh i leave that as the homework so i gave you two you try the third one on your own on this data set and see how it works so uh and that is a little bit of exploring but you notice that the code typically is couple of lines of code that's all so now i first play with the k-means and play with the db scan and then go to the psychic learn library and see how you do hierarchical clustering it's very easy but you learn how to read you know read up the documentation and figure it figuring it out on your own and that will be worth it for Okay. Okay. Okay. Okay. um yes All right guys, see if you receive the three files on your Slack channel. And by the way, the hierarchical clustering, I left it out because it is like the scikit documentation is very good it has given you practically the solution with examples but you should read through that right so it is quite easy actually sir go ahead one question about the k-means clusterustering. In the theory we learnt that when we initialize centroid we should be very far from each other because we had one problem where centroids are parallel and we are not getting proper cluster. So is it something we have to do programmatically or it is taken care internally in the algorithm? Actually internally it takes care of it okay and nonetheless the recommendation is always run not one clustering so there's some there's a parameter called m tries how many times you try to cluster hang on let me show you that This is the ground truth, this is the data. So when you see, I'm saying ncluster, but here right here in the constructor, you can even tell that do you want to try a few times and pick the best one. So you in these libraries you get a few things but if you don't give it and it just sort of picks up does it a couple of times it does the right thing. Usually when you use these libraries they're pretty smart they try to avoid the mistakes. The scikit-learn is a pretty well-established library, and you should use it extensively for your learning data science. Panda, scikit-learn, and NumPy, and matplotlib to make your figures. These four libraries if you pick up, and all of it put together, it's not too much of knowledge. I mean, it is just a couple of months of study. And then it's quite literally it takes you to a job and i don't know how other jobs pay in india but data science jobs pay i mean typically they pay in the range of somewhere from 15 lakhs a year all the way up to like 70 80 lakhs a year depending upon how good you are, 1 crore a year depending on how good you are. And getting started with that and I don't know how much does it, so let me. In the schools, like some kind of state level syllabus, maybe 35. And government school will just give you three, four lakhs a year, isn't it? Yeah. Yeah. So relatively think about the data science you just put into three. I've talked to you now. Just review it and read that book and practice this thing. And after that, you will get a much higher paying job. Really, people start out at 15, 20. Even a lowball, even if you say, oh, I am a fresher and I'm just entering the field, I don't think anybody would offer you below 10. I don't know the job market very well there because I tend to hire people with some experience, but you guys can tell me what the entry level data science salary is. Does anybody know there in Bandalore? Anybody? Data science side, we don't know, but where I'm working on that, like I'm working on the CRM, I know the CRM side. How much is it for CRM? For Fraser, depends like company-wise. So you will get four to five, or you can say that, I can say seven to eight something if you go to that good company. That's right. Actually, data science is a little higher than that. I'm pretty sure because we when we hire we pay our data science people more than that certainly so all right guys this is it you know you learned about clustering you learned about classification you learned about regression and now is the time i introduced you in a way i hope that you found it easy and very mindful that many of you came from non-technical backgrounds i hope it was an interesting journey i hope you learned something from it and this is it at this moment uh i invite, this was the last of the topics, you now know all the topics. I invite you to keep attending. I give a lot of free talks. And you'll start receiving over the holidays some of the other notifications. So stay in touch. This is it. So did you guys find it useful, this workshop? Yes, sir. Follow through with this. I made it as easy as I could. I deliberately avoided going into very hard stuff. But if you follow through with it, it's very, very good for your career. Go ahead, Avilai. Asif, I have a question. I have some data with the labeled and I have some data with unlabeled. Okay, can I combine both the un combine both unsupervised and supervised learning? Yes, yes. You try. See, when you have a data, you throw everything at it. You throw everything at it. See, at the end of the day, what will happen is you'll become very good with all these algorithms. So think of a master craftsman. Think of a carpenter who has all the nice tools, sharp tools, and he's very good at it. Now, what is he waiting for? He's just waiting to buy wood and build something. He's waiting for a customer to come and say, hey, build me, I don't know, build me a table, a beautiful table, isn't it? And then there he would go because he know exactly how to do it. So in the data science, it's a craft. Once you pick up the craft, then the moment you get data, you apply everything you can, every tool that you can, you apply and see what comes out of it. Every relevant tool, not meaninglessly, but everything that's relevant, you apply it because data always has a story to tell, guys. So the big message is data doesn't exist in a vacuum. It's not dull, dead, you know, thing. Behind data is some generative force that produce the data. You get engine data, you know, there's a, there is the whole mechanical engineering of that engine, the whole thermodynamics and mechanical aspects. So suppose you get vibrational data, there's a whole physics behind it. And so that data has some interesting stories to tell you isn't it and if you don't know what the dynamics is behind then from the data you can start working backwards you can start inferring what the dynamics could be isn't it you start getting an inkling and that is the journey of science, to be able to generalize from the data and come up with narratives or explanations for it. So, that's the way you should look at data. It's very live. All data has a story to tell. The only thing is, can you find that story buried in the data so all of these words you know the tools in the beginning supervised and supervised classroom classification occasion it is all right to get started but gradually as you become good the only question you ask is given any data set in the beginning it's a strange thing it's like you know you you as college students or as a fresh eyes, sometimes when you're in your bachelor's, you want to save money. And so you get a roommate, you know, you take a house with two bedroom, one bedroom. Two people will take the house. One person will take one bedroom. Another person will take another bedroom. Do you guys ever did that? Did you guys ever do that? Or in hostels, sometimes you're supposed to share rooms or things like that. So what happened sometimes the first time you meet a person, the person is a total stranger. You have no idea what how the person is close by and you have to work. Is this person shabby and he really just mess up the house and you'll be cleaning have to work. Is this person shabby and he really just mess up the house and you'll be cleaning all the time? Is this person neat? How is he? Is he noisy? Is he quiet? And all of that affects your own ability to sit and study in college, isn't it? So data is like that. It's like you encounter the data very much like you encounter a stranger that you have to spend some time with. Gradually, you make friends with the data. You come to know what the data is about. The more you know a data, the more time you spend with the data, the more you know about it and the deeper insights you can have. One analogy that I give to people is, suppose you meet a person, you're young and not married yet, and in the US there is a custom of dating. People just meet over a cup of coffee and talk to see if there's any chemistry, if people are aligned, and they will always meet at a neutral place, they'll meet at a coffee shop or go to a movie together or something like that. And then if you don't get good vibes, then that is it, it's over. You don't follow up with that. But if you like the person and you do follow up, and so what happens, the first time you meet a person, the person is not likely to tell you much about themselves. They'll put up a good impression, they'll be polite, they'll be nice, they'll smile and so on and so forth. Then gradually as you become friends, you come to know more and more about that person. Isn't it? I think some psychologists once told me that it takes us at least seven encounters before we trust a person. For seven times, we have to spend a substantial amount of time talking to the person. And then six, seven times, actually, I don't remember the exact number. It takes a lot of times before you gradually start trusting the person with anything that about yourself. So it in the workplace, it works like that in relationships outside in the social relationships, it works like that. You make a friend just basic a friend, you don't tell the person everything you tell you your God, it takes some time to know something very similar with the data, guys, I'm giving a very rough analogy. Data, when you first get a spreadsheet or some data CSV or in the database, all you see are numbers, a wall of numbers. And then gradually you start poking the data, looking at it from all sorts of angles, doing summary statistics, visual poking the data, looking at it from all sorts of angles, doing summary statistics, visualizing the data, building little models with it. Slowly you come to know the data very well and it begins to tell its story. And that's how it is. And that's why this field is very interesting. It never gets boring. See most professions get boring after some time, right? If you are into programming, for example, you know that you'll be working on some business app year after year after year making small enhancements to it. But data science is interesting. Every time there is new data, you spend a few months, you solve the problem, you move on to the next data set or something. And there's always fun because no matter how good you are, you'll get a data set that will tease you, that you that wouldn't easily make sense in the first few days. You have to poke around it. Then gradually you start seeing a storyline behind it and that's the fun of the subject all right guys so it was a nice uh nice going through this journey we covered three large topics with classification regression and clustering. I hope you do the labs, follow through with it. I kept it very, very simple for a reason, because I wanted you to realize how actually straightforward the field is and remove the technical details and those details you pick up as you go along. But now is the time to practice and the more you practice, the better you'll get at it. And so what about the quiz test which you told that you'll be giving us after this Varsha? Yes. So I will actually give you guys a quiz. Give me a few days. I will post it in this Slack. Keep your eyes on the Slack. I'll give you a week to do it's totally optional if you do it it's good and if you do it well i'll give you a certificate otherwise all of you um i will give you a certificate of attending and but those of you who take the quiz i'll give you a certificate of if you do reasonably well in that i'll give you a certificate of ability. Alright guys. And Sir, the topics could be on clustering and classification and then the regression, Sir. Only what I taught you. And I limit myself to only what I have taught you in the quiz. It will just review your understanding of the material I taught. All right. Any other questions, guys, before we finish today? All right, guys. Have a nice afternoon. I think one more question. As like unsupervised it is detecting anomalies, right? So we can use for the feature selection as well. See, pattern recognition or clustering, you don't think of it as means to do feature detection, but sometimes you do. See when you do, if you can find clusters in just a few features without needing the other features, it clearly indicates that those other features may possibly not be relevant isn't it you can walk around it a little bit but traditionally feature selection you don't necessarily take the clustering approach no harm in trying that but it's not common well i don't know see there's so much experimentation and playing with it see how do you become familiar with the data, right? Depends on you. You play around a lot with the data to see which fields matter and which fields don't, which features matter and which features don't. Thank you. Thank you. All right guys and then I will let you go. Suresh and Ravi, please stay back for a minute. Sir, good night. Thank you, sir. Thank you, sir. Thank you, sir. Good night. Any other session, so please let us know, sir. We are interested to join for that. Definitely. Thank you, sir. You're welcome. Thank you, sir. session so please let us know sir and we are interested to join further definitely thank you sir you're welcome thank you so much if you are not connected to me on linkedin you should go connect to me so that you can stay in touch share certain yeah Yes, I think. Harini, is all of this looking easy now to you? Yes, sir. Yes. You have made a lot of progress in the last few months, isn't it? Yes, sir. Yes. Good. Are you getting your interviews now? i have to start applying now sir yeah are you ready for your textbook yes reviewing my textbook started off with the project i'm actually going through the videos and all of from the beginning now again okay very good yeah so just a small request can you make uh all the videos of level ml 200 are available on the website because on youtube channel not all are there including the practicals what we've discussed no only the lectures i think only few of them are there did you attend ml 200 few of them are there did you attend ml 200 yeah i attended but then you have access to it because i individually gave access to us private videos no i have that access but i don't think you uploaded all the photo all the videos linked in the page because somebody asked you to take it down and then it you took it down in the on youtube i'll look into that i'll look into that yeah but they're on youtube you guys have access to it but in the course portal i have not itemized it yeah okay i like to do that okay remember that was before i had hired a videographer yes that was before but even on youtube not all videos are there okay i'll go and look at it yeah when you have the time you can have a look sir you realize that in the last uh three four months we have i was just looking at it there are 250 videos now i I've done 200 sessions. 250. Oh, my God. Okay. Yeah. So there are 250 sessions. It's been one to two hours long. So, yeah. That's the review. Yes. Every time I revisit, then there's something new that I know I come up and I have to look back and refresh. All right, Harini. I'll see you tomorrow sure sir thank you bye good night Thank you.