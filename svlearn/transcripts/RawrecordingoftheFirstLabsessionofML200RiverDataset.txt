 All right guys, so I'll get started. This is our first lab walkthrough and the dataset that you got, the river dataset, in the beginning it must have looked rather simple. There are two predictors, two features, x1 and x2, and there is a target variable, t, which is binary. It's 0 or 1. Likewise, the flag-like data set is very similar, except that it's trivalued. 0, 1, 2 are the three values that you see in that data. As if the screen is not shared. Yeah, at this moment, I'm not. I'm talking. So all right, maybe I should start sharing so that I don't get interrupted in the conversation. All right, because now I can draw it out and explain. So this data set, if you visualize this data set and you do exploratory analysis, which I hope that far you all reached. Did we reach that far? Just load the data, do a summarization of the data, visualization of the data, you know know the exploratory data analysis a check for missing values yes right so when you do that then when you visualize the data the first thing that surprises you is this this data has a two decision boundaries, it seems, not one, isn't it? Yes. There is a, to draw it out, the data looks like this. Suppose this is your data. You have sort of, if I remember right, it is something like this. Like this, and this is a little bit bigger i suppose and there's blue points this blue river sort of flows through that and then you have the other points which are here which are here. Does this look like our data, guys? So I'll make this. This is our data, right? And so how do we solve this? You know, what are the problems with this data? The first thing we notice problematic is the decision boundary is nonlinear. The decision boundary is nonlinear. What's wrong with this decision boundary today? Boundary is nonlinear. You also notice that there seems to be two decision boundaries, isn't it? Where are the two decision boundaries? Let's draw those decision boundaries out. So one decision boundary is here. Isn't it? And the other decision boundary seems to be here. So now comes the problem. How do you solve for this? Once you visualize the data and you think about it for some time, you know that you need something more powerful than just a linear classifier, a classifier whose decision boundary is straight. Even if you could somehow straighten the data, there is also the fact that there are two decision boundaries. So how do you you conquer this so we are going to take a methodical approach through the data I hope all of you have tried and if you just try so the approaches that you can try is a simply apply logistic regression to the input data as it is. So we will see whether, let's see if the model is good. We have a good model. Good. Have a good predictive model. What do you expect? Do you think that you'll get a good model here? If you directly apply logistic regression. Remember the logistic regression is something that looks for a linear decision boundary, right? It expects something like this. And this is the distance from the dx is a beta naught plus a beta dot x. This is the distance from the decision boundary. Just going back to your basic theory of logistic regression that we did and the basic idea is that the further away you are from the decision boundary the more sure you are that it is a positive case versus a negative x is log 1 minus p x, right? This is equal to the distance from the decision boundary of that point. So how far it is from the decision boundary, that's an intuition. If it is close to the decision boundary, clearly we know that it is, we a sure answer for example we used to take the example of I believe blueberries some the sorry so suppose this site is all blueberries and what should I say? Some sort of a red berries, whatever those may be. Raspberries or something like that. Suppose those are sitting here. We do know that occasionally there are mistakes. Obviously, decision boundaries are never perfect. Data is never perfect. There will be things that will creep across and so forth, things like that. But the further you are from the decision boundary, the more sure you are that it is like this. Just to recap, see, if you are here, if you are at this point, A, are you pretty sure that it's a blueberry you're pretty sure right on the other hand if you are at this point b you're not so sure because you know you can see that there are a few this uh red berries also here on this side of it so B you're not so sure and if you are at a point like C yeah then once again you're pretty sure that it is a red berry isn't it so we use that to motivate ourselves we said how far you are from the decision boundary determines whether it's a blue it's a red berry or not, the probability of a red berry is that. So far so good, guys. This is just a quick recap. But the point was that the decision boundary was linear in the case of this. It was, sorry, I tend to mix the colors. It's a linear decision boundary but when we look up in this problem do we see a linear decision boundary we don't see a linear decision boundary we have multiple problems we seem to have a nonlinear decision boundary at the same time there seems to be too many decision boundaries one too many there are two decision boundaries so when you apply logistic regression so now i'll ask this question that directly when you apply and that is what i ask you to apply do you think that directly applying it to the data would give good results no sir no it wouldn't We wouldn't expect a good result. Results, because a logistic regressor directly will try to, regression classifier directly, will try to find a decision boundary. And what it will eventually do is the best decision boundary it will find is way beyond the whole data set. So it will declare all the data set probably as the majority data, right? So suppose it draws a decision boundary beyond the scope of this data, then everything by default gets colored or marked or predicted as the majority. Now it turns out that the yellow points are twice as many as the blue points. So you will get about a 66% accuracy. Now 66% accuracy may make you jump into life, like if you're not careful, but you have to. And this is one of the things we'll do. We will ask this question, when do we have a good model? Are we together? And do we have a good model or not? So that is it. So anyway, moving forward, the second approach we will take is, we will take an intuition. So let me give you the intuition. do this. I can treat this beyond the data. I can pretend that this thing turns around, right? And this thing turns around. What about now? Now we have a single decision boundary, isn't it? If you pretend as that or hypothesize, I imagine that beyond this data the decision boundary curves back and meets itself. You would agree that you have one decision boundary which is a deformed circle, isn't it? Are we together? Yes. So then we are only left with a problem that it is a nonlinear decision boundary. Given the fact that it's a nonlinear decision boundary, we can say we have one decision boundary, but it is nonlinear. How do we solve that problem? We can remember, how do you linearize a problem? One easy way is to go to a higher dimensional space, go to polynomial degrees, right? Go to polynomial space, go to... polynomial space go to if you remember I keep saying that for every data there which can be sold there is the hope is if it can be classified there must exist a higher dimensional space some space in which the data is linearly separable, right, and the decision boundary is a hyperplane, is a simple plane. So that, now the easiest thing you can do, or the easiest way to go to higher dimensions, is just to go to higher polynomials. Are we together? Higher degree polynomials of the features. So if the features are x1, x2, you can also start looking at x1 square, x2 square, x1, x2, and so on and may end up linearizing the problem. So if we take that, if we take that thought and then assume a bend, assume that we have a single decision boundary boundary by imagining abandon the data, imagining the two boundaries meeting boundaries meeting beyond the data set, beyond the feature space. Feature space, data, data feature subs know, beyond the bounds of the data, beyond the bounds of current data. So this part of wrapping around comes from the fact that you notice that we are claiming that these things are bending right just outside they're bending over. And so if we take this that will be our second approach we can go to a few high degrees of polynomial and we'll see what sort of accuracy it gives and then we can do a third approach which is that and which is the main approach i would like to sort of bring to bear and the lesson here is that when you get data sets like this, let's go back to the data. When you get a data set like this, your first reaction should be, can I visualize something? Can I impose a narrative from my own experience. Just imagine something. So there are many, many things you can imagine. One easy thing that I take in this particular case is you could imagine, for example, that this is a river flowing through. The blue is the water and the yellow is the sand. So if you think of this as a river, then you would agree that you can draw a decision boundary. The decision boundary that you can draw, well, let me take a color that is visible. What would be that color on this background? Color that's visible could be green. Okay, let's take green. So we'll take green, yes. Imagine the center of this river. You realize that if you think of this blue as water, then it makes sense that how far you are from the river let's say that if you are this point versus this point this point a is blue B and C let's take C but they are yellow what can you tell the about the relationship of a B and C with respect to the green line, with respect to the center of the river. A is nearer to it? A is nearer to it, yes. You can see that points that are very close to the center of the river, they tend to be blue. And at some point the water finishes on both sides, the water finishes finishes and then you can have all the other points which are sufficiently far from the center of the river there is no water anymore it is just sand right so let us use this intuition to build another model and why do we do that we are doing it just to just to learn that sometimes you can feature engineer from the data by thinking about it. This is deliberately a toy data, it's a didactic data to bring home a point that by playing with the data you can impose a narrative, you can do some feature engineering and that feature engineering often can lead you to success so the third approach would be feature engineering so in the feature engineering our steps will be first draw the centerline centerline of the ripple. How can we do that? Fitting a line to the data usually is regression. And so it's a paradoxical situation. We are going to first use regression instead of classification, even though the problem is classification. Do and b feature extract actually we have used abc so let me use the steps as i feature extract the distance from the center from the center line from the center line. And then the third part is after that, build a simple classifier, simple logistic classifier. And let me call this the distance as a d. right in just one variable one feature right from D going to T do you see that based on how far you from the decision boundary you can innocence tell whether it is river or sand right so this is a basic intuition I would like to walk through now this in the code and so I'll switch over to the other machine any questions before we start guys does anybody have any questions I have a question yes please go ahead so it's I mean, from looking at the data when you plot it in the SNS plot with the hue, the ones are all falling in a sine curve. But you have chosen a center line instead of a sine curve to extract the distance from it. Is it the first step towards going close to the sine curve? Or? Yeah, so the idea is when we have this curve this thing see there are two ways of doing with it you can think you can say i don't recognize a sine wave a sinusoidal function so then you go to polynomial space right or you can be smarter you can say hey you know what i can see the sine wave you remember that that in ML 100, we did that. So if you can guess what function it is in the lucky situation, then you can directly model it. And in fact, not in the Python, but in the R version, I will show you how to. And we have done this before. If you remember, this is nothing but your data set 2 from ml100 isn't it if i could extract the river and just the blue part it just becomes like a data set data set 2 which is the sine wave kind of thing so you can model it with the sine wave or you can model it with a polynomial either is good so sir uh i can do log transform and feature engineering but what is the best approach like you said polynomial is okay right do we have like uh uh auto model where it can recognize that this is a you know it will automatically take it to higher transfer right see this entire course is about a ways to algorithms that do the feature extraction for you today is the innovate we I'm saying that before you get too comfortable with those algorithms I wanted to teach you the lost art of feature engineering. It's something that is very essential. See, algorithms at the end of the day, we will pick up somebody, some researchers will do deep research to discover an algorithm. It becomes after that implemented software library. Then at the end of it for us to apply as we were thinking looking at it on sunday it's just a couple of lines of code and those code now will often do a lot of things if we change it but what happens is that you begin you lose something along the way sometimes you lose interpretability uh as we were talking about and you lose something. So the gold standard is before you hastily go and use one of those black box models, it is perhaps useful to see if you can think through the data and do feature engineering. Thinking through the data and doing feature engineering is the gold standard and if you look at the people who do excellent work or win competitions and so forth, they often invest most of their time in feature engineering and data cleanup, data preparation and feature engineering. It doesn't sound very exciting. People get very excited. Oh, I used the transformer to solve some problem in natural language processing or something like that. You know, those are hot topics. Yes, there is a place for that. But you should, my philosophy is you should progressively go to more complex algorithms, because more complex algorithms have embedded complexity like biases that it may not be informing you and also you lose sometimes the legal footing so for example in many situations you are in US any kind of prediction that you make that affects the protected classes you know the gender the age race and so and so on and so forth immediately you're in violation of the law and companies have gotten sued with this in this respect so please try for interpretability and feature engineering is your path to it. To keep modeling simple, use simple tools and do the model. I'll tell you a story. I suppose this is a side. So guys, forgive me for going on a tangent. I'm into photography. So in Silicon Valley, photography, you know, we are all engineers, most of us. A large proportion of people are engineers. And engineers tend to be gadget, sort of gadget fields you go to the beach, well not these days in Covid, but normally when you go to the beach, you can always spot these newly minted photographers, you know, people with a big camera, fancy cameras walking around. So they'll buy an expensive camera, the latest and greatest and god knows 50 megapixel or whatever it is, and they'll start taking pictures. Quite often they'll just leave it on the automatic mode or something like that. So somehow the instrument is supposed to itself produce good pictures. After a little while you get disappointed the pictures don't look good. In fact sometimes embarrassingly your cell phone pictures look better. Because in cell phone there's a lot of computational photography, a lot of software trying to make your pictures look better, whereas a professional camera will give you exactly as it sees. So you get frustrated, then you ask around, okay, what happened? Why do my pictures suck? After a little while you get an idea and you say oh it must be the lenses after all it's the lenses that bring in the light and the image so you can see people splurging on expensive lenses canon l series lens and so forth and you can often spot them long big white colored lenses or cream colored lenses often on the beach and what you can always tell that somebody is still learning because in the middle of the day he'll be asking his sweetheart or children to go pose in the water or something like that on the beach now how do you know that these people are novices there's actually a very good tell, a very good state. A good photographer will never take pictures, rarely ever take pictures in broad daylight. If he is forced to take it, he'll do all sorts of extra lighting support like flashes and so on and so forth to get rid of glares. Because in broad daylight, we all develop stark shadows on our faces. So good photographer, I mean especially if you do landscape, you want to take pictures of people in landscape and so forth, but you want to take it either on a cloudy day or you want to take it when at sunrise or sunset but not during the day. So anyway the joke goes that the nervous photographer is still puzzled and wondering what happened to this camera. You have a great camera, very expensive lens, and by now you're close to a six, $7,000 shot and still no good pictures. And so the joke is supposed to be that finally it dawns on the person that it is not the camera, not the lens, but it is the tripod. Enlightenment is, he will now go and buy the biggest tripod his spouse is willing to carry, because of course he is carrying all this heavy equipment himself. So anyway, so why do I mention that? Tools. There's a parallel to this in machine learning, a strong parallel. These days, there's a tremendous amount of research, and there's an explosion in newer algorithms, more and more algorithms. They all have a place under the sun. They all have a use. But you need to use them judiciously just because you pick a powerful tool doesn't mean that you will get very good results the way to get good results is thinking about the data feature engineering as much as you can cleaning the data preparing the data and going about it some some people say in view of this very complex algorithms feature engineering is dead, right? A be that as it may, you know, if you use any one of those algorithms, let us say you use a big deep neural network, you lose all interpretability. People are doing a lot of research to regain approximate interpretability by making linear models that approximate those networks. But it's a problem. The gold standard is, see if you can simplify the problem. Look at it in such a way that the entire problem looks simple. So for example, the photographers who go and do National Geographic, you know those masterpieces, quite often they will go into a culture into a country with just a simple camera a good camera professional camera pretty old like it might be a few years old and their favorite lens which would be many years old a couple of lenses at most and they'll walk into the wilderness or they'll walk into some country or wherever it is and they will come back in a few months with absolutely stunning pictures that make their way into National Geographic and our pictures we are the blokes who go in by fancy equipment obviously none of our pictures ever make it to National Geographic so in the similar way I like I try to use simpler equipment and put in your expertise deepen your expertise so that you come up with good results so that was the main lesson that I wanted to bring about today anyway just a parallel to this history so all right after this digression let us come back to the data what I'll do now is I'll stop sharing on this screen. I'll go to the Linux and I will talk about it there. So remember, we are going to take three approaches, direct logistic regression approach, which we don't expect to do too well. We can do a polynomial approach and to linearize the data and a feature engineering approach. So let's do that. That's the school. And what do i do here i'll leave it as a exercise for you to go repeat yourself because i haven't seen anyone uh do his submission for the other data sets you can try it out on the other data sets there's a question please go ahead yeah i asked this is so uh my question is like for the center line right that the green line that you had drawn so so so for using regression you assume only the blue water data right yes yes so so you will you will basically do a extraction like you will take a subset of the data that has on you. Absolutely, you hit it on the head. So let's go and see that. Okay, okay. Thank you. Let's do that. So I'm going to now share the set, how do we submit homework, you just said, yes, it's just as well. till your homework becomes polished don't export it to kegel especially don't make it public because you don't want to do that so do it in collab and share it with me that you can share your notebooks with people okay so you okay so you want me to share my exercise with you on Google. Not not necessarily on Google days their ways of sharing. Google kegel directly. Yes, I think in hang on, let me verify. i'm just wondering you, you mentioned two three times, so I would think how do I send you the homework like. Okay, why don't we do it together. thinking how do i send you the homework like okay why don't we do it together right i will figure it out together okay on kegel it seems like we have to paste all the lines of the code at least that's what it seems like to me yeah that is the thing but you can select with whom you can share that particular yeah that is right so let us go there into kegel to col Colab and suppose I'm in Colab I'll just take a random example intro to pangas let's say this one and do you see the link at the top here saying share right you can share warning share will contain you so if you do your own thing let's say that we create your own new notebook here we go I created a new notebook and now if I do a share right you see it gives you means to share it you can get a link or you can add people directly by email to share it so just share your collab with me yeah that's fine sir but I'm doing an art so just I will send you the our markdown file yes that's cool by the way you can do are also in collab and it's one of the things that i'm saying to you yeah and julia too and julia too yes we'll do it in julia julia i don't know if collab supports yet see what happens is i tend to spend a lot of time on a local jupiter or in a notebooks on the google notebook which is more high powered collab is something i like but you don't end up spending a lot of time there. Because it tends not to be powerful for you have Linux server. So I just put the Docker anaconda Docker and you know, we can run Jupyter lab on that. I'm running it Jupyter lab on Docker. Yes, yes. So I have that. So my environment is completely set up for deep learning. Dr. Kirit Parikh Ranjana Thangarajan, Deep neural nets and that's where you see you're seeing. I don't know if you're seeing my entire Linux. I can share my entire screen. But at this moment, you're seeing my notebook screen. Dr. Kirit Parikh Ranjana Thangarajan, Let's go through that. Dr So is the text looking big enough or should I increase the font size? I'm OK. Increase it, please. Yeah, a little bit of a little bit. OK, let me. So for the private notebooks, how do we share with you in the collaborator, I couldn't find the name. In Colab, you're talking about collab in kegel i think a little harder to share maybe there is a way to share you'll have to add me as a collaborator uh yeah in the drop down there's an option like number of people it shows like which you have to select that particular person you want to share it with and you can do that. Okay, so yeah, work with price. What's the name I search for? Okay. Let's take this offline. Why don't we do this at the end of this class, let's take care of the Kaggle and the collab issues. Let us give some time at the end, and we'll do both of that. Prachi, would you like to take that session at the end? Yeah. OK, please do that. So here we go. Hang on. Let me share it again. So is the text bigger now? I'm trying to make it much bigger. Is it big now? It's good. Yes. So, all right. If it may be too big, I might be... It's too big. I might be iterating it. It is too big. Too big, isn't it? Okay. I'll make it slightly smaller so I can myself see some code. All right. So when we look at this, what we'll do is we'll take the first is the imports. So let me go over the imports carefully. NumPyas what do you think numpy does by now we all know it it gives us the linear algebra the the matrices and matrix operations then what does pandas do it gives us the data frame in Python in r of course it comes built in and then what are the stages that we take the data through we we need to pre-process the data uh and we'll talk about it one of them is you standardize the data it is always a good idea to standardize the data you don't need to standardize the data for logistic regression. But for all situations, you should standardize the data. So let me talk a little bit about standard scale. See, suppose you're taking features of an elephant. So the age of an elephant may be like 20 years or 16 years. But the weight of the elephant, if you're especially measuring it in ounces, it will be like, what is it? Tens of thousands of ounces, isn't it? Are we getting that? If you go and weigh an elephant and the unit of measurement is ounces, it would be a massive amount. So the unit of measurement of each of the observations like you know the features in the observation affects the data. Now also there's a wide sort of skew some things are in millimeters for example if you look at the temperature of an elephant or the temperature of either a human being it is within a very short range. Most people have temperatures close to 98. They may go down to 96, 97, which is getting bad, or up to 100 and 405, and that's also getting bad. The numbers are within a very narrow range. On the other hand, the weight of an elephant could have a very wide distribution. And the unit of measurement may be huge. I mean, the number of pounds, the values may be huge. So one of the things you do is you standardize the data. So if you subtract the mean of the data, and I'm just reviewing what we did in ML100, if you subtract the mean from the data the data becomes center aligned isn't it so all the values will be around center means for example for temperature 98 will be re-centered to zero any positive value would mean temperature above normal and any value below 98 would be temperature below normal, below average. So that is the first thing you do, you center the data. After you have centered the data, you still have a problem. Your data is too wide in some areas and it contains the units, you know, your ounces and this and that. And some, a lot of these algorithms, they don't like data to be, to have so much variation from feature to feature. One is in millimeters, you know, one is in decimals and the other is in tens of thousands and so forth. It's cues of the learning process. So the next thing you do is you scale it down to a standard, to a unit area. There are many scalers. One scaling that is very popular it is called z value you can not only take a data subtract the mean from it but you divide it by the standard deviation when you do that your quantity now your number becomes dimensionless there are no dimensions the the ounces disappeared. And when you Z value data, even if you had measured your data in kilograms, you would still end up with the same Z value. That's the beauty of it. It becomes dimensionless. The second good thing that happens is it obviously brings it down to much closer to zero so that the values are typically between minus three and three. Beyond three are the outliers. Beyond absolute minus three or three are the outliers that you have. So that is the standard scaler. Subtract mean and divide by standard deviation. It's called Z value. Some of you may recall, you might have done it in your high school or college textbooks. But that is not the only scaling. You can do min-max scaling. What it means is that you can subtract the minimum from any data and divide it by the range. Min minus max is the range. So then your data will all fall between 0 and 1. If you are at min, your value will be 0. If you are at the max point, your value will be 1. And all the data will fall in the unit interval. And so there are other scalars, a means to scale or normalize the data. Interestingly, once you have done the modeling, sometimes you can squeeze the last bit of accuracy or last bit of juice by, for example, playing around with different scalars, different ways to normalize the data or scale the data into a standard representation. So this is, if you're not used to scalars, that is what it is. So the standard scalars has Z value things. Now, remember one of the approaches we will try is the polynomial features, right? We will expand into a polynomial space where higher degree polynomials can do that. So these are pre-processing we need or we need classifiers so we'll only use one classifier the logistic regression now you remember that in one of the approaches like the feature extraction approach we are also going to use regression to find the midline in the river so i have included the regression library also now when you have a included the regression library also. Now when you have logistics, when you have a classifier, the measures of a classifier as you know are confusion matrix. In other words, if something between a cow and a duck, how often did your algorithm confuse a cow for a duck and a duck for a cow? So that will be the off-diagnose in the confusion matrix. And the principal diagnoses are the values that it got right. And so that's the confusion matrix. Based on that, there's a classification report based on prediction and what the value was. That report has now in classification, there are multiple measures. There is accuracy, there is precision, and there is recall. And then in different literatures, you use different words. There is specificity, there is sensitivity, there's type one error, type two error, and so on and so forth. There's the F-score. There are many, many metrics. They all come by comparing what was predicted to what was the reality. Here, we'll take a few of those and look at it. And the next thing is something called receiver operator characteristic curve, ROC curve, and the area under the ROC curve. That's what these two lines refer to. Now, if I trust you know from prior ML100 what these are, but those of you who haven't taken ML100 with me, at some point, we'll take a remedial session. I'll walk through this classification. Prachi, would you please take us explain it to people uh yes would you like to explain these things if people need some explanation of the roc curve in area and the rsc cover would you rather i did it uh i would rather you did it like okay all right so let's hold a session over the weekend or some some clinic that are like okay all right so let's hold a session over the weekend or some some telecom in which we will cover these things next can we include f1 score as well sir yes yes f1 score is just the harmonic mean of the precision and recall yes i've got it. One second. So coming back now, going forward, linear regression, what do we look for linear regression? We look for the coefficient of determination, the R squared. And we look for in regression in general, if it is not linear, in general, we look at the mean squared error. How much is the mean squared error, residual error that is left behind. And the average of the residual errors, residual squared errors. Or you can take the square root of this, then it becomes a root mean squared error, RMSE. So those are the libraries. Now, going down a little bit, matplotlib inline, this is a statement saying you don't need, well, okay, all it means is that show the graph, any plot, without needing to have the word .show explicitly stated. Any plot you shouldn't have to call show. Just assume that it is there and show it in line. Then the and just show it in line in this notebook itself. This is the normal plotting library in Python. matplotlib is the default library. Good thing is that it's quite powerful, actually. Bad thing is that a lot of people complain that it's not very aesthetically pleasing. And there are better libraries out there and so forth. So Python has now almost a zoo of visualization libraries. And people who love to do data visualization they get carried away and they they do absolutely stunning data visualizations they become very familiar with libraries they they write their own libraries and contribute back to the open source it's a wonderful world out there uh on in the visualization space visualization brings out the best of creativity in people in many ways so um but start if you're not familiar with this, I would say start with matplotlib. It's the default library. When you use a matplotlib, you then import this library called Seaborn. Seaborn is a much aesthetically more pleasing library. Now, even if you don't use Seaborn, include CBON, because what it will do is it will go and put CBON, because it will go and override some of the matplotlib defaults. And so matplotlib itself will start looking nicer. So it is worth doing. The next is colors. colors except if you want to give the colors most of us do web pages so we get used to giving colors in hex form you know hash something something something something or hex form and different forms there's ascii forms and hex forms and so forth so if you are in that mood to give it an ascii or rgb values and so forth this library color helps you i don't think i use it in this notebook but i tend to use it the other thing is nowadays and this is you. I don't think I use it in this notebook, but I tend to use it. The other thing is nowadays, and this is something most people don't know, I think it's fairly recent, Pandas has PlotLib built in. And when you plot with Pandas, one of the things that happens is you can choose a backend. You may choose matplotlib or Bokeh. Bokeh is another exciting library, interactive library for data visualization. Or you can choose Plotly. Plotly is a commercial thing, but it has an open source counterpart. I think I like the lightweight version of it. So that I don't know whether it's lightweight or complete. I am not too familiar with Plotly, but it looks beautiful, is based on D3 visualization. So you can specify those backends and therefore your visualizations will then automatically start using those backends. It's an interesting thing. Now this is very basic. I like to have my pictures in landscape format why because ultimately i have to you know i give you guys notes written notes but there's a few who have been looking at ml100 tabular data the pdf you realize that all of these things make their way into a chapter in the textbook so having it in landscape format means i don't use up a lot of vertical space having it in landscape format means I don't use up a lot of vertical space that's that and then this is basic form setting now plotting style there are many styles of plotting some people like dark background some like white background some like slight gray background and so forth and so there are many many styles if you go and look at the matplotlib styles it's a zoo and people are writing their own styles so by all means pick your style i like ggplot because it gives me a uniformity with the with r and in r ggplot is the reigning champion of sorts a very very good visualization based on the grammar of graphics. So it is. The rest of it is just setting some attributes. Now, what... Asif? Yes? You mentioned about using like yellow brick as like one of the libraries. So you're not using that for this one? Yes, I will be. But there is a deliberate reason I have not included it at the top the reason is unfortunately but their function names are exactly the same as a psychic learn function names so if you if you know whichever you import last takes over okay it says anyway that's the reason you'll see that when I use it at that moment, I will talk about it. Now latex, I mentioned this fact that is that at the end of the day, once you have done your analysis, you need to tell a story. Because that is the story behind the data. So to tell the story, you need visualizations, you need plots and you need good writing, you can fix your writing, but you also need good visualization. Having professional quality visualization makes a world of difference, right, when you take your show on the road and try to convince people. It's very jarring. It's like if somebody were talking to you in English and the English had huge amounts of grammatical mistakes. It would be annoying or like, for example, I'm an immigrant. So I assume that the native speakers in US when they hear my English, they must be getting annoyed that I put accents or stress on all the wrong places. So it's like that and bad graphics is somewhat in the same genre, you know, people tolerate it. It's not that they they they're lenient and they'll acclimatize to that and so on and so forth. But generally it doesn't look very professional. It's certainly curable. Like, for example, I cannot possibly cure my accent, but I certainly can make better graphics and make sure I don't have grammatical mistakes. So in that same way, spend some time becoming good with this. So now I'll move a little bit faster. This line just suppresses some unnecessary warnings when you run. Otherwise your Jupyter notebook gets littered with all sorts of output and that's not pleasing to look at. So what do we do? We first load the data and that's not pleasing to look at. So what do we do? We first load the data and we drop any rows that have missing values. This is not obviously the best idea. I did it because I already knew that there, I mean, they're not likely to be too many missing values. Generally, you should load the data. You should do something called missing value analysis see how many missing values are there in more complex situations then you should do imputation you should impute try to fill in those holes in the data by imputing some values sensibly but that is a whole thing in itself when you do boot camps with me uh you go through a lot of rigorous uh sort of practice with those sort of things but but not for today. Here we are first learning to walk before we run. So I'll just not go into that. I've been a bit sloppy. Just forget about the missing values kind of thing. Describe the data. So this describes the data. And what does it see? It has three columns. These three columns. Now, I already told you that the data has t is the categorical variable right even though it looks like a number it's actually a categorical. When you see that the mean is 68 percent it means that 68 percent of the values if it is 0 and 1 68% of the values are one. So suppose I build a classifier in which I declare all the values to be one. I would still be right 68% of the time, isn't it? So my baseline classifier just picks the majority and declares everything to that. It's also called the zero R classifier. So it forms a benchmark, a baseline. Any classifier you write must beat the baseline classifier. So to illustrate the value of this with a little analogy, see, why should I not be impressed if a classifier has 68%? Or what number is a good number, people ask, in the impressed if a classifier has 68 percent oh what number is good number people ask in the accuracy of a classifier first is accuracy in itself is not a good measure you have to take it in tandem with other things recall and precision they mean a lot for example if you're diagnosing somebody if you have a diagnostic tool and you're looking for if you have a diagnostic tool and you're looking for, let's say, indicators of breast cancer or prostate cancer, you want to not miss any positive case. So then the recall as a metric becomes more important. If you are finding that you're lost with these words, please go and review Chapter 4 of your text book. I wouldn't have time today to review all those concepts but that's that so recall becomes more important than accuracy now speaking of accuracy let me give you a narrator suppose you have a tool and you you're screening people for let's say something pretty malicious let's say some form of cancer, breast cancer, prostate cancer, whatever it is. And most people will be healthy because if you're doing a random sample of the population, then you wouldn't have that, right? So they wouldn't have that. Maybe 1% or less will come out positive. 0.1% will come out positive, let us say. So let us say that you have your very well-researched, carefully crafted tool that can classify a person into healthy or unhealthy. But there is a quack next to you who obviously has no tools. But whosoever comes to him, he says, my dear friend, you're perfectly fine, go home. I tested you, and he'll make a big show of testing, maybe draw the blood, and then just throw it away. And then he'll declare you to be healthy. Now, what proportion of the time would he be wrong? If you think about it, he would be wrong only 0.1% of the time would he be wrong? If you think about it, he would be wrong only 0.1% of the time. He would be right 99.9% of the time. And yet you would rather not go with the quack. You would go with the proper diagnostic tool which catches the problem when it is there. So the other metric recall is more suitable to that. Things like that. So there are many metrics of classifier to judge a classifier based on context precision and so forth. As we move forward, make sure you review it. And again, I'll give some session maybe to review all of this time formatted. So that's that. So that's about accuracy. In fact, the history of, before modern medicines came about, I would say that the history of most of those ancient forms of medicine were often, oftentimes, not always, they were very effective in quite a few chronic cases, but for acute cases, I don't think they were very effective. And a lot of the time, a quack would just sit down and declare everybody to be healthy. He would come out like a genius because the human body has an amazing ability to heal itself. And so the outcome used to always be aligned with his prediction. So anyway, that's a digression now coming back to this data. So one tool that I would like you guys to use is called Pandas profiling. It does in an automated way, it does your exploratory data analysis. You can build your, you know, the histograms, et cetera, et cetera, but just with one line of code, you can generate the entire profile report. So let me show you what I mean. Look at this report. And I wish this could be opened in a new page. No, I don't think it can be opened in a new frame. So you notice that it gives you statistics of the data there are three variables the the these are the number of observations and the boolean variable there are two numerical value and one boolean type so it was smart enough to detect that t is actually a boolean type zero one right it's a binary type which is The variables, it shows you the histogram of the variables. It shows you the mean and all the statistics associated, mean, minimum, maximum, and so forth. And you can even look for details, more details. If you look for more details, it will give you all the quantiles. Do you see how detailed it is? It will give you the median. Those of you, if you want to brush up your statistics, this would be a good thing. See if you can explain the meaning of each of these variables. We don't have time, so I wouldn't go into basic statistics, but here it is. And the same thing for x2. You can toggle the details and see it. T, do you notice the lovely thing? It immediately identified in exploratory analysis that T is a Boolean data type and that there are 4,300 instances of 1 and 2,000 instances of one and two thousand instances of zero right and it gives you the ratio of the two 68.3 percent it means that 68.3 percent any classifier needs to be better than that right can you make a classifier worse than that? Believe it or not, you can actually. You can always do worse than that. The interactions, like how interrelated are these X1 and X2? And when you look at it, you still see the pattern of the data. You see this here. and you'll see this more. You can look at the correlation between the data and you notice that the data is not, if you will just let me, I'll just zoom out a little bit so we can see a bit more. Yeah, you'll realize that most of these correlations are low, the off diagonals are low. So x1 and x2 are not correlated. A t seems to have some degree of correlation to x2. But with x1, it seems to have no correlation. So this is it. And so, by the way, I'll leave it as an exercise for you to read about all of these correlations. These are the different correlations in statistics. It could be a good review of your statistical concept. Are there any missing values? There are none. But then when I loaded the data, I deleted this. This gives you a preview of your data, some sample rows of the data. First rows, last rows and so forth. So this is the value of this descriptive statistics, right? Variables, interactions. So you can just go straight to this. You can look at the samples of the data. So in other words, Pandas profiling is a pretty powerful tool. There are a couple of other tools. R also has data explorer and so on and so forth. Use those. But know that you should know how to do histograms and correlations and correlation plots yourself. Then it is good to know, but you can save a lot of time after your master deck by simply using the tool. So if you remember in the ML 100, we use the basic tools. We did the histograms by hand. We did the correlation plots by hand and so forth. But now we'll just use Pandas profiling for Python and Data Explorer for R. So what do we do? If we want to train a machine learning model, we can, okay, standardize the data. This part I can just remove it. I can just remove it. What am I doing? The first thing scikit-learn needs is a separation between the feature space and the target space. So our target variable is t, and our feature variables are x1 and x2. So we are separating them out into x, which is the feature space, and y x which is the feature space and y that is the target space. Now there is a lot going on in that line if you're not familiar with the syntax. So I'll talk a little bit about it. This is very Pythonic syntax. This x takes the first value here and y takes the second value. Now the first thing you notice is I'm converting t to a categorical. Categorical, I could have made it Boolean but category is good enough. Categorical means it's a category because it's zero one. I don't, we don't want to treat it as a number. Then the other thing we notice is that I use a specific notation capital X and little y. There's a reason for that. You could have used any variable in programming. What names you give to variables doesn't matter. In data science, it tends to matter because of expectation. It is a convention in the field that in the textbooks, the feature space is represented by bold capital X. The data representing data of the features is represented by bold capital X. And the data for the target variable, whether it is numerical or it is categorical is traditionally represented with little y y is for vectors see generally small letters are for vectors and capital letters are for matrices the feature space with all the rows feature space will have like the data features will have a lot of features i mean the feature part of the input data a of features, and it will have many rows of data. So obviously X will be a matrix capital. Y on the other hand is a single scalar value or it is a type cow or duck. But for every data instance, it will have a value. So it is a vector. It is a column vector. So a small letter for it. It's a convention in the field. The next thing we do is we take this data and we split it into training and test parts. This code is essentially boilerplate code. You will see in enumerable notebooks and you get very used to doing this. Are you splitting the data into training and test this random number state is 42 why it is i'll leave it as a mystery for you there's an interesting story behind it but i'll let you explore it has to do with the hitchhiker's guide to the universe so what do we do with the data now Now that we explored the data, we did descriptive statistics on the data. Let's try to visualize the data and see what it is trying to tell us. When you visualize the data, you notice that it is it looks like this. By the way, this is data visualization this, creating the figure and doing this. Actually, why did I do this? I need not. Okay, I'll just leave it at this. We are going through. So the size of the figure, it's more twice as wide as it is vertical. This line is purely optional. I just add some little bit of padding here in there next is I give a title plot then I do a scatter plot these kinds of plots as you know are called scatter plots what do I need to do I need to give the X and the Y value and I'm coloring it by the label the target I use a color map color maps a color scheme see what happens is that we shouldn't randomly pick colors for plotting because of a lot of aesthetic issues as well as disability issues quite a proportion of men and sometimes women are colorblind. I don't know does colorblindness exist in women? I don't know anybody who is but maybe it does or doesn't. One of you, a doctor here can confirm. But in men it certainly is their colorblindness. Or people who have partial colorblindness, they can only see some colors. So people have created after a lot of thinking, certain color palettes color maps. Manoj Mistry, Which are optimized for disability and at the same time are good looking. I mean, pleasing looking so that is why you can look up the dictionary on the web. There are many, many color maps. Matt Plotlib has the documentation. You can pick a color map and give it. You may have a different choice. I just gave this. Alpha is the transparency. You notice that this yellow is not deep yellow and this purple or violet is not deep violet right it just faded out that is the alpha of the transparency of it and s is the size of the dots how big the dots are so if if you don't give anything else if you all you if you if these things were not there your code would still work but this is just adding a bit of aesthetics to the plot likewise you don't have to give the title, labels, etc. It's always good to give it. Now the title is this, scatterplot. One of the things that you can do, again optional, is if you're familiar with LaTeX, you can then apply some LaTeX formatting to it. So that would you feel that this thing looks a little bit more publication ready let's open it up in a new tab and say compared to default it's not perfect but now i'll let you judge would you agree that compared to default it looks a little bit more publication ready. Yes. So that's the point of it. So when you do the plotting, plot it out and then spend some time brushing it up, cleaning it up a little bit, spruce it out. So from this data, we make a few observations. The observations are first thing you notice that there seems to be two decision boundaries. One at the top where yellow meets a violet and one at the bottom where violet meets yellow right and the decision boundaries are non-linear okay we talked about it a little while ago now i explicitly said in the statement of the homework that use logistic regression and that is rather you may say that that's rather painful because this does not look like a linear problem a logistic regression as we talk as we mentioned makes a linear decision boundary and we are quite literally staring at a problem that is not linear so how how do we solve it? Let us say and just to remind you what is a logistic regression? This part is the distance from the decision boundary and this is the odds. P over the probability of success versus probability of failure. In other words, probability that it is a red berry versus that it is not. If you take the log of it, the log of odds is the distance from the decision boundary. That is the essential statement of logistic regression tracephile. So let's take the code. When we do the code, it's quite straightforward. You have this, PLF fit and so forth. You realize that to build a logistic regression model on the data, it's just two lines of code. Maybe I'll now increase the font size, the two lines of code. Once you build a model, if you remember the methodology that I sort of trained you folks into was that Anshul Kaur you do that, you plot it, you make all sorts of visualizations of that afterwards, and then finally you plot the predictions on the data. And if everything still holds out, you say the model may be a good model. Generally, something along the way will fall, right? And so you abandon the effort. So let's look here. We will do the confusion metric and the classification report. When we do that, right away we see a problem here. Do you notice that? None of the zeros are being predicted as zeros. Right? So it's a disaster. Everything is being predicted as one so here but this is the the rows the columns here are the real values zero and one and the rows are the predictions so as you can see clearly we are looking at a problem not only that when you look at the accuracy what do you see 66 so should you be impressed with this accuracy definitely not a little bit louder no why not all all ones were 68 right ah that's right so the thing is even the baseline classifier had 68% accuracy and it got that drunk. Secondly, it's accuracy for zero is like for zero. The precision recall etc is just hopeless. Vaidhyanathan Ramamurthy, Right and F1 score, which is the harmonic mean of these two is again zero. It's a pretty bad model. mean of these two is again zero. It's a pretty bad model. And for this also, it should have been. At least for one, it should have been a completely accurate statement. If you just use a baseline classifier, your precision recall in F1 score for one would be exactly 100% right. But this one seems to not even get that. So at this moment you abandon the effort you say well we are lost so then you take the next effort which is polynomial regression what do you do you take the data I don't know why am I doing this here I must be thinking of something. I did this anyway. Oh, where did the polynomial regression disappear? I think I accidentally may have dropped something at the bottom. Hang on. Give me a chance. Oh, here it is. The polynomial regression is here. Let me bring it to the top. top yeah let me move it to the top okay and we'll go one more step yes so the next thing you do is you do it as a polynomial regression so when we do polynomial regression just the steps remember the first we need to scale the data that is something you should always think about doing unless there is a reason not to scale it you should scale the data the question is what scaling would you use the second is we want to use polynomial features so that is the next transformation will do we pick a degree I just generally picked a degree five or five degree polynomial is a pretty complex model isn't it it will have a lot of interaction terms and so forth so we have that then what you can do is data input data will go in it will be scaled it will be it will become polynomially expanded so you can use this feature called make pipeline that will do you don't have to tediously do it in your code you just give the pipeline and in a single statement the pipeline will do you don't have to tediously do it in your code you just give the pipeline and in a single statement the pipeline will do it for you okay so here we took the data and we uh got a nicely scaled and polynomially expanded feature space feature data that is good then we apply a logistic regression model on this polynomial expanded data let's see if it works yes why do you pick degree 5 here because of the cars there very good question yes I should so here's a mathematical trick i'll tell you one two three polynomial regression count the number of bends here how many bends do you see three at least uh let me one two three and then four five actually six six so at the bottom but because two of these bends are aligned, you may just take six, five, whatever it is, but you need to take a fairly high degree polynomial. So see, this is the fundamental theorem of algebra. It says that a polynomial of degree n has n roots. n roots. In other words, it has n places where it meets zero. And therefore, if you're trying to solve a problem, it means that the curve will meet the x-axis n times. It means it has to bend that many number of times. It's one of those little things of mathematics. If you know it, you can apply it when the situation arises it's a pleasure to do that anyway so we come here we will print the model fitness model coefficients and we'll make predictions with the model and once again we'll see the classification report and see how good it is when we do that you realize that this seems to have been a good idea. Your F1 score goes up, your, this goes up, general F1 score for 0 and 1 also goes up. Accuracy is close to 79%, right? And if you were to print out the coefficients of the model, you would realize that, do you see how complex the model is, there are so many coefficients, 1, 2, 3, 4, 5, 6, 18, 19, 20, 21. So imagine an equation in 21 terms. And such a complex equation is able to get a reasonably good prediction here right so i will pause here for a few minutes let's take a 10 minutes break i'll stop the recording when we come back and then we'll do the feature engineering approach to it so together let's let me go drink some water and then we'll do that record so I need to stop sharing the machine this machine okay All right, guys. It is 823 by my clock. Would you guys like to take a 20 minutes break or 15 minutes break? Should we meet at 840, 845? 845. All right, let's meet at 845. Sounds good. By the way, are you guys understanding so far? Is it proving straightforward? Yes, sir. i love the analogy but i'm 100 sure if you buy canon rf series lens you can take better pictures they just came out with the new series i can assure you that in the hand of a nervous you know there's a saying that nothing is foolproof yes because the fools are so ingenious so people like us you know who are dumbos you can count on the fact that we'll leave it on fully automatic yeah and go out in broad shining daylight and try to take pictures of people and things yeah but that i agree but sometimes you have to shoot in shining daylight. Yeah, there is but then, yeah. But you have to buy ND filters, remember, but but yeah, I like the idea, sir, because, you know, like, you gave us pondered profiling. So, you know, if I started in pondered profiling is it is like too much spin kind of Vaidhyanathan Ramamurthy, Automatically cooked up thing I would rather like to do, step by step, one by one, and later on, when I don't have time I do pondered profiling That is right. So, you know, in a hundred I focused on doing it by hand. is right. So you know in ML 100 I focused on doing it by hand. And here I'm introducing you to a tool so that if you don't want to do, if you're short of time you can use Pandas. See it just gives you quick and dirty results. You still need to do more work on your own. This is a very simple data set. It seems to do everything, but generally for more complex data sets, you have to, you have to do a lot more than that. So yeah, if we the feature engineering and with the log transformation and all the things if I can create a linear relationship that is much better rather than directly using polynomial and just neural network and all those things right see the log transform so the broad key answer is yes but here's the point the the log transform captures the in other words the power transform more broadly speaking yes our transformations are applicable to the relationship of input space to feature to target space. Right? Or looking at any of the inputs and seeing should you do a feature, should you do a log transfer of power transform, right? Then you use a different set of techniques to linearize the feature space itself or linearize the decision boundary itself. Just power transform will not linearize the decision boundary. Sometimes it may, but generally you need more than that. Yeah. Like it looks like a sinusoidal wave. So sinusoidal wave will go to linear if you use. That's right. It does. And in fact, when we go to the r part of the solution you'll see that i have used sinusoidal wave and so this is an exception see i oversimplified the data in reality the data was uh one of the data sets that this reminds me of is a fracture data and the fracture I represented with a model the fracture, the fracture center and then looked at the level of impurities and whatnot, the sort of things, but it was a more complex problem yeah and are you done with your dinner sir that's that's it uh but yeah i certainly need to drink some water so i'll bring something and be back anil are you there? Thank you. Thank you. Thank you. Hello, Asif? He told he'll be back in some time uh biology oh yeah i see the video it's not there Thank you. Thank you. Go ahead. I mean, that's nice analogy we're giving actually. So I was thinking about that. It's true when people know about the, you know, scenery that they are looking at, their composition is better than an automatic machine making guesses. So in future with all the data science development and object detection do you think it's going in the direction where you know the cameras can take a decision like a human would do there is effort so there is this emerging field called computational photography uh For example, these days, all these smartphones, when they take pictures, they know that the person, a face. So what they will do is they will optimize for portrait. The moment they see people in the picture, they'll make sure that the people are in focus. And they try to give a shadow depth of field. You know the techniques that used to belong to photographers, they're trying to incorporate it into the computational aspect of it. So you may have noticed that people actually look much better in their smartphone photos than actually standing in front of the mirror. Observe that and you'll see. It's very interesting. It goes in the direction that you are describing. Even with a simple machine like a phone camera, you can take good pictures if you know how well the subject is positioned and all those things. That is right. That's right. It's very interesting. Yeah, it's very interesting. Sir, I want to quickly remind you uh we gotta buy new nvidia tesla cards are out when did it come out uh they came out with the pca express version so i think we were gonna get it in the next two three months the 3080 ti how much does it cost 30 no no no a100 that's for our uh computational or the computational one a100 a100 we have somebody in our team he was working for nvidia i i remember but i don't know who i forgot the name but anyway so those cards you you need to buy those maybe two three or whenever you're not going to get money for that i have rtx 6000 but it's time to upgrade now the new cards are very powerful and then i think amd is also going to come with this time very powerful card hopefully and now thread ripper is getting cheaper so uh you can get a better deal i think in next two three months actually i've been wanting to make a workstation with the top end fedripper right that yeah 3995wx or something so yeah so the computer is already there uh motherboard and processor we need to buy so you go all right so let's have a install fest let's uh get all the parts together i'm ready so we will do this on the weekend the trouble is with this covet yeah we have to do it remotely virtually that might help you build at your home and be watch each other and you. One question I wanted to ask you when we were, you know, when today you started this lecture, you said we were going to stay in this pandemic, in this pandemic until next year. That's what the mathematical models are saying. Yeah, so I... The Harvard State... How do you define we are in pandemic and we are now out of pandemic? Oh, those are well established. Those are based on numbers and the level of infection. So whether it's an epidemic or pandemic, there is some definition that everybody has agreed to. So by that threshold, we are in the pandemic zone. Yeah, so like, you know, there are so many things people are ignoring. Like, you know, if we put this disease and number of deaths and all those things, you know that we have like, this disease doesn't even come in top 10 or top 20 or anywhere near that right so we are in in pandemic of what cardiovascular disease for long we are in pandemic of cancers we are in pandemic of motor vehicle accidents so number of deaths doesn't define it so that's an interesting perspective i suppose the point is that, see, if I had to guess, those diseases, at this moment we don't know how to prevent. The cancer, the best we can do is deal with the pandemic. The hope for this particular pandemic is that with behavioural change and public safety measures, we can significantly bring down the mortality. Whereas for cancer, we have already saturated, we have done all that we could and when cases happen, we deal with them. But you're right, there is a pandemic of cancer. You know that in India, they run a train I remember, which goes to either Ludhiana or Chandigarh, where there is a big cancer hospital. And people in the various places refer to it as a cancer train, because it is taken by cancer patients to go to that city and that hospital. Yeah. So, you know, if we put all the variables and if we count them uh lockdown and financial stress and you know other things it may not be a hundred percent like it's a good idea to lock down every single thing we can restrict a particular age group to have less interaction but particular age group to have less interaction but I am at the opinion that young people should just continue the work see that idea so first of all I agree with you there are no goods the way I look at it is there are no good solutions still a curious so if we lock down too hard the economy tanks if we open up the economy and mortality rate goes up. So we are in between, we have to make, we have to see saw between one and the other. Public policies, seesaw, people are opening the economy clamping down, going up and down, there are no good solutions. And then mortality rate is again, I'm not sure that they they are defining it properly, like, you know, all the patients who are dying. And they, they, I heard from the doctors, they are forced to label them, the cause of death is COVID-19 pneumonia, right? So let's say if we don't have this disease they were gonna die anyway right so the mortality rate is should be not this this is not a true representative of the death they are exit you know making it more high other countries because all the studies show that while that is happening our into you know detection rate that are we detecting all the cases all the deaths that happen because of kovit no we are capturing only a proportion of the kovit debts and registering it as kovit and then yes the other side also is true occasionally death from other causes are earmarked as kovit but uh in usa we are not missing not even a single patient actually you know that kovit 19 test had very high false positive rate too so let's say patient has died due to some other reason and they are doing kobe 19 test on dead body now so it doesn't matter how he died and because the false positive rate is high, it was going to get counted anyway. But the PCR test is very specific. In the beginning, the test China made, you know that how much false positive they were? India totally discarded both the tests. But the PCR test is supposed to be 96, 97% accurate. Of course, it's supposed to be more than that. It's supposed to be 99, otherwise it's not a good screening test. Yeah, I mean, that was the point that X-ray and machine learning, AI at this moment, people didn't take it seriously because it could not match the RT-PCR specificity. But to your point, everybody has come up with that idea, that let's do one thing. Let the young roam around because they are not going, they're not at big risk. And quarantine the old people. but how do you quarantine people live in multi-generational home so the young come home so the idea is that you can take the old and send them somewhere so every country has at one point or the other coming to that right idea but the thing is where do you send them like do you put them on a grand ship but just one thing brazil and sweden tried that option and you have seen the results very bright bad situation over there both places yeah so but we had a bad are not uh diseases before that there are you you materials and other things they have really bad are not sir yeah they are not for three right close to three i think i'm not sure they are keep on changing it i think two to three but they're bad diseases with bad are not dead we we we have went through them so yeah but india i'm told has a r naught close to 1.3 or 1.4 is that correct but it's a India we as far as I know that data is not reliable even if we have that in the you know some village people will not gonna go and do that kovatest what is the cause of death right so many people person deaths are not reported over but also the immunity system is also different factor like if you look at Europe, death rate is very high. It's not just because they are giving the accurate results that the death rate is high. Because of the people's immunity system, how to struggle with the virus is quite different. And what is the strength of the virus is quite different in Europe versus Asia versus the US. So all those factors also count. That is not true. If you take exactly the same age group, people across the world have the same immune. Actually, nobody has immunity because this is a completely new virus. So immunity doesn't exist. People are saying that in India, we live with so much other germs that we have immunity. All the science shows that's not true for simple reason. It's a completely new germ. Now the reason mortality is high in Europe is because they are a much more geriatric society compared to India. In India 90% of the people are below 40 or something like that. So if you're below 40 the risk is very low. That's the reason they are recovering so well and they're asymptomatic. But you go to countries where a lot of old people live, in US also, go look at Florida, they are panicking now. A lot of older people live. Wherever there are older people, the mortality rate goes up and the problem goes up. So it has to do with age, immunity responding to age, not because of racial or other reasons. I think that India had expectancy around 60, I believe, and USA is above 80 now. So you can Come again, say that again. Age expectancy rate. Yeah, is 60 yeah a few a few decades ago india was less than 60 and now they i think around 60 to 65 and usa had 80 so you can imagine how much proportion of the population is u.s has a massive geometric population i mean look at my white hair i'm getting there too no no you are going to use yourself soon we talked about it okay yes all right guys it's 8 45 let's get started i'll start the recording. I had to drop at 9 because of my work, but just wanted to mention. Sure. And obviously this, as you know, I post the notes. The recording will be posted as a chapter in the you know all right and am i sharing a screen i am sharing a screen isn't it oh no i'm not sharing a screen okay uh cancel let me go to the machine so guys are you seeing the screen or not no I Let's try. How about now? Yes. Yes. Yeah. Okay. I wish I could move this away. So now we're going to do a little bit more careful analysis of the data. So how do we do that? First, what we do is let us extract the river from the data. Remember we talked about if we could get the midline of the river, we can compare every data point to the midline of the river. Isn't it? So let's do that. You have the data, there's hardly any need to describe it. What did I do here? I just kept the river points which are the t is equal to zero points when i do that i have a much smaller number of data points only 2000 data points and out of those data points if you visualize that that data point this is how it looks right you see that this shape of a river stands out and of course this is a toy example that i created but most rivers are not so perfect they would meander here and there and so you would need some sort of a polynomial or something to describe it but it will do for us because here we are trying to learn about feature engineering so then what can we do we can take this data and we can model it as a polynomial. And which is what I'm doing. I just took a fifth degree polynomial. But here I am not fitting a classifier classifier. I'm fitting the regression line. I make a regression line and there are only five terms. Do you notice that it's a six terms, of course, because they're beta zero. Then on those, and the first coefficient is actually zero, so five terms. You notice that the R square is close to 90%. So I have gotten a pretty good model. Let us check. Whenever we build a model, we check the goodness of fit. Now the way, if you remember in ML100, we got trained, first thing we do is we look at the residuals plot to see if there is any heteroscedasticity in the data. When you look at this and as you scan your eyes from right to left, there is no distinguishable pattern, there is no funneling or anything or there's almost no pattern, worth noting so the data does seem to have homo skidasticity this also has if you look at the residuals they tend to have a gaussian distribution here guys that is another criteria for residuals they should have a gaussian distribution bell curve distribution and in the margins you can see that they do have a bell curve distribution. So we seem to have modeled the river correctly. So then we go down a little bit. We also do the error analysis. We compare what is the prediction error, like what is the value of y versus y hat. And you see a strong correlation. They seem to be along the principal diagonal right so a good thing next we go and so this is it uh best fit and identity line so our best fit line is pretty close you know r squared is 90 90 which is very good. The other thing, remember I mentioned, and so it is good to be careful. One of the things that I mentioned is you should look if your linear models are being hijacked by points of high influence, points of high leverage. There is a way to do that, a very simple way, and here's the code for that. And Anil, do you notice the library that you mentioned is being used? Yes. So you notice that now there is in Cook's distance or plots what you look for is a dotted red line and you say that anything beyond that red line red line and you say that anything beyond that red line begins to stand out as points of high influence here there are actually none we don't have those points all the points look normal and fairly good some minor here and there so then let's visualize the model so the the next thing is, so far, no reason to reject the model. One final test, let's visualize the model over the data. So this is a data visualization code, obviously, for those of you who are just getting started, I would say, be patient with yourself, you wouldn't be able to do all of it right away. But do that book, Fonders for everyone and the psychic and the psychic learn book it has example codes the web has example code while this code looks scary you know oh goodness so much but actually it's a very trivial code once you get used to plotting so I'll just walk through what the code is this is just some beautifying parameters I'm'm using LaTeX, why? Because you notice that the title has a publication quality text to it. Right, if you look at it, all the text is nice, good quality text, good fonts, whether, and mathematical expressions look like mathematical expressions, so you can ignore that. I just create a lot of synthetic data in that space to draw the line, and then I just go ahead and draw the line but in the background of the line i put the original data points all the river data points so do you think that this line more or less represents the center of the river occasionally it seems to miss miss it but broadly speaking it seems to get it, but broadly speaking, it seems to get it right. Would you agree guys? Definitely. It does agree. Now what we will do is, now let's do feature extraction. Let's look at the distance from the center of the river, which is very easy. We just define the distance to be the absolute distance of a point from the center of the river. It doesn't matter whether you're above or below, what matters is how far you are from the center of the river for that point. So I can take the x2 value of a point and compare it to what the center of the river is. River middle is this and I just subtract and get the distance. And so you notice that I have one feature, D, in the center of the river. Go ahead. Probably I'll make a comment once you explain it. Sure. So then with this data, now what you do is you create a much simpler dataset. You see that my feature space is now just the d variable and target space is t variable. So now I'm talking about a classifier in just one variable, right, a line. I'm just trying to divide a line, chop a line, and at some place saying on this side of the line is river, on that side of the line is sand sort of and so if you build a classifier with this you expect it to do well and here it is right away when you build this classifier you notice that first thing you notice is you get a good confusion matrix remember the initial one was a disaster that we got zeros right but here you notice that most of the points seem to be correctly classified there are 92 times when one is misinterpreted as zero and zero is and here 77 percent of the time and zero is interpreted as one right so i don't know why I divided it by 10.0. Okay, so that's the thing. And you can see the classification report. What is our accuracy? 89, 92, close to 90% give or take. Now, this is the accuracy that you can have. You usually can't exceed that. Give or take one or two points here or there. You can't exceed that because Give or take one or two points here or there. You can't exceed that because the data inherently has noise. If you go back and look at the visualization of the data, do you notice that there are areas in which the yellows and the purples mix up? The zeros and ones interleave. So there is inherent noise in the data. you you realize at this particular moment you seem to be a pretty close to the best possible situation this is a way of visualizing the uh the same data this data is nicely visualized here right precision recall uh f1 and support which is here F1 and support which is here so this gives it a bit of color to it the next thing I mentioned in classification is the receiver operator characteristic curve the receiver operator characteristic curve is this curve that tells us how good the classifier is another way of looking at it the idea if you remember is that your classifier if if you are this diagonal line or below your curve is below this diagonal line then you're doing a pretty bad job but if your curve is like pretty much hugging the top left-hand corner and most of the area of this page of this graph is below the line, so this graph is one unit by one unit, even though I've made it into a landscape format. If most of the area is below the line, it is a good, it means that you have a good model so here it says that the roc curves look good first of all they look pretty good for zero and for one uh both of them are good and at the same time the area under roc is 96 percent 96 percent is a very good number right for that so that so by roc plot also we realize we have a good model. Now we can, this is a simple plot that tells you how often did you confuse 0 for 1 and how often you confuse 1 for 0. So the level of mistakes. You're making marginal mistakes, right? And that finishes this particular analysis, right? So guys, now that you have this you can use this to do the flag analysis. In flag the only difference is it matters whether you're above the line or below the line. When you visualize it you'll know what I mean. You should be able to do the flag analysis in 20-25 minutes. Now I'll take questions and then I will perhaps do it it in r repeat the same exercise in r so what do you think guys do you feel that this is helpful in trying to solve the problems very yes thank you good good so understand it yeah go ahead i have a quick question so here we looked at the data it's a 2d data and we saw a pattern of where it's all zeros and ones and we saw that river is where it's all zero elsewhere it's one and since we knew about the data we modeled the sign uh sign i mean the whatever the fit was and then subtracted the distance uh so it's like a cheating, I mean it's not cheating, we know about the data and so we, so how does this generalize to, if it's not a river? No definitely, a polynomial can capture many things and your point is valid. See this is a toy example, deliberately crafted to introduce you to the concept that feature engineering is a powerful concept, is a powerful method to use. In every situation, you'll have to think hard and you'll have to decide how to extract features from that data. But the universal truth that remains is that effort at feature engineering almost always pays off. You should put in quite a bit of effort at feature engineering. Because I was thinking like, okay, it was a jackpot. I saw the sine curve. And then I thought there would be a formal way to, you know, i mean to approach this problem um but i think the scope of the problem yeah as we move through this course right we will learn a lot of methods see okay i'll make a statement uh opinionated statement see everything that you will learn from now onwards See everything that you will learn from now onwards or will be methods Powerful algorithms that will somehow linearize the problem and do feature extraction for you We live in the world where deep neural networks deep learning is very hot If you look at what deep learning does actually is that here we thought through and did feature extraction ourselves when you can do it you have a simple interpretable model that you can explain what is happening no you ever how lovely it is that you interpret it as simply saying distance from the center of the river very intuitive but you don't have to if as you will learn when we use decision trees random forest gradient gradient, I mean boosting and support vector machines, deep neural networks, they all can solve this. This problem is too simple. They will solve it in the blink of an eye. But what will they lose? Decision trees still has some interpretability, but beyond that the rest of them become black box black boxes right so that is it but if you like what a decision tree does for example or what a support vector machine does let me give these two as an example when you when we do the theory of deep learning in that boot camp that is coming up, deep learning bootcamp, you will realize that layer by layer, you are systematically extracting representations or in some sense features from the data and higher level features and higher level features or representations, right? In a systematic way. So a deep neural network layer by layer is a method to do the same thing, feature engineering or feature extraction, except that you do it using machines because the problem domains are very hard to do. The same is true when we talk about, you know networks, it's true also for transformers and so forth, attention. And for example, when you apply an attention model to a picture, you ask this attention model that where is the, we are looking for a building, where are the buildings, right? So it will tell you that it found a building in the picture right this is a building but then you look at the whole photo and you're not sure that this algorithm knows where the building is so you can use an attention model to say or attention mechanism and say show me where you suspect there is a building in this picture and it will start highlighting those areas where it says you should be searching for a building for right because let's say that the building is just a way small or not quite visible or whatever it is and the neural network is starting to pay attention to certain regions and it will then go produce a heat map for you showing that that's where the building is now if you think about it what happened heat map for you showing that that's where the building is. And if you think about it, what happened? You went through the neural network somehow went through a systematic process of feature extraction and learning. What we are doing is we are learning the feature engineering ourselves. When you have tabular data, and you can do feature engineering and solve the problem with simpler algorithms, you have a gold standard because it's very interpretable you can explain it to customers and walk away with it okay so this is what we do the complex models will do it automatically for us but now the scope is for us to learn uh how we do it by ourselves and like yeah how we do it by ourselves. And like, yeah, how we do it by ourselves. And also it has value. There are many, many situations. All these shiny algorithms, we will learn all of those. They may not be used for legal reasons and for a whole host of reasons. You're forbidden or it is too dangerous to use them. You're seeing what is happening with facial recognition, for example. It is a disaster. It with facial recognition, for example. It is a disaster. It's become a tool for discrimination. You should always search for interpretable, simpler models to get the job done. While complex models can do it for you, you lose a lot. You have to sacrifice a lot the point within being able to predict and being able to interpret you want to have both and you won't have both if you use very complex models right god i agree all right guys so uh i said just a quick question maybe maybe if I'm thinking it right if you go to the river again they were picture this one or the one here the one below that right this one just the river itself okay uh there we go or the other one very middle line i'm sorry the other one with the lining it yeah by the way as a best practice guys whenever you make your notebooks add a table of content to it you see this icon here it will notebooks add a table of content to it you see this icon here it will help add a table of content and have a table of content on the left hand side so it is easy to navigate quite often you come across people's notebooks which are a mess you just keep scrolling up and down so yeah that's one here. I just want to validate if I'm thinking it right. Do you think if I normalize those blue point, I will get a better middle line like a center line right? I'm not sure if it's evenly distributed or not uh remember that when we were visual when we were doing the analysis we did normalize the point in the visualization we have not you can do that you'll get exactly the same graph okay uh you'll get the same but if you went back here it is that poly data transform but this scales and does polynomial expansion this quality data is a pipeline you already normalize it during that process so basic good hygiene guys make sure you give a good descriptive title. Don't just leave with doing there is a school of thought like for example edward tefft likes principle of leasting these days you don't make a lot of grid lines and so forth so i have made this with some places having grid lines and some not having grid lines all over the place i think when i visualize the data you know here also okay so these days people are trying to remove clutter from diagrams so our visualization so that you can see the information much more starkly all right guys so what i will do now is let me bring up the r version sorry one last question um before the version so how when you made this um middle like this uh midline right through the river so you separated out that river data using that t t as the classifier yes t is the target variable remember t is the target variable okay it's always a target remember t is the target value okay it's always a target very good in the next next time sir if we can have more visualization like if we look at this picture below you know we can interpret that x2 variable all over the place so it doesn't explain anything only expert level has inherited information so if we utilize those things that we can understand better. Yes, that's true. Okay, I'm trying to find where I kept my file for give me a moment guys, I'll have to compile my latest file to generate the R version of logistic regression support. Baba, the vent thing is going to fall off. I know. Thank you for telling me. Thank you. I'll fix it. I cleaned up. OK, I'll do something let me give you a preview of things going forward chapter i just want to watch it out of time so maybe we'll do it next time or will i do that partly because i can't find this note do that partly because i can't find this note so let me share that oh no i shared the wrong one about the three one second i want to show you something this question that you can do better with more sophisticated algorithms i wanted to deal with that. So 100 Okay. So I'll give you a little bit of a walkthrough of what we are going to do in the coming weeks. We have just talked about trees it may be worth asking how well do the trees do so let me share that and how do I share it let me share the entire desk are you folks able to see my screen? Asif, one minute. Can you hear me? Sure, please be a bit louder please. Would you be able to share the notebook that you have today? Today is a little I haven't finished it yet. The Python? The Python one itself. The R is actually finished long ago, but the Python one, I still have to do it. I'm still adding a couple of things. And the way I share it is that, see, if I shared the notebook itself in raw, then you'll just copy paste the code. So I'll make it harder for you. I'll actually give you the chapter of my book that contains all of this. So at least you'll have to key it in. Sure, sure, not an issue. But you'll get the idea. So guys, we are going to talk about trees. Let's look at the same dataset. Are we seeing this dataset guys? Oh, by the way, this is how it will look. These are the chapters of my book. So if you're looking, this is the way you'll get it. And I hope you agree that it is better to get it as a well-written text rather than just a rough notebook. Nice looking. Yeah, so this is the river data set. We use a decision tree that we learned about. This is just taking the data and doing basic statistics on it and visualization. Turns out there are many libraries in art that you can use to do it. We'll use one of them, trees. When you use trees, you come up with this kind of, let me zoom into this. Do you guys see this regions do you see how complex the decision bound boundaries are you know how complicated the regions are each of these rectangles is either a zero or a one would you agree that this looks very complicated doesn't those are extremely complicated extremely complicated then if you prune the tree and it becomes a little bit simpler but not by much compared to our interpretation using feature engineering or i mean this cannot even hold a candle to it uh yes it's a prune tree it's accurate but see still it is not very interpretable what it is trying to do more or less right and if you look at the tree you know the decision tree i deliberately didn't put the labels here because it would be too hard to put it on a page this is the original decision tree on the left on the right and this is the pruned decision tree on the left still to me both of them look much more complex than what we are trying to do yes right and you could do that and then obviously these are the metrics in art by the way this is once again there are tools in art that help you get your precision your recall your accuracy and so on and so forth the area under roc curve with decision tree also is good but if you look at our area under roc curve or we are actually look at this what is the area under roc curve with the decision tree 88.5 percent isn't it guys yeah which is supposed to be hang on i'll zoom out 88.5 what is the accuracy or what is the area under roc curve that we were getting let's go back and check 90 plus roughly 95 exactly right so when we go and do that we are getting an ROC area under our OCC of 96 percent and the amazing thing is we used a very simple model you can't go simpler than logistic regression yeah right using a simple model we have just beaten what a much more complex model right and that is the beauty of feature engineering guys so let me go back to the tag and you can zoom out. Yeah. So this is it. You can do the same thing using different libraries. Then you can use even more complex algorithms. We will talk about random forest, very popular in ensembles, next Monday. We'll do that. When you use that, now random forest is almost the state of the art for table data, tabular data, right? And we wouldn't, I wouldn't go into this. By the way, these are literally the notes that you'll get. Still, even the state of the art algorithm just about comes close to our simple linear model, isn't it? You see the area under ROC? 95% We are at 96% Well, close But it took a state-of-the-art black box model to achieve a comparable accuracy So I hope I've convinced you of the point that feature engineering is a good thing, guys. Would you agree now? Yes, of course. So that is that. Oh, I have a Python. So this chapter is ready, actually. But now that I think about it, the decision tree chapter is ready. And even this chapter was ready uh kate you took this uh last time you took it i did hand out both r and python isn't it i have one in r i'm not sure if i have the one in python have to look through my uh okay lab notes yeah yeah okay so is that your book yeah of my book chapter is it like a meep type concept meep no it is not a book at this moment and now it's in the under publication you can still use the chapters exactly as i'm writing this book yeah as i'm writing this book i'm making it available 11.9 it's python implementation of a decision tree in random forest for the river data yeah but no but this feature engineering part that chapter is probably uh if you look at it maybe i did in python maybe i didn't is probably uh if you look at it maybe i did in python maybe i didn't but i'll certainly add both of those so any chapter will have implementations in both guys if you have ever written a book you realize that writing a book is tedious good thing with writing a book is that things are clearly explained you have a reference right it is you notice that i use much more words than they are in the notebook. Notebooks are just, I put descriptions but not as much in this as in the book. So you'll get the chapter in due time. Anything else guys? So I'm done. We have about 13 minutes. I'll take questions. I just had one question here. So when the deep learning workshop start, will it be on same day or parallel to the deep learning is different. It's a boot camp. The number of hours that you'll have to commit will be much more. So the way it will work is you'll give one evening probably Wednesday or Thursday evening for theory but just like this will cover that theory but then the lab work will cover the entire day of Saturday yeah that will be good because I have my some of the meetings overlapping with these classes. So that's why I wanted to check that if you know, if it can be on Thursday and workshop on Saturday, that will work perfectly. Yeah, it will be like that. I'm not starting the boot camp yet. I thought, let me make progress with this batch, train them enough, because some of these people may also want to join the boot camp. I should probably start the registrations, at least for the boot camp. Are you planning to take it? Are you guys planning to take it? Yes, Asif. Just one question. Is it going to be again the gcp based or deep learning part of deep learning part again it will be a choice you can do it in the you can do it in the cloud or you can do it on your local machines now most people don't have very powerful local machines yes so i would suggest that, see, by the time you get to deep learning, the common man just does it in the cloud, borrows the resources for a couple of hours, lets it go. Because otherwise those machines are $30,000, $40,000. Just my workstation is like $8,000, $9,000. This one is $7,000. The one in the white, I have the black elephant here the white elephant in the office is more than $10,000 so deep learning machines get very expensive the cluster you have in your office is that set up for deep learning no that is set up for big data and cloud like you unity stuff scaled out micro services and so forth. So yeah, and that is one massive, like, six-figure cost. I won't even quote it. Yeah, I'll rather pay you than Google. Oh yeah, you could use it. Actually, that cluster is the cost of a house. That's what I'm thinking, right? I mean, for deep learning, if your setup is available, why not? Yeah, you could use it actually. So here's the thing. Obviously the COVID has hit support rate is hard. Enrollments are down, but as it bounces back at their revenue stream continues. Lambda Labs, I was in the process of purchasing the latest Lambda Lab machine. Two reasons I waited, first is the COVID situation. The other was that I was waiting for the new GPUs to come out, which the quality has already come out. So Lambda Lab will catch up with it. So we'll buy the next generation Lambda Lab server. They pretty much put us back by about 60,000. Then we can do all the labs locally. But if I don't get it, then we'll do it in the cloud. That's that. So guys, yes, I have a couple of requests to you. The deep learning will start and then this workshop, if you guys are liking this workshop, please do spread the word out. At this moment, support vectors would like some enrollment. We're in a bit of a crisis. The enrollment is here because of covid has been surprisingly low well i would like to believe it's because of covid and not because of my bad teaching we like the class as if you're fishing for compliments you know and we all know what an excellent teacher you are so uh no kali actually you'll As if you're fishing for compliments. I'm not fishing for compliments. Just saying. You know and we all know what an excellent teacher you are. No, Kaliya, actually you'll be surprised. One student who took the previous class, the ML 100, because we are doing it virtually, his feedback was the class was completely useless. Oh, okay. So it isn't all positive. is always well you've been even seeking virtual right he was probably watching something on the TV and then trying to follow the class I'll take it at face value so you know they'll always be people don't think so the fact is that there exists at least one student who was frank enough to just say it was useless. If he said it useless, I wonder how many other people felt something. We have to differentiate. You are an excellent teacher. You are one of the best teachers I've ever met. I agree that as if like I have really developed a lot of interest in this science uh that i started reading so much and you know that's why i have re um i came into this class again good good yeah thank you just continue that you know continue that will become good the point the worry is that virtual is new for me i like to see people in the face you know see whether they are understanding or not make sense of what is happening in the classroom so this whole thing is a new experience i'm myself learning to teach online so that's right the advantage here is that the lectures immediately become available online as you know this is being live streamed if anybody would like to set up some time to talk to you we can talk 15 minutes or so after this I have some time stop the recording and then we can talk anyway so I have a plea, do send your friends, if you like it, do send your friends my way. I do need some advice. Sure, sure. Yes, please go ahead. I have a small question, what was the coding? Yes, please go ahead. You know how when we're doing feature engineering, we're doing the vertical distance? Yes. How do you do like distance from all directions? Cause that's what I did on the- Like what is the nearest distance to the midline would be the perpendicular distance. You know, ideally you should have done that, that's hard, but why bother if just vertical distance distances taking you all the way. Yeah, no, that's what I did on R, but I was thinking about, you know, I want to make it more complicated. You can do that. You reach a point of diminishing returns at some point. Okay. All right, I'm stopping the recording guys in one second. And after that, and I'll stop with YouTube