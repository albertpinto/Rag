 All right guys, today's topic or this week's topic is anomaly detection. It's a huge topic. There's a whole industry around this topic. It has to do with finding outliers in data, finding novel features in data, finding fraud, fraud detection. And fraud detection is a hugely important topic. The reality today from a practical perspective is that for example essentially fraud is winning everywhere you hear for example this week itself we heard to our unpleasant surprise that some critical government departments have been hacked right so it means that the anomalous traffic that we got network and login and so forth the traffics that we got are things that happened went unnoticed and the hack could happen so to observe the network traffic to observe user behavior to observe all sorts of things, data about financial transactions, data about medical data, for example, the presence of anomalies in somebody's x-rays. All of it put together makes for a very vital topic tool in machine learning, which it is hard to exaggerate the importance of. It is used everywhere. It is also a topic that is really worth doing. People who can do this do this can become good at this field. Generally, you can join companies where just this one topic will keep you entertained for the rest of your career. So it is a very important topic. We have done some part of anomaly detection in the past. Those of you who attended the boot camp last year, we did one day of anomaly detection. In particular, we did the classical methods. We did not use deep neural networks for that. The way I will do today is I'll broadly talk about the concerns of anomaly detection from first principles. Then I'll talk about some of the classical methods, just quickly going over them, what those are, and then coming to deep anomaly detectors or DADs, as some people call them. Now, the good news is the deep anomaly detectors did prove in many ways, in many areas of anomaly detection or fraud detection and all of that, it proved to be a game changer. It led to much better performance than the previous models that used to exist. So that is, therefore it is important to know this topic very well. We have a substantive lab coming up on Wednesday on this topic. And so that's that. Now there is another bit of good news, which I suppose it's time to reveal now. It turns out that I've already laid the foundation for anomaly detection using deep neural networks. In the previous sessions, in the previous weeks, I was sort of building up to it, you'll notice that I covered auto encoders, and I sort of insisted we do it and not miss it. One of the big reasons I did the auto encoders is they play a central role in anomaly detection. Likewise, we covered GANs, regenerative adversarial networks. GANs are another means to uncover anomalies. We will study these two approaches in the deep learning and we'll look at all sorts of hybrids and variants of it. For example, there is a hybrids and variance suffrage. For example, there is a anomaly which takes a GAN and autoencoder, and by a conjunction of the two, it comes up with a fairly good anomaly detector. So there are various architectures that we will encounter. But given the fact that you all have a background now in autoencoders and in generative adversarial networks are going should be pretty smooth right so i'm hoping that this broad topic of anomaly detectors the entire theory i should be able to cover today okay now i was uncertain whether I would bring in anomaly detection into this workshop since so many topics need to be covered in the ensuing weeks. But there seemed to be quite a high demand for this. So therefore, I'm covering it. And I apologize, given the compacted nature of this workshop, we had just an 18-week workshop. The time that we can give, once you realize this importance, one can start wishing if we could give more, perhaps a month to it. We won't be able to, though my hope is that in some sense, we have given a lot of time to it. We have given weeks to, we have given a week to generative adversarial networks, we have given a week to autoencoders. So in some sense two weeks have already gone into it. This could be considered the third week of anomaly detection from that perspective. All right, so with that administrative aspects out of the way, let's get started with anomaly detection. So our topic, novelty detection. And there are subtle differences between outlier detector and novelty detection. And we'll talk about it, though broadly speaking, they all come under the same umbrella. They're close, they're very close cousins of each other. So we'll talk about that. Now, when you look at anomaly, first way to think about it is to think in terms of outliers or anything that sort of is not representative of normality in the data. So this brings up an interesting concern. How do you know that something is an anomaly, or it is an outlier? It is an anomaly or an outlier which is not anomalous and which is not outlier data, isn't it? If you had just one data point you wouldn't know whether it's an anomaly or not. It is one data point but when you have a lot of data point and those data points show a certain pattern, a certain distribution, and this element stands far from the distribution, its likelihood of happening is low. Then you say it's an outlier, it's an anomaly. So the history of outliers and anomaly, the theories have been pretty long, starting with basic statistical definitions. For example, there is a definition, I believe, from the mathematician Tukey, which dates back to a long time, which basically says that for one dimension, if you, so suppose you have, I mean, obviously most distributions are not necessarily Gaussian, but two are bell curve like, but for the sake of keeping it simple, let me say that it is. For the sake of keeping it simple, let me say that it is. You have a mu, you have a sigma, the width of the bell curve, and Kewke said that see most of the area, as you can see, the mass of it is between minus this is mu minus sigma this point mu minus sigma and this is mu plus sigma so you can clearly see that the mass of this sort of the bulk of the mass of this is between these two points. Let me call this point A and B. So one easy way that I often say to visualize this is imagine that this is a sand hill. If you were to think of it as a sand hill, you would agree that most of the sand is in the yellow area, right? So that is the quintessential definition of inliner data in liner data in other words that is the core of it that is how it should be quite representative of it then by the and i believe for a belt for a normal distribution you can look up the number i believe is 68 percent right then you can go one more standard deviation and I believe that will take you somewhere here mu plus two sigma and here mu minus mu oh, mu minus two sigma. This point AB, let me call it CND. And then we have the area that is here and here. We say that the, so let me just give it a legend. This region is Let me just give it a legend. This region is in liner data according to Tukey's definition. Then he went on to say that let's treat this as sort of not big outliers like soft outliers. Beginning to get outliers because outliers is ultimately a subjective term so let's call this soft outliers and then you you go to the third standard deviation here this point this is mu plus three sigma and this is and one of you please look up the exact numbers what is there because at the top of my head I don't seem to remember. I think by the time you cover the Sand Hill is already present here. Now, if you include this, and one of you, if you have a chance, look up the bell curve and the two standard and three standard deviation. Let me know what these numbers fall to. I believe first is 68, the second is in the 90s, and the third is in the high 90s. So by the time you include... 99.7. Come again? 99.7, I think for the four... 99.7, yeah. Okay, so... Yeah. Okay. So actually, I take it back. I think Tukey called both red and yellow as inliners. I believe he started calling it, I take this back. This is not, this is also by Tukey's definition. I think this is also in-liner data according to him. And then the green becomes your soft outliner. And you know these are matters of definition. It turns out, I just realized that my recollection may be flawed. Soft out. There's like four, past four is the hard outliers, right? That is right. And then is the hard outliers, right? That is right. And then everything beyond that, you can start taking beyond. You are looking at outliers and beyond four, of course, or beyond three outliers are definitely beyond three and three Sigma. And then beyond that, it is very clearly an outlier. So for example, if you got a standardized test score and you were three standard deviations above the average, you have a very, very high percentile. I believe it's somewhere around 99 or 98 percentile at three. Okay, did you look up the numbers? Hold on a sec. Okay. So you can let me know. So this is it. Now this is correct with respect to, this is, this is easy when you have one-dimensional data. But even with one-dimensional data, things do get complicated. What if your distribution is like, it's not very clear, it's like this. So the question is, at what level or where do you start deciding that it is an outlier? If you have a distribution like this, where do you start saying it is an outlier? So you realize that so long as your data has a normal distribution, it is a lot. This whole way of defining soft outliers and hard outliers and outliers, etc. Sort of fits with our intuition. But when the distribution start having very long tails, now what would you call an outlier? This tail just goes on and on and on, right? For example, when you look at the shopping, you know, consumer bestsellers, or the items sold by Amazon, and how many instances of the items were sold. It can be long, so things begin to get a little bit uncomfortable, but still people, one dimension is relatively simple. Now what happens in two dimension, that's where, and two in higher dimension is where the story begins to get more interesting. Suppose you have data like this. And this is the bulk of your data now what about this point a or this point b what can you say about and what what about c let's ponder over a b and c if you just look at the distribution here like along the axis maybe this will show some form of normal distribution so c will stand out if you look along each of the axes probably C will stand out isn't it guys because C will fall somewhere here and it will project itself somewhere here on the y-axis so this is the y-axis and let's say the x-axis right what What about A? Do you think A will stand out? A seems to be falling here, here, and B seems to be falling here and here. So the question therefore arises, how do you discover that A and B are outliers? So the first thing is, of course, you a and b are outliers so the first thing is of course you realize that they are outliers they're broadly outside this sort of dense envelope that i made for the data are we together yes yes so we have that so you you realize that one-dimensional intuition doesn't easily move forward to two dimensions or higher dimensions. You can miss a lot of outliers if you use a simple reasoning like that. And therein lies the complication. Not only that, what if the data came something like this? You had regions, like for example, we keep talking about cows and ducks so here are your ducks in the size weight scale and here are the cows big and heavy let's say somewhere here so now you realize that this particular A and this particular cow B, you would like to call these outliers, isn't it? This A is a rather overweight duck and B is a rather underweight cow. So it is an anomalous in weight. A and B are both anomalous in weight. And of course I could have gone in the other direction, in the size direction, or I could have put, I mean, any one of these places. So what is your intuition? You see that these things are away from the dense regions. So now I'm going to say that we have a way, we have been learning a lot of techniques along the way that we can repurpose to discover these outliers. Can you think of some of the techniques guys? What can we use? So Asif can we use density based that is right we can use uh and more broadly any so suppose we use density based clustering what does it do remember one of the things that density based clustering does db scanScan, DENCLU. Or DENCLU, yeah, I think we can use DENCLU. All of them will create an envelope. You set a threshold of density. Points beyond that are declared as outliers automatically. So a density based clustering, one of its side benefit is that it produces, produces for you these outlier points as a side benefit. Then you can do even other forms of clustering. Any form of clustering, suppose you did k-means itself. So let us say that the k-means did this entire partition it called duck. And this particular partition, this side of the feature space it called cow. Even then, all is not lost, because what you can do is you can go to the center of the cow and the duck, the centroid, and just look at the distance, observe k-means. k-means, you can say clustering. So let's say this was number one, this is number two. What you can do is, k-means, you can go to find the centroid, distance of a point x to centroid, whichever centroid it belongs to. Are we together? So if this distance you compute for all the points, you will then get a one dimensional, we are reducing the problem to one dimension. What will happen is most of the distances, let's look at this ducks here. Ducks, this is the mu of the duck duck and what will happen to this duck? The distance from the centroid of A will be here. It will be far off. This is where the A will fall in terms of this y-axis. This is distance. Are we together, guys? Yes. Does it make sense? Most points will be close to the centroid and they'll have some sort of a tapering of density or proportions or probability density. But A will be literally standing off as an outlier now you can broaden it to any clustering whichever means of clustering you find whether it's k means now it's easy to see that you can find centroids right just need to But then you have the extra step. In dbScan, outliers come free. So outliers come as a free byproduct. Isn't it? But if you use any other form of clustering, then the burden is on you to get the centroid and to compute the distance, to plot it out, and so forth. So typically what happens is the further you go, the more you feel that it is an outlier. There are many metrics to tell whether something is an outlier or not, like local outlier factors and this and that i will just use one particular metric which is just called s of x usually denoted as s of x in the literature it it is called the anomaly score Now, S of X belongs to the interval . So closer to zero, it is not an anomaly. Small values, not an anomaly. Large values, large is relative of course, it goes from 0 to 1. So you can choose a cutoff. Let's say that you can say I will take 0.5 as a cutoff. Now what happens is that people have standardized a sum one score. Let's tell us one score so that we can tell. And people who implement these algorithms for anomaly detection, they ultimately come up with some way of producing a score that is between zero and one. And there's many, there's a bit of mathematics associated with all of them that you can use. We won't go into all of them. It's all, it all agrees with intuition basically. So it's not, we'll move a little bit faster as I said and stay on the big topics because we need to move faster so therefore you realize that you can use clustering so what is a realization we can use all sorts of number one realization one clustering Algorithms, isn't it? Or clusters. Number two, this is one. Then comes an interesting fact. What else can we do? Just look at this data, cows and ducks. What else can we do? So what happens when you are an outlier? Would it be reasonable to say that your distance to neighbors is pretty high? So suppose you are an outlier here at this point its neighbors is this, this, this, this, this, right? So do you notice that these distances are pretty large compared to inliners? Inliners tend to have neighbors very close to them but outliers tend to have neighbors rather far from them. Isn't it guys? So you could for example say I will look at the average of the K and N distances distances averaged. To tell, that could be one way of deriving your anomaly scores. Now, it doesn't always work. The trouble with these naive approaches are they don't always work like K and N. Why? Because you may have a distribution. Let's say that you have data. Why? Because you may have a distribution. Let's say that you have data. Most of the data is here and some of the data, just a tiny amount of data is here. You want to treat this as anomaly. And this is the inliner. So will your KNN work for you? Would anybody like to say, would KNN work in this situation in detecting the anomalies? You mean when the anomalies are a close cluster themselves? Yeah, exactly. They are closely clustered in themselves. And that just alludes to the fact that quite often you have to try things out, ultimately know the distribution of the data before you can choose which particular algorithm to use. Now, there is another thing to it. I give you clusters based on the same idea, how could you have solved this problem? What you could have said is you could have done clustering and you could have called it cluster one, cluster two, and you may just notice that cluster two is way smaller than cluster two is much, much smaller than cluster one. The number of elements in cluster two is much smaller than the number of elements in cluster one. That itself is indicative that it's an outlier or anomaly, isn't it guys? Because this big large cluster is inline data and relative to it C2 is the anomaly or the outliers so that's how you argue with outliers and anomalies there are many many algorithms that you can repurpose for that for that purpose right now let's try something else you could say hey, why not create this as a classification problem? Let us say that you realize that these were all unsupervised learning methods. Right. We are applying KNN,n distance or clustering, they fall under the general category of unsupervised learning. Why do we call them unsupervised learning? Because the data doesn't come with a label saying this is inline data and this is outlier data. But then let's go a little bit further. What if it was? What if you could tell that this is outlier data and this is inline data? So for example, consider a credit card fraud data set. This is a fairly used data set that Harini I think you did a notebook on it just now isn't it okay so yeah and so the idea is that there are a lot of transactions that took place in Europe in a certain year and a few of them are fraud so you get a transaction x and you get a y right x and you get a y right data comes like this this belongs to either a fraud not a fraud or a fraud right it can take one of these two values right now the idea is the fraud is in some sense in the feature space you would imagine that it is an anomaly right right? It cannot be the normal pattern of behavior. Obviously, the fraudsters would like to make it look as normal as possible, but there must be some vector space, some feature space in which the fraudulent transactions stand out, isn't it? And can be separated from the good transactions, the non-fraudulent or the valid transactions. So that will bring up the question, why not just use a classifier, right? So would somebody like to put some thoughts into it? You don't have enough data, right? Normally very rarely. That is one reason. Class imbalance. That's a good point. Typically data that is anomalous can be, there'll be a huge class imbalance situation. By definition, it's an anomaly, so there can't be too much of it. And sometimes it's extreme, sometimes you have let's say i'll hypothesize just 10 records or 20 records that are supposedly anomaly right so for rare diseases that's very true some rare diseases by its very name means when you gather data very very few people will have those rare diseases so you capture a lot of, you know that in some place in the feature space, these two things separate out. There exists some vector space where the disease person, the people with that rare disease and the people without it, the normal people, their biometrics will separate themselves out. Except that you don't know it. And the data point is very small. So when you apply a classifier, you have to be sensitive to huge class imbalance, a massive asymmetry of data. Now there are methods to address the asymmetry of data. If you naively apply logistic regression or any one of the classifiers, a random forest or whatever it is that's your favorite classifier, you will end up in trouble. You will miss a lot of, either miss a lot of anomalies or you will start classifying a lot of normal data as anomalies, right? So you'll have a lot of false positives or false negatives. What sorts of errors will keep cropping up? We went through this exercise, there's a few who participated in the bootcamp last year. You realize that we did this as an exercise. So then you say, well, wait a minute, there are techniques to address imbalance in data. If you remember, we used some libraries. Yeah, the resampling method. That's right. More than sampling, oversampling, and in particular SMOT, which tries to impute data, which is more tries to so under sampling and oversampling a self explanatory, youam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam try to do more of that. The trouble with doing that, of course, is there is a presumption that if you have anomalies, the in-between spaces between the anomalies are also anomalies. And that kind of assumption begins to break. If you look at situations A and B, you would agree that if you start filling the gap between A and b with more synthetic points those points are actually not anomaly not anomalous right they will intersect through very normal regions in this graph do you see this if i connect a to b and i start popping up points here those yellow points they run the risk of not being anomalous points, isn't it? Do you see the problem with that? Yeah. Yes. Yeah. So that is why SMODE, it works. It's very good, actually. It works quite effectively in taking care of class imbalance till the imbalance is extreme and the data is really spread out. But then it becomes counterproductive sometimes. It's not the best technique. It will give you some mileage, but after that you'll start hitting a wall. But nonetheless, having said that, it is still possible to take classifiers, use class imbalance techniques, and get a certain degree of performance, as you will see in the lab. You can get certain amount of anomaly detection using that, using classifiers. So now we have done two kinds of techniques. So unsupervised learning are like things like a clustering, KNN, they're taking the median or average distance for the nearest neighbors, and then plotting it out to detect anomalies. And then we did the supervised manner, in which you essentially create some form of classifier that will distinguish between this. Now, between these two extremes comes a very interesting region of semi sort of a semi supervised learning. And it's a continuum. You know, something is closer to unsupervised, something is closer to unsupervised something is closer to supervised and there's a whole host of techniques that we can deal with and we will deal with in a little bit uh when we come to the deep learning part and even here let me do that so before i move on from this let me mention two very popular techniques that are used in that has been used for many, many years. Those two techniques are one is using support vector machines. One class. SVM one class support vector machine is one technique. The other technique is called independent forest. Am I getting it right? It's not random forest. I think it is called isolation forest. Thank you. Isolation forest. The word was escaping me. And so I'll explain both of these. Let's start with isolation forest first because it's a simpler of the two to explain. What you do is, suppose you have data. So let's go back to the same data. We have this envelope of normality. this envelope of normality and let me put the data points that are anomaly anomalies in red and as if just to clarify so right now you're sticking to classical methods only right this yes i'm giving you a background on classical methods so that these are important by the way, guys. So I'm just quickly reviewing that. And unless you have come to my prior workshop or have read independently, you may not know this. It's worth knowing this before we move to deep learning. I have to read a book. Come again? No. I have to read up to catch up. Come again? No, I have to read to catch up, but I'm just following along. That's right. So this is your box. Let's take two-dimensional data. This is your R2 box. This is your bounding region. Let's say this is your outer bounding box. Now, here is what you start doing. This is very similar to random forest. So let's say, if you recall, forest, what Forest. What do we do? There we create an ensemble of trees. If the word ensemble, more colloquially, lots of. Like a collection of trees. We'll build a collection of trees. Now, if you remember, how did we grow a tree? Well, there was this whole fact that you would project down to a lower dimension space, right? A feature space, you would take random projections or random subspaces, you would do bootstrapping and all of that. So forget that part. Let's just go once you have taken a sample of the data in random forest, how would you build a tree? So what you would do, suppose this is feature X1 and X2, you would find the split point along either x1 or x2 axis that gives you the maximum information gain. Isn't it, guys? The criterion for splitting was maximal information gain. And I hope you guys remember this from our prior discussions. So you can use entropy, you can use Gini impurity index. One way or the other, you want to have a situation where before and after split, we have the greatest, let's say, reduction in entropy once you split, right? Total entropy, for example, if you use entropy. So you try to create the best tree that would fit a data. But in isolation forest, you actually do something quite different. It's a little bit simpler. You forget all this, you don't do any of this in isolation forest. You say that, all right, pick a random axis. Let's say we pick X1. Pick a random point and just go split it. So maybe I'll pick, let me just go pick this point. Are we together? I split it. So maybe I'll pick, let me just go pick this point. Are we together? I split it. Now you have these two regions, go again randomly split it anywhere. Randomness is the most important thing. Let's say I split here, right? And this one, I don't know, maybe I'll split here. Then again, I split, let's say here. Then again, I split here. Then again, I split, let's say here. Then again, I split here. And again, I split here, things like that. Now, what will happen is if I go on splitting like this, this, this, and in this region, because there is a lot of data, you stop splitting if there are too few points in the region. So what will happen is you will end up with a lot of splits things like that as complicated as it can be something like this now when you look at it like this through this mesh do you observe something mesh. Do you observe something? It's a you realize that points that are in liner, they belong to very small sub regions that you reach by successive layers, or successive sequence, a lot of splitting takes you to the white points but observe that the red points they tend to be you you easily get them with a very few splits you know the big regions the regions that were created without too many splits are the regions that contain outliers do you see that guys Do you see that guys? Yes. Yeah. So what happens is that if you now this thing, if you represent it as a hierarchy, there was a region R naught, R1, R2, then this more and more regions R11, R1, r1, r2, r2, r1, r2, r2, and so on and so forth. So what will happen is that, and let me make this tree much deeper, what will happen is all your inliner points are somewhere here. And all your outlier points are a little bit high up in the hierarchy. Let's say that there was another split here. You know, they are achieved. Well, I just screwed myself because I used up all of the spaces. Let's say that I'll just add a third. It shouldn't be like that, but okay. I'll add a third. What happens is that these regions, this point, it will be hanging off a node, a region that is high up. It won't get through splitting. So maybe I should redraw it. This. Okay, so let me just put it this way. Here is this point, right? And here is another point. And sometimes you can have, I'll get rid of one branch somewhere. Let me get rid of this branch entirely. So there may be a point here and there may be a point just here. So you might end up, and maybe there are two points here in this region, and this region is just one point and so forth. So you end up with small regions which are empty, except for a very small number of points, and you can't split it any further because they don't want to split it. And so if you look at it, the outliers have a shorter path to the root, isn't it? In other words, you get to them with very few splits. So this path, for example, is much shorter. This path is much shorter. This path is much shorter this path is much shorter this part is much shorter compared to this part you can see that in liners tend to have much longer parts to the root number of steps number of edges or number of parents you'll encounter till you get to the root will be much higher and outliers tend to have less. So that is the basic idea. What isolation forest does is it creates a forest. In isolation forest you you create an ensemble of trees. Each tree has random splits. do is for each point for each point each data instance xi compute the average path distance path length let me just say distance from the root across the trees right find this average value across the trees, right? Find this average value. In some tree, it may be a little closer, two steps away. Some tree, it may be three steps away. For each point, you can do that. And now all you have to do is, you have a one-dimensional entity, isn't it? For every point, you have a distance from the root. And now you can plot your distances and you can decide where you want to declare it to be outliers. At some point you can say the outlier score I'll put it at 0.5 and now you're looking at outliers and hard outliers and so forth. Do we get the concept guys? So this is a very simple idea and it's a modification of random forest. In fact, a simplification. You don't use any of these information gain and Jenny or entropy or any argument like that. Asif Nadav Thangaratinamayi, You can do like this and you'll have your outliers. Asif Nadav Thangaratinamayi, I hope you understand what I just explained, guys. Tell me that this was straightforward and easy. Yes, Asif. Understandable. Yes. Great. So then there is another one which is a little bit more abstract. It is the one class SVM. So instead of like splitting it along the axis, if we split it along the angle will that also help or yes so people have done that it is called random uh see any one of these algorithms have many adaptations many papers have come out i do remember somebody saying that for this whole business of random forest, you can get better results if you are not always parallel to the axes. If you go in random directions, you get much better results. And I'm sure that somebody else must have shown that the same is true for isolation forest yeah so that's the i think that's called like the extended isolation forest like if you instead of right just like you split at an angle okay i see nice so anyway so there's one question so why was did you split the red regions only few times and the white region so many times i mean what was the logic the reasoning is see um look at this point a you realize that this was split number one let's say that the split number two right this is split number two but by the time you come to a point like this let's say that the split number two right this is split number two but by the time you come to a point like this let's say a point like b here you realize that it is in a very tiny region like suppose you say i'll stop i'll stop splitting when my region has no more than three points or two points right you realize that any inliner because it has a lot of neighbors you will have to do a lot of slicing a lot of chopping of the reason until you come to till you have a little box around the region around that point which contains let's say only three points do you see that right yeah so it's basically the same density or the same number of points. Yeah, that's right. If you just say my stopping criteria for splitting is I need no more than three points. So what happens is for inliners, you'll have to keep on chopping. And so any inline point will be in a tiny box, which has been created after a lot of chopping what it means is that in your region's tree this tree it is way way down somewhere yes whereas what happens to outliers oh they get chopped off and have their little isolated little boxes pretty quickly. Yeah, yeah. After that it is clear, but that point I missed. Why the splitting was so rare on the red points. That's right. I suppose I should have repeated that, but that's how you stop, for example, splitting in decision trees. Okay. Thanks, Sas. Yeah, nice. So we now come to the one class SVM. In one class SVM, it's a little bit more abstract, so I'm not sure I can do complete justice to it. But the way you do that is, it is very different to it. You don't show it the anomalies. And therefore, it is not calling, different to it you don't show it the anomalies and therefore it is not calling i mean you don't show it the outliers at all and that is why strictly speaking one class svm is not called an outlier detector it is instead called a novelty detector so the way the novelty detectors work whether it's one class svm or others and the way the novelty detectors work, whether it's one class SVM or others, and the way they differ from outlier detectors is in an interesting way. You only show it the normal data. Are we together? So suppose you have a way of getting clean data which has no anomalies, which has no outliers, you just show that to the SVM. And SVM in a very abstract way says if this is the normal data and what is the minimum, like how do I shrink wrap it, says that anything beyond it is anomaly. Are we that so suppose you give it a data which is normal it will basically try to create a decision boundary kind of a thing it will try to shrink wrap it and say okay if you now give me a data all i care is whether it's inside this closed region or outside it right so suppose I get a data here now afterwards, after the one class SVM has been trained with only normal data, then you give it a point like A, it will say well it's a novelty. It doesn't meet the definition of belonging to the normal class. Therefore, the one class difference are the class of normal data. Is the idea simple guys? Are we getting it? Yes. Yes, Asif. So here the limitation is with one class SVM and with all novelty detectors, you have to be careful. Your data has to be clean. It cannot have the training data, cannot have outliers. Because if it does have outliers because if it does have outliers or anomalies then the SVM will learn that too and try to consider it normal so you have to give it this and when you do that actually so long as you give it the clean data you'll be rewarded by a pretty good novelty detector anomaly detector detector, anomaly detector. It works quite well, actually. So that is the point of using one class SVM. So for example, try this on the credit card fraud data set and see what happens. That is that. I think enough of classical methods. Should I go on with a few more? Asif, one quick question on the previous one, the isolation forest. Yes. So you explain the intuition right now with a two-dimensional data set. In the two-dimensional data set, when it comes to randomly figuring out you know where a box should be yes a choice of picking you know a point on one of the axis and a point with other axis when this problem becomes multi-dimensional there has to be a logic around which combinations are first picked right in? In terms of- It doesn't matter. I'm saying pick an axis and pick a cut point and go build your rectangular volume space. And so it's totally random. There is no guidance in terms of which- There is absolutely no guidance. The only guidance is when should you stop splitting? And that guidance is that stop splitting choose a criteria maybe at three points you'll stop splitting i think when i was the question was more like this uh let me rephrase the question there uh let's see so x1 is a feature x2 is a feature right we don't have any relative importance between these features right now no you don't have okay yeah you don't have that and so what happens features right now. No, you don't have. Okay. Yeah. You don't have that. And so what happens is that, see, sometimes, and that's the reason you use ensembles. One of the trees may be unfortunate and it may just do it badly. But most of the other trees will do it right. And so on average, A will have a much shorter average distance to the root than inline is B. And it comes out pretty well. Isolation forest is actually fairly successful. It has been one of the most successful strategies for anomaly detection to such an extent that when you guys, let's say that you go and interview, it is quite likely that if the company does anomalies or outliers, they will ask you some question on isolation forest in the interview. Right, thank you. So now isolation forest is a true outlier detector. It detects anomalies as outliers. Whereas the one class SVM is a novelty detector. You have to give it only the normal. So it creates a definition of normality. You see that? It says, now I understand what normal data is. Are we together guys? Yes. And so it does that. And so we'll see this in the lab. It's a very successful approach to doing it. So ask him, is it still like supervised learning or this is semi supervised? Yeah, that is it. See what has happened is it is you have label data. So it is, there's a bit of supervision, but the actual shrink wrapping is a definition of normality, which there is no more labels once you are training it, which is the unsupervised part. So typically this is called semi-supervised learning. I think it should be. One quick question, sorry to interrupt you. So how do we ensure that the normal data without any anomalies is fed for novelty detection in one? Exactly. So that is the labeling part sadir see what that's the limit that's that's what makes it ex it's very effective but it makes it expensive because now somebody has to take a sample of the data sufficiently large sample of the data and from that inspect and throw away the outliers some human being has to sit in judgment know that these are outliers and throw it out and create a reasonably big sample of normal data so that the one class SVM can understand the concept of normality, it can shrink wrap a region of the feature space as normal. Okay, got it. Yeah. That's that. That is the thing. See, whenever you get to some form of label data, like, you know, needing a supervised, any part of it is supervised learning, even partially, the costs go up. But generally, it also means that the performances go up a little bit, quite a bit. So there are more techniques guys, you can apply many, many, many such ideas with simple ideas to detect anomalies. And there has been a fairly large literature of anomaly detection using classical topics. Some of you have been through the boot camp, if you remember we did these things in the lab. Do you guys remember we did isolation forest and novelty detects? Yes, we also did the LOF, local outlier factor. Local outlier factor is a metric, yes. You can find that for points. So for density based clustering for example you come up with the lof factor and so forth so yes we have these techniques now what i would like to do in the interest of time is move on to deep learning based methods deep deep anomaly detectors and this takes a slightly different frame of mind because these are actually a little bit bigger ideas so before i go into it just a question it's a little bit early what we should do is let me just ask if there are any more questions because i'm going to move past Would you have links on your website for additional? Yes, I will. I don't know if I have already added an entire section on anomalies. If not, I'll add it. And so there are some very good papers and review papers. One of them just got published in December itself. That's the most comprehensive review of anomalies in deep learning across the board. I'll post all of these review papers. And any other questions, guys? guys. So how many things we did? We did the Q-keys method for one dimension. We did clustering methods, especially dbScan is a really good one. It automatically comes out with outliers and the LOF factors and so on and so forth. We dealt with the fact that other clustering algorithms can also be used. We use the concept of just the size of the cluster being able to tell whether some cluster of data is anomalous. We also dealt with the taking k and k nearest neighbor, distance, average distance to the k nearest neighbor as a metric, as a distance metric to create a one dimensional measure now Raja Ayyanar?nk Raja Ayyanar?nk Raja Ayyanar?nk Raja Ayyanar?nk Raja Ayyanar?nk Raja Ayyanar?nk Raja Ayyanar?nk Raja Ayyanar?nk Raja Ayyanar?nk Raja Ayyanar?nk Raja Ayyanar?nk Raja Ayyanar?nk Raja Ayyanar all, you tend to come to an anomaly score. Then we did the isolation forest and we did the one-class SVM. Isolation forest is based on the idea that outliers tend to have shorter paths to the root. The regions in which they belong to are not reached by too much splitting, whereas inliners can only be reached after you have really split the regions into fine little boxes right multi-dimensional boxes that's the concept behind isolation first a simple idea and then you take an ensemble of it just to make sure that you don't have errors right you have a good robustness there one class algorithm is a novelty detector. What you do is you take only the normal data and you let SVM then understand, discover the one class shrink wrap it in a sense, right? As the definition of normality. And then whenever a new data point comes, it falls into the shrink wrap regions of yours, then the SVM will declare it to be, by the way, here it is looking like one region, it could be multiple regions in the feature space. So it has those, anything interior to those regions are inliners, anything outside of it are different degrees of outliers. And one class SVM also does these degrees of outliers once again, like the soft outliers, hard outliers and so forth. It creates a definition again of anomalies code that you can use to figure that out. A lot of this will become very real when you do the set of labs we are going to do on wednesday so i'm about to get into deep anomaly detectors guys would you like to take a 10 minutes break before we continue yes yes please ah yes let's do that i'm pausing the recording. In the first part of this session, we talked about the well-known anomaly detection algorithms. They fell into the class of outlier detection and novelty detection, right? There's subtlety between the two. Outliers, you give all of the data, the normal class as well as the outlier data, and you let it do that. Novelty detectors is a little bit more involved. You only give it the normal data to train it and then it detects novelties in the data. So we went through a slew of examples, clustering and care nearest neighbor and isolation forest and one class SVM and all sorts of arguments and there are many more that you can do. The literature is vast and rich. By the way, there is an excellent book by, I believe his name is Charu Agrawal, who has written a book literally called Outliers. I will post a link to that. If you are in this field, if you intend to be in this field of fraud or in any way anomaly detection, I would really recommend that book. It's more a reference. It's perhaps not the best book to learn from, but it has been considered very highly in this field and reading it once at least is a must if you if you are a practitioner in this field it's very readable and after you get the main ideas once you know your algorithms you're just seeing those algorithms being applied to I'll post a link to that. Now I will talk about deep anomaly detectors. See, deep learning has had a profound effect in many, many areas of machine learning. It has taken the state of the art forward. Many problems it has taken the art forward now when when you think about deep learning architectures the very first thing that may come to your mind is just think of it as a classifier between anomalous and normal points then generally the classifiers with deep uh with a deep deep learning architecture neural architecture, tend to outperform other architectures. Not always, but quite often tend to outperform. Therefore, it's our duty to at least try that. And then if you want, you can marry it with other class imbalance techniques and so on and so forth and see what results it gives you. You get a huge improvement just applying deep learning, deep neural network based classifiers to this. But then, so that is one technique and I won't talk about that because we all know how to do that. Use deep networks use deep networks for classification. That is good. But now I'm going to talk, so I'll leave this thing aside. There's hardly anything to teach here. I will focus on two specific things that has had a large amount of success in anomaly detection. In fact, it has really moved the state of the art forward in many, many problems, the state of the art performance in many, many real world problems. So these are two different neural architectures we'll talk about. One of them is called GAN, Generative Adversarial Network. And the other is called Autoencoder. Now, some of you I know have missed some of the sessions. So if you have missed either of the sessions, I would strongly advise you to go to the course portal, look up the associated videos and review it later. At this moment, I will give very quick summaries of these two because I expect most of you have attended the lectures. Nonetheless, I will give a summary of that. So let me start with GAN because that's what I taught first. A GAN is roughly this. It's a cat and mouse game between a fraudster mouse game between a fraudster and a cop who is trying to detect it. So imagine that there is some real data, some real, so one basic intuition could be that you have a pile of real data. Imagine that it is, let's say it's coins, right? If you think of each data instance as a coin, it's data. So X comes in, right? XI comes in from there. So what happens is that you can give particular neural architecture, I'll just mark it like this, the discriminator, the police, think of it as the cop or police. its job is to tell the real from the fake. And then there is a guy who is freely obviously trying to make counterfeit currencies, counterfeit coins in some sense, he takes a random, let's say, Z. Z is taken from some random. Either you can take it from normal distribution or you can take it from just uniform random. Make your pick some sort of a random data and it has the ability to take random junk and transmute it into a coin gz. So let me just call it x prime is equal to g of z some transformation you apply and you give it to this discriminator so the way the idea is that the generator tries to make as legitimate a coin as it can as good an imitation of a counterfeit coin as it can and the job of the discriminator is to be able to tell the genuine thing apart from the counterfeit right so it's sort of an adversarial game between the forger and the just the cop right or the expert who is doing the discrimination and And so what you do is you give it some real data and the discriminator says, okay, this is real. You give it a sample. You say that this is real, label data. This becomes your label data, whether it's real or not, the discriminator gets trained. And now what happens is in the beginning, the generator will produce junk. It won't look anything like the coin of, let's say, of your country, let's say the US coin. And so the discriminator will be easily able to tell them apart. Then what happens is you back propagate that loss here to the generator. say aha i got that wrong by a wide margin now what do i do for my uh how do i you know change my uh you know this weights in such a way that the next time i'll produce a little bit more realistic coin and so you keep doing this the discriminator keeps getting trained and gets better and better in telling apart the counterfeit from the real and the generator becomes very good as good as it can be in being able to generate very realistic coins so at the end of it when this game stops the beauty of this is you end up with a generator which now can generate a lot of fake data which is realistic or if you think in terms of coins in this metaphor then it can generate it can perpetually generate free money now free coins now right and all of those coins will be extremely good counterfeits. So you can pass them along in the nearby, I don't know, 7-Eleven or something like that. Right. Those are one of these stores. Convenient stores and wherever it is that counterfeit money is floating usually. So this is your generative adversarial network. Now, the magical thing here is this generator or the generating function. You take Z, which has been sampled from some sort of a random process, right? Stochastic process. It could be a normal distribution or a uniform distribution, or it could be anything you can pick doesn't matter a stochastic process right some random let me just say a random process from a random distribution sample And then just GZ is magical. It is able to take this. And that's why these are called generative models. It's a generative model, right? Because at the end of the day, you can use the generator to go on making money, right? Or fake coins, counterfeit coins. So this is your generative adversarial network. Now this is obviously a very quick summary. I wouldn't go too deep into it. Any questions guys in this little quick review? Do we recall all of this? Yes, Asif. One question, Asif. Go ahead. So in this context of fraud detection, the DE is the one which is getting trained and No, no, no, I haven't come to fraud detection. I'm just giving you a review of GAN. Okay. So I will relate it to fraud detection, but in a little bit. It gets quite interesting, actually, how you relate it to fraud detection. It's a surprising twist to the plot here. So if you think of this as a game between the thief, the counterfeiter, the forger and the cop, there is a very lovely twist to this whole thing. Now let's think in terms of autoencoder. The autoencoder is typically the under complete, I'll just take the under complete version. Typically, the under complete, I'll just take the under complete version. It doesn't matter the same argument applies to. And now I will just use representation, this and mirror image. So this is, let's say what you do is X goes in, it gets transformed into a hidden state. Let me just call H, where H is encoder. This part is the encoder. This part is the encoder, this part is the decoder. The encoder produced encoder is a box. X goes in, a hidden state, H comes out. That hidden state is some function that is that hidden state is some function is some function of x some non-linear function some highly nonlinear x x being the input. And what you do is obviously the way you realize that highly nonlinear function is by having many layers typically of either convulations or MLPs, multi-layer perceptrons, dense layers, inside it. Right? So there will be, I won't draw the figure to complicate this picture, I'll keep it neat here, but you can realize that there are many many layers inside it. That's why sometimes people use the word stacked autoencoder. In other words, there are many stacks of layers inside it. Those layers could be convolation layers or MLP layers, if you feel how it is inside it. It's quite common. And you guys have seen the code. We did the lab using both convolations and standard MLPs, multi-layer perceptrons. So that's that. then you have another function g which is in essence the inverse of f right it managed to reconstruct it is some function that manages to reconstruct this as closely as possible so by creating a bottleneck, you come up with a hidden representation of your data. Right? Nice hidden representation of your data. The last function that you use is this x minus x prime. Let's say squared in some sense, I'll just use a vector notation, Right? So let me use a more of a matrix notation just to say that this is it. In a matrix notation, which is basically the sigma i, xi minus x i hat squared. Right? This is it. I'm just writing it as like this. This is the reconstruction loss part. We call it reconstruction loss. Right. And it is just a mean like if you divide it by number of data points, of course, it becomes the mean squared error. Right. Usually when you're doing this, when you're doing real value data, let's say image. On the other hand, if you're doing discrete data, then you can change this. And this is one of the points I think I didn't mention. The loss function, then you can use, of course, your cross entropy loss. So either way, then, of course, you put a softmax in the end and you use a cross entropy loss but typically people write this by implying that with the understanding that if you're doing categorical inputs then you will you will sort of implicitly understand that your last term will be reconstruction loss will now move on to the crossing group now this is the basic autoencoder and then we always have an extra term in the autoencoder and then we always have an extra term in the in in many I taught you many variants of it a regularization term omega regularization and that regularization term could be omega could be based on which class of autoencoders you're dealing with omega let's say for this let's say for just l2 regularization simple this is quite literally that you know your weights uh you're doing a l2 regularization is as you know the standard or l1 or l1 l2 regularization you can do whatever you want right you could put it like this then by the way there is a i don't think i taught you we are are talking of the L1, L2 regularization. Sometimes what people do is that we write the loss function in this. You will see in the literature in the autoencoder, we do the log cosh autoencoder. What it means is that you replace this reconstruction loss part with a log of the cosh of the reconstructions. So what it does is, in a simple way, it has the effect that when for small errors it uses L2 and for big errors it tends to use L1 or something like that. I forget what the exact details are. So there are many many variations. I'll just leave the word here in the notes. And there are many many variations of the autoencoder. Then we talked about the contrast, the contrastive loss autoencoder, if you remember, which was stable in the presence of perturbations. And what we did is that we looked at the gradient of the Jacobian. We looked at the gradients here, gradient square. I won't go into the full mathematical expression. You have covered that. But it had to do with the Jacobian, basically, a square of the Jacobian in some sense. So that is one. And this is your contrastive laws. Contrastive laws, I believe. Or contractive laws, whatever. I forget contractive loss. Then there's a third, the more important one that we talked is the variational autoencoder, which we did in more details. If you remember, what does it do? It uses in the autoencoder, it puts, it learns, instead of learning the hidden representation, it learns the hidden representation in terms of mu and sigma of a normal distribution. in the latent feature space, you force it to have a Gaussian distribution shape. So the only thing that matters is a mu and sigma that you learn. And then you sample from that. And then you have your decoder part of it. And if you think in terms of the omega, basically for this, the omega becomes the KL divergence. It's a KL divergence term. And we won't go into it, but I'm just tabulating it in one place. So there are many variations of it. One of the autoencoders that has recently come and gained a lot of attention, which is based on optimal transport theory. And once again, it's mathematical mathematical maybe someday in one of the Sunday sessions I'll talk about it has to do with optimal transport theory and the Wasserstein distance based instead of the KL divergence it uses the Wasserstein distance and it turns out that it outperforms it's very well conditioned and seems to outperform all the other or many of the other most of the other auto encoders in many many situations so so the state of the art keeps moving continuously so this is all about autoencoder but we will keep ourselves for simplicity to this and forget the omega term just look at the contrastive loss term and stay with this basic architecture for autoencoders so let me repeat what i said we have the generative adversarial network gans whose architecture we just understood and we have the autoencoder are we together which tries to minimize the reconstruction loss. So it comes up with a very efficient hidden representation of the input data. Input data, for example, in the MNIST is 786 dimensions, if I'm right. And we did that with what, 8 or 16 dimensions in our lab and yet we got a pretty good reconstruction and very low reconstruction loss do you guys remember that right and we could use it for all sorts of things we could use it for denoising we could use it to get sparse and all sorts of things we could do with the auto encoder So autoencoders are a very powerful thing and of course GANs are also very powerful. You can say that you know these are some of the most successful architectures, the combulations, the recurrent neural networks, the attention, the autoencoder, the GAN. These neural architectures are the workhorses, obviously, of the industry to quite an extent, very ubiquitous everywhere. So now I'm going to connect the dots between them and anomaly detection. So the thing is, how do we do anomaly detection using it? With autoencoder. So let's take autoencoder for anomaly detection. And the idea actually, the simplicity of the idea is just quite amazing. Let me take auto-enrollment. So what happens is that you start just like with the one class SVM, start with take only normal data. only normal data so once again the burden is on you to train this auto encoder only with normal data are we understanding that nice yes just with normal data that is the most important. So then what happens is when you train this with normal data, it will minimize the X hat will approximately be X. Isn't it? And so the hidden state representation that it creates will be, it would have learned that manifold, you know, the latent space manifold. People often use this word for manifold. How should I do it? Manifold. What does manifold mean? Remember, think of a bed sheet in higher dimensions. That's your manifold. So what happens is that all of these input points, they tend to become points in the latent space, but those points are not uniformly spread in the latent space they are sitting or they are embedded or close to some higher dimensional bed sheet of sorts crumpled up bed sheet right the manifold well crumpled up back sheets a very very rough idea uh so curved surface hyper surface curved surface, hypersurface. It is close to that, it is sitting on that or close to that. Are we together? Right? And so in some sense, by making x hat close to x, what it has done is, at the latent space, this latent vector space, it has discovered this manifold. It is the surface on which, hyper surface, on which all the hidden state vectors are sitting. All the HX are sitting. Are we together? And that is occupying only a part of the higher dimensional, the entire latent vector space. Am I making sense, guys? Yes. Right, that is it. So now the idea is that if you train it only with real data, I mean, a normal data, what happens when you bring in an anomaly? So let me use y for, can I use, I need a special symbol for anomaly, what should I use? Would it be okay if I use y for anomaly? Right, so remember, I know why we use for outputs. Let me just say anomaly, xA. I feed it XA and you get X hat A. What will happen is this XA, which is anomalous, it will sort of be not exactly on the manifold and a little bit away from the manifold, then what will this system do? It will, it will, so this is trying to, you take this, it is trying to fit it somewhere here and it is trying to reconstruct what it thinks a close approximation to XA, the anomalous. But remember this autoencoder knows only one thing. It is like you have taught it to speak only English. So whenever it sees characters, it tries to speak English or read it as English in effect. Right. It's something like that intuition that whatever you give it, it will produce normal data as output, right? Your data that will come out will be normal. This will look normal, right? Let's take an example. Suppose you have digits zero to nine, one, two, like that, right right but now instead of feeding that you feed it an h an image that is an h right what is the best it can do maybe whatever it is closest to maybe it is close to four or it is close to eight. It will be close to these two things maybe. I don't know what else I can think in the digits that it will be close to. So it has learned how to reconstruct the digits. Let's say that you have only trained it with digits. Are we together? So what will it do when you feed it something like this? It will not be able to classify. That's right. It will just try to create something closest to this, right? It will enter some space off the charts of this manifold and the closest thing on the manifold would be these points and the reconstruction from these points would look one of the normal data points right it will look like maybe four or it will look like an eight or something like that let's say that you feed it an edge. Are you getting an idea, guys? Are you seeing the intuition here? It may end up producing one of these, something like this. But now is the fun. You have duped the autoencoder because now what will happen, if you look at the reconstruction laws which is the X hat a minus X a what do you expect this to be would it be much bigger yes and for normal data isn't it you fed in h and 8 came out or 4 came out right so you would have a pretty significant reconstruction loss now reconstruction loss is a number isn't it it's again a scalar so you can again have a one- distribution. Let's say that your reconstruction losses are all something like, I don't know, I'll just create a distribution. Let's say it's here. And then suddenly what you will notice is all your anomalies reconstruction losses are somewhere here. Do you see the beauty of this argument guys? It's pretty well argued actually. And that is it. You just train an autoencoder with normal data. And then once it has become very good at minimizing the loss of normal data, it has basically been conditioned. For example, if you think of in terms of MNIST, it has been conditioned to produce zero to nine, some variants of that. So it is reconstructing from this manifold of normality to that. And so you give it something that is off it. The best that it can do is it can pick things close by on the manifold and reconstruct that, right? I decode that from the latent space back to X hat. And therefore you will end up with a significant contrastive loss. will be significantly more than for normal data. So if you were getting very strict and people don't tend to do, it is more like novelty. Technically, I would say it's a novelty detection algorithm for anomalies, right? Because you trained it only on normal data and got away with it. So guys, pay attention and ask me if there is any part of it that we didn't understand in this argument. Asif? Yes. Is this actually analogous to the one class SVM we talked about earlier? Yes, because there also to train, you gave normal data. You carefully weeded out the outliers or anomalies and give it only normal data in the same way here you give it that normal data. Right, you know how like on Sunday was said, you know you can probe so deep neural networks and see like what they're doing like cancer doing like kernels is it kind of like this. Like did anyone actually. related to that paper. Okay. Okay. I mean, it's a rather long argument, but if I were to try to relate it to that, I would do it this way. See, there, those researchers are claiming, and obviously the research community still has to weigh in, but the argument seems very persuasive that deep neural networks are nothing but in some sense support vector machines but with an interesting kernel, a new kernel. It is a kernel which is a path integral over the tangent over time, like basically you take the tangent kernel, right, neural tangent kernel, and then over that you do a path integral, right. So it can do that. But at the end of it, what it does is that the conclusion that they were coming to or the more significant observation they were making it therefore follows that deep neural networks do the same thing as support vector machines they are they are machines that somehow remember the training data or at least the important ones of them the lighthouses right the significant ones of them it remembers some more than others let's put in this but the other ones are less remembered but ultimately it somehow remembers the training data in the network and it but besides that it is just not a instance based uh in which you get it will regurgitate it has the ability to generalize you know the whole theory of support vectors or the kernel machines is that it can generalize from those instances. So when you give it a point in the feature space, the first thing it does is it goes to the kernel space using the mapping function and finds the dot product to all of those training instances, especially the support vectors, the lighthouses. And based on that, it is able to therefore interpret it. A similar statement, now let's bring it back here. See what has happened is if you take your autoencoder and show it on your normal points, right? What will happen? Your input, so long as your input is close to, like, is something normal, right, it is within the realms of the normal, what happens is that the latent vector that you produce, because it is made out of essentially the points it has learned, isn't it, kernels, the similarity to kernel similarity to points, data points that it has seen, it will produce a latent space, a latent representation along those lines. Now you give it a point that is completely different, right, from what it has seen. So now what happens? Remember the kernel similarity function, if you remember that the argument in the paper is the kernels, the K, right, which is based on the loss at this and the loss at, you know, points. I wouldn't go into the full details, but this will, and then you're reconstructing it based on AI. So I wouldn't go into that, the summation of I, the Y is equal to this kernel, plus not to mention the Y's term. the outputs are based on the nearness of closeness or similarity of this point in the feature space to other points that the system has seen. But this is way off. And so the performance of your support vectors or your kernel machines will be poor. And to the extent that deep learning can be thought of as kernel machines, its performance will be poor. But that is the beauty is that that is exactly what you want. You want it to perform poorly for anomalies. Dennis, are you getting that? Yeah, somewhat. So the argument is very simple. You take these neural architecturesures you train it to work well on normal data and you and then you sit back and grin you throw it a curveball you throw it an anomaly and you see it fail right you see it come up with a bigger loss and when it comes up with a bigger loss you know that this is an anomaly that's what you do with autoencoders. Thank you. Any questions guys in this? Is this argument clear guys? This is it. If you understand this, you have understood the autoencoder version of anomaly detector. Is it effective? Tremendously effective, especially as you use more sophisticated versions of the autoencoder, start with variational autoencoder or use the new, the Wasserstein and distance based autoencoder based on optimal transport and so forth. There's a whole flurry, as I said, autoencoders are an industry in their own right or sort of research area in their own right, not an industry, but a subfield in their own right. So you can keep using and trying different autoencoders, more well-conditioned autoencoders, and you can get better, quite good performance on anomaly detection. They were considered a bit of a game changer when they came out for anomaly detection. But the argument is very simple and elegant. Now what about GANs? What are GANs up to? So I noticed that we are a little bit running out of time and soon Shini will say that we are reaching point of saturation. Shini, are you already feeling that for now no no good because we still have the guns to do here comes something quite interesting the generative adversarial network what can you do let's go back to our pi the old example you have a mountain of coins the old example, you have a mountain of coins, right? Data, by the way, I hope you guys don't mind my using a very sort of illustrative, but somewhat imperfect examples, or not very accurate or precise examples, because it's a little easier to think with that. Let's say that you have a discriminator D, you have a generator G, you take Z sample from some sort of a random process, let's say a normal process, normal distribution or uniform distribution or whatever, pick your poison here, it doesn't matter. But G has the magical, generator has the magical ability to produce something which is uncannily similar to X when it gets strained. Are we together? Right? And so you have a discriminative loss. This is a discriminative loss. you have a discriminative loss. This is a discriminative loss. This can be a standard classification loss, right? For example, cross entropy. Cross classifier loss. Classifier loss. Now we are in familiar territory. And the whole idea is that you can keep training the cop in the fraudster. And at the end of the day, what will happen is this will become very good at producing X, realistic coins. It will become very good at that. So now, see, the way you argue is quite interesting. And so this has gone through many, again, many versions of GANs for anomaly detections have come about. I will just take the core idea. There are many variations of it. Maybe I'll talk about a couple of variations. What you do is you again take only because now you want to fool this generator. So what you do is every time you give normal data, this guy will generate, I will try to fake the normal data, right? Something like the normal data. At the end of it, what you do is you take only normal data. The generator will become very good at faking the normal data. And then what you do is suddenly you come back with an anomaly. And then the X anomaly that it will produce, the generator will be forced to produce. It doesn't know. It has learned to produce, produce for example zero to nines and you suddenly you bring up let's say h or something like that what will happen generator has completely not been trained to produce an edge so it will produce the closest thing to edge right and so what will happen to your classification loss here classifier loss here So what will happen to your classification loss here? Classifier loss here? The output, the output that it produces, the XA that it produces. Let's say that it's an image, XA prime that it produces and XA that you have, the anomaly that you have. It's very interesting. So so this will be let's say the letter h and this has produced the the letter four like this when you subtract these two in an image wise what do you get you get regions of error clear regions This, this, let's say that I write the four like this just to emphasize, right? And the parts that will match are. So this part will stand out very prominently as mistakes. Do you agree guys? Do you see how it works? Yes. Right. And so, and not only that, therefore the added benefit of GANSes, not only can it tell you that this is an anomaly, but it can tell you where in the region those anomalies exist according to its interpretation. Do you see that guys? So that's a nice thing. You can literally compare these two and tell in a visual way where the anomalies in the image exist if you're dealing with images right or let's say that the coins are images of coins real in this thing then it will tell you that this these are the places that the counterfeit coin got wrong right so this is the basic idea of using the generative adversarial networks to do anomaly detection. Now, while this is the base idea, people have cultivated it quite a bit. And they have gone and created many architectures, creating the state of the art models. I will give you maybe two architectures I can talk about, which are interesting. So people have created lovely architectures, I will bring one of them here. These three architectures, I wish I could write on them, I can't. Or maybe I can. I don't know. Alright, so when I put my mouse here, is it visible where my mouse is, guys? Yes. Yes. Okay. So this is your basic GAN kind of a thing, the way we talked about it. Now, the first extension that people came is that they created an encoder, which in some sense is the opposite of the generator. They trained it together. So, this is called the EG-BAD. I won't go into too much of it. But basically what you do is you add an encoder layer okay i can't circle it and you train something uh opposite to it basically it just helps the whole situation but the idea is the same and likewise there is a the latest is the c which is the gan anomaly gan and anomaly word put together sandwiched is the anomaly okay what the anomaly does is something very interesting it before it gives you the real data to the discriminator the genuine data it passes the genuine data through an auto encoder so X becomes the the X first goes into the the sort of a encoder part that GE, then it goes to a decoder part. But when it goes to the encoder, it becomes Z, a latent representation. And then when it goes to GD, the decoder, it it is reconstructed as x prime and then finally you pass it through one more layer of encoder which is this to produce z prime right and what you do is you have a discriminator that tries to distinguish between the original x and the origin and the x prime right the usual thing so all of it gets trained but one of the byproducts that happens along the way is that in this process you can also look into z and z prime and by trying to reduce the gap between z and z prime making z and those two latent representations similar, you end up sort of training this encoder also along the way. But at the end of it, it all comes down to there's a bit of, little bit of mathematics and this and that, but the end of it is why would you do something like that to the GAN? It turns out that by sandwiching an autoencoder in front of a GAN or into a GAN, you get more stable, higher quality anomaly detection. So it is not always true for sometimes B does well, sometimes A is good enough, sometimes C. So there are many, many variants of GANs that people use, they have come up with, that they use for anomaly detection. So I won't go into all of those. Suffice it to say that if you understood this idea that I had here, where was that? This, if you understand this much, you have understood how GANs work, right? For anomaly detection. So autoencoders and GANs are quite often used and there's a whole you know sub fields that keep trying to create many many variants of this in fact I can show you one particular thing right somebody has created with a package and sat down and implemented a lot of these autoencoders. Now, this is just an implementation of the autoencoder. You can use any one of these autoencoders for your anomaly detection. Variational autoencoder, conditional variational autoencoder, then the WA, you have to look at it. So each one of these, you can go and see what they're talking. Yeah, this is the Westerstein distance, the Westerstein autoencoder. And this itself has two variants, the MMD and the, what is the other variant of it? MMD. And okay, so, okay, there's one more variant of it. But anyway, it mmd and okay so okay there's one more variant of it but anyway using rbf kernels and using imq kernels but there is one more version that's not implemented yet then you can imagine this is a long list you can keep going down this list right the log stash is what i was talking about all it does is the mean squared error replaces with the log gosh love gosh so what are we seeing between the left and the right side what is the distinction between the two i think it is the reconstruction loss this is the right is the sample left is the reconstruction of the sample, how well it reconstructs. Actually, yeah. No, no, no, I don't think it looks like reconstruction. It looks like some of the sample data is given on the left and some of the sample faces generated is given on the right. I don't think they look anything close to each other. No, I see for left side reconstruction looks better images. If you see image icons. That's right. That's right. Because these are all variational, right? Generating new data points. For example, this guy wearing a cap, I don't see anybody here doing that. Yeah, I think that is it. I may be wrong. So this is it. Especially that one where it's really fuzzy and for the WAE MMD RBF. and for the WAE MMD RBF? That's right. These are the samples. Actually, you know, I have a suspicion that reconstructions are reconstructions of the input data and samples because these are variational autoencoders, they are samples. You'd have to read the documentation. Okay, all the models are trained, the architecture, here are the results. It doesn't give an explanation what is left and right, except saying reconstruction and sample. So we don't know whether the sample is the original samples or it is sample from the variational. Output? Yeah, it is, yeah, the output of the variation. You can generate from the, you know that just use the decoder part and sample from the Gaussian to generate this link come again the link it's too faint Kate I can't hear you the link I noticed that the second column has the light is right yeah yeah so we have to probably go to the paper and we'll have to look into the paper by the way this paper western is a very good paper Raja Ayyanar?nilrohina?nilrohina?nilrohina?nilrohina?nilrohina?nilrohina?nilrohina?nilrohina?nilrohina?nilrohina?nilrohina?nilrohina?nilrohina?nilrohina?nilrohina?nilrohina?nilrohina?nilrohina?nilrohina?nilrohina transport these days has gotten a lot of attention. So this is the optimal transport theory. It looks as how do you convert one distribution into another. So it's in the same flavor as scale divergence but it tends to do much better. So I don't see those images. Yeah, it is just dealing with MNIST. It is not dealing with the... Yeah. So there's one version using something called MMD. We have to look it up. And the other is the GAN using a generative adversarial network. So it's not clear to me, at least, if you guys can figure out whether the sample is real sample or they generated from a variational autoencoder. So guys this is it. This is the theory, at least in the lab we'll cover this much theory. The field of anomaly detection is rich and vast. Deep learning methods tend to focus on pretty much be based off these two core ideas. Any questions? Asif, quick question. On autoencoders, you have mentioned that when you have a low accuracy rate, right, that's where we detect it's a anomaly detection that's how it is done right so there can be any other factors too don't use the word accuracy low performance is okay accuracy is a suspect word because you know when the data is so imbalanced as to have anomalies accuracy of just even the baseline classifier will be extremely high. So you have to look at the precision and recall. False positive, false negative rate. Those are the things you focus on. So how good is the specificity and things like that? So generally what happens is that the broad statement is that deep learning algorithms tend to do tend to fare better in many domains practical domains so this can be extended to even for a language model right like nlp for classifications where you can text classifications where you can even detect anomaly detection there for fake news anomaly detection there for fake news. Yes, yes. So please give me a moment because we're going into Q&A. I'll stop the recording.