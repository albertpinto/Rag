 So the autoencoders. The basic architecture of the autoencoder is like this. It's like this. In your mind, we always think of autoencoders as like this. A symmetrical object, laterally symmetrical. This is the hidden representation. People often write it as H or Z or something like that. This is the latent space representation. And this is your input. goes in and x prime comes out actually literally use capital X x prime comes out and the idea is that if you take a loss function which is X minus X prime square it summed over all the i values, then, and you try to minimize the loss, then you have an autoencoder. The basic idea was that let this function be f such that f takes you from x to the hidden representation. This is the encoder function. This is the encoder function this is the decoder function and G such that you go from a hidden representation back to X prime right so this is the reconstructed reconstructed input corrected this is your basic auto encoder now in this in this autoencoder, this entire thing, you may have just one hidden layer, which is the latent layer itself, but much more commonly you have many more layers sitting here. And this side is literally the mirror opposite of that. Now, the reason we keep more layers is the same reason we have many layers in a deep neural network. Namely, computations are much faster. It is more efficient. The learning is quicker and better. So you have many more layers here in between. And we will see this in the lab today. So this is your vanillailla Autoencoder. The loss function, if you look carefully, it is very common sense thing. It is basically saying, what is the gap between the reconstructed input and the input? So this is the input. This is the output. And the output should be able to reconstruct the input even though we have passed it through a bottleneck right this big bottleneck which comes out with a hidden representation of of the input so once you have trained it then what happens is so let's put it this way this part is the encoding part Let's put it this way. This part is the encoding part, encoder part, and this part is the decoder part, decoder part of the architecture. Are we together? Now what do you do? Once the training is over, then what you can do is you can use the two pieces separately. For example, you could do things like while encoding the data. So suppose you're doing data compression or coming up with a hidden representation. You can just give X and H will be your output. And then if you so choose, you can store this value, you can store it. So one example could be you can, for example, compress images if you want through this process. The other part is later on when you want to do a decoding of it, what you would do is while decoding, you now use the other part of it, the decoder part of it. And remember this was your f and this is your g. And what happens is X goes in and X prime comes out. A reconstructed input comes out. And so one very simple use thing that you can think is, suppose you have a data whose representation is rather large. For example, a TIFF image or something like that. One thing you could do is you could pass it through an autoencoder, through an encoder, get the hidden representation, send the hidden representation over the wire, send it over the wire. Then at the other end, you can reconstruct the original image from this compressed representation. So you can use it for compression and transmission and so forth, it's a very basic thing. But there are other use cases that we learned about the autoencoders. For example, we learned that in the autoencoder, autoencoders are in some sense, a generalization of principal component analysis. Isn't it? In a principal component analysis, you look for a linear lower transformation to a lower dimensional space. The operative word is a linear transformation. And we do our eigenvalue decomposition and so forth of the covariance matrix to get there. It turns out that autoencoder and the PCA are quite related. So let me put a sidebar of PCA versus autoencoder. If you do an autoencoder with only one hidden layer, the edge itself, and nothing else, and you have x going in, and you take the sigmoid function, you take a unit activation function. In other words, you don't activate, you don't do any non-linear transformation. So then what would happen is, once you have trained the data, you will find that your H is related to or equivalent to a PCA. It's not directly a PCA. It turns out that it's sort of a, you need a little bit more, but it is equivalent to a PCA. And that is something. So in some sense, you can think that the PCA is a generalization of your, I mean, sorry, autoencoders are a generalization of a PCA. But autoencoders are more than PCA. They, they have this ability to do a lot of things that you don't expect, for example, to be able to achieve through PCA, and so forth. So one of the examples is, we talked about is the denoising autoencoder. In the denoising autoencoder, what you do is, is the same autoencoder architecture. It doesn't change, but you add one extra layer. That you just call it the noise injector. So you take x, it goes in, what comes out typically is represented as x tilde. It is the same input with noise now. So you introduce noise. The noise can be anything. It could be Gaussian noise, normal noise, particle noise, picture noise, and you can inject that. And then what you expect though, the loss function is not with respect to the x tilde, but it is with respect to x prime. Loss is still with respect to the original image or original data input. It need not be image, it could be anything by the way. So it could do that. When you do this, what you do in effect is you create an autoencoder that cleans out noise and it will clean out the noise from the image and so forth. And in today's lab, we will see how it is done. So here, what you're doing is you're sampling into a noise thing. And basically this is, you can think of it, for those of you who are mathematically inclined, but there is a, the relationship of this is essentially, roughly speaking, you take the input and then you apply, you do some additive noise. Some additive noise you get to come up with a distribution for this. So now comes a third point I mentioned, which was the contracting autoencoder, which is very similar to the above, with the one exception that what it does is you don't inject noise, you pass in x, you have your fg. So it goes in through this. But what you do is, sorry, you play around with your loss function itself. In your loss function, what you do is, x x prime loss function will have the normal x minus x prime term, square term with the appropriate sigma and so forth. But what you do is you actually add something very peculiar. You add a resistance to change. You say that the value, the hidden representation, h, how does it change with, let me just use some vector notation, gradient of h with respect to h, right? In simple terms, in one dimension, it is like the rate of change of H i with respect to any one of the input features x j. You try to minimize the change. It's very peculiar. You're trying to make the internal representation insensitive to changes in the input. You say that, well, you know, one easy way to do that is just make it constant if you do that though this term will blow up in this part of the last term blows up so this part by the way it is the jacobian so you add the jacobian of that so and forget about the uh the let me just put it this way so this part encourages x prime to i mean this part encourages x prime to be to tend to x on the other hand this this part is interesting if you want x frame to catch up to x you would imagine that h itself would be sensitive to X to the input. But that sensitivity or rather over sensitivity is not, it makes the autoencoders unstable. So what you do is you add a resistance term, you add a penalty to the sensitivity. If you look at the derivative, what is the derivative? Derivative is a measure of sensitivity, right? So for example, if you have a curve like this and this is x and that is y, at this point you would imagine that y is very sensitive to values of x at this point, right? So a. Whereas at b, y is not so sensitive to, at b, y is not so sensitive to at b, y is not so sensitive to x. The derivative is small. Whereas here, the rate of change is very high. Your small changes in x will lead to a big change in y. So it's very sensitive to the input. So that is what it is. Derivatives can also be thought of as measures of sensitivity. And what you are basically saying is, let's add a penalty term such that the hidden representation, the latent representation, is not so sensitive to the input. Now, that looks paradoxical. If it is not sensitive to the input, how in the world would you have this x prime close to x, right? And so that creates two competing forces. One part in this loss function says catch up to x, output must catch up to x. But for it to do that, obviously, it will try to make the h the hidden representation somewhat sense quite sensitive to input and then there is a penalty factor that prevents it from being over sensitive to it so what happens is with both the denoising autoencoder and the contractive version by the way Jacobian term that penalizes oversensitivity. I forgot to mention the contractive one in greater detail. Today, I hope I've done that. If this math worries you, ignore the math. Just think of it as intuitively that you have a penalty term for oversensitivity. So what it means, what both of these do effectively is, let us say that your data, see, when you have data, let's say that your data is, and I'm now going back to the picture that is there in the original research paper, and they literally draw a picture like this. They say that suppose this is your input manifold, right, and it has some hidden representation. What you want to make sure is, suppose this is one data point. Let's say if you're looking at MNIST, this is the letter one and this is the letter two and so forth, right. What you want to do is, suppose you get a perturbation of it of the input you get something like this one prime these auto encoders are denoising and the contractive their main goal is to be able to project it down to the this underlying manifold and realize that one prime is actually one and so when it does the hidden does the transformation into the h hidden representation all these perturbations of one and let's say there's one another perturbation here of one prime of one prime, double prime, right? All of these, let me mark the points with deeper color, one, two, and the point itself. What you want to make sure is that all three of these are mapped to exactly the same point in the edge space. Are we together? So small perturbations of the input, your hidden representation stays insensitive to it, which is the value of a Jacobian term, or which is the value of in the direct denoising, inserting a noise, sample noise into the autoencoder. You prevent it from being unstable or being oversensitive to it. So that is the theory of autoencoders. Now you can build autoencoders in two ways. You can build autoencoders with feedforward network. network, you know, you can put a feed forward or dense layers or dense or linear or fully connected, fully connected, fully connected. You can pick your synonym. You can use these to build layers, to build your autoencoder, or you could do other things. You could, for example, use a convolational. Yes, and one of the things I'll show you in the lab is, it's a flexibility of it. So long as you stick to this basic architecture, something like this, you have an autoencoder. By the way, here I create a bottleneck. Now, I did mention that you can have overcomplete networks also. You can have this also for a sparse representation in which you, even though it looks lots of things, what you expect is for a given input, most of the hidden dimensions will be zero. You force most of these neurons to not fire and only some of them to fire. So that is the sparse autoencoder. And the literature of autoencoder is pretty vast and growing. There's more and more work being done all the time with autoencoders because they are so useful. and more work being done all the time with autoencoders because they are so useful. So in the lab today we will do the autoencoder, the denoising autoencoder, the convolational autoencoder, the denoising convolation encoder, and you will see that these are fairly simple beasts. They can easily code them up. Now I have put the sample code on the web. I noticed that some of you had asked me a few days earlier to put the code. Unfortunately, as I mentioned, I was really taking a break and resting this particular weekend, so I didn't post it. I posted it now, so you can download it. So with those words, let me see if I can bring up the lab. Give me a moment. Any questions, guys? Oh, by the way, I didn't cover the variational autoencoder. Again, we won't do variational autoencoder now. Autoencoder, because that's a separate topic. But the basic idea there was, again, it's a generative model. What you do is you have a hidden representation H. But the hidden representation is a parametrized representation. It is basically a Gaussian mixture, typically. If you think of it as Gaussian mixtures, then what happens is all you're caring about is the mu and sigma the mean vector and the sigma vector right there or let you use the little sigma because you have only diagonal terms so you make this part a bunch of mus and this part a bunch of sigmas then there is a sample you sample off this and then when you train this autoenc, what you can thereafter do is sample off this and produce your output. So that's the variational autoencoder. Now, the theory was, if you recall, we talked about variational inference. It's a very Bayesian argument. I won't repeat that today because we have a lab to do but this is just sort of a background to that so these were the auto encoders we covered there are more and on the website and a course page i put a whole link to some way the whole section for auto encoders and you will see that their entire websites dedicated to the research on auto encoders all the autoencoder work that has been done emerging and so on so that is a huge deal uh so with that i will now uh go to the lab part sir one question here go ahead so the jacobian is a constant, right, sir? Can you scroll up a little bit? I just wanted to check. I think it's Jacobian is a constant kind of a Lambda, right? It has some values. No. Or the whole term is Jacobian. The Lambda, that delta H. That is true. See, okay, so let me explain Jacobian. I'm going to have to switch to my Linux workshop, Linux workstation for a moment for the rest of the lab. But let me explain the Jacobian. See, the concept of Jacobian is very simple. It refers, of course, to the Jacobian. See the concept of Jacobian is very simple. It refers of course to the Jacobi brothers. So suppose you have a function y is equal to fx in one dimension. Okay. So now the sensitivity to a small change in delta x you'll give it simply as delta y is equal to df dx delta x isn't it right this is a very straightforward a small change in that so now on the now what happens is suppose y is a function of f x1 x x2. Now what happens? Now what happens is delta y is equal to delta f, a little bit, it will vary, it will have some change because of the response to changes in x1 plus some change in x1 plus some change x2. Would you agree with this? All right. Yeah. And now let's make y into a vector. So suppose there is y1, y2, right? So suppose this was y happens is we can now think of y1 as y1 is equal to f1 x1 x2 and y2 is some other function. Let me just call it g. Let me call one the f and the other g, or let me f and g, x1, x2. Are we together? So you would agree that this is still the same, delta x1 plus df dx2 delta x2. What about y2? Delta y2 is dg dx1 plus dg dx2 delta x2. You realize that suppose y has, there are two different y's components. So they will independently vary with respect to changes in the x-axis. You know, this is it. So now this entire thing, vary with respect to changes in the x-axis, you know. Correct. This is it. So now this entire thing, you can actually write it as a Jacobian. But this thing, do you notice that this thing here is, so you can say delta y1 df dx2 d g dx1 d g dx2 this thing applied to delta x1 delta x2 right you can you can work out the matrix decomposition so all i did is i took a matrix out of it in writing it this way So all I did is I took a matrix out of it in writing it this way. Can you scroll up a little bit, sir, that I just wanted to see that term, the one that you have written? Where? Little up. Yeah, I'm coming to that. This is it. So this is edge, right? So now, but anyway, but before that, I'm just explaining to you what a Jacobian is. Now this, so you agree that I can write it like this, right? That's right. It is like a matrix representation of what I wrote just above. Okay. Therefore, this is called the Jacobian. Okay. That's it. So it basically says that each part of the output, how sensitive it is to each part of the input, right? So now come back to your autoencoder. Just look at the encoder part. This is F, right? Right. And H is, this is, H is the output. And this is X, right? Basically the Jacobian is, so now let's say that h has two pieces, h1, h2, just as your example, right? Or maybe an x has three pieces, x1, x2, x3. I'm taking an example, right? So now what happens is I can say that I have two functions, h1 is this value here, is f1 of x vector, and h2 is some other value, some other, let's say, what you would call G there of the X input, right? And once again, I can create this matrix. Since F1 and H1 are the same, people often just write it as the output, the response of the output. By output, I mean just the latent representation H. H1, dx x1, d h1, d x2, and then I can talk about d h1, d x3, d h1, d x2, d h2, d x1, d x2. You realize that all i can do is i realize that this component is is the one because then i can write delta h is equal to this thing this jacobian times delta times the delta and i'll just write it in vector notation the delta x vector isn't it correct so this thing if you notice that it is what it is made up of terms which is the derivative of the output with respect to the input isn't it that's what the function is these are all derivative terms like component y y is derivative right this jacobian so one notation that people use in the literature is they say that it is just a generalization a vector generalization of the this this is this this is it that's right that's the matrix representation of the whole differentiation part but in the equation above there is lambda as well because why I was gonna wonder is the suppression term so okay let's go back so good point right the patient is not the the suppression part like because if we keep the term more that means it will be less sensitive like it depends on that is so see what happens is if lambda is very strong it it is emphasizing insensitivity. It is damping down. It is making H very insensitive to changes of the input. Right. Correct. You don't want it to make it too big, because if you make it too big, the neural network will say, OK, I give up. H is constant. Right. The minimum loss is when H is constant and you don't care for this loss. Even if the reconstructions are poor, no matter. But this part is the reconstruction loss. So what you want to do is you want to find the middle ground in which the reconstruction is very good, but your reconstruction is not sensitive to the small changes of the input. I see. That's fine. So is there any, like how do we decide that part, like the value of- Like the parameter of the model. It will be obviously between zero to one, but it has to be based on experiment. Yeah, see it also depends upon the degree of noise in your system. See, if your system has a lot of noise and you want it to be, you want to still not be unstable, what will you have to do? You will have to keep your lambda pretty strong because at that moment, you would rather be able to reconstruct something than be unstable. So application is basically as well. Yes, data specific. So you have to play with data. Remember always, it goes back to the old saying, data is king. Thank you, sir. So guys, just bear with me for a moment. What I will have to do is I have to get to my other machine and work from that one more quick question on the context before we switch to the lab so looking at the tooth i'm just focusing on the two examples that you had photo encoders one the denoising one the other is the the contractive encoder yes i'm assuming denoising auto encoder. Yes. I'm assuming denoising autoencoder would be suitable for let's say situations where I have a grainy image and I need to extract meaningful content from a grainy image, building a denoising autoencoder would use. I lost you for a little bit, I apologize. Could you say that again? I'm trying to look for an example to map each of these and build an understanding so for denoising and auto encoder i'm assuming it'll be useful to have a denoising auto encoder if i expect grainy images and it can be used to clear out the grainy part of the image is that a valid assumption that is a perfectly valid assumption that is one remember it also fills in the blank That is a perfectly valid assumption. That is one. Remember, it also fills in the blank. One of the things you do is if you take an image and you hide a part of it, it will learn to interpolate. Okay. Yeah. So the grain image could have been some lossy compression happened. And when I got it back, when I decompressed it, there were some lossy areas and the denoising autoencoder could enhance it. Could enhance the image and both the contractive autoencoders also will do that so i was trying to make a difference for the contractor so the way i was trying to understand the contractive encoder was yeah i have a situation of trying to interpret alphabets and they're written by different people there could be variations in how the alphabet is written that is a good case for using a contractive encoder is that correct yes yes yes certainly okay so uh pretty much you you will see that is literally a lab today huh you will see that in a moment okay all right give me a second okay so this just as a background So let us say that I have a function f that takes the input x and makes it into a hidden representation. moment ago and then you need a convo and then the point is that it's a lower dimensional presentation quite often except when you use a over complete autoencoder that too is possible but your basic intuition should be that it's a compressive form like this now if you take a unit activation function and you don't have any other hidden layers, except the latent layer itself. You come to something that is essentially equivalent to a PCA. Not exactly the PCA, but essentially equivalent to it. And then you need a decoding function, the G, that will blow up. So remember neural networks are just functions, continuous differentiable functions in some sense. So there it is. You recover your some sense so there it is you recover your uh your input it will come out as x prime it won't exactly match and so now your loss is in the simplest auto encoder your loss is the gap between the input and the the reconstructed input you know the output and the reconstruction, x prime being your reconstruction. Are we together guys? Does this all look very obvious now, so far? It's fine. It is fine, okay. So now there are many variations to the autoencoder. We have the regularized autoencoders, the sparse autoencoders, the denoising autoencoders, the contractive autoencoders, and these are all the autoencoders, the sparse autoencoders, the denoising autoencoders, the contractive autoencoders, and these are all the autoencoders. And then the variational autoencoders are a completely different beast, even though they follow the architecture of autoencoders. They're generative models. What they do is they hypothesize, so the hidden representation can be, for example, constructed as a set of, as a mixture of Gaussians. Then your only job is to find the use variational inference to find the best matching values of mu and sigma. I mean, where exactly the bell curve is sitting or the bell Hilsa sitting rather rather Gaussian-Hilsa setting that is that. So in this workshop you'll realize that I've given you some code and we'll get through some code. Before I go into the code I will just walk you through what imports you need. So by now this import should be looking very familiar to you guys right. What will you do do you'll change it to your own computer's value wherever you have downloaded the project so the only import is this these are the three new classes and one file vanilla encoders that i have added to your project asif sir can i ask one more question before so in the generative versus discriminative model in the generative model when like is it always generates the data like input data based on um based on the parameters like how we get it works in different different forms when we talk when we say a model get late works in different different forms when we talk when we say a model pretty we cover that i'll refresh it for you see in a discriminative model right all you care for is given the input given a certain input what is the output right let's take a classifier model classifier model. All you care for is how, what is the, is it a cat, more likely a cat or a dog? Is it more likely a cow or a duck? Right? Given the input. So you just care for the probability of a label given the input and whichever label has the highest probability you'll pick. Right. Now in a generative model you go one step further. Generative models are also models capable of making predictions or being discriminative but you do one extra step. You say no wait a minute, I don't want to just figure out whether it's a cow or duck. I want to figure this out. In the feature space, like the feature space of weight and height of a cow and all the features of the animals, what is the probability distribution? What is the shape of all the cow data? And what is the shape, what is the probability distribution of all the cow data and what is the shape what is the probability distribution of all the duck data are we together so if i know the actual probability functions in the feature space for a cow and for a duck anytime you want a cow i can generate a completely new cow for you for you all i will do is i'll sample from that distribution isn't it not right so it it does in addition to constructing the input it it will anyways do the what the discriminative model can do yeah which it can do so for example if i get an input all i need to figure out at that point now that i have a probability distribution for both the cow and the duck which of these two have a higher value. Right, it will know that. So that so it definitely implicitly is automatically it is a discriminative or a classifier but the generative part is that it goes above and beyond that. That's why generative models are a little bit longer to train and more computation, more data needed because they go further. They try to figure out what the underlying distribution in the feature space is. Okay and once you figure that out you have the lovely thing. You can go on generating data and that those in practical terms have far-reaching consequences. So for example you can take two faces. If you have faces, right? You can basically say that, find what these two faces map to in the latent feature space, and then find all the intermediate faces. Give me 100 intermediate faces in a continuum from this to that guy. Right? Right. Right. So for example, you can take Pradeep and you can take amitabh bachchan and you can put a continuum in between right that's true so if they have much more usage much more use for that so generally when you have a generative model you have a bigger success you can you can find many practical uses for it model you have a bigger success you can you can find many practical uses for it got it thank you very much sir nice so guys there are these there are four classes i wrote for you the base class is the real one in which and we'll walk through the implementation but imagine that it's an auto encoder okay we'll walk this is the signature of the auto encoder. We need to know the input dimension, right? We need to know the latent dimension of our data and we need to know if you want to give a lot of hidden layers, how many hidden layers you do. So I have implemented the autoencoder in such a way that you can specify many hidden layers. People often call those hidden layers autoencoder with many hidden layers. People often call them the stacked autoencoders in older literature. Nowadays, you don't see much. You just assume autoencoders will have many layers in between. So this is it. And now you give the hidden layers as just assume autoencoders will have many layers in between right so this is it and now you give the hidden layers it's just a list you can say how many nodes you need in each layer so here we are talking about fully connected layers and fc layers so the way to use it and first at this moment just play with it guys see how it works so here i've instantiated your base autoencoder i'm saying input dimension 28 square now why what what does the word 28 square remind you of what is 28 by 28 photo come again the photo photo i didn't hear that the photo image dimension fro image is it some particular data set mnist oh yeah mnist certainly yes it is the mnist data set yes mnist MNIST, they are all standardized to 28 by 28. ImageNet is 32. Now people are creating 64 by 64 images and so on and so forth. So you can pick at this moment, I'm going to do the lab with MNIST. And your homework is to try out other data sets. Now, my latent dimension is 16. Right? What does that mean? It means that my, what does that mean it means that my what does that mean my hidden representation latent representation is a 16 dimensional vector so we are going from 784 dimensions to 16 dimensions so as you can imagine that is a massive dimensionality reduction is a massive dimensionality reduction. Do we agree? Definitely. Yeah. And so what you do here, I've printed out and I've created a utility function so that if you print out the autoencoder, it will declare itself. And here it declares, it says the encoder is made out of, do you notice how the numbers keep decreasing? So 784 to 64. And what it has done is it has automatically figured out that it's a pretty steep dive from 784 to 16. so helpfully it has gone and added a hidden layer one more hidden layer of 64 dimensions so it is 784 to 64 64 64 to 68. How did I come upon 64? It's very easy. You take the log base 2 of this, log base 2 of this, these two, and find the middle point. And you'll see it in code in a moment. So that's that. And then a decoder is just the reverse of the encoder, except that the output of the decoder output so you notice that everywhere there is relu but in the output there is a tanh or why tanh because the m list images we have standardized to standardize it to the scale of zero to one so there are two ways of getting values in zero to one either you can use sigmoid or you can use tanh if you remember i mentioned that tanh are much easier to train faster to train right but sigmoids are way slow to train do you guys remember that from our earliest one of the first sessions we did that prefer tanh uh if you have to go sigmoid use tanh instead you could have used sigmoid it would have worked just as well but this is it so now you train the network here i am just training the network uh can you take a moment to explain once more on the the hidden layer so why did you decide on the hidden layer what was the motivation for it see by adding hidden layers you optimize you make the learning more efficient if you so if you think about it uh you need not have had hidden layers the original auto encoders were like that then what happens is and your universal approximation theorem and all of that but basically you will get some representation latent representation but the learning there is not as good and you can play around this is actually your lab your homework by adding hidden layers you you do much better right at this moment i've added only one hidden layer you can choose to remove it and just go modify the code and remove that. Or add more to it. And this is something we'll do, is a good word. Add more and more hidden layers. See the whole point of deep learning is when you add a lot of hidden layers, if you can successfully add it without having your vanishing gradients and other problems, if you can add more hidden layers, the more efficient is your learning. So instead of making very, very tall neural nets in which each layer has gazillions of nodes, you prefer making it out of many smaller, I mean many layers with more modest number of nodes. In the case of autoencoder, it becomes a direct choice. You're starting with 784 and you're diving down to 16. So you have a lot of freedom to stick in many more layers in between of smaller and smaller size, right, funneling down to 16. And the learning is more efficient. Oh, by the way, I mentioned it eight uh please correct it because initially i did it to make it eight is your homework i shouldn't say it 16. it's a typo so uh we go from there to 16. you may wonder how did i come up with 64 so we'll obviously you can look up my code and see then i've made this see guys i've made the api very easy you and you instantiate the auto encoder you see how it looks like and then you ask it to train itself by default it will train itself on uh five epochs right so when it trains itself on five epochs you can see Asif, there is an error in the training line. I get an error too. Yeah, it goes back to the vanilla encoder Python file. I'm not able to understand what it is. Yeah, I see. Yeah, it is related to the size of tensor A, not matching tensor B. Okay, let me just walk through and then i'll make sure that it works on all your machines do you see the same error message that i put in the chat let me see what you guys are putting in uh in the chat in the in the zoom chat. Oh, in the Zoom chat, okay. Yeah, the same error. All right, guys, I'll look into that. Give me a moment. At the last minute, I was changing the size back to 16 from eight. So something may have happened, we'll look at it. The learning rate, so there's some default values I've put in when you learn, all of which are configurable, the learning rate, the number of epochs. I've put five epochs. The reason is, you notice that by five epochs, the loss has stabilized. This is your standard loss plot. You must be familiar with this from your previous labs, plotting the loss. Now let's visualize the results. What do we do once we build a model and it looks good? Let's visualize the results and see if it makes sense. So here, let me make it even bigger. And here. Oh, sorry. So, guys, this is the original image here and below it, below every original, first row is original images, and below it are the reconstructed images through the autoencoder. Would you agree that the reconstructed image is a little blurrier, but it is still a fairly good representation of the original. Would you feel that, guys? Yes. It is, right? And now you can make it even better. Now here I used a latent representation of 16 dimensions. You can play with it. You can make it less than that. You can make it more than that and so forth. What happens if you reduce it down to four dimensions and or three dimensions? For example, can you capture the essence of a digit with just three dimensions? Something worth thinking, isn't it? Let me ask an extreme question then. What if you go down to one dimension what happens so as you play with this there are some surprising findings and i thought that i'll leave those findings as some fun things or some pleasures to be had as you do the lab it is quite amazing actually how data is quite amazing actually how data can live in or the essence of the data can live in much much lower dimensional spaces it's just amazing so i want you guys to please play with these images and see if you can uh with this auto encoder and see what you can do. Now, here's the, and by the way, I'll take you guys through that bug and whatever it is, we'll fix it in a moment. Now, first thing is, see the effect of reducing the latent space or dimensionality. This is very easy. You can just go here. What do we need to do to change the dimensionality, guys? Latent dimensions, you can make it something. Make it four. Make it three. Make it two. See what happens. Make it five, eight. See how it varies. how it varies. Then the other thing you can do is to see what you can do and how much it degrades your images. The next thing you do is how about adding more and more hidden layers. So in the vanilla version, you didn't give the hidden layers, so it automatically picks 64. But suppose you actually explicitly give a few hidden layers of different sizes, then what happens? I used ReLU as the activation function. What if you change it to CELU? What if you change it to something else? So try it out guys, play with the different activation functions, see what it does. Look at the runtime, how slowly or fast it learns. What if you just make it a sigmoid or tanh for learning? See how well it learns. Then does increasing the number of epochs have visibly better reconstruction of images? Check it out. What happens if you set the number of epochs to 50? You should get some hint just by looking at this graph. But try it out. Try it out and see what happens. Then other thing is I've tried it with MNIST. But a fashion MNIST is a drop-in replacement. And it is a drop-in replacement as in your torch vision also comes built with it. Instead of saying MNIST, you'll just have to say Fashion MNIST. Fashion MNIST is a collection of, you know, dresses, shoes and So you have a bigger data set to learn from. You can try that out. You can try it out with CIFAR and CIFAR-100, CIFAR-10 and CIFAR-100. Do you guys remember what was CIFAR-10? Anyone remembers? Yes, another image data set. Yes, with 10 classes. And CIFAR-100 is 100 classes. They are all subsets of the ImageNet dataset. So that is that. Then let's look at, so this is easy and we'll walk through the code in a moment, but let's first be just the user of it. You can use a denoising autoencoder. So this is easy and we'll walk through the code in a moment, but let's first be just the user of it. You can use a denoising autoencoder. So this time around, instead of giving it the input, we'll give it the input and we'll sort of add noise, inject noise into the input and see what it does to this picture. Now, if you notice that here what i've done is something quite surprising i have reduced it down to eight output dimensions to eight and i've added three hidden layers just for fun why did i do that for no particular reason it's just playing around and seeing that you can do that when When you do that, this becomes your architecture. 256 to 64 to 32 to 8. And 8 goes back to 32, 64, 256, 784. So these are your input hidden dimensions. Then I'm storing the data in a directory called noisy. And when you do that, the data in a directory called noisy and when you do that you can give whatever names you want some directory to put it in well here is the loss function and the interesting thing is that let's look at it and this picture i find quite amazing so guys would you agree that here actually let me make it just a little bit smaller oh sorry yes so we can see all of them together look at the picture the first column nine can you guys tell that the middle one has noise injected in it it's noisy uh can you make up the noise that I've injected into this image? I don't know if on zoom it comes. So maybe it's Gaussian? No, no, no, it is a straightforward noise. So see, look how clear the nine here looks. Sorry. How clear the nine here looks. And do you see the dottiness little dots all over the place? Okay, hang on. In the background, the background, I did my dots. Yeah, that's right. In the background, I've added white dots. It looks like a white noise, basically. It looks like white noise. exactly. And then you look at the image that we have reconstructed. Do you see that the white noise is gone? Let me make it bigger. I don't know if over Zoom this comes through. Can you see, guys, that this noisy image has been completely cleaned out? Yes, correct. So this is the beauty of it, you know, you can give a photograph on which you have a poured coffee and there are coffee stains and a denoising autoencoder will clean out and remove your coffee stains for you. And if you look at it, it is quite decidedly different, right? Look at the zero here and the zero here. It is much more cleaned out. So that is one purpose of autoencoder. Now let me just decrease the fonts a little bit. So this is the value of the denoising autoencoder. Sorry, I am jumping around. Denoising autoencoder. By the way, I've simplified the code because at this moment, first play around with it. These are all one-liner codes. To train, you just have to say train, and then you can plot the loss, and then you can just visualize these images. can plot the loss and then you can just visualize these images. Once you do this, oh, so no, no, no, I've gone too far below. So there's more to it. Homework. So then this is your denoising autoencoder. Yes, it has done its job. Then you have convolational autoencoders what are convolational autoencoders i said that we built our autoencoder out of fully connected layers isn't it but there is no reason why you should just use fully connected layers you can use any layers you want so long as the architecture the overall architecture is that encoder decoder architecture So here we have built a autoencoder out of convolutional layers. And as you will see with convolutional layers, actually the results are in my view better for images. So I took three, three convolution layers for the encoder. And so if I train it, you notice that the losses are actually, if you look at the losses, 0.01. Let's look at the loss that you ended up with in the previous ones. Do you notice that when you train, hang on, a normal autoencoder with a deep layer, this is fully connected layers. What was the loss you ended up with? 0.0664. Right guys, this is 0.0664. Relatively speaking, if you notice that when you use combulation neural net, you will 0.012 right it is almost 1 6th of the just in the same number of epochs you end up with an error which is a loss which is six times smaller compared to that and we will see whether it leads to cleaner reconstruction and look at this original image and restored image you notice that they look almost identical guys yes right you have to look really hard to see the difference if you look at this four uh the in the in the middle image last row second column look at this four and you look at this four here you will see a subtle difference here right so all over the place you will see some subtle difference this eight in the first image top left image the eight here has a little marking inside it and this one doesn't seem to have that marking inside it so it does a pretty good job of reconstruction okay and that's the beauty of it that's why should use it and then there is a denoising can this when you use the congulational layers can you use them to denoise your images very much so and when you denoise again it does a better denoising look at this it's the same code essentially i'm using a different class a different auto encoder now notice that this is look at this here i hope you can see the noise here in this picture are you guys able to see the noise that has sprinkled it yeah background like sprinkled snow right that's right and so if you compare you look at this any noise one uh let me make it a little bit bigger no little smaller oh gosh too big okay i said this is fine yeah you can i hope you can see the difference between the two that you have completely removed the noise now this congulation layer isn't it so you give it a noisy image and it will be able to clean it out for you. You have trained it to clean out noise. Now at some point I'll give you another lab, actually I was going to do it in the vision part of this workshop, where we are going to do, I would give you other things like some parts, somebody's ear masked, you take a face, the nose is missing or the ear is missing or something like that and you see that the auto encoder goes and paints a nice fills in a nose or fills in an ear and things like that into the image and it is quite magical actually to see how well it can go and fill in the gaps. So from a practical use perspective, guys, do you see the usefulness of autoencoders, at least for the purposes we are seeing, the compression and for noise reduction? Any questions before I go into the code? Asif, one quick question. The example that you just narrated right now where it fills in a nose. Yeah. Is it filling in a feature or it'll just fill in based on the local info that it has it doesn't have the notion of features right not based on local info it has but it fills in based on the memory of all the faces it has seen and learned okay so it has the it's building the memory of the features yes it brings in a nose but not because there is no nose in the picture so if you interpolate it will begin to look like the cheeks. But it has memory that this place needs a nose, and it can fill in a nose. OK. OK. It's quite amazing, actually, the autoencoders, the way they work. And I always find that remarkable. Yeah, I mean, I'm sort of exaggerating the fill in the blanks part of it. You just hide a part of the nose, it'll go fill in the nose. Yeah, thank you. Yeah. So it has a memory that this is what it should look like. And that's a very sort of a hand-waving way of putting it. But it was still very remarkable, actually. I've always been very fascinated by the power of autoencoders, especially, and then a variational autoencoders with their generative capability it's just remarkable so guys what i will do is we will look into some of you are having struggles with the lab what i would like to do is go over the core quickly and then give the last hour to make sure that all of this is working on your machines also i think some of you are having weekly issues should we take a 10-minute break guys uh i think some of you are having weekly issues uh should we take a 10 minute break guys before we go into the code let's take a 10 minute break and we'll go into the code so if you notice in the code you'll find a directory called auto encode there is only actually this thing can be deleted it is not there when I gave it to you and I need to update it. You won't find these directories. I cleaned it out because when you run your code all these other data directories will get created. This data directory is where the MNIST when when you run the code it will download the MNIST data. This is where it will get stored. These are all your output image directories. The only thing that you need to read is the Vanilla encoder. See, one moment. You said the MNIST data will get stored in a directory called data will get created? That is right. Because when you look at the code, you'll see why it is okay that didn't happen on mine so i need to figure out why okay okay i don't think we have that minimal data um in the sv learn auto decoder okay so we'll see that in a moment let's see so guys let me walk you through the code should i walk you through the code to to Jupiter? Or would you, is this visible? Like this black thing. This is PyCharm. Is it too small on your screen? Okay with either. Jupiter was better. Jupiter was better. So let's go through Jupiter. So this is your code as usual. I've written the code in classes and objects and so forth. So there is a reason for that is you don't repeat yourself. It's very easy when you create a base class with all the common functionality, then creating other encoders, you just reuse that class and you write just the deltas, the small differences between the two base class and so forth. Of course, the downside is if you are not careful and you're trying to do like I was doing, somewhere along the way you'll end up breaking everything. So I need to go back and fix it. It's an easy fix, I'll fix it. So this is anyway the base encoder, let me explain what it is the input dimension is of course the input dimension latent dimension hidden layers by now you know what it is okay so only supposed to have that vanilla encoder pi file in the sv learn autoencoder directory because i've got the init pi vanilla, vanillaencoder.py, nothing else. Yes, that is the only file you should have in that directory. Okay. Yeah. When you run that, other files will get created. Right. So actually don't bother at this particular moment. I gave you a very clean data set. I mean, clean project with all the unnecessary directories and files removed. So let that be as a library. So here we go. I will again. Explain this. So this is line forty one to forty seven. I hope is self-explanatory, right? You don't want the input dimensions to be less than two because there is no further compression you can do. If latent dimension at this particular moment, we are not doing the over-complete autoencoders. We are just assuming that these are under complete the hidden dimensions are less the if latent dimensions are less than input dimensions so line 44 guys is that self-explanatory i hope so then obviously latent dimensions people can't specify to be less than zero. So these are just sanity checks. These in programming it's called a preconditions check. You don't want to build an autoencoder which is strange. So if you notice that the user has not specified any hidden layers then what you what i do is i this is the bit of code that i mentioned which comes up with at least one hidden layer which whose size is a good one i take the log of the input log of the output take the average of the two and of course then i the log of the input log of the output take the average of the two and of course then i exponentiate it back right so i come up with a hidden dimension there that's how i came up with 64. this is it these are just stirring it and by the way this is i just added it just now you won't see it in your code is because of the fix the fix that i'm putting in for you guys i mixed up my with and without noise situations that's why we're having this issue so there we go with uh guys i'll give you an updated version of this file the forward pass by now guys who would like to explain what the forward pass does what the forward method does anybody would like to explain what is the forward method in Python to do? It's the feed forward network. It is literally the inference part. You know, you give it an input, output comes out. It does all your matrix multiplications in the forward journey right so it will do the encoding and the decoding and return it right the wrapper is just a helper it just you know that when you say print on this object itself it should print out the things properly, which is what it does. Build encoder. Here is how I build the encoder. And this is something that is worth paying attention to because I introduce a new syntax here. This syntax sequential, I was waiting to insert it somewhere in your thing. It's a little bit like Keras. It helps you make your neural nets pretty efficiently. So what we do is I decide what my layers are. Let's walk through the layers. We agree that the first layer, there is no choice. It has to be this input dimension. The input dimension will be the size of the first layer. Do we agree guys? Because it's the input layer. And then what will be the output layer, whatever the size of the first hidden layer is, the output to that, right? Then the first layer, we are activating it with a reloop so this by the way i'm just adding it to a list at this moment i haven't built it i'm just i'm creating these little pieces and i'm adding it to a list so far so good guys does it make sense guys give me a feedback does this make sense is it very easy to see? Yes, sir. Yeah, this is easy. After that, what I do is you have a lot of hidden layers because I gave the functionality that you can have as many hidden layers as you wish. And now I need to dynamically figure out what those hidden layers are and add it to this modules list, which is what I'm doing here. And then finally, is the innermost central layer which is the uh which will be the output here now what do we do here is your latent this is the add the encoder the actual there uh i should have called it not encoder but the latent layer this is your latent layer right the hidden layer minus one and later after that what do we do uh this is a syntax nn sequence you give it a list of these pieces and it will stitch them together into a neural net right so which is what i'm doing so this syntax will be a bit new to you. By the way, what does star mean in Python? Variable number of arguments. No, basically, if you give it a list, it will unwrap the list and make them the, yeah, like a variable r, like you are saying. Right? Like, some place, sequential is expecting literally an unwrap list like you can go on adding things to it and so it will take a list and it will unwrap it and a double star unwraps a or a map single star in python unwraps the list so this is it it just so happens that the sequential takes arguments like that all right so one of the common things I notice people do on the Internet is that they will just repeat the encoder backwards, the code of the encoder backwards in the decoder. The trouble is then if you change your encoder, now you need to go and change your decoder. you know the if the decoder is a mirror image of the encoder why repeat yourself in backwards it's a bad idea you can introduce bugs in it so try to do do it the way i did it or some other better way what i did is that i took the list of modules that i have here and i have just reversed it do you notice that this is by the way a very pythonic way of doing it this creates a list that is the reverse of the hidden layers list right so now what happens instead of each layer being smaller than the previous layer now each layer will be bigger than the previous layer in the hidden in the reverse list. Does that make sense, guys? So suppose you go 256, 64, 32. What will be the reverse of it? 32, 64, 256. Isn't it? That's that. So I take a reverse. And now what do I do? I repeat exactly what I did before. But this time around, I weave it out of these layers. And I put it. The only thing to note is that in the output, the last activation is a tanh, right? And the reason is we want values essentially between zero and one. And what we do is we tanh produces values that get minus one and plus one. we can always shift and scale it if you add one to tan it it will go from zero to two and then take half of it it will become like for example you see me do it here so that explains the encoder decoder part uh there is a helper method, shape to image. What it does is, if you see, the point is that if you take a 28 by 28 image with one channel, which is the monochrome channel, it is basically a three-dimensional object, three-dimensional tensor. So when you take the three-dimensional tensor and you flatten it because a deep neural a dense layer can only take a linear input isn't it guys that's why in python it's called linear means it can only take a vector as an input not a tensor so how would you flatten it you would just flatten it into a single vector up to 284 dimensions you when you flatten it to a 784 dimensions you do all your work later on when you are looking at the result you want to shape it back to a image isn't it so this undoes this so this x you imagine that it's a 784 dimensional vector. Now this vector we want to shape back into an image. Actually it's not really a vector because there is one more thing there. The mini batch size, the mini batch let's say 32, means that it will be 32 cross 784. But that 32 is implicit. it will the linear the pytorch works transparently with mini patch so you don't even speak about it you just refer to the fact that the rest of it is a vector the other dimensions are right so now this is here what we do is we shift it back you remember tanh produces values between minus one and plus one we We need values between zero and one. Why do we need values between zero and one? Because the MNIST, we standardize it to zero one. So if you want to compare input to output, we better be on the same scale. That is it. This is not needed. It just makes sure that if we used any other activation function, then uncommon disguise. If you use something that produces values above and beyond, if it does something strange, then you may want to truncate the values to 0, 1 interval. Then you know that this data, when it comes, it is actually initially a two-dimensional vector data. It is the batch size times one dimension is the batch, other dimension is the other batch size and the other dimension is the actual data right so for every batch every batch element there will be one vector one image so imagine 32 images coming together through this in the shape you're shaping 32 of these images you're trying to do that. So what do we do? What we do here is, how do we find the batch size? The rule in TensorFlow is in the image tensor, at least, the torch vision, the tensors are annotated like this. First, the batch size, second, the channel, third, now comes the image size uh image uh you know height and width so uh so it so the the zeroth element which is the first element would be the batch size a channel is one we we know that the channel is one it's monochrome image right at this moment and by the way this you'll have to change when you try other data sets. I'll leave that as an exercise for you. You'll have to change it to three or something like that. Now, we know that image net data is 28 by 28. So what do we do? This is just a view. You remember, this is the tensor by its standard function. Any tensor you can reshape to any other tensor. So here it is. We do the view and you give it the batch size, the channels, image size, height and width. So now what will you get? You will get images, right? 32 images showing up in . That is that. That's all it is. Right is right so how do we build and by the way this was where the bug was um i need so no this is not the book i'll tell you where the bug was okay a build input one i see from the previous function can you touch upon why you had the static method oh it's pythonic it's a weak pythonic, do you notice that this X, this function, it does not need any member variable, any property of the object or the class to work with. Right. When a function is invariant to everything, you can you leave you give the facility for it to be called. From anywhere. Yeah, anywhere. Okay. So it becomes like a C function. That's what it is. You notice that it doesn't have self as the first argument here. Okay, yep. Thank you. So a build input is simple. Suppose you, is the opposite of that. Suppose I get an image 28 by 28, what do I need to do? I need to, and let's say batch size 32. So it'll be 32 times one times 28 times by 28, right? So what you do is you linearize it into one linear vector. The way to linearize it would be you take it and then you, for here, you keep the number of dimensions. Of course, that's a number of batch size. That's a mini batch, you ignore that. And the rest of it, what do you do? You put a minus one means take everything and put it in one long vector, right? Which it does, and we can print it out. You can see it. This is the fix I was putting in your code when in this break. It's a minor fix. I'll give it here. So let's now look at the train part. The training part by now you should be very familiar with. Self batch sized. So I assume a default batch size of 32. Now those of you who attended my loss function loss landscape thing you know that batch size how do you decide a good batch size if your batch size is one it is stochastic gradient descent if your batch size is let's say the entire size of feminist well then it's just gradient descent a patch creating descent but typically you do a mini batch creating descent in between sizes of eight four eight 16 32 are common 32 is pushing it sometimes you go for 64 128 also the bigger the batch size the two two problems with it one is that uh you may not have enough memory on your video card. That is one issue. The second problem is bigger batch sizes introduces its own instability on the loss landscape. So you have to be careful. A batch size is a hyperparameter of the model. In the autoencoders it doesn't matter terribly. 32 was a reasonably good number and I took it. code is it doesn't matter terribly 32 was a reasonably good number and i took it epochs how many epochs i trained for just in this particular case you know that it easily trains into three epochs five is pushing it learning rate you can play with the learning rate as i told you learning rate is a hyper parameter of the model right and you can play with it of course you can do better things like one cycle learning and all that but at this moment i've taken a fixed learning rate now the output directory where is it that you're going to put your output the images as it is learning it will keep outputting some uh output images so forth, where it is. So this is it. And show directory is those of you must be remembering from past labs. It is just my helper method that makes sure that that directory exists. If it doesn't exist, it creates it. Because you may specify a directory that may not exist. So it's just a convenience. A transform does what? It just converts it into an image. For MNIST normalization values, half-half are good. These are monochrome images and the zero to one. So you normalize it on a scale with the middle being half and half. So this is the point of where the data is stored. You know, I've written tilde data, means your home directory and create data. So when you do this, your MNIST data will be downloaded to your home directory. If it is not already there, download is to make sure that you, and once the data comes, you apply these transforms. so your data set now is made up of tensors this is the data loader part so remember what's the difference between the data set and the data loader just to wet your recollection if you think of the data set the whole data set is a huge pile of sand your data loader is the guy with the shovel. Data loader will give you many batches of data. When you ask it for the next and the next and the next, it will take a shovel and each shovel full it will give you one mini batch of data. That's the data loader. Loss function, of course, we talked about it, mean squared loss. And optimizer is the one that will help you learn this is it now this code should be pretty familiar to you from the past history loss history something is a class that i created to keep track of the loss losses you can use it so epoch so now what do we do for every epoch for all the mini batches you do the learning. The learning is, you'll realize, a very straightforward. It is given, you take this class itself onto the input and you'll get an output. So this will, what do you think this will call? Which of the methods of this class will it call? Init? Yes, no. Remember the module itself is a function and the function is directed to call the forward method. So this is a this is a thing that trips people when you make the neural network, the training inside the neural network itself you have to call self as a function right if you want to call init you would have to do self dot underscore underscore in it but when you call the object itself as a function you can do it only if as in python your class is a functional your class is a functional. So it's a very Pythonic thing to do guys. As you become more familiar with Python syntax, you will realize that it has all these nice conveniences and PyTorch has really been well-designed with Python in mind. So this is the part, this is me putting the fix in for the mistake that is happening. I'll put it basically a sort of muddled up as I was trying to make it for multiple classes and muddled up my loss from the, am I finding the loss function with respect to the input or the input image or what am I finding it with respect to? So I need to distinguish between these two cases that's all it goes yeah um the question maybe i should have asked like with two lectures back how how can i find more about self function because i'm a bit confused with the self declaration here in the previous few pages also oh boy or maybe i shouldn't have used it see basically self refers to the object itself okay right so in java it is that this okay except that in java and in c plus plus this refers to the object right yeah python also self refers to the object, right? Python also self refers to the object, but for one exception. When in Python, function is an object in itself. So if your entire class is also a function, now how do we know that it is a function? If you look at it, that comes from the fact that, ah, look at this line, line 29. A base autoencoder takes nn.module as input, as a superclass. So what it says is that a base autoencoder inherits from module. Module class in PyTorch is a functional, right? So it is both an object and a function. So if you know that there's a function called X, you can just call X on some input, isn't it? So that is what is happening here yeah this is a little see guys i did it like this so that you know you the sub the size of the code base remains small because we are talking of four auto encoders typically if you look at the web you'll find that people write independent classes with completely different implementations, but I have a unified implementation, putting it all together. And you'll see that with this base class in place, the rest of the auto encoders are just a few lines, three, four lines each. So this is the magic with object oriented programming. You make a very good base class with a lot of core functionality. And then all the inherited classes have just a little bit of work to do. So that's that. But I could do it only because I could use syntax like this. So just think, okay, here is one way to look at it. I forget line 149. Imagine that you're seeing the word self dot forward on the input. Then it would make sense. So can you add a comment there? So when we go back, we know. Yeah, I will add it. I will add a comment in there because I have to give you guys anyway, a fixed version. I'll add that comment. So now happens you you so it's very straightforward this gives you the forward pass it gives you it produces an output image from the uh from the auto encoder so i'll add a lot of comments in this i thought that this most of this loop you guys are familiar with now then this output i need to compute the loss from once i have the loss i need to back propagate the loss right and then the optimizer will take once i back propagated the gradients of the loss what do i do optimizer takes one step forward means updates all the weights but then the rest of it is just bookkeeping you know i'm storing some information and you know some of the files do you notice that i store some files into the directory so that's how i visualize it in the jupyter notebook i save these there right the plot loss function is literally the plot loss that you're already familiar with from your classifiers and other things. So I've just reused it. If you look at the plot loss, it is simply reusing the plot loss that we already created. Remember those days long ago in the classifier when we were writing our regressor and classifier with deep learning long ago. So all of that code is being reused. long ago. So all of that code is being reused. That is that. That is your training. Where is it? That's your, there's a main training loop, right? This is it. And you plot the, so this is the beauty, you know, again to the point that once you have created a function, reuse it. That is why I encourage you not to put everything in Jupupiter notebooks it's very hard to call function written in one jupiter notebook from another jupiter notebook but if you write it as proper python library then it's very easy to recycle code or reuse code so now i did that see guys look how easy is it to create a denoising auto encoder what do i need to do i just need to insert the noise, isn't it? Into the input. So what I do is before I feed it into the input, I have just one method here, build input. So you give it data, it will take the data, it will do whatever transformation needed, like for example linearizing it and so forth. And then what it will do, transformation needed like for example linearizing it and so forth and then what it will do i'm just adding at this moment i'm adding just random noise you can add gaussian noise you can add uh you know sparkle noise you can add whatever noise you want but just as a illustration i add random noise okay and you can decide how much noise to add for For example, you can add a 10, by default I add 30 percent noise. That's a pretty aggressive noise. Or you could add less or more. Don't go like 90 percent or something like that. It will destroy your thing. That is it. So do you notice that when you have classes, I managed to create a denoising autoencoder literally with only like one, two, three, three or four lines of actual code from an autoencoder and that is why it is of value. Now I can build an autoencoder out of convolational layers instead of a feedforward or linear layers. If you look back here, the default autoencoder I created out of linear layers. Where am I? Build encoder, yeah. Do you notice that everywhere I'm using linear layers? Linear, linear, what are linear? These are fully connected dense layers. You don't have to use that. You can instead use convolutional layers. So here it is. And once again, because I have my base autoencoder class in place, using this is very easy now. By the way, you always search for good layers, and it comes through a lot of trial and error. So I just took it literally from the Toronto AI group. They had this example for the autoencoder. It works very well with MNIST. Obviously, these numbers you come up to by playing along. So instead of cooking up my own numbers, I just took the actual layer values from this particular link. I'll say one other question right on convolutional network layer. Can you describe what exactly that? CNS? Oh, okay. This is a... see convolutional are like think of them as filters or stencils. So suppose you have a page in which you have a stencil mark, X marked, X cut out, and you go over the wall or over some surface and you look for a match to that X as you move that stencil all over the surface. Somewhere it will match, right? Let's say if there's an X on the wall, it will go match it. So now you know the location of an X in the image. That is the basic intuition of a convolational filter. This process of applying that stencil to the wall is called convolution and then seeing if there was a match that is the intuition of convolution okay right so convolutional unit we covered in part uh one and and i just remembered that you you weren't there in part one were you i wasn't here yeah yeah that's right that's why it's always like relu and i'm getting like lost right when he's talking yeah so it's you know learning is a cycle you know you'll come back to it again in march february march when i'm teaching it's not too far off join the next batch again okay Join the next batch again. OK. So by the time we finish this, he'll be ready for the next batch. Alright, so this is all I'm doing is compilation and here you have to see the how many filters to how many you know channels and filter to have a filters to have in each layer. The basic rule is the number should keep on increasing. You see these numbers increasing? That is it. The size of the filter also is increasing. This is a very common thing to do when you are dealing with convolutions. Now what happens is that if you look at this thing, we are not using max pooling here, right, in this example. Take this as a homework. Go add a few max pooling layers in between. So when you do your convolution, throw in a max pooling also. Do you guys remember your max pooling? When we did ENN in part one, guys, do you remember we did every, every combination layer would have combination filters and pooling. Yeah. So here you don't see you can add them up. So anyway, that's that. You're in the vanilla encoder or just using that as a function in a Jupyter notebook. So anyway, that's that. In the vanilla encoder or just using that as a function in a Jupyter notebook? No, no, just do it here. Go modify it here. Or you can copy this code into another class, and then in that class you can go and put that. Or you can do it in Jupyter notebook, whatever you feel comfortable with. So max pooling layers in the building coder. Yeah, encoder decoder. See what happens. So this is that and then when you do this, as we saw in that thing, now I tried to run my notebook and I am in the same broken state as you guys are at this particular moment but for what it is worth let me open it here. So yeah see I managed to successfully reproduce the same problem you guys were having so now I know the same problem you guys were having. So now I know. And I'm pushing the effects in a little bit. So if you, and this also training that Denoiser autoencoder. Anyway, we'll visualize the results and the results were here still. I haven't removed it. move it so i'll have to make this fonts a little bit smaller for this page to be reasonably sized okay and then when you go to the convolutions you see it what 16 16 channels becomes 32 channels become 64 channels you have a lot of channels and you see the kernel size increasing right but when you use it it is very much the same you don't notice the difference at all right except that the results are better and then there's a denoising version of it add some 30 percent noise to, and then you can run it. At this moment, it won't run because things are screwed up. So you guys have the results. That is it, guys. So how did I add the noise? Exactly the same way. Take the original image and add some extra noise to it. And that is autoencoders, guys. And that is autoencoders, guys. Now, remarkably, these autoencoders today are used for one set of purposes. They're fairly versatile. You can use it for many, many things. When we do anomaly detection, we'll again come back to autoencoders. And you'll see that we can use them quite, quite a bit. And at some point I wanted to introduce a lab dedicated to variational autoencoders, but then I realized that it's becoming a bit too much of autoencoders, so I'll make that optional. And those of you who want to attend the lab can attend it later. Thank you.