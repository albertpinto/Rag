 Give me a moment. Folks, if you haven't subscribed to our YouTube channel, I would request that you do. We keep posting a lot of material these days, putting in a lot of effort to do on a regular basis. And we are live. I'd like to introduce you folks to Chander. Chander and I, we were together during our PhD years. He, while we were working on theoretical physics, theoretical particle physics, he is from IIT Chennai, one of the rank holders. He did an outstanding PhD on black holes and quantum field theory. After that, he went back to India. He joined Infosys for a few years and then he joined Evolve and he joined me and we have worked together to a startup and now in the day job that we work with. Chhanda is going to talk on word embeddings. It's a topic that is central to natural language processing. In any language there are lots and lots of words and how do you deal with a large vocabulary where words are related they're interrelated so that's the topic I'll now let Chandu continue yeah you're able to hear me right yes thanks a lot Asif for the introduction and thanks to all of you. Good afternoon. And what I will do today is I will try to keep it as interactive as possible. So feel free to stop me with questions as and when you have them. Of course, I mean, because it is a webinar, we don't want it to get too interrupted as as I said let me so what I will do is I'm right now going to use a whiteboard which I will use to describe this word embeddings you're able to see my screen I have a whiteboard is it yes it's visible, it's very plain. Okay, yes, of course. Yeah. So I'm going to use this whiteboard where I will actually start writing. So let me do this. So what are we going to do? Today, we are going to cover word embeddings. And I will try to cover two specific approaches. So one of them is Word2Vec. So I will go through Word2Vec in quite some detail and I hope time permits, I mean, depends on how many questions are there and how interactive the session is. But I will also try to cover the GloVe embedding. So I don't know in this audience whether how varied the understanding on word embeddings is. Maybe just to have an understanding, can I have a quick, like we have around 14 participants. So is it possible to let me know if you have come across these words so far or is it completely new so that I know how fast or how slow to paste the whole videos? Guys, if you know what the settings, raise your hand. In this webinar, you can raise your hands. I see two hands raised and we'll count the hands raised so quite a few five okay nice so about uh one third of the audience knows what and what is it so that's okay i will anyway i mean if for you some of the things may be a repetition so in case it is um and you have some additional questions it's okay but because i'm not going to um what i will try to do is i will try to cover from the ground up if you have any questions you can bring them up but some of them may be repetitive for you is that okay uh so let me uh so let me start with the word to wake which is yeah so i'm going to start with the word to work, which is, yeah. So I'm gonna start with the first topic, which is word to work. So you're seeing my screen as of now. So first let us understand what are words in any vocabulary. Let us, for specificity, let us say we have the English language. Usually in what we look at are around 100 000 words in the english vocabulary i mean there may be more but as far as the commonly used words surely it is less than 100 000. so let us assume that we have around 100 000 words in english now why do we want to convert them to vectors? The reason why we want to convert them to vectors is ultimately any kind of machine learning, any natural language processing we want to do, we need to convert words to vectors which can be input. vectors which are can be input can be input into one of the ml algorithms so whether it is i mean we we can have a classification problem like let us say given a text we want to understand the sentiment whether it is positive or negative so that a classification algorithm, or it could be some kind of where we are trying to do a translation, where translation is really trying to convert from say English to French. So any one of these, there are several algorithms, but ultimately all these algorithms need vectors to be the input. So words that I mentioned. So, these are W1, W2, W3, dot, dot, dot, up to W, 100,000. So, what we can do is we can convert them into vectors. So, let us say we have 100,000 words that I mentioned. So, these are W1, W2, W3, dot w3 dot dot dot up to w hundred thousand so we want to convert each one of them into a vector the usually the most naive way one uses is called the one hot encoding and this is typically used not just for words it is also used for any what are called as categorical variables so categorical variables are basically variables which are one out of several like let us say the days of the week you have sunday to saturday sunday monday tuesday so you have seven such possibilities so whenever you have these categorical variables, which are one of seven here it is, it so happens that it is one of 100,000. So the the typical naive way one does it is what is known as the one hot encoding. So what do you mean by that is you say W1 is the vector which is one only at the first position. So you you treat it as a hundred dimensional vector. And you say that w1 is this and what is w2? Obviously it is going to be one at the second position and zeros everywhere else. So again, it's a hundred dimensional vector. So this, this is the way it is done. The most naive way and of course w100,000, which is the last of these words, would have been one which is zero everywhere and one only at the last position. So this is the one hot encoding approach that is done as the most naive approach to convert words to vectors. Now, the thing is, first of all, there are many problems with this. Okay. What are the problems? Can somebody try to list out these problems or should I go ahead? I mean, I don't know if it can be interactive. Asif, how do you suggest? Should I wait for... Yes, this would be a good question for people for us to wait and I have given everyone permission to speak so people can chime in. Let's wait a minute. Anyone would like to chime in? Memory alone and then the processing time yes so uh this is i guess kate yes yeah memory problem because we are using huge vectors it's sparse right sorry it is a sparse like the matrix yes it is a very sparse uh vector and which means that we are not really using even though we are having a hundred thousand dimensions not all of them are getting used. Correct. So it is basically we are saying we are having a lot of unutilized that is that is what you want to say right? Unutilized space in some sense yes can the network issue like passing data across the chip the bandwidth that would be needed for that is in some sense related to this i think whatever you're bringing is related to this but there is a more fundamental problem which i think is very important to identify i mean yes these two are very valid problems um i'm looking for one more probably a very um obvious problem with this that's not relation between the world of machine learning word the context is not lost a context is lost right here relations yes yes yes so what it means just for the sake of everyone else's let us say we have very closely related words like for example I would think of pros and cons. Now pros and cons are two words which are, they come always together. You very rarely say a pro without a con. So these two words, now in this approach of using each word represented by a vector, there is really no difference. I mean, you would have had something else like camel and pros and cons, all of them, they are as different from each other or as similar to each other. Whereas we don't want that, we want a representation where it is, so I will mention it as a, we want a vector representation where relation between words is maintained. So, we want in some sense the pros and cons so if we are thinking of a some kind of a vector space i mean this is of course only three dimensional i have drawn here x but let us say we have a much bigger dimensional vector space though not as big as this hundred dimensional now in this vector space we would like i don't know pro the pro vector to be here the con vector to be somewhere here so it's not very far away whereas the camel something which corresponds to the camel is quite far so this is what to do this is what is our first goal. So, we need to go from the one hot encoding, naive approach to a better representation where we just saw where a better representation so which will address all these drawbacks it will address not only the drawback that related words are nearby but it also address in this in the process it will address these two issues the fact that we are no longer having to deal with 100 000 dimensional vectors or the fact that most of the data is zero and just one of them is one so all of them will get addressed so this is um uh one one of the ways of doing it i mean there are this is uh we are going to start with word to work today today. So I will first go with word2vec where how it is this whole process of going from one hot encoded vector to this representation where words have relation is achieved. And subsequently I will go through another approach, which is the GloVe embedding approach. So these are the two approaches I will cover today. let us go through the word to work first so word to work what actually happens is um also if in case there is anything where i have glossed over and you think there is something i need to emphasize please suggest okay so the way it is done in word to work is ultimately it is done with a neural network architecture. So let me just show the neural network architecture that is done. It is a neural network architecture where there is an input layer which is basically having 100,000, which is essentially one to one with the number of words in the vocabulary. So there is an input layer which consists of that many nodes and there is a hidden layer which is much smaller in dimension. Typically, for the purposes of invertive, quite often the 500 dimension is used. So this 500 is the number of dimensions in which we are representing these words. And once we represent in these 500 dimensional words, it is not one hot encoded any longer. It is, they are more dense representations where we have continuous values, unlike the case where R. Vijay Mohanaraman, Ph.D.: Only one of the values had a one interest had zero, it is no longer the case in this reduced dimensional representation and in this reduced dimensional representation words also will R. Vijay Mohanaraman, Ph.D.: Have this relation. So for example, the vector representation for like I said, an example of pro and con would be quite close compared to a third word which is like a man so that R. Vijay Mohanaraman, Ph.D.: So the way it is achieved is we have this neural network which is a fully connected layer. I'll explain how it happens. So you have a fully connected hidden layer and then you go again to the another 100 dimensional output layer. Now 100,000 sorry 100,000 dimensional output layer again it is fully connected everywhere and what is the interpretation here? So I'm just stating that this is the neural network used but I have not explained how exactly the input is fed, what is the output, and what is the minimization done. So I have not yet explained all that. So I'm going to tell that right now. So let us think of some random sentence, I had bread and butter for breakfast. I'm just giving a random sentence. Now, think of this is just one sentence. Now, let us think of the whole corpus of literature. Whole corpus of literature that is available in English. So whole corpus of literature available in English, which means it could include all of Shakespeare's works. I don't know, all of Leo Tolstoy translated to English. And I mean, and it could have the encyclopedia whatever. So let us say we are having all this corpus of information and each one of these corpus ultimately is consisting of several sentences. I mean I have just given one simple sentence here but imagine that there are each novel or each story is going to have millions of sentences, each sentence will have several words. Ultimately, it is going to be a corpus of literature which is covering all these 100,000 words with different ways in which the sentences are structured. Now, I'm just giving one example here. This is one example of a sentence. So the way this Vertovic is done is it is converted into what are called bigrams. In fact, it could be engrams, but I will start with bigrams and then the generalization to n-grams becomes kind of straightforward. So let us say we out of the sentence, we are considering all possible bigrams. So that means I and had because they are the first two words. And then you have had and bred. This is the second bigram. So bigrams are basically all these groups of two words which are occurring one after the other now in a straightforward fashion we can also consider trigrams and then four grams and n grams in general so i am just starting with the example of bigrams so let us say we had this i had had bread bread and so you you can construct all these bigrams for um this one sentence now imagine instead of just this one sentence we had all these sentences coming from the entire literature you'd have had tons of these bigrams now what are we going to do with these bigrams? So if you go back here if you see there was this neural network where I said there is a input layer which consists of 100,000. So what is done in during the training phase here is the input is the first part of the bigram. The input is the first part of the bigram. So for example, in this case, I and hat. So the input vector corresponds to really the, let us say the word I in this lexicon, in this whole vocabulary, it was probably the fifth word, for example. So let us say it was the fifth word, which means we are having an input which is 0 0 0 0 1 because that is how we have defined the one hot encoding and then zero everywhere else so we are feeding in this vector which is the first part of the bigram i comma had and the output is going to be the second word had and let us say the had was actually the word number two right so if it was the word number two it would have been zero one and then everywhere else is zero so this is the input and this is the output and because this is a fully connected layer let us say this is represented by a weight matrix w so in any neural network we have a matrix so here the matrix would have been let me say this was the vocabulary size was v which is 100 000 here there would have been a matrix of dimension v w v n which is going to connect this vector to the vector in the hidden layer. And then likewise, because there is one more fully connected network here, it would have had a W dash, another matrix, which will connect from N to the same V. So what we have done in the process of looking at these bigrams and feeding into this in a network that i just drew so let me redraw the network because i have i don't want to go back and forth again and again so i have this and then i have this and then i have this And then I have this. This was size V, this was size N, this was size V. V is of course much bigger around 100,000. And N is much smaller around 500. So we have this whole lot of bigrams where the input is I and output is had. Or it could have been had and the output is bred. So you'd have had, imagine that you're creating these bigrams from the entire corpus of literature. So you have a whole lot of training vectors. You have a training and the corresponding, I mean, the training input and the training output. Training input and output is available and you have this fully connected W and W dash which you do not know, right? So what is done in the process of training the word-to-vec model is to arrive at the w and w dash which ends up predicting the appropriate output for the given input so the input being the first part of the bigram and the output being the second part so so in some sense let us say the input word was let me call it what should I call it I'll call it v i let us say v i was the input and this is getting multiplied by the matrix w and let us say the v i is a i represent it as a row vector okay and this row vector is multiplied by a matrix this row vector had dimension of v right it is basically it is a 1 cross v matrix a 1 cross v multiplied by a v cross n matrix you end up getting a 1 cross n matrix 1 cross v into v cross n becomes 1 cross n and then this is multiplied so this was the w so this was the vi this is the w and then you have one more matrix which is n cross v which is the w dash so what do you end up getting you have this vi multiplied by w multiplied by w dash and this again becomes because this was one cross v and this was v cross n and this was n cross v so if you have a one cross v matrix multiplied with a v cross n matrix and multiplied with a n cross v matrix you finally end up getting a one cross n matrix sorry one cross v matrix right one cross v into v cross n cross v becomes a one cross v so this again becomes a vector of dimension v now the only catch is that here it was a one hot encoded vector. So only one of them was fired. Like if you see the 001 that was corresponding to the word I and everywhere else was zero. Whereas what we finally have is going to be values in all these places, A1, A2, up to AV. Right? Now we need to somehow convert this, which is occurring in all these, which is having values at all these places, we need to convert it into something which is uniquely determining the word had, because had is only one of, for every input, we are only having one unique output in the bigram so this is done using the approach called softmax so the the way this is done is um whenever you have a so what we finally had is a a n cross a v now from here we need to go into something which is a n cross a v now from here we need to go into something which is only having one at one of the places right because only then we can really make the connection between the input and output so the way this is done is through the soft max function so the soft max is essentially the way it works is these are all values which could have been double values which could have been positive or negative so the way the softmax function is defined is you convert these two probabilities so you do for each value you do an e power a1 e power a2 dot dot dot up to e to the power of av now when you do the exponentiation from here to here you are guaranteed to have all positive numbers but then they are still not probabilities because they could have been bigger than one so you actually go through a normalization step where you do e power a1 divided by summation of e to the power of ai and so on up to e to the power of av by summation e to the power of ai where this i index is over all these values now we are guaranteed that these are actually the having the same meaning as probability because first of all they are all positive and if you add all of them they become one so what it is what is done is from here to going to this we look at the one which is maximum so let us say we have some vector which was 0.1 0.1 and then there was a 0.8 and then there is a 0.1 and then 0 of everyone so if this was what this was from this the maximum value is 0.8 therefore we conclude that the word which is predicted is zero zero one zero zero zero and so this is this is how the softmax i mean this is i'm sure those of you have already gone through softmax in the past. I mean you know about it. I'm just explaining so that we are… Quick question. Yes, please. Before you scroll that up, the equal, the 1 times v, the v times n, the n times v, and then that's equal to… I cannot read that. Sorry, I think I should have… Just what are those letters? to, I cannot read that. Sorry, I think I should have, I'll- Just what are those letters? Oh. Let me erase that and rerun it. Okay, thank you. How do I erase it? I'm trying to figure it out. Or let me do one thing. Since I'm not able to erase it. Just write above it. Yeah. So let me go back here. So whenever we have matrix multiplication, the way it works is the 1 cross v into V cross N becomes a 1 cross N matrix and 1 cross N matrix multiplied by N cross V becomes a 1 cross V. So this was a 1 cross V matrix, a row vector. 1 cross V is basically a row vector multiplied by a matrix which is V cross N. basically a row vector multiplied by a matrix which is v cross n this is the matrix multiplication 1 cross b into v cross n into n cross b you again get a 1 cross b matrix so this was a one hot encoded vector this is the weight matrix w this is the weight matrix w dash so these two are unknown so as of now when we are starting with this whole word to work training we only know that there are these bigrams there is this uh like for example in the example i gave there is a i for a left word i you had the output had and then for the word had you had the output bread so all all you knew was you have this input vector and you have a weight matrix which you do not know and using this unknown weight matrices it is going to create this one cross v now what we have done is out of this one cross v we are trying to predict the word which is most likely to occur using this concept of softmax because what you have in this 1 cross v is a vector which is having values in all these. I mean, unlike the one hot encoded vector which was having only a value of 1 in one of the dimensions and zeros everywhere, we have something which has values everywhere. But if you look at what we had as our output, we had only one of the words being fired so to get that we go through this approach of softmax and predicting the output to be the one which has maximum value for the softmax so now this is a standard neural network which can be trained so the training uses what is known as cross entropy because whenever we have, so we have this, like I said, we have this pair of I and there was a had, we had a had and there was a bred. is a vector and there is a corresponding vector here and we train this so our job is to identify the values of w and w dot dash which end up achieving this training so there are standard ways to do it i mean we can use keras or any other approach but essentially it is a training of a neural network using the standard method of gradient descent because we we have a input and an output and our goal is to figure out what the w and w dash are of course it's not going to be a straightforward because it is the corpus is really huge we have to train over all possible bigrams in the entire literature and i'm starting with bigrams here but in reality we don't do with biograms we do with n grams where typically we do n is equal to five i think uh as if is that right we use n equal to five yes so yeah so normally we use n equal to five i mean this is how the best work to back models are built. And there are also, I mean, I have glossed over one part here. There are two approaches here that are taken in this whole word-to-vec approach. I have started with bigrams and said that we could go to n equal to 5, is 5 grams but there is also another there is a bag of words approach and there is a skip gram approach there are these two approaches used typically in word to way so what do they mean in bag i have one question yes please sorry so Sorry, so actually how did you ensure that the like in your previous example that the third or the fifth one will have the highest probability like you showed in that example 0.8 right. No, no, that was just an example. What I am saying is let us say I start with a 1 cross v matrix and i i start with some random weight matrices which are v cross n and n cross v i i don't know i'm just starting usually the way the training works is we iteratively figure out what is the weight matrix which is going to achieve the output that we want right oh okay in any neural network we start with the way the neural network works is it uses this gradient descent to gradually reach the w and w dash which is the ideal w and w dash okay okay in in neural network the answer but then you try to design w and w dash so that it you approach close to the answer right yes yes exactly so that is the gradient descent approach and there is a whole technology for doing gradient descent whenever we have a soft max and we have the actual output and that there is a loss function called as the cross entropy in case there is something called cross entropy loss so this loss function is what is minimized so the whole neural network works on minimizing this cross entropy loss and as as we keep minimizing this loss we keep getting to better and better w dash okay okay thank you so you got it so we have this training which is input and output so we have an ideal input and ideal output this is what we don't know we start with some random matrices and then we gradually using gradient descent reach the w and w dash which comes up with the minimum value for this cross entropy loss chander how many uh how many layers are needed typically for this in inverted is just one layer word to make this the approach is just using one single layer that is the beauty of it i mean it will take a long time to train but it is just one layer one input one hidden layer but it is just one layer one input one hidden layer okay okay okay for more technical languages uh let's say like like in the scientific languages does an n-gram with n equals five still apply or do you prefer a little bit yeah tell again what is the question with n equal to five for form yes for more technical languages let's say in the scientific community or in medicine where english or if it's not it's not spoken language do you use the n or does n equals five still still normally n equal i mean it is a flexible parameter. I mean, you can try with different hyperparameters, but normally I think in Word2Vec, the standard, see whatever has been done and published, which we can end up, see, we don't do this training every time we come across a problem in natural language processing. So what we do is we borrow the embedding vectors that have already been created and they are available publicly for download. So when we do that, normally the n is equal to 500. That is typically used to and also i didn't yet explain what bag of words and skip gram is but the skip gram is what is typically considered as the better approach i mean it gives more accurate values for the word vectors did i answer your question? Yes, thank you. Okay. And yeah, so this was a point which I have not explained yet. So I said there was, let us go back to the same sentence I had. I had bread and butter for breakfast. And let us say I'm looking at 5 grams. If I look at 5 grams, 1, 2, 3, 4, 5 is a 5 gram. Then had bread and butter for is the next 5 gram. And then bread and butter for breakfast is the third 5 gram. I mean, here we don't have really any more 5 grams, but of course, when you are having a much bigger corpus, you are going to have many more 5 grams. Now, in this 5 gram, there are two ways in which you can look at the input and output. So, let us say you looked at the input as bread. You could have used the output to be I or it could have been had or it could have been and or it could have been butter. Basically, it is all the surrounding words of bread. I had and butter, right? So this is called the skip gram approach. Skip gram, where you're using one word to predict multiple. But then there is another approach, which is a bag of words approach, which means you start with I had and butter as the input and so this was the input and this is the output okay now there is another approach where you choose this as the input and the output would be the word in context so here the word in context is predicting the other words which are surrounding it whereas here the bag of words is predicting the output so this is called the bag of words approach so i mean this is i'm just mentioning this because there are these two approaches to doing word to vec the whatever architecture i showed practically with slight differences you can use it to do either this or this and experimentally this is what people prefer rather than this i think it takes slightly more time to train but this is what is giving more accurate results so this is the approach yeah uh just a quick question uh so the w and w1 uh would it uh have a reverse uh values in the back of the world no actually they are called the dual i mean they don't they are not they are one is not the inverse of the other so it's like this so your v input times w times w dash is equal to your v output right this is and you are asking whether w dash and w are in some sense transpose of each other so i i was asking for the skip gram on the back of words with the w and w dash uh you know uh become yc version like w dash will become w i don't think so no okay no okay because i have two different approaches yes i see i see so how do you give i had and butter as input because there is one hot encoding i will have one one hot encoding i had will have one hot encoding so you so the way it is done is then there are there are multiple neural networks considered so you have this so you consider not just one but you consider uh like you got it. I mean, you consider if you had, I had and better, one of them would be the I, another would be the had and then and. So you have, basically you have four of these. And I mean, four layers, which all get connected to this hidden layer. Which like a summation yes so it is basically it is the same weight matrices get so if you had a weight matrix w here the same weight matrix will apply in all of them but it is just that the inputs are different so so the first multiplication which is vi into w will get summed up for 4 words like i, i and but correct and then the output is only to 1 here whereas in the bag of words so this was the bag of words approach whereas in the skipgram approach what would have happened is the converse you would have had one input and then the hidden layer and then it will end up predicting uh four four different words so you would have that there will be four spikes in the softmax right yes you don't need a four different output but 100k output will be there in which four of those outputs will have high value high problem no no we we actually consider four layers at the output but with the same w dash for each one of them dash uh for each one of them oh okay got it i see but it'll be the same value at output like for all the w dashes the output has to be the same because the n is same w dash is same so the output will be the same okay what you're saying is because it is the same vector no when you are doing the minimization see we are trying to ultimately we are trying to minimize the cross entropy loss right hello i think there is some background i'll mute shankar okay okay yeah he's on mute go ahead the way it works so the way it works is we you're you're saying because it is the same w dash you will end up having the same output vector everywhere that point yeah yeah there'll be only one output vector there is but we we use all these to do the minimization i mean the fact is if you had only one w dash when you are trying to make the the minimize the loss you would have got only for predicting one word whereas you are trying to come up with a w dash which is in some sense predicting not just one of these context words but the others also oh i think you are so the training really depends on the fact that there are multiple layers got it got it got it i can imagine a little bit but i will wait for your further lecture no no i mean what you are saying is absolutely right if you have a given w dash and if you have a given vector the fact is that it is only going to have one output at a given point and you if you exponentiate it and give it you're only going to have one value for the predicting the softmax but the way it works is as far as training is concerned it determines the w and w dash we are done we are not going to use the w and w dash to later on do the actual prediction of context words got it got it got it so you take one of those matrix as the embedded embedding and then use it downstream exactly exactly so for example let us say you have found the w matrix that actually becomes your embedding because w matrix is it's a as i already told you it is a v cross and matrix which means every input word which is a one cross V matrix will get converted. Sorry, I'm, yeah. Every input matrix will get converted by multiplication into a vector, which is of N dimensions, because this was a one cross V matrix multiplied by a V cross N, it became a 1 cross N. So any word, like for example, I said I, which gets multiplied by W, which is a V cross N, I mean, this becomes a vector which is of N dimensions now. Is that clear? And this is no longer a one-part encoded representation it is a dense representation so this is how the once we have w and w dash we end up having vectors which are n dimensional for each one of these words so i have one more question so now the w is of importance after you do all the training because that is the transformation from one part to the uh embedded vector yes uh do you throw away w dash or will it be of any use no no i mean we could have used w dash also um as a representation um it's not thrown away i mean i i think um in Word2vec both have been used. I think experimentally either one of them works as good as the other. There is no reason to use one as compared to the other. We could have used W to get vectors out of words we could have also used W dash to get the nearest word to the vector yeah see so long as we use one of them consistently we still end up having a conversion of V dimensional I mean the whatever is the V dimensional vectors to N N-dimensional vectors. Either one of them can be used to get the vectors from words. I mean, actually, they are called dual representations. You could have used either one of them. It doesn't matter. Hi. Chander, you have a question asked by one of the participants. Does the bag of words pay respect to the order of the words? No. Bag of words does not. It does not pay attention to the order. That's a good question, but it does not. At least the way the word to the neural network is defined, it does not pay because it is a bag of words. That is why it's called bag of words. Neither does skip gram. It is just that when you are having a whole lot of these skip grams, in some sense you end up having the because more often you would end up having words which are only one word away coming more often than words which are coming five words away. So just by the fact that in the huge corpus, you end up having more cases where words which occur one after the other end up happening frequently. But as such, there is no, in this whole architecture, there is no order used to define which is coming first and which is coming later and how much later. Okay? So there was one more thing which I wanted to cover before I wrap up on word2vec. So at the end of all this, ultimately the proof will be in what are the nice properties that have come out of these word2vec vectors. So some of these nice properties which have been experimentally observed like king minus queen it turns out that this is equal to man minus woman so what i mean by that is the king has king is a vector i mean it's a word which has a n-dimensional representation. After having converted the king using the w into n-dimensional. Likewise, queen also has an n-dimensional representation. Man also has an n-dimensional representation, woman also. So it turns out that when you do this word2vec, originally it was all one-hot encoded. So a difference between any two vectors um there would be it would be equal to the difference between any two vector i mean there is no connection you would have got because the difference uh would have been only having the ones and minus ones at two different locations and those two would be different from for whatever the ones and and those two would be different from for whatever the ones and minus ones for any other two words right whereas in this n-dimensional representation it turns out that when you convert king to an n-dimensional queen to n-dimensional and you do this minus this you get something very close to this minus this which means that word to work has actually understood the English language. And this is not, I mean, this is just one example. There are more, I think France minus Paris. I mean, this basically the country minus the capital, it turns out that it is equal to India minus Delhi. So this means if you convert these two vectors in this N-dimensional representation, this basically tells that how much is a country different from the capital of the country. So again it's a very nice relation. I mean these are all close, it's not that it is exactly equal, but when you do this calculation it turns out that you get some values which are all close. It's not that it is exactly equal. But when you do this calculation, it turns out that you get some values which are very close. So it means it really has, it is behaving like proper vectors, where the relation between words is coming clearly having done all the neural network training. Okay, so this is about work to that any other questions I will take it now otherwise probably with a brief pause for five minute I will go on to glove we have time for covering we do have time there is second you're not audible am i audible yes i hear asif i can hear both of you there's time for glove all right so uh chanda there is a question can you guys hear the question chanda uh for some reason you're not able to hear me all right um so guys there is an open question from dave chander can't hear it can one of you please repeat it for him look into the question and answer panel and uh read out the question for him that one he already answered i think the question for him that one he already answered i think he posted a new question instead of doing a soft max why can't you that's the question you're talking about no there is a question all right maybe that question is just meant for me so we'll skip it uh yeah there is a i have you hear me Chandra can you hear me Chandra can you hear us Asif can you hear us yeah I can hear all of you personally okay let me give him a call okay hello Chandra friend calling me again i wonder something's wrong so you're able to okay okay uh but i am not able to hear anyone should i disconnect and reconnect no we're able to hear each other yeah I hear you okay okay I think we can hear him he cannot hear us yes there's some what so I've requested that he continue on with glove guys I'll take the question at the end hopefully we'll have resolution of this issue hello can they proceed yeah so you are able to hear me right let me then continue with the glove and so let me probably move on can you hear us so any questions anyway i i see one question here listen if you look at i see a question here from Dale. Should I probably I'll go through that first. Is there any way to make word embeddings that depend on the context? Because the same word can mean different things depending on the context for example so this is a i think asif this is something that you are going to cover in probably next week or the week after that that is right contextuality though i cannot hear you as if i think you are acknowledging it yeah okay so yeah um i don't know why what happened to my audio i don't know the price let me try just quickly let me try with my microphone it's okay let me continue because i am not able to figure this out okay let me continue because i am not able to figure this out is going to cover this in a subsequent lecture but yeah in word to work the what you are saying is spot on there is going to be the same vector for the word regardless of the context in which it occurs so that is a drawback of word to it um yeah so let me go on to Glove embedding. Any other questions? There is no other question here. Does the conversion here have to be in this same set of dimensions? Does the conversion here have to be in the same set of dimensions? I'm not sure what the question means. Let me take this. Yes, what happens is it's a bottleneck. A word to wake is a bottleneck. You go from a very… I didn't understand the question. So let me move on to glove embedding. Okay, I'll answer that later. Go ahead. So I have covered the Word2Vec and before I go to GloVe, there is one small probably I will addendum to Word2Vec which is in Word2Vec, if you noticed, we used individual words and from the individual words we came on to the vectors using from starting with one hot encoding and then moving on to the vectors in the lower dimension. Now, there is a slight improvement that one can do on Word2Vec using what is known as WordPiece where you don't do one hot encoding on the words, but you do one hot encoding on word pieces. And why is that done? Because many times there are sub words, which make like, for example, the very word sub verb. So, in some sense, sub verb is the words sub is not a word by itself, but it occurs in sufficiently many words as a prefix that it actually makes sense to treat it as a word piece. And you do one hot encoding on this word piece rather than on the full word. And it turns out that doing this does an an improvement on word to web and there is a algorithm called fast text which goes through this approach where instead of doing word to work on that word as tokens it does a similar architecture on word pieces i i thought i should mention this for completeness let's go go on to GloVe importing. What is the meaning of GloVe? First of all GloVe is an acronym for global vectors. So nothing fancy about it. They are just global vectors and what is global about them? Why it is global compared to Word2Vec? I mean the reason it is considered global is in word2vec, we look at either bigrams or trigrams or engrams, which means we are looking at five words at a time, for example. At a time, we are not looking at the entire global corpus of words. We are only looking at any given point, we are looking at five words at a time to come up with what is the input and what is the output. Now global vectors is, the approach is taken is slightly different. So what is done is, let us say we have the same 100,000 words. So you have W1 all the way up to W100,000. Now you write it along the column of a matrix of a square matrix and then you have the same w1 all the way up to w100000 so you you have a matrix which is basically having the rows as well as the columns as these word words And what do you have in the values? So the values, for example, the diagonal entries really don't matter. So let me leave out the diagonal entries. The off diagonal entries, the 1, 2, W1, W2, that's really, it is denoted as X1, 2. It is the number of co-occurrences of W1 and w2 similarly w13 would be x13 which is the number of co-occurrences that means the number of times the word 1 and word 3 co-occur in the entire corpus so how is the corpus defined corpus ultimately is a collection of documents so like i said earlier in my previous example, I said all of Shakespeare, all of Leo Tolfi. So you have, let us say you have around 1000 documents. I'm just, that may be more. Let us say you have 1000 documents and let us say each of the documents has around 1000 words, which are basically sentences, sentences in the form of words. Let us, I mean, this is of course far from the truth. In general, there are many more documents and many more words per document, but just for specificity. So let us look at each one of these documents and in each one of these documents, let us look at the co-occurrences of the different sets of words. So let us say there was a word one, which was like, for example, I mentioned bread butter. So let us say word one is bread and word two is butter. How many times do they co-occur? And then not just bread and butter, but bread with each one of these 100,000 words. So you look at how many times they co-occur in each of these documents and you sum up all the co-occurrences. So it's going to be a huge number, but then there are these co-occurrences for a word one with each one of these all the way up to X, one, 100,000. So likewise, you have these words here and it's all the way up to X1 100,000. Okay. So likewise, you have these words here. And it's a symmetric matrix because number of times word one and word two co-occur is same as number of times word two and word one co-occur. So you have an X2 1, which is actually equal to X1 2. So you have these co-occurrence matrix. So this is the co-occurrence matrix. So in GloVe what happens is you first start with this matrix as your starting point and from here you try to derive the vectors. So how is that done? So the way it is done is first of all let me call this matrix which was c which is the co-occurrence matrix which we just defined. From here we define matrix of probabilities. So and how is that matrix of probabilities defined? It is done by doing it, dividing by the normalization. So if you remember, I said X1, 2, X1, 3, all the way up to X1, 100,000. And of course, the first element is something which doesn't matter because it is the number of times word one and word one co-occur, which is practically all the time. because it is the number of times word one and word one co-occur, which is practically all the time. So what is actually done here is you convert this into a probability matrix, P12, P13, all the way up to P100,000. Similarly, you have P21, this diagonal element doesn't matter. And then you have P31, all the way up to p sorry p21 p23 p22 doesn't matter and all the way up to p200000 what is what is the meaning of all this so the way it is defined is i'll just write it here pij is defined as xij divided by summation over x i no sorry summation over xij a summation um over j of xaj so this is what i mean this means the what we are saying is the while while the absolute counts may12, x13, and all the way up to x100, the probability that word j occurs in the context of word i is the number of times it occurs divided by the total number that any of the words occur. So this is pij and likewise so the way it is from here to here you are converting the actual co-occurrence count to the probability of co-occurrence so once this is done we still haven't figured out how to get the vectors out of it so this is there is a little bit of trial and error that is done so the glove paper actually explains it very well but i will explain it here and you can further read the glove paper again so the let us say we are we want to come up with glove vectors so let me say and let us say we come up with those glove vectors which have the same nice property you remember in word2vec we had these nice properties v king minus v queen is me v man minus v woman this was in word2vec now we know that this is our i mean we want to achieve because we have we have noticed that this kind of understanding that word2vec brought in was really powerful so we want to arrive at this kind of a relation, even for GloVe. Even for GloVe, we would like to get an embedding which kind of replicates this behavior. Okay, so the motivation is, but what we have is the PIJs. From here, how what we have is the PIJs. From here, how do we get this? So there is a nice argument in the glove paper. So I will, probably what I will do is, the way to explain it, let me think of a third word, which is clothes. Okay, so corresponding to clothes, let me say i have a glove embedding i mean i don't know yet i don't know what the glove embedding is i'm going to argue for how that glove embedding can be derived from this pijs so let us say for clothes there is a corresponding glove embedding called v clothes okay it all sounds a little abstract right now, but very soon it will become obvious. So, once you have vectors, you can actually do what is known as an inner product. So, v king minus v queen. Let us say I do a dot product with v clothes on both sides. dot product with v clothes on both sides so because of the fact that v king minus v queen is supposedly equal to v man minus v woman which i don't know yet but i want it to happen so which means i can take the dot product which is the inner product of these vectors with this v clothes and what do i end up getting i end up getting v king dot with v clothes minus v queen dot with v clothes is equal to v man dot with v clothes minus v woman dot with v clothes. Okay, so this is what i have ended up getting if i assume that this relation is true and i am just doing some random third word now how do i now connect this with my pijs now there is something interesting that one can see from let us let us know sorry okay let us now think of what what can happen peaking clothes so let let i'm just i'm not doing anything here i'm just writing these different possibilities and somehow trying to connect these with these vectors. So what is the meaning of P king, clothes? It means the number of times, the probability, the probability that clothes will occur whenever the word king occurs. So that is the probability of P king, clothes. So let me write P of queen, clothes. This means what? This means the probability that clothes will occur whenever queen occurs. The man clothes. I'm writing these four P woman clothes. Okay. So what do we expect as far as probabilities to be concerned? We know that we know something like this has to be true. King is to queen. I'm just writing some very heuristic here. Okay. This is not very precise. But we know that we want something like this to be true. King is to queen as man is to woman. This we know has to be true if our words are in any, if we have understood the language well, we know that king is to queen as man is to woman. Now, what does it say about probabilities? So, whenever we have king is to queen as man is to woman, we expect P king man clothes by t woman clothes see unfortunately i can't hear any of you if any of you is asking question uh maybe i should look at the okay okay, so far no questions. I'm assuming you're with me because I'm just trying to argue how we can get a glove embedding from these probabilities. So, so far I have a, I would like to get something like this. So this is, this is what I would like to get. But what I have is this. What I have is this because of the fact that king is to queen as man is to woman. Therefore it means that the ratios of the probabilities of clothes occurring in the context of king to clothes occurring in the context of queen is equal to this. So this is what we expect whenever we say that there is a similarity in this fashion. But from here, if we need to get this kind of an equation, here we have a ratio, but here we have a difference. So this is the insight that came in the Glove paper. It's a very nice insight i think the way they have argued is this can work only if which means v king comma clothes should be related to logarithm of p king comma clothes because if if this were true peaking this ratio gets converted to a difference how does that happen because if you take logarithm of both sides you end up getting log of e king comma clothes P king comma clothes because log of A by B is log A minus log B. Log of P queen comma clothes is equal to log of P man comma clothes minus log of P woman comma clothes. So this, we see that this ends up becoming whatever we wanted it to be. This is what we wanted. We wanted vking.vclothes minus vqueen.vclothes to be vman.vclosed minus vwoman.vclosed because this is a relation which we like, which we want. We have seen in word2vec that it is a very nice relation to have. Therefore, what I have argued is using the co-occurrence matrix and from the co-occurrence matrix coming to the probability, we see that we can get a similar kind of a vector which is called the glow vector so now the glow vector is defined as logarithm of p sorry i shouldn't call it king any longer the maybe i shouldn't even follow it like that so what I want to say is gender a time we start wrapping now of a word one dotted with V of a word 2 is in some sense equal to logarithm of p word 1 word 2. So this is what if this were true then obviously everything else follows. So if we can come up with a vector which follows this, we end up having the GloVe embedding. Now from here, what do we do? The way the next step is done, here is to recollect so that is a minimization of the gender time check yeah let me just quickly go through the glove embedding paper which i have just a second oh my gosh it's my best friend calling me again hello yeah almost i'm almost done i'm just finishing the last part okay nice length Yeah. Okay. So I think, yeah, more or less, this is what it was. So from do not know is the vectors we do not know these but we do know these right so and this is the error function is defined as this minus this whole squared yeah this is this is the part i just referred to so the way the error function is defined you are summing over all combinations of w i w j so i should no longer call it WIWJ, WIWJ. So what we have here is we have a whole lot of probabilities which are actually coming from the co-occurrences. So this is something we know. This is something we know. This is what we do not know. We do not know these vectors. And what we try to do is we try to minimize the this is the error function because we want to we want the dot product of vectors to be same as the logarithm of the probability of co-occurrence so we do this vwi dot vwj minus log p w i w j we square it and then we actually also add a weighting factor um that weighting factor I'll explain quickly. So this is the error function. So just like in word2vec, we were trying to minimize the cross entropy loss, right? In word2vec, if you remember, we had all these n-grams. I showed the bigram case where we were trying to minimize the cross entropy error. Here, what we are trying to minimize is this error, which this error is the dot product of vectors. And we do not know what these vectors are. So we will start with some random values. So let us say we go to some n-dimensional. So ultimately, it becomes a similar problem. So if you have n-dimensional representations, let us say again we are doing 500 dimensional representation for these vectors, you end up having a weight matrix, this is a 500 and this is also a 500. So, actually have a matrix of 500 into 500, which is similar to the weight matrix which we are minimizing. So you have a weight matrix which have all these weights which is uh that weight matrix is being the so let me call that as some w i j this is a weight matrix because it's a bwi dot vwj in general it is indexed by ij and then you have log of t ij which is something you know so this is something you know and you have a whole square and then you are summing over all possible ijs and you have a f a weighting factor which is made to depend on these vijs okay so the why is this done this weighting factor is made to depend because you you want it to have a high value only if pij is high so that is why you have this weight matrix and this error is minimized and when this error is minimized you end up arriving at just like in word2vec, you arrived at the matrix. Here also you arrive at the ideal matrix, which ends up minimizing this error. And once you have the minimum value, you can use it to define your glove vectors, just like you did in the word2vec. So it's quite parallel. The fact is we are using, instead of using n grams we are using the probabilities of co-occurrence and then we are doing a minimization of this error so more or less i think i have covered glove so any questions on this i can take it now maybe i should look at the una too can you hear me um you need to can you hear me can you hear any of us probably still not there okay is there a question on glove what is an advantage of glove vector on top of the order hello um it is just a different approach i think uh the people who the glove when they wrote glove it came after word to egg i think the author said that it is better but as on date there is the i think both there are different contexts in which each one of them works well i don't think there is a clear which one is better as if you have any comments on this yeah so the world to where looks at local relationships you looked at the skip grams and so forth now uh glove because it's based on the co-occurring matrix it looks at more global relationships unfortunately i can't hear ourselves but any questions guys yeah i hope you thought that any other questions Any questions guys? I hope you got that. Any other questions? So Asif, I posted in chat the Glove paper. Yes. Asif, when Chander was saying the goal is to minimize this error to get that V, WIWG, minimize to what? Yeah, see the error, see in machine learning, the way- Sorry, I'm not able to hear anyone. Minimized to what? See the error. See, in machine learning, the way... Sorry, I'm not able to hear anyone. I don't know what happened to my audio. I have to get this fixed. Let me include Chandra through the cell phone in this so he can hear this. One second, Amrithu, and then I'll answer you. Sure, sure. No rush. Chandra, can you hear me? No. Not yet. I'm not ringing. Alright, so let me answer this. See, what happens is you need a loss function to train the weights. Ultimately, the point is the same. You need a lower dimensional embedding space. You need the lower dimensional embedding vectors. So the question is, how do you find those vectors? They are weights again taking you from higher dimension to lower dimension, the hidden space. So now the question is you know that you need weights. You can initialize it with some random values. But how do you train it? You need to create a notion of a loss function. So this loss function that Chanda is showing you, it's a very natural thing. It is saying that the dot product of two vectors in the embedding space... The person you have called is speaking to someone else. It should be very close to the log of the probability weights. So it's just that. Because that makes logical things. He explained king minus queen or something. So with that loss function written like that, you're basically saying that maintain that relationship, king minus queen. I see. Yeah, yeah. And so now that you have a loss function, what can you do? Your random weights can now get optimized to the best weights that will essentially give you those relationships and that is enough in this scene yeah it's not okay yeah very good can we go can we scroll over that one just yeah i wish i could i get you in and then one second i'll try him again sure sure this has been a bit of a audio issues his phone is coming out busy for some reason which is okay there is a question yeah i see a question uh for gloves hello hey chanda i'm going to point i pointed you at the my speaker, so that you can hear other people, when people ask questions. Right, so. Telegram. Telegram. I have put this cell phone next to the speaker, my speaker for the web conference so you can hear people's questions. Now, just to let you know. So go ahead and answer Dave's glove vector question and the other questions that Mridul asked, I already answered. Yes. Okay. Yeah, for glove vectors vectors are the probabilities of, just a minute, let me mute here. Yeah, are the probabilities of co-occurrences, the co-occurrences in the document or co-occurrences in an n-gram or? These are the global co-occurrences. That is why it is called global. These are the global co-occurrences across all the documents. Did I answer? Yes, you answered that. To be more precise, what you do is you take a window, a window of five words, a word and its neighbors on the left and neighbors on the right. So anywhere in any of the documents, if a word co-occurs with some other word in the window, you say that it has co-occurred. Dave, I hope that answers your question. Can you can you please scroll up where you wrote that probability equation, P i j is equal to... Okay, so Chandra, can you scroll up a little bit? Mridul is asking for something. Okay. Yeah, what are the scroll up a little bit and say we had to move this screen. Yeah. More. Should I go further up? Yes, please. Amrit, now ask your question. I think the one you are right, Pij where you define the probability, right? Pij is equal to when you're defining the matrix here. No further. Yeah, this one. Yeah, yeah. Pij, ij okay for every j in the bottom okay is it clear yes yes it's better okay maybe you are referring to this this should have been k yes some arbitrary word k yes yes some arbitrary word k here then not that g okay thank you i have a question so i'm just trying to put it in perspective like the whole thing why we are doing this uh you know the n gram andgram and like two gram, three gram, cent gram basically. So we are trying to predict the next word, right? Is that correct? No, no, we are not predicting. I'll let Chandra answer it. But the bottom line is the original one-hot encoded space is useless because all the words are orthogonal to every other word. The dot product between two words is zero. It means none of the words are related. We are trying to come up with another hidden space, a latent space or embedding space, in which the words mean something. They are near other words that have similar or related meaning. We are just addressing the embedding part. Embedding part, yes. The word embeddings are all about embeddings. Now Chander will say more. Chander, would you like to add more i i think as if you covered it so there is nothing more with one hot encodings two words have no relation so we do not want that. We would rather go with words where the context and the relation becomes apparent. See, also the fact is, once we train this on vocabulary, we do not want each and every time to relearn the vocabulary because the same vectors can be reused in multiple contexts. Makes sense. Yeah. So once it created word embedding, like it could be in any dimension and then it can be reused wherever needed. Is that correct, sir? Correct understanding? Yes. Correct. What people do is train the word embeddings on huge corpus of documents. And once they have been trained, people use those embeddings in their work. Yeah. I think many times in our real world problems we let us say we want to do the sentiment analysis or let us say we want to do some classification of a set of documents into different kinds of topics for example For example, our training will consist of very less data in a normal, let us say, we have the set of documents and we have the set of subjects that need to be assigned. We may have a training data of just some thousand data points. With that, we cannot really hope to interpret the entire language. Whereas we know that this language is English, which is already coming from the corpus of literature we are aware of. So we train it beforehand on this corpus. And then we are left with a much simpler training to do with with we should be able to achieve it even with the thousand data points so do we have any libraries aware you know which can be directly used with you know problems like this if somebody wants to solve you know the NLP problems I guess these algorithms are already implemented and can be directly used as libraries. Chandu, you got the question? Chandu, you got the question? Sorry? Did you get Pradeep's question? No, I didn't get it. Tell again. But the question that he asked is that, so he assumes that there are libraries that can help us do word to work and glove so that we can just use it. These embeddings are available, directly important to use it. Correct. Yes. Yes. So these are all there, both in Keras, PyTorch, everywhere. These embeddings are well uh available now and these algorithms are fully implemented so using it is just one line of code but the important thing is to know and use it all right correct correct thank you very much sir thank you very much any other questions all right guys thank you I'll close this session. Thank you very much. Those of you who are waiting for the next session, which is on Python, please, I'm deferring it by one hour. Let's regroup in one hour and then we'll do it. I need to grab lunch. So I'll post it on Slack. You're welcome. It's again an open session. Everybody is welcome. clock and thanks a lot to everyone for participating uh i'm sorry about the technical issue on my end yeah no problem and uh thank you chander for the great talk i uh this was fun for all of us to learn about word to vec and uh announcement guys tomorrow next week we we have yet another very illustrious speaker but his Dilip Krishna Swami in India some of you may have heard of the reliance industries but they have they are heavy and now they are creating their own currency blockchain and so forth digital currencies and so forth so the lip Krishnaamy is the Vice President of Research and Innovation. He's leading the blockchain initiative and he'll be giving us a talk on two things. The first this coming week he'll give us a talk on quantum computing. Quantum computing is a very hot topic and it is going to change everything including the way we do AI and so forth. So it's a sort of I thought it would be a good thing to do. He has published papers in quantum computing. And in a couple of months, he'll also talk about blockchain and its relationships to all the computing and AI and stuff that we do. And he'll speak from the perspective of the implementations that he's doing. Those implementation have already won awards. They're considered tied in as the best in the world. So it would be good to hear from the horses. With those words, guys, I'll end today's session. Thank you for participating in this. I look forward to seeing you all again next week.