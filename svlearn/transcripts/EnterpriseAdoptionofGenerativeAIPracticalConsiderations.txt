 After doing my IIT, I came here to pursue my graduate studies. For a few years, I shifted away from engineering and was doing doctoral studies in quantum field theories and gravity. Then came back to engineering and then worked for short stints at NCSA and NASA, then joined a company called Siebel, which got acquired by Oracle. So I was with Siebel and Oracle for many, many years, then got into startup mode, was the chief architect, and way back in 2011, started a big data machine learning group research program that work led it to be acquired the startup to be acquired by a much bigger learning platform and spent about 12 years in that leading that research program. After that, I started Support Vectors, which is again a pure AI-centered company. We have three legs in the AI lab. We provide, obviously, we solve problems, practical problems, implement things. We give guidance to companies. We do, for example, due diligence for acquisitions and mergers or companies that are launching out on AI. We provide them with a lot of guidance in their AI journey. And at the same time, we also give corporate training in AI, especially in some of the things that are very fast moving and in which, frankly, there are no textbooks, things like the large language models and multimodal learning and so on and so forth. So our internal work here at Support Vectors includes a lot of multimodal learning. We do a lot of, for example, image to text, text to image, and healthcare related aspects. So there are many, many areas we work on. We look for fraud detection using large language models. We do knowledge extraction from text, and we do speech synthesis. So there's a whole range of activities that we do with large language models and sometimes we have to do it at fairly big enterprise scales so that's my background thank you asif um that's you know as as you have heard that's a a vast silicon valley kind of hat that you know you could imagine um so let's start So let's start with a couple of sentences. So I'm a Twitter freak. I'm going to come out and say that. I basically stay on Twitter for hours pretty much every day, read a lot of stuff. One of the things that kind of amazed me this morning I was reading about this whole open AI developer you know thing that they kicked off believe it or not I'm sure you're probably aware of this they have named GPT-5 as Gobi that was so funny and I couldn't even stop laughing at it looking at the name so there is always a new model that coming up to your point, Asif, from the overall AI evolution that's going on, Gobi, Grok, you know, Grok that Elon Musk just introduced and, you know, he's been pumping quite a much and there's a lot of uh good response for that based on the real-time capability uh that grok is kind of demonstrating so can you um i know you've been doing a lot of research which i know for firsthand um can you take this group through the commercial models that large language models that currently do exist and you know and you know the strength and weaknesses at a brief level of those and then we'll dive into the open source models equivalent to those and and then we'll do the comparison. So if you don't mind taking us through that commercial models that do exist in the current market. See, when it comes to commercial models, there are two ways of interpreting it. Models that companies have built and they are keeping it to themselves they are not revealing or they are not sure opening it out to the public so there is a huge number of models which i would put in that class so for example google's some of the models gemini work currently is not yet released baidu is doing a lot of work, but quite a bit of the work is not released. What does happen, though, is every once in a while, these giants, these huge AI related companies, most of these are obviously Facebook, OpenAI, I mean, Meta, OpenAI, Google, these are the usual suspects in Microsoft, and then in Tencent and Baidu, and so forth. They tend to release APIs to their system. For example, the poster child of all of this started out with ChatGPT. When ChatGPT came, and even before that, when GPT-3 came, we saw that OpenAI was not keen on releasing it to the public. But after some time, they did. But 3.5 came out, GPT-3.5 and GPT-Turbo. And they were only accessible through APIs. And the reason given were varied. In the beginning, it was said it's too big, it's too dangerous, and so on and so forth. And finally, there was an API, OpenAI API, through which it was accessible. And it became sort of the most well-known commercial model. If you are in the Google Cloud, then, of course, you're familiar with a lot of the models that Google exposes under its umbrella of Vertex AI. In particular, when you're talking about healthcare, they have a significant lead. The models that they have trained in AI, I think, in the healthcare space are better than anyone else's model as of this moment in the commercial space. So those are, for example, BARD, which is a competitor to ChatGPT, rests upon their internal models. Then there is Cloud, which is again a conversational agent, conversational thing. And it is backed by Anthropic and their commercial models. Then there is Cohere. Cohere, we started out with its roots in semantic search, now is trying to catch up and compete with the rest of it. So these are the large commercial models in the sense that they're generative models. Then there are models that do more limited purpose. For example, if you were to go to look at pine cone and so forth, and a lot of these, they are more focused on retrieval, data retrieval or semantic search, and they do a very good vector embeddings and so forth. So there is a fairly rich collection of commercial models that are accessible that you could obviously paying a certain licensing fee, you could use them and integrate into your systems. Thank you. Are these models available as a service for the enterprises? Yes, they all are available through an API and OpenAI's API, for example, is perhaps the most popular because it's a general purpose model. Now, by now, it's beginning to show some limitations. So, for example, cloud and the anthropic model, it does not have a token limit. You can give it a text with instructions and prompts that can span close to 100,000 tokens. That's practically, as far as practical use cases are concerned, that's unlimited length of instructions and tokens. And it is also available as an API. You could use it. Likewise, BARD AI has quite a bit of their models available as an API, assuming you subscribe to the GCP platform. So there are a lot of these, and now Amazon is coming out with its own services based on some of these models and so forth. So most of these commercial models are available as an API that you could use. So they are all residing on their servers, their clouds, and you use them or you integrate with them using the API. So how do they measure the profitability in their models? Any thoughts on that? At this moment, it's basically a cold war it's an arms race so while you could say that if i set up a cluster of so many gpus so many thousand gpus and we do know that for example open ais and just chat gpt takes a massive amount of, I believe, 16,000 or some very huge number of servers that we hear about, GPU-enabled servers, that form the backbone of these services. If you ask this question, are they profitable? The answer could be, it's hard to tell. For example, OpenAI just came out with a statement that they're close to 100 million users, daily users of their OpenAI and ChatGPT systems. People are paying $20 a month to use ChatGPT. People are paying a licensing fee to use and integrate with the OpenAI's APIs. Are they profitable? I don't know, but if I were to venture a guess, I don't think so. OpenAI just announced that they are going to do what I heard is a 10x reduction in price of their service. That makes it very, very inexpensive to use it. In contrast, if you were to host your own models, it would be more expensive. Now, this is by design, because all these commercial models at this moment, besides the fact that they do compete with each other, but at this moment, the biggest threat that they face, and that showed up in one of the documents that leaked out of Google, the biggest threat they face is from the open source community. And at this moment, the tendency seems to be to prove that instead of hosting your own model using open source, your own environment, why don't you just use our services? Our services are much cheaper than you requisitioning a lot of hardware in GCP, in the cloud, and putting any open source model and fine tuning it. That seems to be the message they're driving for at this moment, which explains why they're so inexpensive. It cannot be, it's not an economically viable situation. They can't go on long term like this. Yeah. Yeah, I mean, to piggyback on that particular point. So if I were to, the audience that we have here are more of CXO folks, you know, not necessarily technology. So if I were to kind of look at, you know, a service that probably I could work with Google or Microsoft or any one of them, build a company for that service. It's more is a mimicking of how the cloud service typically is provided to the enterprise. And there is nothing net new here other than if you are going with gcp you get to you know use the google models and you know is that their their typical approach here i said yes that's the way the cloud providers and this big ai and open ai is the exception but open ai is deeply integrated now with microsoft's azure cloud but the way the way the commercial companies have always said is that everything is a service, cloud service, just use it and integrate with it. And so the AI itself has been reformulated as extensions of cloud services. If you're used to using database as a service in the cloud or anything as a service in the cloud, using database as a service in the cloud or anything as a service in the cloud this is also essentially um ai software as a service in effect okay um we'll on this topic will uh sushant has a question so go ahead unmute yourself and ask go ahead i think uh first one is a comment and have a question after Madhu you were asking about the you know profitability right I don't think all these companies are actually thinking of profits right now because in one of the interviews with Satya Nadella, Microsoft CEO, he was like, we are investing a lot on generative AI. And then he said, it's, yeah, we do invest a lot in R&D, we have been investing, but with generative AI, it is really a lot. Right. So I don't think they're actually starting to think of how to make money yet. Because like what as i've mentioned it's a you know cold war arms race kind of thing right so yeah that that is just a comment but uh so as if a question i have is uh so when when so many people are providing different uh you know services of their own generative ai models right so and say for example you're hosted your application in one cloud is there a way to actually leverage the service from a different uh different provider oh yeah that is happening all the time for example a lot of the enterprises that are sitting in gcp i know in particular they are integrating with open ai and a lot of lot of so-called generative ai companies that are happening and because i do a lot of due diligence and acquisition mergers and look under the cover. Quite often, they are value-add or very specialized applications built on top of OpenAI or BARD or things like that. But OpenAI in particular, because it was the first entrant into this field in November, they practically shook the world coming out with their chat GPT. So a lot of them, irrespective of which cloud platform they are on, they're integrating with OpenAI. Likewise, there are many people who are sitting, for example, if you're doing healthcare, it doesn't matter whether you are in Amazon or where you are, you tend to integrate with GCP's models, which are actually pretty superior. So for example, if you give it a patient's case history and a consult transcript and ask, what are the differential diagnosis you can come up with? Clearly, you can see that the Vertex AI's offerings, the Google's offerings, tend to do significantly better than others. So multi cloud strategy or multi cloud traffic going back and forth at this particular moment is very, very common. So my question is more towards like, if I have my own data, right, I mean, I want to, like a hospital system or any any Pepsi or Coca Cola kind of companies or whoever it is, I want to train my data with this LLM model. And I do my due diligence and I understand, okay, I like Google's model better, but I have my cloud hosted in Amazon. Right. So, so do you pump your data into Google, you know, train it there, do the, and get the inference from there and route it via Amazon or? Yes. So there are two aspects to it. Do you need to? So the way we speak in AI, especially in the context of large language models, are three terms that you hear. Foundational pre-trained models. This is what these guys will give you. If you use those and it serves your purpose, then all you're doing is sending your data for inference and inferences come back. You give your data as input, inferences come back. So that across crossing cloud boundaries is common. You just have to be careful because all of these cloud providers, they charge you for data egress. Whenever data leaves their system, they charge you quite a bit of money, right? So you have to watch out for that cost. Then the other thing is if you need to fine-tune the models. In other words, from the base models, you need to use their models and create a fine-tuned version. Let's say with OpenAI or with Google and so forth. Now we are talking of a significantly higher volume of data that will go to pre-train and fine-tuning the model. significantly higher volume of data that will go to pre-train and that fine-tuning the model. And yes, it is something to worry about. How much data are you shipping? But generally, fine-tuning is a one-time cost, so you don't worry too much about it. Then comes the last phase. Generally, in the cloud, if you say, I'm going to requisition a few hundred GPUs and then train a model from scratch, a pre-trainer model from first principles, then you are getting into a very expensive multi-million dollar play. Because now you're talking about a lot of hardware, a lot of data, and a lot of time that goes into it. Even the data with which you pre-train these foundational models on your own, for your own purposes, data comes very noisy. It has a lot of toxic things and noise and errors in it. So you'll end up using the statistics at this moment is just one to 3% of the data will survive and go into the model because your pre-processing steps will eliminate it. So with that level of data transport and model training and time consumption, it becomes a very, very expensive play, which is why companies that do end up training their own models from scratch make a big splash in the media telling you and telling the whole world that they have trained one more foundational model of their own. It is not very common to do so. And in fact, for most commercial purposes, I would strongly discourage people from doing it unless there is a proven need for it. Yeah, that kind of makes sense. So we'll just cover the open source models similar to the commercial models. So I'll see if you could um based on your research what what are the evolving open source models competing with with these big players at this point um and you know the kind of strengths that you are seeing uh from your you know research perspective see open source came upon the scene I would say from March onwards, still from November till March, April. If you look at the academic and the open source community, there was a sense of pretty grave concern. The term being used was industrial capture, that a few big companies only have access to all the gpus and all the hardware resources to train these large models and they were not willing to share it anymore right they were not being open for example even today we don't know how gpt4 is implemented uh through rumors we do know that it is a mixture of gpt 3.5 turbo models but these are all at the level of rumors. There is no definitive statement. More or less, we have figured it out. But through a lot of alternative means. GPT-5 is about to come out. Again, these people are holding on to it. All of these commercial people will assure you that the only way you can train the model is with a thousand thousands of GPUs the only way you can run inferences is again with thousands of GPUs the open source Community's foundational approach is what if this is not true what if we can get equivalent performance using much less Hardware and if you look at the open source community they have taken an approach they have they have done groundbreaking work in creating parameter efficient fine tuning of the models they create for example the concept of lora low rank adaptation is a most remarkable idea it takes a large language model a pre-trained model, and it says we can fine-tune it for any commercial purpose or any real-world purpose, but we can train it on very relatively modest hardware. For example, if you have a single server with four GPUs or eight GPUs, a typical A100 or H100, four GPUs sitting there, and that will cost you, let's say, anywhere from, depending upon what hardware you're putting, whether you're putting A6000s or H100s, anywhere from $50,000 to $300,000. One server would cost you. You can actually train for your business, for your real-world situation, completely fine-tune a foundational model for its purpose and get remarkably good results. That is for training. When it comes to inference, you need even less hardware. You can run a pretty high, for example, here at Support Vectors, we have a four GPU server and we index the entire Wikipedia. We created a vectorization of the entire Wikipedia, every single article, every piece for a semantic search engine. And the entire time that it took to create vector embeddings of it was eight minutes. This was unheard of. If today you use open AIs, for example, the other 002 model, the state of the art embedder, it will take Wikipedia, if you do embedding of that, it will take you hours, if not days. And so open source community has actually moved ahead and they have found ways to do things better with less hardware. And that has been their mantra. And it is very liberating. Otherwise, you will have industrial capture. All we will be doing is, you know, integrating with the APIs of commercial companies. And they are not going to keep the prices low because they are losing money at this moment. Soon they'll jack up the price. On the other hand, with the open source coming, that whole sort of industrial capture was broken open. Today, the situation is this. Especially if your data is very confidential, you don't want the data to be floating around, no matter how much the commercial companies assure you that they will keep your data safe, that they won't do this, that they won't do that. And I'm sure they mean it. There is always the fact that they can be hacked and your data can be stolen. It happens all the time. After all, Equifax got broken up and all our private financial information is out there, right, in the dark web. And so the last thing you want is your company's confidential proprietary data to be sitting there in the dark web just because one of these providers got broken into. The big safeguard you have is you can use an open source model as a foundation, fine-tune it on your data, keep your data in-house, have your own model that only you guys have access to, and work with that. And so open source should be a very, very serious consideration if you do have talented engineers who know what they're doing great point um I we do have a couple of hands so let's let's go with the Ajit uh yeah so yes so uh I said my question is actually I'm working at gsk um i'm one of the gen ai lead there and we are looking the various use case patterns right in in terms of the patterns for mainly specific the content development side document summarizations question answering document generations all those different ways of looking and we followed a different architectural pattern there so prompt engineering is one then fine tuning model what you just explained with that is something we have not achieved yet but we are following uh rig retrieval augmented right and we have knowledge base graph and a few dvs which are integrated with that process and we found it more cost effective as of now looking to the volume of the data right and that's something we are integrated with the vector dvs doing semantic search getting the right prompts minimal form prompts to retrieve the informations so i would like to know like two questions i have on that right one is uh we i found it very cost effective before we go to the fine-tuned model but i would like to know from the productions ready point of view how you see in difference in these two approach and second thing is uh support vector i'm still trying to understand that on the product is it a kind of a vector db similar to azure cognitive search or pinecon or whatever there in the market how it is how it is bringing the solutions there right These are the two things what I would like to know. Okay. So your first question is to talk about your approach. Actually, Ajit, I have some of your technical people. They participate in the AI training that we give. In fact, in the current cohort of large language model bootcamp that's running we have participants from the from glaxo from your from your technical team so i am quite familiar with the let me not name those people i'm quite familiar with what you guys are doing because they keep taking my advice I'm quite familiar with what you guys are doing because they keep taking my advice over time. And as if we have, you know, multiple verticals. We have GP&T, R&D. I'm part of the R&D. And so there are multiple communications happening. So if one specific vertical is out to you for the training, definitely if R&D and decide if something uh has not done yet it's not something we can discuss eventually no i would love to actually the experience of these people has been very positive they say they find everything here very useful they're coming at the individual level and trying to get it expense uh reimburse educational reimbursement for glaxo that's the situation they are in uh they're not but as a i would love to do a corporate training deal but let me answer your questions first though see you hit upon the mark first of all prompt engineering now if you look at and when you talk about drag um in this bootcamp actually we covered half of the bootcamp was almost first third was on drag or first fourth was on drag see if you can answer your question with just proper instruction uh creation and prompt engineering nothing like that because the development cost is relatively modest and you're using a large language model for it. Just this week, a research came out, which, who published this research? I would like to say, yeah, DeepMind just came out with this research this week, I think. What it says is that these things are not, they go back into their memory. So, you know, in the parametric memory of the large language model. And from there, if they can extract the answer then with the instruction with prompting you can get those answers right and is very good at that now the fundamental problem that you suffer is that of hallucination and the way to so what rag solves are two fundamental problems one is it mitigates hallucination because you say substantiate your answer based on the information retrieved from the semantic search, right? From the retrieval, information retrieval from a knowledge base. And especially for a domain like medical or for pharmaceutical like Glaxo, you have a pretty large knowledge base. And so when you do a semantic search and you get that and you say, this is part of your instructions, here are the documents or here are the results. And now here is the question that is being asked. And when you respond, then you are benefiting from two things, what people call the parametric memory of the LLM itself and the non-parametric, the knowledge base that is there. Beauty of the knowledge base is it can be far more recent. You know, the knowledge base keeps evolving, but your model, the OpenAI model or whatever it is, has been trained last dated in 2021. So you get the best of both the worlds. But more than that, you also mitigate hallucination. So it is definitely the right approach, especially for large enterprises, which are very knowledge intensive is doing. Second thing is you talked about knowledge graphs, absolutely the right thing to do. The trouble with knowledge graphs has been, Ajit, as you know, that creating a knowledge graph, creating an ontology or a semantic web is a very painstaking process. If you look at the world, Google has a very good knowledge graph. Wikipedia Foundation has a knowledge graph. But in the open world, there are not that many knowledge graphs actually available. But companies, especially pharmaceutical and medical companies, because they have a narrow domain, it makes a tremendous value to create knowledge graphs. It's expensive. Recently, and I'm sure you must be doing that, one of the things large language models have become good at is they can extract the triplets you know for example delhi is the capital of india kind of thing those triplet relationships out of which knowledge graphs are built and so that accelerates the creation of knowledge graph and and by orders of magnitude reduces the human cost the very tedious process of creating knowledge graph the third thing to remember is that, see, when we talk about large language models, behind them are transformers. And one of the things that has happened is, and this is a thing that not many people appreciate, is the tremendous strides that graph neural networks have made. When you hybridize graph neural networks, and especially attention-based graph neural networks with knowledge graphs, you practically have a revolution. And when you put all of that together with RAG and so forth, you get excellent results. Now, I'll come back to your old question, prompt engineering versus fine-tuning versus full training of the model. As I said, prompt engineering should be your first approach. And you always do something for a business purpose. If your business purpose is served, you stop there. If it is not served, then you talk about fine tuning a model. Fine tuning is still within the realms of what you should do if you get improvement on it. And of course, as a matter of last resort, at some point, because for example, in Glaxo, it does make sense that for some very specific things, for example, I'm sure you're using AlphaFold, but in some particular case, some particular use case, you may decide that we do need to train a model practically from scratch for this, because no such model exists either in the commercial or the open source space so that would be the rare rare exception I don't know if that answers your question no no that that helps uh thanks for it and one last question I have um uh we we started exploring uh as in driven architecture uh when I say agent driven is more specific to the our domain specific agents, not like tech agents. And what we are trying to define is different patterns of the architectures for each of the agents. And I would like to know like, because it's a new concept, and we're still exploring to define it. What do you thought on this? How you think? Good question. So first of of all it's a great direction to pursue the coming year that you will see you will see an explosion of work done with agents so what agents do is they facilitate so imagine you have delegates you have a mix experts or a mixture of experts so you can take an approach that there is an agent who is very good at each task but there the purpose of agent is to dynamically infer to take a task break it up into smaller tasks decide who to delegate to and therefore use that in fact to the best of everything that we know that is exactly how chat gptT has been implemented, right? Through, it is just a magic of agents. And if you look at, for example, land chain, its main value comes from the dynamic chains that are created by the agent breaking a problem down into pieces. So agent is certainly the right approach to do. Now, a bit of a caveat. See, if you take a book on artificial intelligence, and for example, if you were to take this book, which is now in its fourth edition, and is the great classic, like, you know, you must be recognizing this book, right? It is the great textbook. It has been for many, many years, for decades in artificial intelligence. Even today, when you open this textbook and you go beyond the first introductory chapter, the first meaningful chapter starts with agents. So AI, real AI has always been very agent-centered or believe that agents are the way to go to get to real AI. And that is coming to fruition in this. As we move forward, it will more and more be coming to fruition. Okay, cool. Yeah, thank you. Thank you. We'll go to Parth next. Go ahead. Bless you. We'll go to Parth next. Go ahead. Hey, good evening, everyone. I think you answered some of my questions in your previous answer. So let's say for somebody who has a lot of sensitive data on premise, who cannot afford to send anything outside. I mean, there are so many companies like that, right? and anything outside. I mean, there are so many companies like that, right? So in that scenario where you cannot really invoke an LLM on the cloud or a cloud service, for example, like you said, using an open source LLM on-prem to start training it from scratch, is that the only option? I mean, and also I think i saw the question from shushan that i have the same question too because i challenged my engineering group um sometime ago hey you know what guys go figure out like facebook llama or some other open source model just try to deploy it on-prem where nothing can leave outside? It has to be so secure that you cannot do anything that can go out. So have you come across those scenarios and what exactly people are doing in those kind of scenarios and what models are they using? See, Pata, very good question. So I'll unpack it into parts. First of all, as I said, your progression should be prompts, RAG, this retrieval augmented generation through internal LLMs and on-premise LLMs. These should be your methods of first use. Your method of last resort should be using a commercial LLM like OpenAI. I'll explain to you why. First of all, open source has made a lot of strides. By the way, if you're using Meta's Lama 2, then consider Mistrel. And now Zephyr has come in. They're coming in quick succession. Mistrel, by the way, is very, very good. It's a significant advancement. And it is more or less a drop-in replacement. So many companies I am seeing going to full production using just internal prompt engineering, RAG. I mean, now we have something called Active RAG, Flare, and using the open source models, widespread adoption. What is now, having said that, there are also a lot of companies that are saying, why bother? Open, excuse me, so inexpensive. Cloud is so inexpensive or Bard is so inexpensive. Let's go integrate with them. The problem is they are artificially inexpensive. Your architecture, once your code base is written written a typical code base is a lifetime of 10 years what it will mean is that once you create an architecture around a third-party software it the moment the cost increases you will have internal debates now that is it more expensive to re-architect or is it more or should we just keep paying the price right those debates will start happening sooner than you think in companies it's it's just like amazon right amazon used to be way cheaper than everything used to be far less expensive than in other places or in local shops now the the tables are turned. At this moment, the federal government is apparently suing Amazon because it seems to have made a billion dollars using a secret algorithm to price optimize and convince people to pay more. Those days eventually do come with commercial companies. I mean, not to single out Amazon. I guess they all do that. And at this moment, if the commercial companies are running at a loss, they can't be running at a loss forever. It's an arms race. They're just trying to capture the market. Right. So again, back to the question. So for somebody who cannot leave even a single bit of data from on-prem, but it's secure, what is the LLM that you think you will recommend for somebody to run on-prem in a private network? Oh, certainly start with Lama, Lama 2. Like for general purpose situations, Lama, Lama 2 and Mistrel. But see, it depends on your situation, right? If you're talking healthcare, there are the Gator Trance and so on and so forth, which are very good open source models. Bring them in-house. If you're doing text-to-speech, then you're talking tortoise and so on and so forth. So, you see, pick your problem and you will be surprised how far open source models have come in, in each of these situations. So, if you were to tell me the specific problem, perhaps I could tell you at this moment in that specific problem, which open source model is appropriate right I mean let's say if there is a lot of unstructured data there's a mix of a big data Lake or even a data Ocean kind of a thing sitting across on-prem uh a very very large one um and if you want to build like a basic chat GPD kind of a model with on-prem securely. So Lama 2 is the best, is it? No, Lama 2 used to be the best. Now replace the Lama 2. It's almost a drop-in replacement. Replace it with either Mistral or with Zephyr. Those are the new things. They have just come out in the last month or so. You could use those. But I mean, if you think L Lamatu, you're not far off. RAG is the way that you want to go. If you want to just chat to the knowledge base that you have and the data that you have internally, you're looking at RAG, retrieval augmented generation, which is basically semantic search, a vector database, a semantic search, vector databases you can create in-house very, very easily. I mean, of course, people who don't have the, I mean, if they want the convenience of it, or they're willing to pay the money, they could use Pinecone. There is no reason to. For example, I, in my last year, until last year I was working, I was the, obviously leading the research in the leading corporate training company, Cornerstone on Demand, the entire AI stack that I created. And for years, we used in-house RAG. Okay, perfect. Thank you. Thank you, Pradha. I'm going to, before we go to Fernando, I'm just going to read a very prominent person actually tweeting this on Twitter. And I am not making up these words words so it's important you hear me after Elon Musk introduced grok so grok appears to be the way more real-time spicy and fun compared to I'm I'm stressing this word, work chart GPT. I wish he didn't say the word work in there. So let's go to Fernando. I know I think you're about to ask that question regarding the side of the EIML bias and stuff like that. Please go ahead, Fernando. Yeah, thank you. Thank you. Really a thank you. Thank you. Really a great session. Thank you for that, for putting this together. Just to give you some context about my profile before asking, I used to architect a VMware for big techos, federal government, and cloud providers, right? So now I transition into architect VMware for financial services industry in US and Canada. So probably I'm offering myself to help with the AI side, right? So my question is about the following, how to prevent raising ethical questioning when training the models for instance preventing racial racial profiling when provided a great service for small business or things like that how we can in general avoid any ethics conflicts under ai when implementing AI? Yes. Fernando, this is a very good question. And I must say, it's a minority of people who are very seriously looking into the ethics of AI. I'm very passionate about it. So I'll tell you this. First of all, you have to want to be ethical. And it's surprising how few people want to be. They just want to not do anything illegal. But if it is unethical, it still makes them profit. By all means, they'll go with it. The other aspect is you have to be ethical. In the US, for example, they are the protected classes, like you said. The protected classes are race, gender age uh orientation uh and and so on and disability and so forth and if if it is demonstrated that your system ai system is doing bias against them then then you are in violation of the law now how do you prevent it it cannot be prevented it can be mitigated People have tried to feed data and to train the AI in such a way that data is unbiased. And then they say that, see, look, our data is, we have made sure, taken a lot of pains to show that the data is unbiased and therefore our models are unbiased. That's not true, actually. Even if your data is unbiased, the model can easily become biased because the data will have asymmetries or it will have artifacts or artificial patterns in it that will mislead an AI to take shortcuts and get biased. So you need a system to check for biases in the inferences of the model. You need to put safeguards there. You need to put test suites to check for that. And in fact, that is one of the things we help companies do to do that. And surprisingly, almost always we discover that people haven't taken care of even the basic things. They don't even have a test suite. They just say that, you know, our data was pretty clean. We did not try to be biased. So it must not be biased. Of course, our inferences would be good. That's not how to think about it. It's a pretty nuanced thing. AI, the cleverness of AI models, the complex models is that it will take every possible shortcut to make a good prediction. And sometimes those shortcuts introduce bias into the system. It's a fact of life. Now, European Union has has a AI act US government is trying to bring one now these rules are not perfect and if you look at the technical Community they are all like oh this is so horrible and so forth but at least it's a step in the right direction there's a imperfect but then those laws need to be brought in just to get people to seriously consider the ethical implications of AI. The second aspect is that because people are not looking into the ethics of AI carefully, we are in imminent danger. We are facing an existential danger to civilization itself. More and more, AI is getting into the hands of or is commandeered by very few extremely wealthy people and it is leading to a unprecedented concentration of wealth throughout the world and democracies are being toppled and many things are happening i hope i don't sound too woke after that comment on work but it's a fact it is objectively seen as a fact today. Many of you remember the Cambridge Analytica. And the work that Cambridge Analytica did, they were implicated in the Brexit. They were implicated in toppling democracies in Latin America. They were even implicated in playing mischief with the US elections. And that was before these large language models came in. In fact, it was all done with the previous generation technology. So you can imagine what potential for harm there is and why it is important to be worried about the ethics of all of this. Perfect. Thank you. Appreciate it. Thank you, Asif. I just wanted to add a little bit to what Asif said. There is a tremendous discussion going on, especially on Twitter, which I follow rigorously, about the White House executive order in terms of AI safety and almost forcing the open models to be much more explainable. And you can't just roll out a model without basically explaining how the heck it works. And one of the debates that's been going on across the board is that all this data that these models were trained on, especially from a transformer's perspective, is the known human or animal knowledge. That's how they put it. If it is known human and animal knowledge, Transformers are not going to basically do its own thing and make it existential threat. So that's the debate. There was a paper that just came out as if I shared that with you today. You may not have had a chance to see that, but there's a lot of discussion on that. So being said, we go to Gopi for the next question. Sorry, one second. If Gopi is trying to switch topics, I just wanted to check with Asif regarding the bias, right? Since we just finished that topic. So in bias, I've heard that these systems are actually better in bias than actually human beings, because we do have a lot of bias and a lot of things. So what is your opinion on that? See, that's a very good question. And here is how my position is a nuanced situation. See, if you, first of all, large language models mirror civilization. They have learned from all that we have expressed. Human beings speak. They are a gregarious community and a very talkative one. So we write, we speak, they are a gregarious community and a very talkative one. So we write, we speak, we express and all those expressions feed into the training of a large language model. So here's a reality. Human beings are on a spectrum. Today for example, I mean bias is always there. There is a statement I remember when I was in college. There was a person who was very passionate about social issues. And at some moment, I said, you know, I'm a scientific person. I'm not biased. And she immediately said, Asif, everyone is biased. And it took me many years to realize that she was right. We all have biases. Biases that we know of and biases that we don't know of. One illustrative example that I would give is Aristotle, the great Greek philosopher. Even he had biases. He used to believe that men are brighter than women for amongst other reasons, men have more teeth and a bigger brain. Obviously he did not open Mrs. Aristotle's mouth and count the number of teeth there, right? Otherwise, he would have discovered that's not true. And well, what about whales, which have even bigger brains than us and so forth? So we all carry biases. Aristotle was considered a very enlightened philosopher. Today, when we looked at much of what he taught, we consider it to be pretty ignorant and biased. taught we consider it to be pretty ignorant and biased right there are gems of sparkling gems of depth and wisdom there but there's also a lot that is wrong and so every successive generation finds that what we were had quite a bit of ignorance so there is progress within human society itself there are people who are deeply biased and they defend their biases. The very fact that there are wars going on means there are segments of people who are fighting identity wars. People identify with one thing and they are biased against the other side. It is going on in America. It's happening globally. The wars are everywhere. So bias is implicit are human beings better uh or are our llms less biased than human beings that is a point that has been made for a very technical reason what people have done is that once her llm is trained on human knowledge then the second thing that it goes through is some fine tuning for a domain and the third thing that it goes through is something called reinforcement learning with human feedback. So what you do is you take the LLM, a causal language model, to produce, let's say, 10 responses to the same input. And then you have a human being rank those responses. And you weed out the most toxic responses. You tell it, you give it a penalty that these responses are not good they are terribly biased and these are the better responses but who is going to do the ranking it is human beings for example in the case of chat gpt it was a whole army of nigerians who were the human beings in the loop rating the answers that this large language model was producing and saying which is better and not so at the end of it it is only as unbiased as perhaps the best effort of those people trying to be unbiased and that is the limit of how unbiased LLM can be but is it truly unbiased no actually a long topic we could go on and on, but wonderful discussion. We have a couple more hands, and I am trying to respect everybody's time. We're almost at the top. Yeah, we are on the top of the hour. So I will just take 30 seconds in the end to, you know, so I'll kind of relinquish and let's have Krishnan go ahead. Oh, thank you. Thank you. So I have a couple of things in my mind. One on the biases. Today, there was just a comment. I saw the tweet from Vinod Khosla, but he was talking about yesterday or today's tweet. But how the AA and, you know, potentially can help us replace, not replace, augment doctors and help us get more access to doctors and in that sense you know you are talking about biases that doctors and some of us are some of the spectrum in US across the world don't have access to the doctors right they're all biased at certain way so he was projecting that you know you and though a may have some bias it may be still better than what's happening in the reality. So that's a comment coming from Vinod Khosla. So that's just a comment. On my second question, more about the operational aspect. So a lot of this enterprise who wants to either use open LLM or fine-tune LLM or even self-model LLM, fine-tuned LLM or even self-model LLM. They are now thinking about how do we manage, operate, and monitor these and making sure things are working as expected. So what is the maturity of that LLM ops today? And where do we go from here in terms of the maturity of that? So, Krishnan, to both the questions. First of all i'll speak on the llm maturity i'll let me go to the vinod kursala comment yeah that is a realization see in united states in particular there is a vast and throughout the world there's a vast asymmetry between the number of people who are there and the ratio to doctors very few doctors too many patients. Especially in rural America, that's a problem. Secondly, especially in today, medical community, sad to say, are far behind when it comes to racial equality or gender equality or sexual orientation equality. It's a given fact, right? There is data after data that shows that. A black woman, pregnant woman gets a far worse treatment than a white pregnant woman. It's a known thing, right? So obviously, large language models are useful. The other thing is that it makes it more accessible. So instead of waiting for a long, long time to get access to a physician or driving 200 miles to get to a physician, which is the reality in rural America today, isn't it far better that you can have access to an AI assistant, which can act as a primary care, as a triage system that can figure out, are you in dire need of help? Or is it just simple cold and flu and so on and so forth and give you some help in the absence of a doctor those would be the big use cases of ai in medical now the second part of your question was llm mature the ml ops maturity it is a fast evolving thing lots of startups are coming and doing it for example if you look at frameworks like ray so Ray is created by the same guys who created Spark. You know, some of you in the technical field know that for large data computation, Spark is the leading distributed big data compute engine. Same guys are now creating the Ray compute framework. And in particular for AI, it's very AI centered. It's very much, it is maturing very, very fast. We use it heavily with great success. It gives you monitoring and dashboarding and everything. It is not the only framework. There are many, many such frameworks coming out. Ray is leading the pack, but others are trying to catch up or having differentiators or having different things. Of course, cloud providers like Google, they give you their own frameworks, the whole vertex Ai and monitoring framework and so on and so forth, so those things are tooling up very rapidly. Gopi Ghosh- And I would say that they're there have achieved quite a bit of maturity in a very, very short time, I would say in this year itself. Srinivasan Parthasarathy, CIS, Great Thank you. Great. Thank you. All right. Thank you, Gopi, for the platform. So I'll leave it to you for the final for fantastic questions that made this even more. It took it up a notch. You know, it's one thing to come with a set of content to share, but to kind of be able to pivot it around the kind of questions you were asked. I think you did a fantastic job and I really want to say a big thank you to that. And I think this is the kind of way we would like to see this forum try, right, Sushant? I'm kind of looking for any of you, your end points, please come forward with, you know, facilitating or, you know, offering some topics where we can kind of have these rich intellectual discussions, informal, you know, Q&A, back and forth, that we all can learn something together and take away from. So that's just my public service announcement. We'll be meeting in two weeks. That's I think November, what's that? 15th or something like that. So we don't have a speaker or a topic lined up for that. We'll be trying to do that between now and then and share in our chat group. So stay tuned for that. And if you're willing to, please reach out to me or, you know, we'll put it on the chat and then we'll pick it up from there. But coming back to you, Madhu, thanks for, you know, preparing and, you know, anchoring this session. I really appreciate all the efforts, the energy, the thought that went behind this, and thanks for taking my feedback. And I think it's very much in alignment with what this group has been interested in talking, as you could see. So on that note, I think good night to you all. And I think once again, thank you. Thanks a lot. Appreciate it.