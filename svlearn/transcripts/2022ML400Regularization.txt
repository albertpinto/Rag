 With machine learning algorithms or with models, when you train a model, let's say, let's take a very basic model. Suppose you have a polynomial model. y hat is equal to let's say beta naught plus beta 1 x plus beta 2 x squared up to the nth polynomial x n there is a problem that these any model especially even very simple models like this the problem that they have is they tend to listen not just to to. So suppose you have data. You want the model to, like, suppose this is the ground truth. You don't know the ground truth. Data presents to you like this because data always comes with noise. So suppose this is your data, and I can go ahead and complete this. Give me a moment. Suppose data shows up like this. And I will make it more. You get the idea. So I won't be too precise in putting the dots to have a normal distribution around the ground truth. distribution around the ground truth but something like this. So suppose this is your data and it shows like this the data always has noise so fact is fact data has inherent noise. So in other words, it is always, the y is always some function of x plus an error term, a noise term, that is, that you can't really wish away, no matter how good a model that you try to build. What happens is if you take a model that is too inflexible or too simplistic, the simplest model that you can build is that it is called a high bias. This is the bias, people often call it the bias variance trade off. It's not really a trade off, but people think of it like that because you can actually contrive a situation which suffers both from bias and from variance. But here, let us look at the hyperparameter. N, degree of the polynomial, is the hyperparameter. Why? Because you can decide if you're doing polynomial regression, what the degree of the polynomial will be. If your degree is zero, then you're saying there is no relationship. So let's say n is equal to zero. Where am I? Right. Suppose you say n is, why am I not able to write this? Something strange happened with this one note. Let's see what it has done. At n equal to zero, wouldn't it be like a straight line? Yes, exactly. So let's go down to the bottom. Where am I? Yeah. So Outlook has this issue, this particular version, that every once in a while it stops responding. So I'll close it and I'll reopen it. No, not outlook. One note has this issue. Now with that, this issue would have disappeared. Yeah, so when you say and so let's take the case what in the world is going on so let me try another color this so this reads We said that suppose this is the ground truth and you have data where data has showed itself to you as let's say like this. Why? Because data always has irreducible noise. I'm just repeating what I said a while ago with this bringing back the picture. And you're trying to approximate it with a polynomial regression. What does the polynomial regression say? That y hat is beta naught plus beta 1 x plus beta 2 x squared all the way to beta n x n degree of the polynomial. Now if you remember one of the facts of algebra this is basic it goes back to the fundamental theorem of algebra it says that if you have a polynomial of degree n then at most it can have n roots or n places where for n values of x where y will be zero. In other words, another interpretation that you can give is at most it can have n minus one bends, curves that you can have. So when you look at this curve, how many bends do you see in this curve? You see two bends. So it means that you can only capture it with a polynomial of degree at least three. That's how people think. But suppose you don't know, you have seen the data, but you don't know what degree of polynomial you're talking about. You can start by making all sorts of hypotheses. You can say, well, n is equal to zero. Suppose that's your that is your hypothesis. Right. So what does that make it? It makes y hat is equal to just beta not. This is a constant. It is saying it is invariant of it is invariant of x. The y value doesn't depend. And what would be the best fit Y? It will simply be, the solution would be the average value of Y, which would here be of course zero or close to, or whatever it is, this is your, the actual Y bar, the average value of Y, y hat will be y bar. In other words, beta naught will be your y bar, the average value of y in variable y, right, as seen in data. This is what it could be. It is basically the null hypothesis. This is your null hypothesis that says there is no relationship between y and x. So it is foolish to be searching for a model at all. The next best thing you can do is you can say n is equal to 1, in which case y hat becomes beta naught plus beta 1 x. So what's the degree of the polynomial here? 1. Therefore, how many bends can the line have? 1 minus 1 is 0. You can't have any bends. You will end up with a line that best fits the data. Let's say that, let me write this in another color, just so that I can illustrate that. Let's say that y1, y, in this particular case, is y hat is equal to beta naught plus beta one x. If you try to fit a straight line, a straight line, when you try to fit a straight line, after some bit, you can convince yourself that the best straight line that you could possibly fit to this data is possibly this. In fact, this was your homework. If you remember, this was the best straight line that you could fit. You can say, well, you know, that is it. What about going to the next degree polynomial and saying n is equal to 2? So if you take n is equal to 2, then your degree of your guess is now a beta naught plus a beta one x plus beta two x squared and you can sort of figure out what would be the best value of what I haven't drawn this out if you try to fit the data you can figure out it will have one bend. If I had to guesstimate, it's very hard to guess what would be the best curve that would fit the data. I'm guessing it would be some, I'm guessing, and this may be wrong or right. I'm guessing it would be something like this, but it may be wrong. One bend curve. You look at this, you see that n is equal to one is also not enough. So you can say, obviously, the Goldilocks value where it best fits the data is n is equal to 3, in which case you have y hat, let's say, y hat is equal to beta naught plus beta 1x plus beta 2x squared plus beta 3x cubed. If you do that, you will get a curve something like this. And you can see, you can convince yourself that up to n is equal to 5, it does well. But what happens if you do n is equal to, and suppose you have 100 data points. You go to n is equal to 10. What happens? Because the curve can have nine bends, what it does is, after getting these two bends right, this bend and this bend right, now it needs to fit in seven more bends somewhere. bend right now it needs to fit in seven more bends somewhere and so what it begins to do is let me pick a color here that may be different from all the colors that I've used let me use this color so what will happen is it will start it will start you see what the pink line is doing it is beginning to add in extra little bangs to overfit to the data. So you start seeing that it starts listening to the like, listens to the noise. Because it doesn't know that it is noise, it overfits. It overfits the data, which is bad. This is also a situation of high variance in the strict definition of the values, but we won't go there. Now, what about the extreme limit? If I go to y is equal to, let's say that n is equal to Let's say that n is equal to 99. So now how many parameters there are, including beta naught? There are 100 parameters. There are 100 data points. What does it become? It becomes simultaneous equations with 100 equations, 100 variables. What will happen? You'll end up with a curve. You'll end up with an exact solution. In other words, you will end up with a curve that sort of wiggles around and does all sorts of things and it exactly fits the data. Training data. The trouble is training data is just a small window into the reality. When you fit to the training data so hard, then another data set with a different noise characteristic, slightly shifted, will now not fit, will not work. So it begins to fail on the test data set or the validation data set because it has really overfit to this data. So there are two ways to solve this problem. First thing you can say is, well, you know what, if instead of 100 data points, I've had 1000 data points or 10,000 data points, then having a hundred degree polynomial is not bad. You can't exactly fit to the data. Right. And I would like to illustrate this with a little example. So the first way you can fix it. The easiest way you can fix it is more data. Let's bring in more data. Yeah. So this is an example. This is a code that you have actually access to correlation and I'm just wetting your collection that you do have this code now look at this you have five data points if you claim are you guys able to see this graph yes yes five data points and you take a model, polynomial of degree one. All you can do is set up a straight line. You would agree that this is a fairly okay straight line, but it doesn't quite fit. What happens if you go to degree two? In fact, I was curious to know. Yeah, look at this. So my guess was wrong actually. So now you notice that it looks like this. Suppose I increase data. I go to more data point. See how it looks. I go to even more data point. Suppose I go to 200 data points. Ah, 200 data points has just made it look like a straight line again, right? But I'll keep it down to five, but i'll just show you something degree two was there degree three ah it seems do you notice that by the time you go to degree three it has more or less started fitting the data exactly you see that case isn't it now it so happens that it was from a degree three polynomial so it does make sense why what happens when i go to degree four even better degree five as i said even better right but observe what happens at the tail end of it now what what has happened when you went to degree six? You have five sample points and you go to degree six. Degree six means it must have five bands, one, two, three, four. What is it beginning to do now? Do you think that this is justified, this part? We don't know. It gets worse as you increase the degree, as you make your model more and more flexible. And it will come back. You realize that something weird happens. The y value starts widely oscillating at the tails. In other words, the tail begins to wag. This phenomenon is called the ranch phenomenon after the great mathematician branch who discovered it and pointed out that if you try to fit a very high degree point on data, you start seeing this wagging tail or this oscillations in the periphery of the model. Pretty bad, actually. So now this is a classic case of overfitting. It's listening too much to the noise. Notice how it perfectly goes through every data point. Well, so one of the easiest ways to fix it is to just go get more data. Well, this looks a little bit better. It's not as bad. More data. Yes, it doesn't look like that now. And notice that when you don't have more data the values go up to 60 like wild oscillations whereas you know that the actual values were just close to three four right 1.5 and minus 1.5 so 60 is just bizarre now it's beginning to get tamed what happens when you go to more data even you notice that it's getting much more tamed. What happens when you go to more data even? You notice that it's getting much more tamed now. You go even further. It begins to fit even with the ninth degree polynomial. And it begins to behave even more. You bring in more data, 200 data points, and it has pretty much tamed this very highly flexible model. But then you try to make the model more flexible and the game begins. Do you notice that the more flexible you make the model, once again, the tail has begun to wiggle? I don't know if you can see where my mouse is. Yeah. Yeah. And here also at this end also the tail has begun to wiggle. I don't know if you can see where my mouse is. Yeah. Yeah. And here also at this end also the tail has begun to wiggle. And it gets worse if I go to 20, right? Here it's really beginning to misbehave. So what happens is that another way to say this is your data, when the model is very flexible, it easily adapts and fits to data so the only way you can the simplest way you can avoid it is giving it more data the trouble is more data is more easily said than done because data is precious and you can't simply ask for more data quite many situations especially in medical situations and so forth, data is very, very hard to get by, right? So, because each data point probably represents one patient, right? So you can't just tell a hospital that, okay, on this clinical study, instead of the 25 patients you gave me, just go give me 1,000 patients. They'll say, well, 1,000 patients have never shown up here with that particular disease. So you can't get data. The question therefore becomes, what do you do to prevent this kind of a behavior? So everything that you do is called regularization to prevent the model from overfitting to the data or in particular fitting to the noise of the data. See what happens if the model is very flexible, another way to put it is it tends to memorize the data, the training data. So in the extreme case, all it needs to do is internally in some form, maintain a dictionary and say, if this is the input, that is the output. It will get your training data perfectly, isn't it? If the model does that. Now, if you remember, in college, we have all two different degrees done that. If you don't like a subject, what do you do? You go to the seniors and say, why don't you tell us what questions appeared last year? And does this professor tend to repeat the questions? And if he does, well, what does, what do the students do? They, instead of studying the subject, they just go memorize the answers to the questions they expect to come. Then they keep hoping that if, if they are memorized 50 questions, hopefully in the exam at least 30, 40 will come and they'll pass and they'll get over the subject. Does that resonate with you guys? Yeah. So that is overfitting. What you did is machine didn't learn or you didn't learn. You instead just memorize the data. You memorize the responses, the correct responses, and you keep hoping that you are tested on exactly the data that you have memorized. But the moment the data is different, you are in trouble. So if the professor asks some widely different questions on the same subject, that's when you get into trouble. The same is, that is overfitting so how do we prevent it this problem is much more so in neural networks uh where you have a lot of parameters you must have seen that if you put five layer neural like each of them with hundred layers the number of weights that you have is hundred times 100 times 100 times 100 times 100, right? So it's a massive number of layers. Like at each layer, you have 100 times 100 layers, right? Sorry, 10,000 plus 10,000 plus 10,000. Yeah. So you'll end up with some 50,000 or so weights. Am I getting my arithmetic right? I hope, yeah. So massive number of parameters. So it can easily fit to the data. So how do you prevent it from fitting to the data? There are many techniques that people have brought in. There are traditional techniques called L1 and L2 regularization, which is the LASSO and RIDGE regularization. What I would suggest is because they are a big topic, the best way is we just post the video that I have from comprehensive intro. It's there on the Support Vectors YouTube channel also. So we'll just post a link to that. It will take time because it takes three, four hours to get it right. And there's a beautiful geometry to it on how you do it comes under the field of constrained optimization i'll just write the word here constrained optimization are we together now Constraint optimization. Are we together? Now, what does that mean? It means you're not looking for the absolute optimal. So suppose your error surface is like this and this is your error direction. Again, this will totally not make sense to you unless you have been through my background on this thing. These are the parameters, beta i, beta j, right? So what happens is that these things form contour lines. Now, what you do is the absolute minima is here, but you're not willing to take that. You insist that you will only go with minima within a certain radius, within a certain radius, within a certain radius r of this origin. The reason for that is if you keep the beta small, you make sure that the model does not overfit to the data for a whole variety of reasons. And there's a lot of technical discussion we went to. So what happens is that instead of taking the absolute minima you take this point as your constrained minima this point here is your constrained minima and this value of beta this is of beta, this is your beta star. This is your best beta here. You pick this. And there's a L1 and L2. They have to do with Minkowski norm, which basically says, how do you define a circle? Do you define a circle as a round thing or as a diamond, right, geometrically, and that determines how your optimization surface looks, whether you're in this circle of constraint, does it look like a diamond or does it look like a circle? But the end result of L1, L2 regularization is that it does pretty well for more situations. Now deep learning also l1 l2 regularizations are pretty common but there's some extra regularization techniques that come in that i would like to mention so one of the two things one of them there are many things actually that come in generally when you do batch normalization it helps and many things help but more properly there's a dropout. There is another technique called early stop. And then there are things that there's something called skip connections, etc. Residuals that I haven't taught you. We will do it in a future section. So these things and how they affect novel regularization will come to much later. But I'll talk about two things about today. One is drop out and early stop. Early stop is easy. It says that when you see the loss, you know your loss. Asif, just a quick clarification that I need. In the picture above, what was the circle to the which intersects the all three the Xs here? Which circle is this? He's just circling beta star. It is the beta. Yeah, it is basically in the hyperparameter. Actually, this looks as though it's rising up in the e direction it's not it so if you just look at the beta i beta j space it is a circle in that okay a constrained surface you're saying basically that don't look for solution outside this region right so one example that i give is that maybe in are familiar with Ramayana, isn't it? Yeah. So there is this story that Ram and Lakshman and Sita, they are in exile in the forest, but Ram and Lakshman, they have to go gather food. So Lakshman draws Lakshman Rekha around the cottage where Sita is supposed to be there and tells her, don't come out of this circle, isn't it? Outside this you are not safe but inside you're safe. And so it's somewhat like that. Obviously, if you step out as she did, then all hell breaks loose after that because there's a demon Ravan sitting there to do that. And so the demon here is this the temptation is this big global optima the global globally best solution which tends to be an overfitting solution and effectively what you have is a circle of safe region and you want to say that i don't want the global solution i want whatever is best within this region right yeah so that's what it is but so in this case when you selected the beta x here instead of the the the i would say the uh bottom most part of the polynomial upstairs up there so i guess what we are saying is we are satisfied with whatever beta x is and then we don't want to go to the center is that what what you're trying to say yes that's what we are saying is we are satisfied with whatever beta x is and then we don't want to go to the center. Is that what you're trying to say? Yes, that's what you're saying. You're saying that we will go within a circle, whatever is the best beta. We will take that for the error. We are not going to step out of the circle. Now why in the world do we put such a restriction? Why do we draw this Lakshman Rekha around it? There's a lot of interesting discussion and the whole theory of constraint optimization comes in. Unfortunately, that will take us the better part of three, four hours. So I won't go into that, but I suggest watch the video, right? Or if you come to the ML 100, eventually, we do this in very careful detail. This is a lot of territory to cover. And this is the traditional territory of regularization in machine learning, even outside deep learning. But so shouldn't this be beta j and beta x on the bottom here on the on the y axis? Yes. Let me just say I wrote it as beta it's actually beta star beta star means star usually is optimal beta got it okay i got it i think so basically what we are saying is that if this is um like if this whole circle was basically we are trying to stay within this the um within this polynomial right and not go outside the bounds of it because we will yes yeah i got it okay so what you do so let me come to this deep learning specific techniques early stop is very interesting you don't keep on learning epoch after epoch right what happens is after certain number of epochs or steps number of steps you just say i'll stop i'll just stop here instead of going all the way you'll just stop here it's called early stop you stop training the model any further it's a little dangerous because it might not have gone to the global uh it might not have gone to the minima that you expect and it takes a lot of experience to do it see a neural network gets very complicated you remember that there is the there is no convex loss surface it's a very complicated surface but one experience that people have had is if you keep on training your neural network for a very very long time Even training your neural network for a very, very long time, it will overfit the data. So one way to prevent it from overfitting the data is that once your loss function, your loss has become small enough as you are training it, you just stop. Don't try for perfection. You stop when it is good enough. When you do that, it has, see in the beginning, your model is learning from the signal because the signal is strong. But after it's picked up most of the signal, it is beginning to pick up on the noise. So you want to stop early and say, I'm not going to train it any further because I fear that if I train it any further, it has already learned from the signal. There is no more signal to learn from. It is beginning to learn from the noise. So you stop it. That technique is called the early stop technique. Then comes another technique, which is quite interesting, actually. It showed up in the world of autoencoders and later on was generalized to all of the deep learning. Even now, it is used. But not as much because other things have proved more effective. So dropout was actually generalized to all of deep learning by such luminaries as Hinton and Jeffrey Hinton and other people themselves. They're great guys. They initially said that let's do one thing. Suppose you have a layer of neural net. Lots of nodes are there. Another layer of neural nets, right, another layer of neural nets. What happens is that when you train it with the data, excite, all of these things are lighting up. If you think of each neuron as a bulb that's lighting up, right, what happens is that there is way too much learning taking place. So let's handicap it in some way. What you do is you sort of simplify the network at each stage by going and shutting down arbitrarily, randomly, a small proportion of the nodes, right? You just go and shut it down. So when you pass through one batch of data, or mini batch of data, each mini batch sees a different neural architecture because randomly some of the nodes have been shut off. So when the back propagation happens, both the forward pass and the back prop, they only see a subnetwork, a random subnetwork. Typically, let's say that you turn off 10% nodes. a random subnetwork. Typically, let's say that you turn off 10% nodes, right? So you say that the dropout is 0.1. In other words, 10% of the nodes arbitrarily get switched. Initially, there was an observation that when you do that, it prevents the system from overfitting because you can't, this network is not getting enough place to save the data and memorize it. It is, in my view, it is very similar to autoencoders. Just like an autoencoder is forced to summarize the data when it comes in, in the same way, there's a sort of flavor like that, that by doing dropouts, you're forcing this thing to understand or have redundancy in the network so that it understands the input-output relationship much better. Dropouts worked. They did quite well. But one of the things is, when you look at the, so go to that Lost Landscape website, you will see that what Dropout does is, it sort of creates spikes, little porcupine-like spikes all over the Lost Landscape. Right? And you wonder that, what is it it doing how in the world is it regularizing it's a little hard to explain but it does regularize these days though regular dropouts are pretty much the norm for quite some time see the history of deep learning is not that long 2014 is i believe in the dropout as a technique uh 28 2008 uh ages when it was first discovered for auto encoders 2014 is when jeffrey hinton and all these big guys said oh yeah it's a great thing right then the residuals came about and skip connections also came about resnet came about and with a lot of other things coming about gradually people began to realize that you know you get almost all the benefits of a dropout regularization by other means right the the trouble with dropout is the main problem with dropout is that the network that is trained is not the network on which you will do the inference because when you do the inference all the notes are little but when you are doing the training you arbitrarily go about switching bulbs everywhere notes everywhere right and then you do the back forward pass and back back pass and so it led to a bit more instability in the learning process and people are more and more averse to using dropout but these days they use it. It is still a very useful technique. You should certainly try it out. Small amounts of dropout is always good. I still put throw in a little bit of a dropout, but not so aggressive. But given the fact that you have all the other techniques that you have been learning in this course, you may not find that it is all that useful. Earning stop is still very much a practice. L1, L2 regularization in your optimizer is still a very good practice. Dropout is still very prevalent, but not considered absolutely necessary anymore in terms of regularization. So that is it. So people have all sorts of, this is just the tip of the iceberg. There's L1, L2 regularization, then there's a mixture of those elastic nets, et cetera. There are these skip nets and skips and residuals and so forth, they all have effect. Everything, actually it's hard to say, everything has a positive effect. The batch normalizations that we talked about has a positive effect. Gradually, everything has a positive effect. And at the end of it, what happens is that then you start asking, asking okay is this one technique still of value or not and you can say that about any one technique about early stop about you know drop out or so on and so forth should i do it or not and ultimately the data decides right every situation is unique. By playing around with that situation, you learn and you decide whether it works or not. But the topic of regularization is important. You must always regularize in some form or the other, because without that, there is a grave danger of overfitting to the data. So that is it. That at a very high level is regularization. It is preventing somehow overfitting to the data, either by bringing in more data, data augmentation, or by L1, L2 regularization, or by dropouts and by early stops and then by many other techniques. And the field is fertile people keep coming up with more and more techniques for regularization and it will go on go on hopefully another year or two there'll be another big breakthrough and it will be a game changer but at this moment it is experimental you have to play around with it and see what works is experimental. You have to play around with it and see what works. How do you prevent the network from overfitting? See, these days you build networks that are massively, massive in size with hundreds of millions of parameters and weights, weights and biases. So the question is, how do you even prevent such a huge model from overfitting? Right? And that makes the topic of regularization very important. You have to play around with it and make sure that the system is not overfitting to the data. Right. So we have a lot more parameters than the data points, right? Not all, not necessarily. What happens is, for example, look at the next topic. So this Monday I'll be teaching GANs, after that I'll be teaching Transformers, and then CNN, and CNN is the final one. Transformers are massive models, but to train them people feed pretty much they literally what seems to be the equivalent of the entire knowledge base of mankind is fed to train a transformer that it doesn't overfit. So while you use regularization, you apply all regularization technique to it right pretty much you do not only have you good optimizers and regularization in between and residual layers and skips and so forth at the same time you have give it as much data as you can to prevent overfitting so those things chomp through all of human knowledge, not just Wikipedia. Wikipedia would be too small a knowledge base. They chomp every single available, publicly available text in the world. Right. And then they're and those those models you cannot even train on your own machine. If you take a cluster of something like 20,000 or 100,000 GPUs, really, really powerful servers, and you train these one model, goes through training with data for two, three years. Then it gets to these transformers that we'll talk about. They are such absolutely marvelous models but so ginormous that it is not even conceivable that we can train them on our laptops okay they're trained on massive clusters with massive amount of data with massive effort to prevent over fatigue everything is thrown at it they represent the state of the art in human to prevent over fatigue everything is thrown at it they represent the state of the art in human uh in ai at this particular moment someone did a study that said that the amount of energy required to train a transformer as it is going through training is the same as the amount of energy needed to light up a city in the u.s the total consumption of a city equivalent amount of energy needed to light up a city in the US. The total consumption of a city, equivalent amount of energy goes into training an AI model these days. It's literally, it's that, it's that hard. But when they get trained, they become general purpose agents that can be used for many things. And we will do that in this workshop in the labs. We will do that.