 I'll just quickly review the California data set for those of you who missed it because I know some of you missed the lecture last time. Obviously we all live in California. Some of us own homes, some of us aspire to own homes. We either own homes or aspire to own homes one day. The thing in our mind says, is it a time to buy and if you do own a house, are our houses appreciating? Like most Americans, our assets are a lot of part of our equity, our wealth is in the homes. So this is a good problem to look into. It's old data from the 1990s for the California housing prices. This particular thing I believe I did it in Python and a very good exercise for you would be to do it in R. See if you can do it in R. It will be very easy. Just follow the way I have done the previous data in R. And for example, the Irish data set, I believe I did it in R. So that illustration and you'll be fine. So we have this data. The only categorical variable is the opening proximity. So, this is a basic summarization of the delta, during what it is, projecting it onto a map. This is using the four-year maps, and then this is putting it onto a scatter plot. In this scatter plot, the color ranges are the home value. The region from the . Asif, I just would like to say, in this scatter plot the color ranges are the home value as if so i should just would like to say there's a calling user too not sure who the there's a lot of background noise if people could mute themselves we could listen properly yes please uh please mute or ask if you could mute or ask somebody to mute. Call user to indeed. Could call user to identify themselves? We have no idea who it is. All right. I'll mute it. Thank you. So now looking at this, we have a scatterplot where these are the home values. One interesting thing to know that 25 years ago, $500,000 homes were the expensive side of homes. Do you notice that? They're deep colored, the 500K homes. Now in 25 years, I suppose a 500 500 K homes would it still be considered expensive what do you think is anybody considered cheap it could be considered quite cheap yes so this is how times change this is it and here the size of the burb is the population density so one thing is pretty obvious and we know it from experience high population density areas tend to be expensive for housing those are the urban areas and metro areas and homes they are going to be expensive just looking at this map you can tell that this is the San Francisco Bay area and that this is the Los Angeles area. And down there you can see the movement from Los Angeles all the way to San Diego. And then if you go up here, you can sort of make out the progression towards the Sacramento area. You can see the homes along the Central Valley, all the Merced and Fresno and so on and so forth. So there is a lot you can decipher just from this catapult. So that's that. That's that. Now, so there are certain observations that there is a, so when you look at the histograms, you'll see, and again, I talked about the skew in the data. Think about why there's a skew in the data, right? It is something worth thinking about. Whenever you see a right skew like this, or a right skew here, median income, is it true what it is trying to say is that the median income are taper soft, and then most people have a median income comparable, in a comparable short range. Then likewise, population density. My population here, but this population per block. Total number of rooms most people don't tend to have that many rooms in a block. The violin graph is of course the density estimators. We talked about it the box plot. So guys if you don't understand these things I've explained it once so post it to slack and then I will explain it and remember we also have the extra sessions so we covered some of this in the extra sessions that I give I wouldn't go over that again but this is just a review the correlation parts and so on and so forth. Once again, if you guys need me to explain anything, the team or this is a pair of plants. One more thing I added was the missing value. Whenever you get data, data never comes perfect. You always have some rows which are missing some features. For all sorts of reasons, somebody forgot to take measurements taking measurements was not feasible right or the values were invalid and somebody had to delete it and things like that so you should see whether the how clean the data is looking at this data it seems that all the fields all the features are there the only place you have some missing values about seven eight about seven eight missing values is in the total bedrooms and so total these are total bedrooms seven eight missing values out of more than twenty thousand a sensible approach is to just go delete those rules it wouldn't affect your system so this is the way you would remove those rows from the data. So this is that. Now there is one thing I just wanted to warn you. These days network science is becoming a fast emerging subject. We have all sorts of networks in the world, the social network, the professional network, the network of power grids and power transmissions, the highway network, and many, many such networks. Even at the molecular level, in the biological systems, we have the protein networks, and the genetic code networks, and so on and so forth. and a lot of them have tremendous impact on like you know their structure their topology have a lot to say for example you can tell if there are vulnerabilities in the network some of you remember that at one time the whole eastern seaboard went dark because some critical power stations failed and because they failed or into overload they led to cascading failures and the whole instant C would went out and so we can today study those things those vulnerabilities in network infrastructures through a subject to exploratory analysis to a machine learning exercise and all of that comes under the subject of network science. So when we talk about networks, we start with simple networks. The first thing is networks is something you are not probably used to studying in depth. So I put a mention here, but these notes are very preliminary, I'm still writing them. But see, you can have, let me give you some intuition. This is called a connected network. Means node you notice is connected to every other node right so from any node you can go to any other node in a single walk I'll get you get it guys so when you have that the number of edges is much more than the number of nodes in fact it is n choose 2 which is n n minus 1 over 2 which is so in other words the number of edges grows quadratically we can see that for any ten nodes you have so many edges by the time you go to 50 nodes it begins to look like a beautiful and abstract art the same graph isn't it you can hardly see the links from one to the other right but the reason I put it here is to show how quickly the number of edges explode it's a quadratic growth curve and these days of course we are all very well aware about exponential rewards your current corona and so forth so that is it that's one there's another kind of network topology which is a hub and spokes model you know at the center is one hub node and all other nodes are directly connected to the hub but they are not connected to anything else so one of the things we speak of in network in networks are when we explore networks is the diameter of the the entire network it is that what is the maximum distance between any two nodes so what would be the maximum distance between any two nodes right but somebody tell me what's that what's the maximum distance it's in any two minutes diameter that's right so how many edges are there let's say that if I take this guy and like you know any two nodes in this except the hub remove the hub um yeah yeah how would I go? You realize that all you have to do is from any node, you have to go to the center, the hub, and from the center, you can go to any other node, right? So it takes two edges at most to reach from any node to any other node. You see that, guys, in the star topologyology the diameter would be two, right two edges. It takes two edges to go somewhere else. Then a variation of the star is a little bit more connected. It is the wheel. You see the wheel? It is like the star except that a node is connected to its adjacent nodes. Every node is connected to two nodes, two neighboring nodes. Do we see this guys? The wheel graph. This is the wheel graph. And then there are other kinds of nodes or networks. This is very interesting and it tends to show up often in practice it is called the cliques graph the cliques graphs are interesting each of these do you see this bunch of nodes together so each of these nodes are highly interconnected means every inside a clique it is a practice it is always or almost always i mean sometimes it's approximate clique, but strictly clique is in a bunch or in this group, it's called a clique, every node is connected to every other node. So it's a completely connected subgraph. And each of these cliques are connected by just one edge. Do you see that there is only one edge connecting a clique to its neighboring peaks ask a question sorry the document is in which language okay oh at this moment yes it's very nice I trust you guys all know Latin no I'm joking actually I still have to write the text. As I said, this chapter is unfinished. This is just baseholder text. There's a development now. So up to here, up to clicks, it was fine. After that, there is a whole bunch of things that are handwritten and I need to type that in. So that is why you see this strange language and remember this chapter is unfinished as I'm still writing so now one of the things that you speak of in network is something called a degree of a node what we find in networks is that real-life networks are very different from these simple conceptual networks the real clicks and star and so forth it's not that these things don't show up in nature they do in the complete track they don't they do show up for example the complete graph that this one used to be one of the very popular networks for a parallel computing but the earliest apology a parallel computing. The earliest topology for parallel computing was this. Every processor could, had a direct link to every other processor. Some of you from hardware and parallel computing background will probably remember that. And of course we went to different architectures, like torus architecture, et cetera. And if you really think about it, all of those parallel processing architectures are networks, different topologies of the network. And we'll have a lot more to talk about it. Actually here, I discuss the parallel processing architecture that you find in practice quite a bit. I speak of the torus architecture and I will speak of the so-called binomial tree architecture and so forth. I have not finished writing that. So now to analyze it, real networks like what you see for example in the social graph or a professional network or a citation network. Citation network is when a researcher writes a paper he cites, so she cites other other papers other resources and you don't have to be a researcher just an author for example if you look at the bottom of this document itself you will notice that i have citations to all things that i found worthwhile so now those people refer to somebody else and so on and so forth so there is a web of citations they form a network users uh you know web pages form a network so all of these networks in real life they tend to show some very interesting behavior uh qualities that are worth remarking. So I'll start with the simplest quality. The first quality that you observe in a network is the nodes have different degrees. What is the degree of a node? How many other nodes it is connected to? So given a node, suppose it is connected to seven other nodes, its degree would be considered seven. Now in directed graphs in which you know the direction matter, you can talk of two kinds of degrees, the in degree and the out degree, how many nodes are coming in and how many edges are coming in and how many edges are pointing out. So for example a citation network could be treated as a directed network. But let's keep it to a simple thing, undirected in this moment. So the degree is the number of edges incident upon a node. How many nodes is it connected to? So that's a degree of the node. It is quite instructive actually to see in a graph how the degree of a node is distributed. One of the earliest breakthroughs in this field was actually studying the degrees of the nodes in various network topologies and if you can understand the degree distribution of a network you it is somewhat like you know doing descriptive statistics on a bunch of numbers it gives you a quality very good sense of what is it that you're looking at now degree is the simplest of it now there are other measures for example connectedness measure then clustering measure and so on and so forth and we'll talk more about it but i'll just take the degree here so this is a little utility I have which I use to compute the degrees because at this moment you know what libraries that automatically give it to you now the library that I use is a very popular library called Network X in Python and for R it is I graph both of these are very mature libraries. iGraph is much faster because the underlying implementation is in C. Network X is implemented in Python so its advantage is that it's very Python friendly. Its disadvantages is perhaps of all the industrial strength network libraries, it is perhaps the slowest. But the fact that it is very intuitive makes for a great beginning. The general practice is that if you are in the Python world, you start with network x. If your network becomes too big or it becomes too slow then you move on to another library and i graph exists in python network kit there are many libraries these days uh that you move on to but for learning certain networks and for experimentation with smaller graphs this is certainly one of the best so here's a little utility i wrote that you are more certain to Here's a little utility I wrote that you are most looking to read through. What it does is, and so I look at one particular graph. It is the so-called Karate Club network. It's a classic, it's one of those things, it's like the iris for normal tabular data. Think of it as the iris dataset of of the network world there was a karate club in which if you look at this karate club let me explain how to interpret this graph do you see the color bar on the right hand side guys this color bar gives you the color of a node the color of a node is the degree so things for example that have let's look at this node here where my mouse is this node is yellow in color the color matches here right approximately this it is because its degree is 2 why is the degree 2 because there are two edges coming out of it right now let's look at this node this seems to be dark sort of a maroon kind of a color. Now, how many edges are coming out of it? How many nodes is it connected to? 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15. eight nine ten eleven twelve thirteen fourteen fifteen right so there are 15 uh edges to it so the degree is 15. so you can see the color of this is approximately the color here do you see the relationship guys so the color and the size is also a measure of the degree. I've made the notes with bigger degrees, bigger, quite literally bigger in size. So looking at this, you can quickly establish that there are two key people in the karate club who are highly influential so that a lot of, almost everybody is related to one or the other. So in this karate, if you look at this, what can you infer as a data exploration exercise? What is it that you can infer? So, well, at this moment, it's a little hard because you're new to this. I'll give you the idea of what it was. See what happened with this Karate Club is, after some time it split into two. It became two different camps of people, right? The Karate Club became two sub clubs. And in a way you can see who the sub clubs will form around. One sub club was formed around these people and one sub club was formed around this hub. one sub-club was formed around this person, this hub. So in real networks, you always find these hubs or nodes of high influence, of high degree. And you should always search for that. Like how much, how strongly is the influence spread out in a network? So one example, which I agree in the extra challenge was yes but in one company there was a problem the problem was that the upper management the executive levels whatever they wanted to convey to the company and they made a lot of effort to communicate to the employees of the company, and those employees would somehow come to entirely different conclusions. Their information would almost always be distorted or not correct. The company was very frustrated on what was going on. Why is it that they were trying to communicate with what was being believed as truth on the ground and the grassroots level was entirely different. And why transparency was not having a . So if you then, one of the researchers made, actually Barabasi, one of the great researchers in the street, made a graph, a network of all the people who are communicating with each other. And that's a point of interest because it shows who the influencers are, who the central people are, or the hubs are, to whom a lot of people reach out for information. So one would imagine that in a hierarchical organization as companies are, the people you would reach out to be would be up the food chain, up the management chain, it would be the vice presidents or things like that. But actually that's rarely true. In this particular case they found the person who was of very high influence can you guess what that person turned out to be anyone would like to guess that the safety training i read i read the books ah something like that very close actually it turned out to be an i.t guy who was going about fixing printers and computers. So because he met people from every department and he talked with them, he became the vehicle to spread gossip from one team to another or misinformation from one team to another. So they realized that actually it was through his movement that a lot of information or misinformation was flying through. So it raises an interesting question. What do you do with this person? Do you fire him or do you do something else? Anyone would like to suggest? Leverage him to spread good information. Exactly. So what you can do now is you know somebody who is a good conduit for passing information, correct information along. So you make sure that he is educated on the right information. So that as he goes from team to team, he conveys or propagates a truth rather than gossip right so that's the value of network analysis it's quite an emerging field in fact if you look at the number of research papers coming out on this area of machine learning it has a exponential growth let's say that it's spreading like a virus like a coronavirus almost it's an exponential growth huge amount of research is being done in this field at this moment very very hot so I thought and this is the first time I'm doing this that the time has come to integrate it into a standard course of machine learning itself now the work in space, the earliest work was done by a person called, by two people, Erdos and Renyi. Erdos was of course such a great mathematician that everybody has an Erdos number, like how many links away are you from Erdos? So this work was done I believe in 1957, but he and Renyi, another great mathematician, they're positive that if you take n nodes and you take a probability p that any two nodes, random nodes are interconnected, then you can have what mathematicians call a binomial distribution of what the degree will be. So there's a certain probability that the node will have degree one, certain probability that it will have degree two, and so forth. So if you look at the degree distribution, it looks like this. This will be reminiscent to some of you of a bell curve or a binomial distribution. I hope it does look like a binomial. Now, in the case of very large numbers of nodes and very, very low probability that two graphs are connected, it's a fact, a mathematical fact, that binomial distribution becomes a Poisson distribution. Now, I won't use all these mathematical terms, but because many of you may have forgotten what binomial is and what a Poisson is, it's there in the notes. Gradually you will become familiar with it. And as you go into intermediate machine learning, of course you'll become much more familiar because I'll systematically be teaching it to you. Those of you who've done the mathematics of data science, Vishal, you must be remembering this, one of the distributions we covered. So, yeah, this is it. So it's a very interesting thing. Do these distributions happen in nature or in real life? These are called random graphs. I should have said random. Do these happen? Yes, there are situations where these sort of graphs show up but it turns out that the vast majority of real-life graphs do not look like this they look like something which in an idealized form is like this oh okay by the way this is in the the Ardor's venue I have colored the degrees of the mode you can is in the Erdos venue. I have colored the degrees of the node. You can see that the degrees of the node are pretty well distributed. There are very few nodes of very small degrees, very few nodes of very high degrees, and most of them are intermediate degrees. So what is the alternative to that in real life? One of the alternatives that was proposed by researchers Barabasi and his student Albert in 1999 was the so-called Barabasi-Albert network. It was considered a landmark in this field and right after that there was an explosive growth in this field. A lot of people were studying it, but now we know in hindsight that the work that Balabasi Albert did, I'm told, was actually done by a great mathematician named Price. It wasn't quite understood at that time, importance was not well understood. A great Price that time importance was not well understood a great uh price actually discovered this sort of networks and that was in the 1960s that he discovered 1965 i believe is that he wrote his first paper and that paper was way ahead of its time it already contained some of the central ideas and people essentially rediscovered it 35, 30 years later. But for what it is worth, what does this graph look like? Can you see that this graph looks different from the previous graph, please? Now, if you stare hard, you may start noticing that some of these nodes, they have very high degree. You see that they're highly interconnected to other nodes whereas more but but they are also nodes which have very poor connect we have which have degree just one two three like low connectivity so if you plot this graph uh and this is a mathematical relationship it is called a power law behavior or the scale free behavior it is not in today in experience we know that it is not strictly followed but it is approximately followed that the degree of the probability that a node will have degree d in other words the probability that a node will have d connections to other nodes is actually proportional to D to the power minus gamma where gamma can be 2, 2.4 or something like that. Let's say let's say gamma is 2 so then what it would mean is probability is 1 over D square right D to the minus 2 is D square 1 over d squared right so it falls quadratically with d and this sort of graph and we can see that when i when i draw it out with the degree of a graph visible do you notice this that most nodes have very low degrees you know they are light yellow colors only some nodes are orange and very few node in fact only one node and maybe another node and look like maroon and maybe reddish right degree 40 or 50 so there is a very skewed degree distribution in this and it becomes more evident if i replot this graph and you should always do that when you're observing an equal data trying to plot it like this you notice that this dark node and very quickly it shrinks in sizes and most of the nodes are yellow they have no degree and if we make a histogram a frequency plot look at the frequency plot here which is so different from the frequency plot of the RDoS Renye graph it is some nodes most of the nodes have degree close to one two three four right whereas the nodes with the with 45 degree let us say 42 degree is right here, where my mouse is at this point. One node seems to be out here, one or two nodes, and just one node seems to have a degree of 22, 24 or something like that. Most of them have smaller degrees. So you see a power law decay, something like one over x squared decay, one over d squared decay. So this was the crucial observation that was initially made by Price in 1965 and later rediscovered by a whole bunch of researchers and amongst them Baraversi and Albert. And it created a, it sort of created a renaissance in network science as a network science as a subject. The number of papers that began to come out in this field went through exponential growth after that. And it is still rapidly growing. People are finding all sorts of very, very interesting things about these networks. networks. And when you couple this real life network, social networks, with one particular class of machine learning algorithms called deep learning, or deep neural networks, there's something most marvelous, some very good insights begin to emerge. And that is one of the really hot areas of research where people are creating a lot of companies and doing a lot of startups and a lot of things are happening it's a cutting edge at this moment so now I'm going to look at another example a real-life example so till now we were looking at the theoretical graphs you know what do you find in nature there is a Barabasi-Albert model there is an Erdos-Renier model there's complete graphs star graph wheel graph and so on and so forth so now let's look at a real example at this moment i put one example this is a facebook social graph we all use facebook so this is a small group i think centered around some topics of facebook a friends network and we are going to look at this so ignore this latin thing now see how rich um actual graph looks like i hope if you just stare at this guys you realize that it looks it does not look like any of the basic wheel graph or compete graph or something like that it looks very rich I hope you can appreciate that and you can see literally communities here and in networks you have communities communities of people who are very closely knit with each other are we together and you find this nodes which are very high degree you see this this node here where my mouse is it seems to be connected to so many many nodes out here there seems to be a big community of nodes out here yeah you can literally mark circles around these communities and in a later version of the graph you'll see that i would have colored the communities differently so So I don't know guys, when you look at this graph, it is absolutely breathtaking. It tells you a lot about human relationships, human friendships and how people relate. I hope some of the beauty of this sort of graphs, real life graphs, it does show through in this. And this is part of the beauty of this sort of graphs, real life graphs, it does show through in this. And this is part of the exploration of graphs and its properties, which is quite interesting these days. The amount of code needed to do this is actually pretty small. We read the graph and then we just visualize it. Couple of things, why did I, there's an alpha transparency, I've made the line semi-transparent, otherwise it will become a big blur of ink right excessive ink so to not have a big blur of excessive ink uh oh there is a typo here this thing should be here it should start with that okay I'll fix that you copy paste this code has gotten a little bit manual the The indentation is wrong. Line 11 should start here. So if you look at this graph, this social graph, and you ask what is its degree distribution? Its degree distribution is this. So of all the topologies that you saw, the Erdos, Rengi, the all of those, the the complete and the barabasi and which of these do you think it is closest to anyone would like to venture a statement which of these is it closest to guys Closest to real network you mean? Yeah, which of the, this real network, which of the theoretical networks that we talked about in the previous sections is it closest to? So see, remember this. Barabasi. Barabasi, right, exactly. Yeah. Because closest to the Barabasi network. And this is a very crucial observation and Barabasi's network it may not be exactly that but it is somewhat similar to that and here I have visualized it with the degrees highlighted. You can see that there you see there's this node, this node here, this at the very center is dark maroon so it has a very high degree actually I wish the y-axis and I'll do that but you can see how well the distribution of this shows through so this is an example of network science guys there's a lot to it and I will mention because I haven't talked about clustering I haven't dealt with communities, but you can see the communities. That's the next topic I'll add to this. I would highly invite you to go pick your own networks. And there are repositories of networks. There is an entire website called networkrepository.com or .org where people have archived all sorts of networking data. It's become that hot. There is also a repository maintained by Stanford called snap snap repository for network grants in fact this particular data came from the Stanford repository it is a sample of Facebook data and of course if you look at the kinds of network we live in for example Facebook it is a gigantic network of a what about four five billion nodes unless you have a really powerful computer it's a bit hard to visualize so but anyway something for you to play with now the now the second thing I wanted to mention about this is that last time we did univariate linear regression. So the lab of it, actually these are the old nodes, I'm still converting it into the new format, but the content is all there. So this is the linear regression. At this moment, please follow through and repeat it in this thing and in due course of time I'll reformat it. in this thing and in due course of time I'll reformat it. So this is the lab we did last time. Please follow through. We covered it last time. I won't cover it now. Today we have a new topic but I'm just summarizing what we did last week. So here are the labs. Please repeat it. If you get stuck reach out. We'll help you. And then the next lab which again today I won't cover I'll let you try to do it on your own and then we'll cover next time because we are alternating between theory and maps so today is a theory session so I won't go into that it's a pretty here's the Python version of the same lab. Then I'm just obviously scrolling through it very, very fast. And you will see that then we do the nonlinear version of this lab, nonlinear least squares. Read through this guys, follow through, read through. We have a lot of material to cover so I won't be able to do every single example in slow depth but I will certainly come back to it next time. So this is for the univariate but non-linear least squares. Suppose you know a function and you're trying to fit the function to it, how would you do that? Well this is a little bit of a page which says that people often ask R versus Python. Again I should move it to the top of the document. Never ask that question guys. It's not R versus Python. It is always R and Python. And in a few years it will be R and Python and Julia. So start doing these labs guys. If you haven't been doing the labs, you'll be surprised at how much work or homework is waiting for you to do. So get started. You have to practice a lot. And this is the third one. This one deals only with real world data sets. It deals with the famous data set of cars, the mileage of cars. It deals with the famous data set of cars the mileage of cars it deals with Boston crime data set it deals with you know pricing data set for car seats these are all classics of this field really work learning is a tremendous amount of learning embedded in these data sets I won't go into too much of it because we're going to cover it in depth in the next section. But I'm just giving you a very quick preview of these, a lightning fast preview of these things, just to give you a sense that you need to get busy doing things, practicing the labs. Then this section of the document will contain all things interesting. So I happen to put it here. One of the things is the bias and variance errors. Remember in the previous session I talked a lot about the bias and variance trigger. Bias is when your model is simpler than the ground proof. Variance errors come from your model being more complex than the ground truth variance errors come from your model being more complex in the ground and so it turns out that the way I explained to you was a good way to get started with and everything that I said in those handwritten lecture notes I'm going to reproduce it here in due course of time it is work waiting to be done I will do that but just but you'll find something more more than what I got time to talk about in class which is something that I will talk about in the deep neural network class but here I'm just mentioning the paper you can actually realize the state of the art says that today when you look at modern machine learning algorithms you know the state-of-the-art algorithms like support vector machines random forest gradient boosting neural networks the bias versus variance trade-off has become not a trade-off anymore it has become a situation where you have to deal with bias error separately and variance error separately. There's a lovely YouTube video to watch. I highly encourage you to watch and obviously ignore this Latin text because it's waiting for my explanations and so and this is the paper if you like reading it in paper form this is a lovely paper a very thought-provoking paper which gives you the state-of- of the art on bias and variance. I'll be talking about bias and variance, obviously this chapter is work in progress, I'll come around to it. Then another beautiful thing is something called the exponential families. I haven't talked about it, I won't talk about it in the whole of the introductory workshop, I'll only talk about it in the next workshop. When I do talk about it, you realize that this is of tremendous beauty and importance. If you can understand something called the exponential families of functions, you pretty much you feel that you have understood the root of a lot of machine learning ideas because they all one way or the other center around as a sort of a universal family or universal formula which on the face of it looks rather our king but when you unpack it you realize that it pretty much explains in some sense or captures the gist of so many of the distributions of functions that you see underpinning much of machine learning. So certainly a thing of great beauty and when you understand that you will look at data with fresh eyes, completely fresh eyes and why is it that I'm bringing in such an arcane topic into machine learning class. Well if you talk to any good machine learning researcher they would tell you that they absolutely love the exponential families. They're very well familiar with it. They see it everywhere. You don't see this talked about in the usually coding or programming oriented books on machine learning libraries but I've always felt that if you understand the exponential families deeply you have a much deeper understanding of one of the pillars of machine learning so it is really good so you'll see it there then I'll move the tips cheat sheets documentation all these things that you're familiar with I've made it the last section a section of its own you guys are all familiar with this so it is all here the cheat sheet so go to this cheat sheet and you'll find it there now in case you're wondering this lapses a lot of labs to do it is indeed and i i hope that you guys are spending time doing it because we are at this moment in the monkey see monkey do phase believe it or not this is the way to practice in the beginning you just see and you repeat but after a little while very soon it won't be repeating i would not be giving you solutions to the problem i'll just leave the problems with enough hints enough milestones that you need to achieve or directions, and you'll have to do those homeworks on your own. And if you do all those examples that are there in this notes, trust me, you will be ready to deal with any kind of data set after that, using these techniques. And then one of the things that I've started doing it is adding a glossary, glossary of terms. Like for example, I use terms like supervised learning. So what is supervised learning? So I've given a little bit of an explanation of those terms. Obviously this is very much a work in progress. So for example, I don't know if you're amused or irritated by my explanation of what mathematics is. Mathematics is what mathematicians do, I suppose. Then obviously you have this bibliography of all the resources relevant to this field. And this bibliography will expand. You know, when I learn a subject, it may surprise you to know that whenever I encounter a new subject and I want to go deep into it, I read books in a peculiar way. I go and read the preface of the book. I might read the introductory chapter if it gives a historic background. And then guess what I do? I look for the glossary and at the end of the book or in the beginning wherever it is and I go and look for the bibliography the references because becoming familiar with the references gives you a very high level view of the subject right it is always good to read a book in conjunction with other books next to it. Because some things are well explained in one place, some in the other. I highly encourage you to get some of these books. At this moment, I've asked you guys to get only three textbooks. One is the theory textbook, the yellow jacketed book, the Springer book, ISLR. The second is the R book and the third is the pandas book that should be sufficient but think of them as a core text book right but that should not dissuade you from reading a lot of other books and other websites so this is it guys this is a summary of what I have to say about the notebook now I'll quickly come to our reviewing our previous lecture I'll give no more than five minutes to that let me know guys when you see this screen the writing screen are we guys seeing the writing screen yes we can so this is a refresher of what we did last time we did multilinear regression what is it what are the factors that affected what is multicollinearity what are outliers what are the Gauss Markov assumptions and Gauss Markov what is Gauss Markov theorem we had a little quiz by the way in the quiz there were a couple of sort of a poorly worded question. In fact there was a missing and crucial not missing so that actually the answers were hard to give because so we have fixed that. If you go back and look at the question you can see the fix and I apologize for that. but I hope that quiz was of some use to you then classification is something we'll talk about but multilinear regression what is it when you can write a equation or in terms of multiple features when you do that geometrically it represents a hyper plane a plane in two domain like with two features it becomes a plane in three dimension space otherwise it is a hyperplane a generalization of a plane to higher dimensions is called a hyperplane the gauss-markov assumption what is it random means that your sample should be a random and true reflection of reality linearity means your hypothesis is actually a linear relationship like this. Y is beta naught, beta one, x is zero. The next assumption is the normality of errors. Your errors at the end of the model should show a normal or a bell curve distribution. The next is homoscedasticity actually i believe i may have written the spelling wrong it is homoscedasticity and heteroscedasticity so these words are misspelled here let me fix it homoscedasticity Let me fix it. A more skedasticity and heteroskedasticity. This sort of reminds me of the story of Mary Portman. She taught the children to say a very long word that gave them a lot of confidence. Anybody remembers what that kid's story was and what the word was? You know, supercalifragilisticexpialidocious. I don't know if you have small children who remember that. So it's somewhat like that. I always say that in machine learning, when you sometimes you, it's a little bit of a fun to know that, to actually know the meaning of such cryptic words, right, big words. So well, there we go. So homoscedasticity is when the variance of the error doesn't change with y, the variance of the error doesn't change with Y, the target variable, does not depend on the target variable. Heteroscedasticity is when the variance of the error does depend on Y, right? You see this funneling effect. Whenever you see this funneling effect, you know that you're looking at heteroscedasticity. know that you're looking at heteroscedasticity. I always mispronounce it as heteroscedasticity. I apologize for that. And then what is multicollinearity? Multicollinearity, would somebody like to volunteer? What is multicollinearity? Multiple predictors being related to each other. Yeah, highly correlated to each other. Or one predictor could be expressed as a linear combination of other predictors. It's highly correlated with one or more predictors. So, it leads to problems. In other words, the problem that it leads to geometrically is it makes gradient descent. It makes the gradient descent go into a func. And so you end up with a very unstable solutions. I wouldn't go into all the details of it, but that's fine. Then, if we talk about interaction terms, you get interaction terms that are mixing and multiplying the features together. Polynomial terms. We talked about outliers. What are outliers? In one dimension it's much easier to define. There's the Cheeky's definition amongst others which just says that beyond two standard deviations it's soft outliers. Beyond three standard deviations it is a hard outlier. it's soft outliers beyond three standard deviations it is a hard outlier which is easy to do in one dimension but in higher dimensions it's a little hard outliers are data points well let's begin which stand out from the rest of the data and there's a whole body of literature and research and techniques algorithms if you say of that help you find outliers in higher dimension data right in two dimensions you can still look at it or in three dimensions and visualize the outliers in higher dimensions it's not so easy so there's a whole set of techniques that you can use to find the outliers and remove them then you also have set of techniques that you can use to find the outliers and remove them. Then you also have points of high leverage that can screw your model. What are points of high leverage would somebody like to enlighten us? Where value of predictor is very high, I think unusually high. No, not value. Anyone else? It's an observation within the already existing set of observations kind of hiding within them which unusually affects the model outcome. Yeah, very good. So, basically it's an insidious data point it is hiding in there looking like normal it doesn't look like an outlier outlier standard stand aside you can spot them but the high leverage points you can't spot they're insidious data points hiding in there but if you remove that data point from your sample while training you get one model if you include them you get a completely different model right so they hijack your model building process it is important to find points of high leverage and exclude them from your model building exercise so that is it so this is a summary guys of what we did last in regression I will close that book on regression the rest of the regression will learn through labs next time when we do the labs will have a lot to learn and it will give us a very good review of everything we learned but today is a theory day I will now take a five minutes break and we will start with a completely new topic classifications and classifiers. But before we take a break, any questions, guys? So far we have just reviewed what we did last time and in the extra session. All right, if not, let's take a really quick five minutes break and no more than that. Let's regroup in five minutes and we'll start. Can I just ask a quick question? Yes, please do go ahead. So I have three textbooks. One was the ISLR book. The second one is the modern data science with r yes bomber and then i bought the third one which is the art of our programming is that though i think the third one you mentioned was pandas pandas yeah the art of our programming is a good book we can use it but the one that i would highly recommend is now that you have the modern data science with r for python also So get the Pandas book. Okay, that's fine. I had already ordered that book. It'll be here on the 18th. So that's fine. No problem. And then there is another book, which was by the O'Reilly book on Skicket Learn and TensorFlow. Okay. So you're recommending to get that one as well? Yes, very much. So guys, it looks as though I'm recommending a lot of books. Here is a way to approach it. Always first read ISLR because nowhere are things so clearly explained as in ISLR. It will get your foundations right. Treat these rest of the books as practitioners book. What will happen is today these libraries are there, Skicket Learn is there, TensorFlow is there. Tomorrow these libraries won't be there, some other libraries will supersede them. So the rest of the textbooks will become obsolete. But the one book that will not be so obsolete, in fact, will still be shining, would be the theoretical book, the ISLR, because math changes very slowly, you add to shining, would be the theoretical book, the ISLR, because math changes very slowly. You add to it, it never gets obsolete. So start with the ISLR, make it your root book. It's only 400 pages, guys, out of which we'll do only 350 pages. The other 50 pages refer to things that actually are not commonly used. We won't do that. 350 pages, but these 350 pages pages if you can get it into your mind absolutely clear you will have rock solid foundations in the school there are not that many people who know it or have studied it so thoroughly that they remember everything that is there in the book if you can you have a winner treat the other books as helpful guides while doing the lab that is their purpose you won't learn theory from them you will learn the specifics of libraries the R library the R language the Python library is cricket library sorry and the TensorFlow library and the pandas library so those are library coding related books. Treat them as references and use them as such. They're excellent books, but they're also the sort of knowledge that gets dated very quickly. So for example, we are talking about Scycid Learn and TensorFlow. Scycid Learn is still going very strong. TensorFlow, well, the world has surprisingly gotten quite divided. TensorFlow was released just a few years ago. It burst upon the scene, became the runaway success. And then already now there is a contention because there is a PyTorch. And most people are finding, researchers are finding that PyTorch is much easier, much more straightforward. And in fact, there is a huge migration away from TensorFlow to PyTorch these days. So you don't know, libraries keep changing all the time. Maybe TensorFlow will be there 10 years from now as the leading library, maybe not. Maybe PyTorch will be the leading library or maybe something completely different will be the leading library. Likewise with Skagit Learn and Python and uh r and who knows maybe julia will take over all of these but while all these libraries come and go the one thing that will not change is the foundations of the subject for which you should refer to the yellow jacket at two other things have a shelf life they are of practical value any other questions guys so I'm going to pause the recording let's take actually it's 816 let's take a 15-minute break and it's regroup at 830. All right guys, we are on the record. Somebody had a question before I start. Otherwise, I'll get started. See, so far, we solved this problem. Regression was, simply put, you have a box that X feature vector goes in, XI, and what comes out is a YI hat, which belongs to the set of real numbers. A number comes on. This is a regressor. You use the word regressress model or something like that. Right? Now, somewhat similar boxes this, the x vector goes in, features go in, whatever features there are and what comes out is again a label of why if we call an output, but that output belongs to a Category an Example of that would be easy How's or ducks how are that you're trying to identify? Is a data instance belonging to a cow or is it a duck where this could be let us say the weight of the of the animal the size of the animal the beak whether a beak is present or not and whether webbed feet is present or not webbed feet horns etc etc right So this sort of exercise is a classifier. And this whole process is called classification. It is as simple as that. So now we will start studying the classification process or the classifiers. We'll start with the simplest classifier today. It is not just the simplest but also one of the most powerful and most often used classifiers. It is called the logistic regression. Now the name is misleading because the name has the word regression in it but it is actually a classifier but before I go into that let us clarify the meaning of the classifier word a little bit more the quintessential example regression that I gave you is an interpreter trying to sell ice cream on a beach and trying to decide how the relationship between the amount of ice cream he'll sell in his to temperature of wind speed and so forth in a similar vein a quintessential example I can get to a classifier is that of right who you have taken or some children whom you have taken to a special kind of a metal imagine a very simplified meadow in which there only two kinds of animals roaming around either there is a duck or there is a cow and then suppose you are looking at a animal and you take your child let's see look at that animal it's a small animal it has it has feathers it has webbed feet it has a beak that's a duck then a little while later you encounter a cow and then you point out to that cow and say, look here my child, you see those big horns and you see that big animal with poor feet, right? And a swishy tail and so forth. And what you're looking at is a cow. So now in the beginning, let's say the children are all lost, right? They can't make head and tail of what we just said because we gave, we did what would be called adult talk a lot of information and suppose these children are two year old they are not getting it right or one year old or two years they're not quite getting what you are saying so now you point out to a random animal and you ask is it a cow or a duck let's say that you point to a cow new cow you encounter you ask is this a cow or a duck now those children if they have absolutely no idea whether it's a cow or duck what will they do they will just guess so half of them will guess it's a cow half of them will guess it's a duck isn't it or the same child you keep on showing it animal after animal it will just randomly say cow or duck and it will have a pretty high error rate then gradually the child begins to form a notion and all these children begin to get the concept of a cow and the concept of a duck they have some internal hypothesis in their mind so when one of them is that the smallest duck biggest cow right haunts his cow webbed feet is duck beak is duck and so forth so now there is a formation of here or a there's a conception of an animal right you're beginning to distinguish between a cow and a duck because you have a concept of a cow and you have a concept of a duck, right? So once you form this concept, you're essentially forming a model or a hypothesis in your mind. And then you ask this child, is this a cow or a duck? The child will likely make less mistakes, may not get it perfect, but as the concept of cow and duck gets more and more refined, more and more accurate, more and more precise, the error rate will go down, the prediction or the identification error rates will go down. So that is the nature of classification, or this whole process is classification. What the child is doing is classifying the animals as cow and duck. And inbuilt to the notion of classification is the same concept of generalization that you saw with regression. In regression, we notice that you might have a temperature on a day which is not exactly there in your training data set. And yet your model is able to predict the sale of ice cream at that temperature, which the model has not seen so far. It is, in other words, able to generalize from the specific training instances to the concept of a hypothesis about the relationship. In the same way, in a classifier, you see that the classifier has learned when it can generalize beyond the training data and it can make prediction for an animal whose weight and size and things may be different or may not exactly match any of the data instances you have encountered before. But if you can make predictions for that animal correctly, if you can identify that animal correctly, you say that you have a good classifier now because you have a model that can correctly classify. So far so good guys. So in classification when you predict it is not so much a prediction, it is more an identification, your best guess of what it is. You're not measuring, you're identifying is it a cow or a duck or maybe the meadow has three animals, cow, duck, horse. You're trying to now identify one of the three animals. The distinguishing feature is that in classification the target variable is categorical. Means the output belongs to a finite set of possible outcomes. The outcome can be or the prediction can be or the identification can be an animal but you cannot say some animal which is not there in the training data so for example there are no penguins in the training data there is no way you can expect a classifier to all of a sudden call an animal a penguin and your data your subsequent data shouldn't contain a penguin. So you have to define a finite set, a finite set of outcomes or objects, and the classifier will identify one of those objects. Those objects are typically called classes, because it captures that notion. There are many, many ducks in the world, but they all belong to the class of ducks, class of animals called ducks. Likewise, there are many, many cows in the world. They all belong to the class of ducks class of animals called ducks likewise there are many many cows in the world they all belong to the class of animals called cows so these are called classes and we use the word category category is a finite set of classes example a category is category of animals on a meadow And they seem to have a medal. Or if you want to extend it, let's make it three animals just so that we don't stop at two. When we do that, this is your category this is your category this set of three possible classes and you say that these are these the classes so this is the sort of the vocabulary that we use how is the class duck is a class horse is a class, duck is a class, horse is a class. And the set of all of these is the category. And so the classifier will take every feature vector, in other words, a data containing the weight of an animal, the size of an animal, whether or not it has beaks and webbed feet and swishy tail and horns and then it will be able to identify which of the animals it is. Are we together guys? Now do we understand the concept of a classifier? So let me ask a quiz question just to make sure that we understood. Suppose you want to make a prediction machine that can tell based on some factors. Let's say how much traffic you're seeing outside a shop on a given day, right? And how much the items are on for sale, what percentage discount you have on the items and so forth. Suppose you are trying to predict the amount of revenue that the shop will earn in a given day. Is that an exercise of regression or classification? Regression. is that an exercise of regression or classification regression regression and why is it regression we use it continuous data can't come to predict an output yeah we are predicting a number I'm measuring a number yeah some measurable thing the the revenue. On the other hand, suppose I ask you to do this. I give you wine. Now, it turns out that wine can come from two kinds of barrels, some good barrels that were well-preserved, and some barrels which for some reason have developed problems, and the wine has gone bad. You are allowed to take drops of wine and measure some chemical attributes. Based on the chemical attributes, you need to tell whether the wine is good or bad. Is it still good or has it gone bad? So a prediction machine, an algorithm that can take the feature vector containing the chemical attributes of the wine sample and tell whether it's good or bad wine would you call it a classifier or a regressor classifier and why do you call it a classifier? Because you'll get two different outputs based on multiple inputs. Yes. Maybe more than two, but it's not something we are not predicting a number. We are just predicting yes, no, yes, no. Yes. We are identifying the type of wine. When you identify things, classification, it is not a measurable thing, good or bad, at least the way the question was posed. A wine can either be good or bad. you hold it up to the light and it will have qualities like it will look clear and so forth and if it is bad wine it will look terribly sort of foggy but you you are not looking it up to the light you're just trying to determine is it good wine of that that's another thing another example is suppose you are looking at it's a common thing we do you know the pregnancy test kits you're dipping urine which looks at I presume some chemical chemicals present in the urine and it determines whether a person is pregnant not so is that device a classifier or a regressor I I would say it's a regressor that's the classifier no but it but it's it's predicting one output right so yeah we assign a reason yeah so when it predicts a binary number what is it is it a classifier or a let's see binary is this or that like a spam or not like yes that's the binary or not it is one or the other outcome right you're identifying one of the outcomes so is it a classifier or a regression? If you're predicting binary, I would say classifier, but. Yes, it is a classifier because it says only two outcomes, right? It is not predicting. I did not pose the question as saying, predict the degree of pregnancy, if there is such a thing. I mean, generally, when predicted degree of pregnancy if there is such a thing I mean generally when we talk of pregnancy either a person is or is not pregnant so I have a question on that because I someone can correct me if I'm wrong but I thought the idea was it's based on on a threshold value right if it's measuring some hormonal balance, right? So if your hormonal balance is high by some value, therefore you're pregnant. If you're lower than that value, then you're not, which is where they fail. So that's where I'm unclear. Is it binary or is it by threshold value? It's actually a very good question. So I'll just repeat what Irfan said he's saying that see internally that device must be measuring the levels or the concentrations of certain hormones right now I'm not a medical person I don't know which hormones it would be maybe progesterone or whatever it is I really don't know I'm is. But whatever it is, you look at the concentration of that. And therefore, because you're measuring the concentration, it looks like regression. And you will encounter situations like this. But you have to ask yourself, what is the outcome? Whatever it does internally, what is it producing as an outcome? It's producing an answer which is pregnant or not pregnant. In other words in in US it is customary I believe for these pregnancy kids to have one bar or two bar or something like that. So the question is is it one bar or two bar or something. So it can be either one or the other so therefore because the outcomes are finite and you are picking from the two outcomes it is actually classification though internally it may be measuring the measuring actually the concentration so when you decide whether something is regression or classification you actually don't look at the internal mechanics. You look at the outcome and you ask from an outcome perspective, is the outcome a real valued number or is it a class within a finite category? So just to further that point, suppose you are looking for a value. So if you're saying, I don't know, maybe you're supposed to return a value of 50. When does at any point does this problem become a regression problem or is it always going to be classification? No, you pose it as a classification problem. So any regression problem, if you attach a cutoff, becomes a classification problem, isn't it? So, for example, you can take the problem of sale of ice cream on a beach and I could pose the problem like this did you sell at least 30 gallons of ice cream or not right so now what will happen you predict how much ice cream you will sell and then you ask yourself, my threshold is 30. Is it exceeding 30 or not? If it is exceeding 30, the answer is yes. If it is not exceeding 30, you say my answer is no. So you just converted a regression problem to a classification problem by applying a threshold. Is that becoming a little more clear now? So, this is Balaji. I think the main doubt is regarding the optimization techniques. One is for each continuous value for which you want to hit the right number that is regression and here you want to be correct between the classes and the optimization techniques uh i don't know you will introduce would be quite different probably that is where the difference is lying i guess actually the optimization techniques would be surprisingly similar. There are differences. The last one is not some squared error anymore. It will be something called cross entropy loss, but we'll come to that gradually. But think of it like that. What is your end goal? To measure or to predict one of the few types. to measure or to predict one of the few types. If you have to pick one from a category, then it is classification. If it is to measure, then it is regression. So let's take a few more examples before we continue. Suppose I were to ask you this question. Using all the parameters and all the turbulence that we are going through in the economic landscape what will the do Jones Industrial Average look like in July what will be its value like can you bring come again 22,000 thousand. Twenty two thousand. Yeah. So what are you predicting? Twenty two thousand is a number, isn't it? Yes, sir. Therefore, this this process is regression or classification. Regression is exactly so. So I said on the on the same token, right? So, so I said on the on the same token, right? If we were to flip the question as to in July, whether Dow Jones will be up or down, that becomes a classification problem. Then write the same same calculation, but a different outcome. Exactly. It becomes a classification problem. So this is this is where generally people who learn the subject get confused that any regression problem can be made into a classification problem by simply putting up right putting a threshold and above the second we can call it one thing below the threshold you can call it an or variable so we can change it to categorical variable and that's it by putting a threshold yes but the other way is not always true so for example if you are to predict whether your apples based on all the attributes will be a green apple or a red apple when it is right now generally green and red apples are entirely different it will be either one or the other there is no I don't usually see apples which are halfway green and red isn't it so it is not always true that it is a measurable and you put a cut off apples are. You're identifying the species of apple, right? Or let us say that I give you a dog in a box. You're not allowed to see the dog. The dog barks. You can take the weight of the dog in the bag, in the box. You can hear the bark you can see how much food the dog eats and you have to tell which species of dog it is and let us say that they I'm giving you only three choices it can either be a golden retriever which is what I have and let's say the other choices it could be a poodle or it could be a chihuahua and now you have to choose between a chihuahua, a poodle and a golden retriever based on some features like the amount of food the dog is eating, the bark sound, the weight of the animal and so forth. So now you realize that you're identifying one of the types. This is a problem of what is it classification or regression classification the telltale sign is that I said outcome has to come from one a finite set chiwawa poodle or golden retriever and I was reminded of this example because I can hear my dog barking in the background. It's a golden. So things like that. I hope I'm making it clear what is classification and what is regression. Another example, and we will deal with this example in this workshop, is the breast cancer dataset. Suppose you're trying to determine whether a person has breast cancer or not. Now the history of this is breast cancer is actually one of the scariest things for women. It is somewhat, it's very common as cancers go. It is not at all a pleasant thing is very so the test for it used to be invasive you would go cut open surgically take a piece of the tissue put it under the microscope and do all sorts of tests and come back and tell you the person has it on so now over the years people have come up with the progressive set of tests you don't do one test. The first thing you do is you just do a mammogram. If everything looks fine, you say, all right, the second thing is if something looks suspicious, at least in many countries, if something looks suspicious, you would do what is called a fine needle aspiration. Go to the thin needle, take out, aspirate out a little bit of a tissue typically from the lymph nodes or something and then you would put it under the microscope and you would observe whether the person has cancer if it even if there's a small chance that the person has cancer then you would go and do perhaps more invasive things you would do a core tissue biopsy in other words now you would go and take a core of the tissue out, a pretty good section of the tissue out, and then observe that under the microscope and so forth. So there are layers. You try to start with the simplest and most non-invasive. If suspicion develops, you get progressively more invasive. So we will deal with one particular one called fine-need needle aspiration for breast cancer this work was done actually a while ago again in the 90s and the data set is very instructive all sorts of measurements features about the cells that you are seeing under the microscope their diameter their shape concavity and whatnot, many many attributes, about 13 to 17 attributes you look at. Based on that you have to come to one conclusion, the person does or does not seem to have breast cancer. So is this a question, is this a problem of classification or regression? problem of classification or regression anyone what is it classification why because the possible outcomes are does have breast cancer does not have breast cancer yes and no yes that is it now let's take another example in the last example now we dealt with the california housing data the target variable suppose we take is the median house price in a given block city blocker so you have to predict based on all sorts of attributes i believe believe we had nine attributes, based on the nine attributes such as longitude, latitude, proximity to the sea, population density, and so forth, and the income in that neighborhood and so forth, median income in that neighborhood, total number of bedrooms, total number of rooms. total number of bedrooms, total number of rooms. All of these are what features about houses on a block. And you have to predict the median house value. A median house value could be, let's say, three, $400,000, 700,000. so was it by okay all right so um thank you for muting so this is an example of classification or regression what is the target period it is you have to predict the median how house value is it a real is it a number or is it a type is it a class to doctor the number it's a number and therefore a prediction engine that can do this would be called a what a regressor or a classifier. Regressor. Right and so we will do this exercise actually as a regression exercise so make sure guys that you have done the California dataset the whole example you have followed through because the next step is to do regression of the data set. And I'm about to add the exploratory analysis of breast cancer data also, because we're going to do classification of the breast cancer data. So these things will become very real in your lab. So now that we have understood classification versus regression let me take a very toy example so asif is there a use case where we use both of them together or regression as well as classification instead of or it is end oh many such situations for example do you have diabetes or which category would you fall in? Not diabetic, pre-diabetic and truly diabetic, right? Based on a whole set of markers. Now that is a classification problem. To have that, the next thing that becomes important is can I predict the severity of your diabetes and usually that is measured by something called a hemoglobin a1c based on your features can you predict what your a1c is and this has become particularly relevant in view of a recent and a remarkable discovery that was made I believe just last year. So let me talk a little bit about the discovery since you asked this question. There is such a thing as an opto map. Some of you who went to your ophthalmologist must have gotten your picture taken of your eyes inside of your eyes. It's called an opto map. Do we all know those big round maps of the eye? It's called an opto map. Do we all know those big round maps of the eye? So in the opto map you see blood vessels, capillaries and things like that. Now usually the opto map is of value only to the ophthalmologist to check if your eyes are okay. But one of the most recent and startling breakthroughs that have happened in artificial intelligence and machine learning is that somebody decided to see whether it can predict more than just eye condition. And to their great surprise, it could fairly accurately tell your blood pressure. It could fairly accurately tell your A1C, hemoglobin A1C level, the degree of diabetes that you have. So now you can take the OptoMap and you can use it either as classification, you can break people up into categories of non-diabetic, the pre-diabetic and diabetic, and you can also treat it as a regression problem and i guess the a1c hemoglobin a1c or the degree or severity of the diabetes that is quite remarkable do you see how you can take data and use it for different purposes Yes. So that is one example that you can do. So anyway, all of this will become much more real when I take this toy example now. So I'll take two toy examples. One, we'll come back to our cows and ducks. Our cows and ducks are very illustrative let's take this cows and ducks so suppose I take only two attributes volume of the animal or size of the animal and weight of the animal so if you look at the weight of the cows and ducks which and maybe horses also in the neck would you you agree that the ducks, what are ducks? Ducks are typically white. So they would be somewhere around this. On this weight versus volume coordinate axis, they would be somewhere around this. And cows should be, well, there are no red cows around, are there? Well, some of them can be somewhat reddish okay would you agree that they are somewhat like this would you guys agree that the data would look something like this and it ducks would look something like this can you guys tell in see my writing? Yes sir. And the cows are in which color? The data points for cows? Red. Red. And the ducks are there. And while we are at it, let's throw in the, the, should we throw in the horses too I don't know horses we need to give them a color well for lack of a better option I will throw in blue now the horses tend to be not so big but they are certainly heavy so let me just say that they are like that these are not up to scale and not very realistic. But let us assume that this happens to be something, a somewhat rough approximation to truth. Are we together, guys? And now we need to determine whether an animal is a cow or a duck. Or a horse. Let me mark it as a horse so how would you do that question yeah it's quite interesting so to answer this question first actually let me do one thing let me simplify the problem and let's take away the horses we go to the middle and we ask and we shoo away all the horses from the middle so now you have a meadow which has only cows index and let's try to solve the simpler problem so if you make the frequency plot of this along the weight axis would you agree that the ducks their histogram or the frequency plot would look like this the weight of the cows this will be the frequency plot for the cow cow histogram this would be the duck histogram. And along this axis, it would be like this and along this axis, it would be like this, something like this, guys. Does it make sense? let me make it like that do you see what what these curves are blue curves are these are just frequency plots i've made it upside down it shouldn't be upside down but it's just because i didn't want to override the data themselves but in the margins i've drawn the histograms the frequency plots i hope the frequency plots are visible to you guys does it need interpretation guys it's just a plot so the frequency is the number of cows in a given point yeah Yeah, yeah. So you can see at the center, right, there are a lot of cows which have mass around the center. And to make this thing obvious, let me put lots and lots of cows here. A lot of concentration of cows, right? Something like this. So this is the frequency of cows by the weight and frequency of cows by volume. the frequency of cows by the weight and frequency of cows by volume so if you look at only one axis let's just look at one axis in this axis you have the ducks and then you have the far away you have the cows uh sorry you have the cows and let's say that this axis is what would you like to have weight or a one doesn't matter let me just take size so you really the decks are small in size and cows are rather big in size. So how would you write a classifier that distinguishes between a cow and a duck? You can say well that is actually it couldn't be easier. What I could do is simply take do this find the center of the ducks you know this peak of the duck or find the peak of the cows, and these two lines are there at the peak. This is where their frequency plots achieve their maxima, the bell curves achieve the maxima. And what can I do? I can then take this thing there. Let me. What I can do is I can find this distance and take the perpendicular bisector of this. What is the middle point of this? This. And I can call this a big, actually let me give it a nice big color. What should I call this? Your boundary. Let me give it a nice big color. What color can I give? All colors. I've already used up yellow. I have used up blue. Let me use orange. So this thing, oh, no, it didn't become orange that's okay suppose this line suppose I put this line and I say everything on this side will be a duck on this side will be a cow anything so this is you this is your cutoff point along the size axis. Do you think this will work for us? What do you think? Will this classification work? No. Why not? Sure. You didn't take the standard deviation into consideration. All right. We can do that. In other words, what you're saying is the the the spread of this will be a Sigma 1 and Sigma duck and Sigma curve they will be different all right we will take that but generally speaking so okay first of all just assume that the standard deviations are the same to the first approximation don't worry about this time so now that the standard deviations are the same to the first approximation. Don't worry about the standard deviation. So now suppose the standard deviations or the spreads are the same. In other words this hill looks exactly like this hill, which it doesn't. But we'll sort of squint our eyes and not look too carefully and pretend as though they are more or less identical but they're just positioned at different places on the axis along the size axis then would it be reasonable to take the perpendicular bisector of their peaks this point and on this side it will be duck on this side it will be calf guys let's say that the ducks are approximately 20 pounds plus minus some, actually 10 pounds plus minus some, and the cows are like 500 pounds plus minus some. So would it be reasonable to pick a cut of something like 250 pounds and say, or 200 pounds and say, well, you know what, above 200, it must be a cow below 200, it must be a duck, something like that. We may be occasionally wrong, there might be a cow that is probably less than 200 pounds or something like that. But it seems a fairly good approximation. Anybody is convinced of that? Especially if we assume. Absolutely, yes. Yeah, it is right. Yeah. So there is actually a name for this algorithm. To do so, it is called, and we did it in one dimension. dimension the name for this algorithm is linear regression discreet me meant analysis so in linear discriminant analysis we pretend as though these two bell curves or these two frequency plots they are identical the only thing different is where is it located along the axis so mu of duck the mu of cow and so you say that the cutoff is mu cutoff is roughly speaking a mu of cow plus mu of duck divided by two in other words is the midpoint between the two and so if your data point has size less than cut off you would say it's a cow it's a duck if it is more you say it's a cow so that is in one dimension now look at this data it is in two dimension we could have played this game with respect to both the weight also in a similar way. So when we generalize it to two dimensions, all it means is you take the center here of the ducks and the center of the cows. center of the house and join it with a dotted line once you have joined it with a dotted line then your bisector of it this is your separator this line or this thing is your separator. This line or this thing is your separator. Any point on this side of it is a cow. I'm sorry, is a duck. And any point in the feature space on this side is a cow. And there is a nice, lovely word that people use for this. that people use for this this is called a decision boundary in classification a decision boundary right and here in one dimension this is what's your closet stupid this is the decision boundary. So in one dimension, a decision boundary is a point. In two dimension, it is a line. So can we generalize, guys? In three dimension, what will the decision boundary look like? Plane. Plane. It will look like a plane. So this particular algorithm is actually the simplest one you can think of. And it was interesting. It was discovered, if I'm right, by the great statistician Fisher, who wrote the paper called Discriminant Analysis, literally. And the idea is very simple. Do you see how simple the idea is very simple do you see how simple the idea is so this is one of the classifiers that is mentioned in your chapter forward and you should review it I say one question here so why do you say that the both of the distribution should be identical of the duck and the cows is that is that a requirement see to have a linear discriminant analysis to use this concept of a straight line boundary a hyperplane boundary plane boundary that is a requirement now in practice what happens is they won't be the same example cows and ducks their distributions are certainly not the same ducks are what 10 of in weight they will be 10 pounds plus minus maybe six pounds right on the other hand a cow will be 500 pounds plus minus 200 pounds so the variance of the cow will be much bigger than the variance of a duck but it turns out that see when you build a model you make a lot of simplifying assumptions one of the simplifying assumptions you make with linear discriminant analysis is that the two assumptions you make one is that the instances of one class are all clustered in one place of the feature space like you know one region of the feature space and the points of the or the instances of the other class are again clustered in a different part of the feature space that is the first assumption right or might as well write those assumptions let me do that so that it becomes yeah assumptions and by the way these assumptions need not be true it is a hypothesis that you are making about the world about the data that may not be true each class instances are clustered around a different in the future space number these clusters are approximately at least a bell hill a bell hill you know like a bell curve generalization or a normal or more formally or more formally normal or gaussian people use the word gaussian or Gaussian distribution. In simple terms it looks like a bell hill. A third assumption for linear linear discriminant analysis LDA or linear is that the hills similar i.e. share the same variance. Variance is the spread. At least approximately. and the fourth is what I'm saying if linear if Bell is have not if if the bell hills really do not look alike, do not look alike, then there is a bit more Quadratic discriminant analysis. Are we together, guys? If the bell hills really don't look alike, then we have another process called quadratic discriminant analysis, which is a little bit more complicated it is not as simple as taking the perpendicular bisector of the two hill tops but you need a little bit more complication to find the decision mark. So I leave it as that. Another thing is is this always true? No it is not always true this brings us to an important point about machine learning see every algorithm it assumes a worldview it assumes that the data looks like this free lunch yes no free lunch theorem exactly but if the underlying data if the ground truth does not match with the assumption and there's no reason it should sometimes it will and sometimes it won't if it does match your assumptions even approximately you will get a reasonably good model good predictive model on the other hand the underlying does not match your assumptions at all. It's completely different. Then your model will perform quite badly. And you can never tell. You can never tell whether an algorithm will or will not work for this data set, because unless you already know something about the data, since But in the absence of any prior knowledge, any one algorithm could be the right algorithm because each algorithm has a different worldview of how the dataset is represented in the feature space. Are we together guys? So I will just stop at linear discriminant analysis for now. Notice how it sort of draws a perpendicular bisector a line or a hyperplane plane or hyperplane by connecting the hilltops of the two classes are we together now the question is what will happen if I bring in the horses also into the middle now life gets very little bit more complicated so let's try that example what if there are horses too that's the question given us Right. So now category C is cow, duck, red, isn't it? Where is the red? laser first, I don't know what else to show. And I'm making the cows now in the feature space. These are my cows. And let me mark horses with green. Let's say that the horses are here. They're rather heavy, but they're not as big perhaps as cows. And so, I'll mark them here. These are all illustrative. I don't think any biologists out here, or zoologists, you may perhaps laugh at these values. I don't know how accurate these are, but we learn from this. So suppose this is the data, and this is again your size this axis is size again using the white marker this is size and this is weight right let me call it x1 x2 right this is it and so your feature space is spaces is basically art and your target is your category is a cow, duck and horse so how would you do that the way it turns out is not that hard it's easy to generalize once again you find the center the hilltops and what you do is you make dotted lines now you have a triangle do you guys see the triangle here guys do you guys see the triangle here yes yes yes yes what color is the triangle green screen is you find the perpendicular bisector of the triangle so the perpendicular bisector of this they will all meet at certain point it will be uh i don't know sort of uh yeah i don't know i'll just cheat i don't know how accurate it will be I don't know sort of yeah I don't know I'll just cheat I don't know how accurate it will be roughly speaking I'm pretty sure I didn't get it at 90 degrees here but close enough here I certainly didn't get it at 90 so I didn't place it at exactly the right place. Maybe it should have been somewhere here. Sorry. And maybe better like this. This to me perhaps looks a bit more accurate. May not be. Okay. But you all know that in a triangle, if you do the perpendicular bisectors, they will all meet at a point. In fact, it will be true for any polygon, right? So here we have. And so what you do is it divides the space into three regions. The region, this region region let us give this region to the cow this I mean sorry not come this region belongs to the ducks this region which I will call like this let it be belong to the horse and this region of the feature space belongs to the cow so if I get any data instance let us let us get arbitrary data instance and let's pick another color for that suppose I take this if I get this point here what would you call this point is this a cow or a duck or a horse is is this point a cow duck or a horse where a model oh why is it why is it because it belongs to the cow yes it belongs to the feature space we have marked as a cow. Likewise, the point here would be a horse and the point on this side would be in the lower left hand side would be a duck. And on the upper side, upper left hand side would be a horse. So what you do is you partition the feature space into the classes. Let me write it down. A classifier, this is an interesting observation, a classifier partitions the feature space into disjoint regions with one region belonging to one of the classes are we together now the number of partitions will be equal to what number of classes the number of your classes at least it could be more but okay in this particular algorithm it would be the same as the number of classes so as did we understand the linear discriminant analysis is it simple guys let us recapitulate all the ideas we are saying that suppose we are dealing with a thousand ducks and suppose the underlying assumption is that they form clusters a bell hill shaped clusters then all we need to do is find the hilltops and connect them and find the perpendicular bisector. That could be a good decision. The underlying assumptions is that both the bell hills look more or less identical, but they are placed at different places in the features space. That's our decision boundary. And decision boundary, the word is very evocative because it says it helps you decide whether something is a cow or a duck. That's why the word decision boundary, or in the case of three classes, whether something is a cow, duck, or a horse. And so the decision boundary is that line, which is the sum putting together the three perpendicular bisectors okay so that's that this is now there is one more complication there is something called quadratic discriminant analysis qda QDA, I will just mention briefly because today is the first day, later on we'll talk about it more perhaps. In QDA, the only thing that you relax is that the variance of cow is not equal to the variance of that. In other words, the Bell Hills don't look alike, Bell Hills do not look alike. In that case, what happens is, I won't go into the theory, but much of it remains the same, except that the when you have the cows, let's say the cows. That's right, the ducks and the cows. Let's bring back our cows quickly and I'll just be brief about it. For reasons that are slightly mathematical, what happens is that your decision boundary becomes sort of quadratic, I believe in this particular case. Let us just make it like Do you notice that this is curved? Actually, I usually use now I apologize I usually use green for this I wish there was a way to substitute the color or mean of making a mistake. So this color or mean of making mistake. So this again the same thing this is for your future nodes. Decision boundary is a quadratic curve or surface. When we do the lab this will all become very clear. But do you see the bend? It's not a straight line anymore. Are we together, guys? So this is the distinction between linear discriminant analysis and quadratic discriminant analysis. So you have encountered one form of classifier. Now I'm going to stop here today guys because we have learned new concepts and it will take you some time to absorb this concept. I would recommend that you read chapter four and only read the discriminant analysis section. Don't read the logistic regression. You're welcome to read it but it might be a bit hard. Go read the discriminant analysis sections and see now if it makes sense or if it is easy to understand in view of the discussions we had today. So now I'll stop. I'd like to give the last 20 minutes to question and answer. The one algorithm we didn't cover is the one that I thought I would start out with but we'll do it next time because it needs a whole session of its own. It is called logistic regression and we'll keep a whole session dedicated that could be next time any questions guys no questions is it clear to you guys is all of this becoming clear is it simple to understand yes sir as if sir I have a question so I when I read the book I think I understood but if you could please explain one more time that might help okay the discriminant analysis and come down no not the not the three percent like the wording why it is calling a discriminant analysis oh it's very simple because the decision boundary look at the shape of the decision boundary in when you assume that the two hills look alike the decision boundary happens to be a straight line or a hyperplane, plane, hyperplane. These are linear equations. These are linear surfaces, smooth surfaces, isn't it? Linear surfaces. And discriminant is for the different classes, right? Yeah, discriminant just comes from the word, it discriminates between a cow and a duck between the classes so discriminant is almost synonymous with classifier isn't it it's an old term for classification discrimination remember this was done in the 1920s, 1926 I believe, when Fisher did this original work. So the terminology from those years have stuck around. Thank you. Now one strength of this discriminant analysis, whether linear or quadratic is, it is easy to generalize to many classes. We went from cows, duck, and horses, three classes. We could go from cows, duck, horses, and I don't know, giraffes. It could be easy to do that and so forth. You can add as many classes as you wish and the discriminant analysis approach still works. As you will see, not all algorithms have the benefit of that. There are quite a few classifiers that can only distinguish between two categories, can have only two. For example, is it a cow or not? Is it a cow or a duck? That's all. You can't add horse to the mix easily. You can do it, but it's a sort of a different thing you have to add on top of it. So what are the assumptions of discriminant analysis? Assumption is each of the class instances are clustered around a different center in the features. That's important. That may not be true as you will see next time but for it to work for this algorithm to work that must be true. The clusters are approximately bell hills is wrong bell hills Hills. Oops, sorry. What just happened? It behaved strangely. Okay. Bell Hills, or more formally, have a normal or Gaussian distribution. A third is only for linear, the hills look alike. If the hills don't look alike, then you use quadratic distribution. So please read it in your book I'll send out these notes and if you guys don't have a question I will stop the recording was that clear guys I want to know are these ideas did you get the concept of a classifier that's all yes good yes I'm stopping the recording remember guys if you get stuck I'm available anytime reach me out in slack and in the evening times any evening I'll come and help you also there are TAs who can help you but one thing that you must do is actually do the labs in the notes. Follow those examples and repeat it. At this moment you guys have a lot of catch-up to do. As of this moment we are on chapter 4 which means the first three chapters of the book are done which means hundred pages of the book are done. We are now marching on to chapter four, review the discriminant analysis section of chapter four. And at the same time, as you do the labs, refer to those reference books.