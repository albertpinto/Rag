 and for the fun of it also put it that some of, so I'll get started. This is our first lab walkthrough and the data set that you got, the river data set, in the beginning it must have looked rather simple. There are two predictors, two creatures, x1 and x2 and there is a target variable t which is binary, it's 0 or 1., the flag-like data set is very similar, except that it's tri-valued. Zero, one, two are the three values that you see in that data. As if the screen is not shared. Yeah, at this moment I'm not, I'm talking. So, all right, maybe I should start sharing so that I don't get interrupted in the conversation. All right. So, now, because now I can draw it out and explain. So this data set, if you visualize this data set and you do exploratory analysis, which I hope that far you all reached. Did we reach that far just load the data do a summarization of the data visualization of the data you know the exploratory data analysis a check for missing values right so when you do that then when you visualize the data the first thing that surprises you is this this data has a true decision boundaries it seems not one isn't it there is a to draw it out it is the data looks like this suppose this is your data you have sort of if i remember right it is something like this like this and this is a little bit bigger i suppose and there's blue points this blue river sort of flows through that and then you have the sort of close through that and then you have the other points which are here does this look like a data date this is a data and so how do we solve this you know what are the problems with this data? The first thing we notice problematic is, the decision boundary is nonlinear. What's wrong with this decision boundary today? Boundary is nonlinear. You also notice that there seems to be two decision boundaries. Where are the two decision boundaries? Let's draw those decision boundaries out. So one decision boundary is here. Isn't it? And the other decision boundary seems to be here. So now comes the problem. How do you solve for this? If you, once you visualize the data and you think about it for some time, you know that you need something more powerful than just a linear classifier, a classifier whose decision boundary is straight even if you could somehow straighten the data there is also the fact that there are two decision boundaries so how do you conquer this so we are going to take a methodical approach through the data I hope all of you have tried if you just try so the approaches that you can try is a simply apply logistic later as it is as it is hang on so we will see whether let's see if the model is good we have a good model good have a good predictive model what do you expect do you think you'll get a good model here. What do you expect? Do you think that you'll get a good model here? If you directly apply logistic regression, remember the logistic regression is something that looks for a linear decision boundary, right? It expects something like this and this is the distance from the dx is a beta naught plus a beta dot X this is the distance from the decision boundary just going back to your basic theory of logistic regression that we did and the basic idea is that the further away you are from the decision boundary the more sure you are that it is a positive case versus a negative case. So the probability that the given point x is log 1 minus p x, right, this is equal to the distance from the decision boundary of that point. So how far it is from the decision boundary? That's an intuition intuition if it is close to the decision boundary clearly we know that it is we have a sure answer for example we used to take the example of I believe blueberries some the sorry so suppose this site is all blueberries. And what should I say? Some sort of a red berries, whatever those may be. Raspberries or something like that. Suppose those are sitting here. We do know that occasionally there are mistakes. obviously decision boundaries are never perfect data is never perfect there will be things that will creep across and so forth things like that but the further you are from the decision boundary the motion sorry the more sure you are the more sure you are that it is like this just to recap see if you are here if you are at this point hey you're pretty are you pretty sure that it's a blueberry you're pretty sure right on the other hand if you are at this point B you're not so sure because you know you can see that there are a few this red berries also here on this side of it so be you're not so sure and if you are at a point like see yeah then once again you're pretty sure that it is a red berry isn't it so we use that to motivate ourselves we said how far you are from the decision boundary determines whether it's a red berry or not. The probability of a red berry is that. So far so good, guys? was that the decision boundary was linear in the case of this it was sorry I tend to mix the colors it's a linear decision boundary but when we look up in this problem do we see a linear decision boundary we don't see a linear decision boundary we have multiple problems we seem to have a nonlinear decision boundary at the same, there seems to be Vaidhyanathan Ramamurthy, Too many decision boundaries one too many that to decision boundaries. So when you apply logistic regression. So now I'll ask this question that directly when you apply and that is what I asked you to apply. Do you think that directly applying it to the data would give good results. No, no, it wouldn't be wouldn't expect a good result. Results because a logistic regressor directly, we try to regression classifier directly, we try to find a decision boundary. And what it will eventually do is the best decision boundary it will find is way beyond the whole data set. So it will declare all the data set probably as the majority data, right? So suppose it draws a decision boundary beyond the scope of this data, then everything by default gets colored or marked or predicted as a majority now it turns out that the yellow yellow points are the are twice as many as the blue points so you will get about a 66 percent accuracy now 66 percent accuracy may make you jump into life like if you're not careful but you have to and this is one of the things we'll do we'll we will ask this question when do we have a good model are we together and do we have a good model or not so that is it so anyway moving forward the second approach we will take is we will take an intuition so let me give you the intuition suppose you take the intuition that see hey i have a way to do this i can treat this beyond the data i can pretend that this thing turns around right and this thing turns around what about now now we have a single decision boundary isn't it we have a single decision boundary isn't it if you pretend as that or hypothesize I imagine that beyond this data the decision boundary curves back and meets itself you would agree that you have one decision boundary which is a deformed circle isn't it nice are we together yes yeah so then we are only left with a problem that it is a nonlinear decision boundary given the fact that it's a nonlinear decision boundary we can say we have one decision boundary but it is nonlinear how do we solve that problem we can lean remember how do you linearize a problem one easy way is to go to a higher dimensional space. Go to polynomial degrees, right? Go to polynomial space. Go to... If you remember, I keep saying that for every data which can be solved, there is, the hope is, if it can be classified, there must exist a higher dimensional space some space in which the data is linearly separable right and the decision boundary is a hyperplane is a simple plane so that now the easiest thing you can do or the easiest way to go to higher dimensions is just to go to higher polynomials. Are we together? Higher degree polynomials of the features. So if the features are x1, x2, you can also start looking at x1 square, x2 square, x1, x2, and so on and so forth. x1, x2, and so on and so forth. Once you go to the higher degree polynomial, we may end up linearizing the problem. So if we take that, if we take that thought and then assume a bend, assume that we have a single decision boundary by imagining a bend in the data, imagining the two boundaries meeting beyond the data set, beyond the feature space. Feature space, data's, data features of space, where the data exists. I don't know beyond the bounce of the data, beyond the bounce of current data. So this part of wrapping around comes from the fact that you notice that we are claiming that these things are bending right just outside they're bending over. Right. And so if we take this, that will be our second approach, we can go to a few high degrees of polynomial and we'll see what sort of accuracy it gives. And then we can do a third approach, which is that, and which is the main approach I would like to sort of bring to bear. And the lesson here is that when you get data sets like this, let's go back to the data. When you get a data set like this, your first reaction should be, can I visualize something? Can I impose a narrative from my own experience? Just imagine something. So there are many, many things you can imagine. One easy thing that I take in this particular case is you could imagine, for example, that this is a river flowing through. The blue is the water, the the yellow is the sand right so if you think of this as a river then you would agree that you can draw a decision boundary the decision boundary that you can draw well let me take a color that is visible uh what would be that color on this background a color that's visible could be green as well green okay let's take green so we'll take green yes imagine the center of this river of this river you realize that if you think of this blue as water then it makes sense that how far you are from the river let's say that if you are this point versus this point this point a is blue B and C let's take C but they are yellow what can you tell the about the relationship of a B and C with respect to the green line with respect to the center of the river is a is nearer to it yes you can see at points that are very close to the center of the river they tend to be blue and at some point the water finishes on both sides the water finishes and then you can have all the other points which are sufficiently far from the center of the river there is no water anymore it is just sand right so let us use this intuition to build another model. And why do we do that we are doing it just to just to learn that sometimes you can feature engineer from the data by thinking about it. Vaidhyanathan Ramamurthy, This is deliberately a toy data. It's a didactic data to bring home a point that point that by playing with the data you can impose a narrative you can do some feature engineering and that feature engineering often can lead you to success so the third approach would be feature engineering so in the feature steps will be first draw the center line center line of the ripple how can we do that find fitting a line to the data usually is regression and so it's a paradoxical situation we are going to first use regression instead of classification, even though the problem is classification. Do that and be feature extract. Actually, we have used ABC. So let me use the steps as I feature extract The distance feature extract the distance from the center from the center line and then the third part is after that build a simple classifier simple logistic classifier classifier in let me call this the distance as a D in just one variable one feature right from D going to T do you see that based on how far you're from the decision boundary you can innocence tell whether it is river or sand so this is a basic intuition I would like to walk through now this in the code and so I'll switch over to the other machine any questions before we start guys does anybody have any questions I have a question yes please go ahead so it's a I mean from looking at the data when you plot it in SMS plot with a hue the ones are all falling in a sine curve but you have chosen a center line instead of a sine curve to extract the distance from it uh is it the first step towards going close to the sine curve or so the idea is when we have this curve this thing see there are two ways of doing with it you can think you can say i don't recognize a sine wave a sinusoidal function so then you go to polynomial space right or you can be smarter you can say hey you know what i can see the sine wave you remember that in ml100 we did that so if you can guess what function it is in the lucky situation then you can directly model it and in fact not in the python but in the r version you will i will show you how to and we have done this before if you remember this lever is nothing but your data set 2 from ml100 isn't it if i could extract the river and just the blue part it just becomes like a data set data set 2 which is the sine wave kind of thing so you can model it with the sine wave or you can model it with a polynomial either is good so sir i can do log transform and feature engineering but what is the best approach like you said polynomial is okay right do we have like uh uh auto model where it can recognize that you know it will automatically take it to higher transfer right see this entire course is about ways to algorithms that do the feature extraction for you right today is the in a way we i'm saying that before you get too Today is the, in a way, I'm saying that before you get too comfortable with those algorithms, I wanted to teach you the lost art of feature engineering. It's something that is very essential. See, algorithms, at the end of the day, we will pick up somebody, some researchers will do deep research to discover an algorithm. It becomes, after that that implemented software library. Then at the end of it for us to apply, as we were looking at it on Sunday, it's just a couple of lines of code. And those code now will often do a lot of things if we change the code. What happens is that you lose something along the way. Sometimes you lose interpretability, as we were talking about, and you lose something. So the gold standard is before you hastily go and use one of those black box models, it is perhaps useful to see if you can think through the data and do feature engineering. Thinking through the data and doing feature engineering is the gold standard. And if you look at the people who do excellent work or win competitions and so forth, they often invest most of their time in feature engineering and data cleanup, data preparation and feature engineering. It doesn't sound very exciting. People get people get very excited oh I used the transformer to solve some problem in natural language processing or something like that you know those are hot topics yes there is a place for that but you should my philosophy is you should progressively go to more complex algorithms. Because more complex algorithms have embedded complexity, like biases. That it may not be informing you. And also you lose sometimes legal footing. So for example, in many situations, you are, in US, any kind of prediction that you make that affects the protected classes you know the gender the age race and so and so on and so forth immediately you're in violation of the law and companies have gotten sued with this in this respect. So, you should always try for interpretability and feature engineering is your path to it. To keep modeling simple, use simple tools and... Simple. Do the model. I'll tell you a story. I suppose this is a side. So guys, forgive me for going on a tangent. I'm into photography. So you in Silicon Valley photography will you know this we are all ingenious most of us a large proportion of people are engineers and engineers tend to be gadget sort of gadget fields they have a love affair with gadgets camera is something that most engineers tend to splurge on so if you go to the beach well not these days in covid but normally when you go to the beach you can always spot uh these newly minted photographers you know people with a big camera fancy fancy cameras walking around. So they'll buy an expensive camera, the latest and greatest and God knows 50 megapixel, whatever it is, and they'll start taking pictures. Quite often they'll just leave it on the automatic mode or something like that. So somehow the instrument is supposed to itself produce good pictures. After a little while you get disappointed the pictures don't look good in fact sometimes embarrassingly your cell phone pictures look better right because in cell phone there's a lot of computational photography a lot of software trying to make your pictures look better whereas a professional camera will give you exactly as it sees so you get frustrated then you ask around okay what happened why why do my pictures suck after a little while you get an idea and you say oh it must be the lenses after all it's the lenses that bring in the light in the image so you can see people splurging on expensive lenses Canon l-series lens and so forth, and you can often spot them, long big white colored lenses or cream colored lenses often on the beach. And what you can always tell that somebody is still learning because in the middle of the day he'll be asking his sweetheart or children to go pose in the water or something like that on the beach. Now, how do you know that these people are novices? There's actually a very good tell, a very good state. A good photographer will never take pictures, rarely ever take pictures in broad daylight. If he is forced to take it, he'll do all sorts of extra lighting support like flashes and so on and so forth to get rid of glares because in broad daylight we all develop stark shadows on our faces. So good photographer, I mean especially if you do landscape, you want to take pictures of people in landscape and so forth, but you want to take it either on a cloudy day or you want to take it when at sunrise or sunset, Jay Shah, Dr. Jay Shah, Dr. Jay Shah, Dr. Jay Shah, Dr. by now we are close to six seven thousand dollars shot and still no good pictures and so the joke is supposed to be that finally it dawns on the person that it is not the camera and not the lens but it is the tripod enlightenment is he will now go and buy the biggest tripod his spouse is willing to carry because of course he is carrying all those heavy equipment himself so anyway so why do I mention that tools there's a there's a parallel to this in machine learning a strong parallel these days there's a tremendous amount of research and there's an explosion in newer algorithms more and more algorithms They all have a place under the sun. They all have a use. But you need to use them judiciously. Just because you pick a powerful tool doesn't mean that you will get very good results. The way to get good results is thinking about the data, feature engineering as much as you can, cleaning the data, preparing the data and going about it. Some people say in view of this way complex algorithms, feature engineering is dead. Right? Be that as it may, you know, if you use any one of those algorithms, let us say you use a big deep neural network, you lose all interpretability. People are doing a lot of research to regain approximate interpretability by making linear models that approximate those networks. But it's a problem. All right. The gold standard is, see if you can simplify the problem. Look at it in such a way that the entire problem looks simple. So for example, the photographers who go and do national geographic, you know, those masterpieces, quite often they will go into a culture, into a country, with just a simple camera, a good camera, professional camera, pretty old, like it might be a few years old, and their favorite lens, which would be many years old, a couple of lenses at most. And they'll walk into the wilderness, old and their favorite lens which would be many years old, a couple of lenses at most. And they'll walk into the wilderness or they'll walk into some country or wherever it is and they will come back in a few months with absolutely stunning pictures that make their way into National Geographic. And our pictures, we are the blokes who go and buy fancy equipment, but obviously none of our pictures ever make it to national geography. So in the similar way, I like try to use simpler equipment and put in your expertise, deepen your expertise so that you come up with good results. So that was the main lesson that I wanted to bring about today. Anyway, just a parallel to this story. So all right, after this digression, let us come back to the data. What I'll do now is I'll stop sharing on this screen. I'll go to the Linux, and I will talk about it there. So remember, we are going to take three approaches. Direct logistic regression approach, which we don't expect to do too well. We can do a polynomial approach to linear linearize the data and a feature engineering approach so let's do that that's the school and here I leave it as a exercise for you to go repeat yourself because I haven't seen anyone do his submission for the other data sets you can try it out on the other data sets. There's a question. Please go ahead. Yeah. This is a return. So, My question is like for the center line right that the green line that you're drawn So, so, so for using regression you assume only the blue water data. Yes, yes. So, so you will you will basically do a extraction you will take a subset of the data that has only absolutely you hit it on the head so let's go and see that okay okay thank you so I'm going to now share so how do we submit homework you just said yes so just as we tell you homework becomes polished don't export it to Kegel especially don't make it public because you don't want to do that so do it in collab oh and clear it with me. You can share your notebooks with people. OK, so you want me to share my exercise with you on Google Drive? Not necessarily on Google. In Colab, there are ways of sharing. Can you share it on Google Kegel directly? Yes. I think in Kegel. Hang on, let me verify that. I'm just wondering, you mentioned two, three times. So I was thinking, how do I send you the homework? Okay, why don't we do it together? We'll figure it out together. On Kaggle, it seems like we have to paste all the lines of the code. At least that's what it seems like to me. Yeah, that is the thing. But you can select with whom you can share that particular. Yeah, like yeah that is the thing but you can select with whom you can share that particular yeah that is right so let us go there into Kegel to Kola and suppose I'm in Kola I'll just take a random example intro to pangas let's say this one and do you see the link at the top here saying share? You can share warning share will contain you so if you do your own thing, let's say that we create your own new notebook Here we go. I created a new notebook and Now if I do a share right you see it gives you means to share it you can get a link or you can add people directly by email to share it so just share your collab with me yeah that's fine sir but I'm doing an art so just I will send you the our markdown file yes that's cool by the way you can do r also in collab and it's one of the things that i'm saying to you yeah and julia too and julia too yes we'll do it in julia julia i don't know if collab supports yet see what happens is i tend to spend a lot of time on a local jupiter or in a notebooks on the Google notebook, which is more high powered. Pull up is something I like, but you don't end up spending a lot of time there because it tends not to be powerful for You have Linux server. So I just put the Docker Anaconda Docker and you know, we can run jupyter lab on that I'm running it jupyter-lacquard yes yes so I have that so my environment is completely set up for deep learning deeply relaxed and that's where you see you're seeing I don't know if you're seeing my entire Linux I can share my entire screen but at this moment here seeing my notebook screen let's go through that so is the text looking big enough or should I increase the font size please a little bit of so awesome I make notebooks how do we share with you? In the collaborator, I couldn't find the name. In KOLAB, you're talking about KOLAB. In Kegel. I think a little harder to share. Maybe there is a way to share. You'll have to add me as a collaborator. Yeah. In the dropdown, there's an option like number of people it shows like which you have to select that particular person you want to share it with and you can do that. Okay. So yeah, work with What's the name I search for? Okay. Let's take this offline. Why don't we do this at the end of this class, let's take care Rajat Mittal- Know what's the name I search for Rajat Mittal- Okay. Sajeevan G. Let's take this offline. Why don't we do this at the end of this class. Let's take care of the cable and the collaboration. Sajeevan G. Right, let us give some time at the end and we'll do both of that project. Would you like to take that session at the end. Prachi Naikin, Yeah. Sajeevan G. Okay, please do that. so here we go so is the text bigger now I'm trying to make it much is it big now it's good yes so all right if maybe too big I might be it's too big like isn't it okay I'll make it slightly smaller so I can myself see some code all right so when we look at this what we'll do is we'll take the first is the inputs so let me go over the imports carefully numpy and pandas what do you think numpy does by now we all know it gives us the linear algebra the the matrices and matrix operations. Then what does pandas do? It gives us the data frame in Python. In R of course, it comes built in. And then what are the stages that we take the data to? We need to pre-process the data and we'll talk about it. One of them is you standardize the data. It is always a good idea to standardize the data. You don't need to standardize the data. Sajeevan G. For logistic regression, but for all situations you should standardize the data. So let me talk a little bit about standard scale. So suppose you're taking features of an elephant. features of an elephant so the age of an elephant may be like 20 years or 16 years but the weight of the elephant if you're especially measuring it in ounces it will be like what is it tens of thousands of ounces isn't it are we getting that if you go and weigh an elephant and the unit of measurement is ounces, it would be a massive amount. So the unit of measurement of each of the observations, like the features in the observation, affects the data. Now also, there's a wide sort of skew. Some things are in millimeters. For example example if you look at the temperature of an elephant or the temperature of either a human being it is within a very short range most people have temperatures close to 98 they may go down to 96 97 which is getting bad or up to 100 and 405 and that's also getting bad the numbers are within a very narrow range on the other hand the weight of an elephant could be could have a very wide distribution and the unit of measurement may be huge I mean the number of pounds the values may be huge so one of the things you do is you standardize the data so if you subtract the mean of the data and this I'm just reviewing what we did in ml100 if you subtract the mean from the data the data becomes Center aligned isn't it so all the values will be around Center means for example for temperature 98 will be the recenter to zero any positive value would mean temperature above normal and any value below 98 would be temperature below below normal below average okay so that is the first thing you do you center the data after you have centered the data you still have a problem your data is too wide in some areas and it contains the units, your ounces and this and that. And a lot of these algorithms, they don't like data to have so much variation from feature to feature. One is in millimeters, one is in decimals and the other is in tens of thousands and so forth. It's cues of the learning process. So the next thing you do is you scale it down to a standard to a unit area there are many scalars one scale scaling that is very popular it is called z value you can not only take a data subtract the mean from it but you divide it by the standard deviation. When you do that, your quantity, now your number becomes dimensionless. There are no dimensions. The ounces disappeared. And when you Z value data, even if you had measured your data in kilograms, you would still end up with the same Z value. That's the beauty of it. It becomes dimensionless. The second good thing that happens is it obviously brings it down to much closer to zero so that the values are typically between minus three and three. But beyond three are the outliers. Beyond absolute minus three or three are the outliers that you have. So that is the standard scalar. Subt subtract mean and divide by standard deviation it's called Z value some of you may recall you may might have done it in your high school or college textbooks but that is not the only scaling you can do min max scaling what it means is that you can subtract the minimum from any data and divide it by the range min minus max is the range so then your data will all fall between 0 and 1 right if you are at min your value will be 0 if you are at the max point your value will be 1 and all the data will find will fall in the unit interval and so there are other scalers or scaling up means to scale or normalize the data interestingly once you have done the modeling sometimes you can squeeze the last bit of accuracy or last bit of juice by for example playing around with different scalars different ways to normalize the data or scale the data into a standard representation. So this is if you're not used to scalars, that is what it is. So the standard scalars has Z value, Z value things. Now, remember one of the approaches we will try is the polynomial features, right? We will expand into a polynomial space where higher degree polynomials into that so these are pre-processing sweeties or we need classifiers so we'll only use one classifier the logistic regression now you remember that in one of the approaches like the feature extraction approach we are also going to use regression to find the midline in the river so I have included the regression library also now when you have logistics will you have a classifier the measures of a classifier as you know our confusion matrix in other words if something between a cow and a duck how often did your algorithm confuse a cow between a cow and a duck, how often did your algorithm confuse a cow for a duck and a duck for a cow? So that will be the off-diagnose, the confusion matrix. And the principal diagnose are the values that it got right. So that's the confusion matrix. Based on that, there's a classification report based on prediction and what the value was. The report has now in classification, there are multiple measures. There is accuracy, there is precision, and there is recall. And then in different literatures, you use different words. There is specificity, there is sensitivity, there's type one error type two error and so on and so forth there's the F score there are many many metrics they all come by comparing what was predicted to what was the reality here we'll take a few of those and look at it and the third and the next thing is something called a receiver operator characteristic curve roc curve and the area under the roc curve that's what these two lines refer to now if i trust you you know from prior ml100 what these are but those of you who haven't taken ML100 with me at some point we'll take a remedial session walk through this classification. Prachi would you please take us explain it to people? Yes Asif. Would you like to explain these things if people need some explanation of the ROC curve and area and I would you rather I did it I would rather you did it like over the weekend or some some clinic hours in which we will cover these things next and I include f1 score is just the harmonic mean of the precision and recall. Yes, I've got it. One second. So coming back now, going forward, linear regression, what do we look for linear regression? forward linear regression what do we look for linear regression we look for the coefficient of determination the R square and we look for in regression in general if it is not linear in general we look at the mean squared error how much is the mean squared error residual error that is left behind and the average of the residual errors residual squared errors or you can take the square root of this then it becomes a root mean squared error rmsc so that those are the libraries now going down a little bit uh matplotlib in line it just this is a statement saying you don't need well okay all it means is that show the graph any plot without needing to have the word dot show explicitly stated any plot you shouldn't have to call show just assume that it is there and show it in line then the and just show it show it in this notebook itself. This is the normal plotting library in Python. Matplotlib is the default library. Good thing is that it's quite powerful actually. Bad thing is that a lot of people complain that it's not very aesthetically pleasing and there are better libraries out there and so forth. So Python has now almost a zoo of visualization libraries and people who love to do data visualization they get carried away and they they do absolutely stunning data visualizations they become very familiar with libraries they they write their own libraries and contribute back to the open source it's a wonderful world out there Jay Shah, Dr. Anand Oswal, He or He you then import this library called seaborn seaborn is a much aesthetically more pleasing library now even if you don't use seaborn include seaborn because what it will do is it will go and put seaborn because it will go and override some of the matplotlib defaults and so matplotlib itself will start looking nicer right so it is worth doing. The next is colors. Except if you want to give the colors, most of us do web pages. So we get used to giving colors in hex form, you know, hash something, something, something, something, or hex form and different forms, either those ASCII forms and hex forms and so forth. So if you are in that mood to give it an ASCII or RGB values and so forth, this library color helps you. I don't think I use it in this notebook, but I tend to use it. The other thing is nowadays, and this is something most people don't know, I think it's fairly recent, Pandas has PlotLib built in. And when you plot with Pandas, one of the things that happens is you can choose a backend. You may choose Matplotlib or Bokeh. Bokeh is another exciting library, interactive library for data visualization. Or you can choose Plotly. Plotly is a commercial thing, but it has an open source counterpart. I like a lightweight version of it so that I don't know whether this lightweight or complete I am not too familiar with partly but looks beautiful is based on d3 visualization so you can specify those backends and therefore your visualizations will then automatically start using those backends it's an interesting thing. Now, this is very basic. I like to have my pictures in landscape format. Why? Because ultimately I have to, I give you guys notes, written notes, but there's a few who have been looking at ML 100 tabular data, the PDF. You realize that all of these things make their way into a chapter in the textbook. So having it in landscape format means I don't use up a lot of vertical space. That's that. And then this is basic form setting. Now, plotting style, there are many styles of plotting. Some people like dark background, some like white background, some like slight gray background and so forth. And so there are many, many many styles if you go and look at the matplotlib styles it's a zoo and people are writing their own styles so by all means pick your style I like ggplot because it gives me champion of sorts. A very, very good visualization based on the grammar of graphics. So it is. The rest of it is just setting some attributes. Asif? Yes? You mentioned about using like yellow brick as like one of the libraries. So you're not using that for this one? Yes, I will be. But there is a delicate reason I have not included it at the top. The reason is unfortunately, but their function names are exactly the same as scikit-learn function names. So if you, whichever you import last takes over. Okay. Okay. Right. So anyway, that's the reason you'll see that when I use it at that moment, I will talk about it. Now latex, I mentioned this fact that is that at the end of the day, once you have done your analysis, you need to tell a story. That is the story behind the data. So to tell the story, you need visualizations, you need plots, and you need good writing. You can fix your writing, but you also need good visualization. Having professional quality visualization makes a world of difference when you take your show on the road and try to convince people. It's very jarring it's like if somebody were talking to you in english and the english had huge amounts of grammatical mistakes it would be annoying or like for example i'm an immigrant so i assume that the native speakers in u.s when they hear my english they must be getting annoyed that I put accents or stress on all the wrong places. So it's like that and bad graphics is somewhat in the same genre, you know, people tolerate it. It's not that they they they're lenient and they'll acclimatize to that and so on and so forth. But generally it doesn't look very professional. It's certainly curable. Like for example, I cannot possibly cure my accent, but I certainly can make better graphics and make sure i don't have grammatical mistakes so in that same way spend some time becoming good with the with this so in that so now i'll move a little bit faster this is this line just suppresses some unnecessary warnings when you run otherwise your jupiter notebook gets littered with all sorts of output and that's not pleasing to look at so what do we do we first load the data and we drop any rows that that have missing values this is not obviously the best idea i did it because i already knew that i that they're not likely to be too many missing values. Generally, you should load the data. You should do something called missing value analysis. See how many missing values are there in more complex situations. Then you should do imputation. You should impute, try to fill in those holes in the data by imputing some value sensibly. But that is a whole thing in itself. When you do bootcamps with me, you go through a lot of rigorous sort of practice with those sort of things, but not for today. Here, we are first learning to walk before we run. So I'll just not go into that. I've been a bit sloppy. Just forget about the missing values kind of thing. Describe the data. So this describes the data. And what does it see it has three columns these three columns now i already told you that the that the data has t is the categorical variable right even though it looks like a number it's actually a categorical when you see that the mean is 68%, it means that 68% of the values, if it is 0 and 1, 68% of the values are 1. So suppose I build a classifier in which I declare all the values to be 1. I would still be right 68% of the time, isn't it? So my baseline classifier just picks the majority and declares everything to that. It's also called the zero-R classifier. So it forms a benchmark, a baseline. Any classifier you write must beat the baseline classifier. So to illustrate the value of this with a little analogy, see, why should I not be impressed if a classifier has 68%? Or what number is good number, people ask, in the accuracy of a classifier. First is accuracy in itself is not a good measure. You have to take it in tandem with other things, recall and precision, they mean a lot. For example, if you're diagnosing somebody if you have a diagnostic tool and you're looking for let's say indicators of breast cancer or prostate cancer you want to not miss any positive case so then the recall as a metric becomes more important if you are finding that you're you're lost with these words, please go and review chapter four of your text book. I wouldn't have time today to review all those concepts, but that's that. So recall becomes more important than accuracy. Now, speaking of accuracy, let me give you a narrative. Suppose you have a tool and you're screening people for let's say something pretty malicious, let's say some form of cancer, breast cancer, prostate cancer, whatever it is. And most people will be healthy because if you're random, if you're doing a random sample of the population, then you wouldn't have that. Right? So they wouldn't have that. Maybe 1% or less will come out positive. 0.1% will come out positive, let us say. So let us say that you have your very well-researched, carefully crafted tool that can classify a person into healthy or unhealthy. But there is a quack next to you who obviously has no tools but who say what comes to him he says my dear friend you are perfectly fine go home I tested you and he will make a big show of testing maybe draw the blood and they just throw it away and then he'll declare you to be healthy now what proportion of the time would he be wrong if you think about it he would be wrong only 0.1 of the time he would be right 99.9 of the time and yet you would rather not go with the quack you would go with the proper diagnostic tool which catches the problem when it is there. So the other metric recall is more suitable to that. Things like that. So there are many metrics of classifier to judge a classifier based on context, precision so forth as we move forward make sure you review it and again I'll give some session so that's that so that's about accuracy in fact the history of before modern medicines came about I would say that the history of most of those ancient forms of medicine were often oftentimes not always they were very effective in quite a few chronic cases but for acute cases I don't think they were very effective and a lot of the time a quack would just sit down and declare everybody to be healthy he would be he would come out like a genius because the human body has an amazing ability to heal itself and so Jay Shah, Dr. Anand Oswal, He or He way it does your exploratory data analysis you can build your you know the histograms etc etc but just with one line of code you can generate the entire profile report so let me show you what I mean look at this report and I wish this was this could be opened in a new page no I don't think it can be opened in a new frame so you notice that it gives you statistics of the data there are three variables these are the number of observations and the boolean variable there to numerical value and one boolean type so it was smart enough to detect that t is actually a boolean type 0 1 right it's a binary type which is good the variables it shows you the histogram of the variables it shows you the mean and the you know all the statistics associated mean minimum maximum and so forth right and you can even look for details more details if you look for more details it will give you all the quantized do you see how how detailed it is it will give you the median and those of you if you want to brush up your statistics this would be good thing see if you can explain the meaning of each of these variables we don't have. So I wouldn't go into basic Statistics, but here it is. And the same thing for x2 you can toggle the details and see it a T. Do you notice a lovely thing it immediately identified an exploratory analysis that T is a is a boolean is a Boolean data type. And that there are 4,300 instances of one and 2,000 instances of zero. And it gives you the ratio of the two, 68.3%. It means that 68.3%, any classifier needs to be better than that. Any classifier needs to be better than that. Right. Can you make a classifier worse than that? Believe it or not, you can actually. Right. You can always do worse than that. The interactions, like how interrelated are these x1 and x2. And when you look at it it you still see the pattern of the data you see this here and you'll see this more you can look at the correlation between the data and you notice that the data is not if you will just let me i'll just zoom out a little bit so we can see a bit more yeah you'll you'll realize that most of these correlations are low the off diagonals are low so x1 and x2 are not correlated or T seems to have some degree of correlation to x2 but with x1 it seems to have no correlation so this is it and so by the way I leave it as I exercise for you to read about all of these correlations. These are the different correlations in statistics. It could be a good review of your statistical concept. Are there any missing values? There are none. But then when I loaded the data, I deleted this. This gives you a preview of your data, some sample rows of the data. First rows, last rows rows and so forth so this is the value of this descriptive statistics right variables interaction so you can just go straight to this you can look at the samples of the data so in other words pandas profiling is a pretty powerful tool there are a couple of other tools are also has data explorer and so on and so forth. Use those but know that you should know how to do histograms and correlations and correlation plots yourself. Then it is good to know but you can save a lot of time after your master that by simply using the tool. So if you remember in the ML 100, we use the basic tools, we did the histograms by hand, we did the correlation plots by hand and so forth. But now we'll just use pandas profiling for Python and data explorer for R. So what do we do? If you want to train a machine learning model, we can, standardize the data this part I can just remove it what am i doing the first things psychic learn needs is a separation between the the feature space and the target space so our target variable is T and our feature variables are X1 and X2. So we are separating them out into X, which is the feature space and Y that is the target space. Now, there's a lot going on in that line if you're not familiar with the syntax. So I'll talk a little bit about it. This is very Pythonic syntax. This X takes the first value here and y takes the second one. Now the first thing you notice is I'm converting t to a categorical. I could have made it Boolean, but categorical is good enough. Categorical means it's a category because it's zero one. We don't want to treat it as a number. Then the other thing we notice is that I use a specific notation capital X and little y. There's a reason for that. You could have used any variable in programming. What names you go to give to variables doesn't matter. In data science it tends to matter because of expectation it is a convention in the field that in the textbooks the feature space is represented by bold capital X the data representing a data of the features is represented by bold capital X and the data for the target variable whether it is numerical or it is categorical is traditionally represented with little y. Y is for vectors see generally small letters are for vectors and capital letters are for matrices the feature space with all the rows feature space will have like the data features will have a lot of features. I mean, the feature part of it, input data, a lot of features, and it will have many rows of data. So obviously, X will be a matrix, capital. Y, on the other hand, is a single scalar value or it is a type, cow or duck. a cow or duck but for every data instance it will have a value so it is a vector it is a column vector so a small letter for it it's a convention in the field the next thing we do is we take this data and we split it into training and test parts this is this code is essentially boilerplate code you will see in innumerable notebooks and you will get you get very used to doing this are you splitting the data into training and test this random number state is 42 why it is and leave it as a mystery for you there's an interesting story behind it with and let you explore it has to do with the hitchhiker's guide to the universe so It has to do with the Hitchhiker's Guide to the Universe. So what do we do with the data? Now that we explored the data, we did descriptive statistics on the data, let's try to visualize the data and see what it is trying to tell us. When you visualize the data, you notice that it is, it looks like this. By the way, this is data visualization code it will talk about it here the only thing that is important is this creating the figure and doing this actually why did I do this I need not okay I'll just leave it at this we're going to so the size of the figure, it's more twice as wide as it is vertical. This line is purely optional. I just add some little bit of padding here and there. Next is I give a title, a plot. Then I do a scatter plot. These kinds of plots, as you know, are are called scatter plots what do I need to do I need to give the X and the Y value and I'm coloring it by the label the target I use a color map color maps a color scheme see what happens is that we shouldn't randomly pick colors for plotting because of a lot of aesthetic issues as well as disability issues quite a proportion of men and sometimes women are colorblind I do not desk colorblindness exists in women I don't know anybody who is but maybe it does or doesn't one of you a doctor here can confirm but in men it certainly is their colorblindness or people certainly is there colorblindness. Or people who have partial colorblindness, they can only see some colors. So people have created after a lot of thinking, certain color palettes, color maps, which are optimized for disability and at the same time are good looking I mean a pleasing looking so that is why you can look up the dictionary on the web there are many many color maps matplotlib has the documentation you can pick a color map and give it you may have a different choice I just gave this yeah alpha is that transparency do you notice that this yellow is not deep yellow and this purple or violet is not deep violet, right? It just faded out. That is the alpha of the transparency of it. And S is the size of the dots, how big the dots are. So if you don't give anything else, if you all, if these things were not there, your code would still work. But this is just adding a bit of aesthetics to the plot. Likewise, you don't have to give the title, labels, et cetera. It's always good to give it. Now the title is this, Cataplot. One of the things that you can do, again, optional optional is if you're familiar with latex Sajeevan G. You can then Sajeevan G. Apply some latex formatting to it so that would you feel that this thing looks a little bit more publication ready Sajeevan G. Let's open it up in a new tab and say, compared to default. It's not perfect, but now I'll let you judge. Would you agree that compared to default it's not perfect but now i'll let you judge would you agree that compared to default it looks a little bit more publication ready yes so um that's the point of it so when you do the plotting plot it out and then spend some time uh brushing it up cleaning it up a little bit let's close it out so from this data we make a few observations. The observations are first thing you notice that there seems to be two decision boundaries. One at the top where yellow meets a violet and one at the bottom where violet meets yellow. And the decision boundaries are nonlinear. We talked about it a little while ago now I explicitly said in the statement of the homework that use logistic regression and that is rather you may say that that's rather painful because this does not look like a linear problem a logistic regression as we talk as we mentioned makes a linear decision boundary and we are quite literally staring at a problem that is not linear so how do we solve it let us say and just to remind you what is a logistic regression this part is the distance from the decision boundary and this is the odds P over the probability of success versus probability of failure in other words probability that it is a red berry versus that it is not if you take the log of it the log of odds is the distance from the decision boundary that is the essential statement of logistic integration classifier. So let's say the code, when we do the code it's quite straightforward. You have this, DLF fit and so forth. You realize that to build a logistic regression model on the data it's just two lines of code. Maybe I'll now increase the font size the two lines of code once you build a model if you remember the methodology that i sort of trained you folks into was that you build a model then you check the model diagnostics you check all sorts of metrics to see is it a good model or not and even if you if you do that you plot it you make all sorts of visualizations of of that afterwards and then finally you plot the predictions on the data and if everything still holds out you say the model may be a good model generally something along the way will fall right and so you abandon the effort so let's look here we will do the confusion metric and the classification report i wouldn't when we do that right away we see a problem here do you notice that none of the zeros are being predicted as zeros none of the zeros are being predicted as zeros. Right? So it's a disaster. Everything is being predicted as one. So here, this is the rows, the columns here are the real values, zero and one, and the rows are the predictions. So as you can see, clearly, we are looking at a problem. Not only that, when you look at the accuracy, what do you see? 66%. So should you be impressed with this accuracy? Definitely not. A bit loud, isn't it? No. All ones were 68%, right? That's right. So the thing is, even the baseline classifier had 68% accuracy, and it got that wrong. Secondly, its accuracy for zero is like for zero, the precision recall, etc, is just hopeless right and f1 score which is the harmonic mean of these two is again zero it's a pretty bad model all right and for this also it's it should have been you know at least for one it should have been a complete completely accurate statement if you just use a baseline classifier your precision recall an f1 score for one would be exactly 100 percent right but this one seems to not even get that so at this moment you abandon the effort you say well we're lost so then you take the next effort which is polynomial regression what do you do you take the data I don't know why am I doing this here? I must be thinking of something. I did this anyway. Oh, where did the polynomial regression disappear? I think I accidentally may have dropped something at the bottom. Hang on, give me a chance. Oh, here here it is the polynomial regression is do is you do it as a polynomial regression so when we do polynomial regression, just the steps. Remember the first we need to scale the data that is something you should always think about doing unless there is a reason not to scale it. You should scale the data. The question is, what scaling would you use you use the second is we want to use polynomial features so that is the next transformation we'll do we pick a degree i just generally picked a degree five a five degree polynomial is a pretty complex model isn't it it will have a lot of interaction terms and so forth so we have that then what you can do is data input data will go in it will be scaled it will be it will become polynomially expanded so you can use this feature called make pipeline that will do you don't have to tediously do it in your code you just give the pipeline and in a single statement the pipeline will do it for you okay so here we took the data we got a nicely scaled and polynomially expanded feature space feature data that is good then we apply a logistic regression model on this polynomial expanded data let's see if it works yes why do you pick degree five here? Because very good question. Yes, I should have. So here's a mathematical trick. I'll tell you one, two, three, your regression, count the number of bends here. How many bends do you see? Three, at least one, two, three, and then four, five, actually six. So at the bottom, but because two of these benches are aligned, you may just take six, five, whatever it is, but you need to take a fairly of degree n has n roots in other words it has n places where it meets you and therefore if you're trying to solve a problem and if it means that the curve will meet the x-axis n times means it has to bend that many number of times it's one of those little things you know of mathematics if you know it you can apply it when the situation arises it's a pleasure to do that anyway so we come here we will print the model of fitness model coefficients and we'll make predictions with the model and once again we'll see the classification report and see how good it is. When we do that, you realize that this seems to have been a good idea. Your F1 score goes up, this goes up, general F1 score for zero and one also goes up. Accuracy is close to 79%. And if you were to print out the coefficients of the model you would realize that do you see how complex the model is there are so many coefficients 1 2 3 4 5 6 18 19 20 21 so imagine an equation in 21 terms and such a complex equation is able to get a reasonably good prediction here, right? So I will pause here for a few minutes. Let's take a 10 minutes break. I'll stop the recording when we come back and then we'll do the feature engineering approach to it. So together, let me go drink some water and then we'll do that. Record. So I need to stop sharing the machine. This machine. Okay. Am I sharing the screen? I am sharing the screen, isn't it? I'm not sharing a screen. Okay. Not yet. Okay, so let's try. How about now? Yes. Yes. Yeah. Okay. Oh, I wish I could move this away. So now we're going to do a little bit more careful analysis of the data so how do we do that first what we do is let us extract the river from the data remember we talked about if we could if we could get the midline of the river we can compare every data point to the midline of the river isn't it so let's do that. You have the data. You hardly need to describe it. What did I do here? I just kept the river points, which are the t is equal to 0 points. When I do that, I have a much smaller number of data points, only 2,000 data points. And out of those data points, if you visualize that that data point this is how it looks right you see that this shape of a river stands out of course this is a toy example that I created but most rivers are not so perfect they would meander here and there and so you would need some sort of a point on me or something to describe but it will do for us because here we are trying to learn about feature engineering so then what can we do we can take this data and we can model it as a polynomial and which is what i'm doing i just took a fifth degree polynomial but here i am not fitting a classifier classifier i'm fitting a regression line, right? I make a regression line and there are only five terms. Do you notice that? It's six terms, of course, because they're beta zero. Then on those, and the first coefficient is actually zero, so five terms. You notice that the R square is close to 90%. So I have gotten a pretty good model. Let us check. Whenever we build a model, we check the goodness of fit. Now the way, if you remember in ML 100, we got trained. First thing we do is we look at the residuals plot to see if there is any heteroscedasticity in the data. When you look at this and as you scan your eyes from right to left, there is no distinguishable pattern. There is no funneling or anything. There's almost no pattern, anything worth noting. So the data does seem to have homoscedasticity. This also has, if you look at the residuals, they tend to have a Gaussian distribution here, guys. That is another criteria for residuals. They should have a Gaussian distribution, guys that is another criteria for residuals they should have a Gaussian distribution bell curve distribution and in the margins you can see that they do have a bell curve distribution so we seem to have a model the river correctly so then we go down a little bit we also do the error analysis we compare what is the prediction error like what is the value of y versus y hat right and you see a strong correlation they seem to be along the principal diagonal right so a good thing next we go and so this is it best fit and identity line. So our best fit line is pretty close. You know, R squared is 90%, which is very good. The other thing, remember, I mentioned, and so it is good to be careful. One of the things that I mentioned is you should look if your linear models are being hijacked by points of high influence, points of high leverage. There is a way to do that, a simple way and here's the code for that and Anil do you notice the library that you mentioned is being used yes so you notice that now there is in Cook's distance or plots what you look for is a dotted red line and you say that anything beyond that red line begins to stand out as points of high influence here there are actually none we don't have those points all the points look normal and fairly good have those points all the points look normal and fairly good some minor here and there so then let's visualize the model so the next thing is so far no reason to reject the model one final test let's visualize the model over the data so this is a data visualization code obviously for those of you who are just getting started I would say be patient with yourself you wouldn't be able to do all of it right away but do that book finders for everyone and the cyclic and the psychic learn book it has example codes the web is example code while this code looks scary so much but actually it's a very trivial code. Once you get used to plotting, so I'll just walk through what the code does. This is just some beautifying parameters. I'm using LaTeX. Why? Because you notice that the title has a publication quality text to it, right? If you look at it, all the text is a nice, good quality text, good fonts, whether, and mathematical expressions look like mathematical expressions, so you can ignore that. I just create a lot of synthetic data in that space to draw the line and then I just go ahead and draw the line. But in the background of the line I put the original data points, all the river data points. So do you think that this line more or less represents the center of the river? Occasionally it seems to miss it, but broadly speaking, it seems to get it right. Would you agree, guys? Definitely. Yes. It does agree. Now what we will do is, now let's do feature extraction. Let's look at the distance from the center of the river, which is very easy.'s look at the distance from the center of the river, which is very easy. We just define the distance to be the absolute distance of a point from the center of the river. It doesn't matter whether you're above or below, what matters is how far you are from the center of the river for that point. So I can take the X2 value of a point and compare it to what the center of the river is river middle is this and i just subtract and get the distance and so you notice that i have one feature d the center of the river go ahead uh probably i'll make a comment once you explain it comment once you explain it. Sure. So then with this data, now what you do is you create a much simpler data set. You see that my feature space is now just the D variable and target space is T variable. So now I'm talking about a classifier in just one variable, right? A line. I'm just trying to divide a line, chop a line and at some place saying on this side of the line is river on that side of the line is sand sort of and so if you build a classifier with this you expect it to do well and here it is right away when you build this classifier you notice that first thing you notice is you get a good confusion matrix. Remember the initial one was a disaster that we got zeros, right? But here you notice that most of the points seem to be correctly classified. There are 92 times when one is misinterpreted as zero and zero is, and here 77% of the time when zero is interpreted as one right so i don't know why i divided it by 10 0.0 okay so that's the thing and you can see the classification report um what is that accuracy 89 92 90 close to 90 percent give or take now Jay Shah, MA, RNIDA 1.929 close to 90% give or take. Now this is the accuracy that you can have you usually can't exceed that give or take one or 2.0 then you can't exceed that because the data inherently has noise. If you go back and look at the visualization of the data. Jay Shah, MA, RNIDA 1.929 Do you notice that there are areas in which the yellows and the purples mix up the zeros and ones interleave. So there is inherent noise in the data. So you realize at this particular moment, you seem to be pretty close to the best possible situation. This is a way of visualizing the same data. This data is nicely visualized here, right? Precision, recall, F1 and support. It is here. So this gives it a bit of color to it. The next thing I mentioned in classification is the receiver operator characteristic curve. The receiver operator characteristic curve is this curve that tells us how good the classifier is, another way of looking at it. The idea, if you remember, is that your classifier, if you are this diagonal line or below, your curve is below this diagonal line, then you're doing a pretty bad job. But if your curve is like pretty much hugging the top left-hand corner and most of the area of this page, of this graph is one unit by one unit, even though I've made it into a landscape format. If most of the area is below the line, it is a good, it means that you have a good model. So here it says that the ROC curves look good. First of all, they look pretty good for zero and for one, both of them are good. And at the same time, the area under roc is 96 percent 96 percent is a very good number right for that so that so by roc plot also we realize we have a good model now we can this is a simple plot that tells you how often did you confuse zero for one and how often you confuse one for zero so the level of mistakes you're making marginal mistakes right and that finishes this particular analysis right so guys now that you have this you can use this to do the flag analysis in flag the only difference is it matters whether you're above the line or below the line right when you visualize it you'll know what mean. You should be able to do the flag analysis in 20, 25 minutes. Now I'll take questions and then I will perhaps do it in R. Repeat the same exercise in R. So what do you think, guys? Do you feel that this is helpful in trying to solve the problem? Very. Yes, thank you. Good, good. So understand it, this- I'll just- Yeah, thank you. Good, good. So understand it. Yeah, go ahead. I have a quick question. So here we looked at the data. It's a 2D data and we saw a pattern of where it's all zeros and ones and we saw that river is where it's all zero, elsewhere it's one. And since we knew about the data, we modeled the sign, I mean, whatever the fit was And since we knew about the data, we modeled the sign, I mean, whatever the fit was, and then subtracted the distance. So it's like a cheating, I mean, it's not cheating, we know about the data and so we, so how does this generalize to, if it's not a river? No, definitely, a polynomial can capture many things and your point is valid. See, this is a toy example, deliberately crafted to introduce you to the concept that feature engineering is a powerful concept, is a powerful method to use. In every situation, you'll have to think hard and you'll have to decide how to extract features from that data but the universal truth that remains is that efforted feature engineering almost always pays off you could put quite a bit of effort at future that is it because i was i was thinking like okay it was a jackpot. I saw the sign curve. And then I thought there would be a formal way to, you know, I mean, to approach this problem. But I think the scope of the problem. Yeah, as we move through this course, right, we will learn a lot of methods. See, okay, I'll make a statement, opinionated statement. See, everything that you will learn a lot of methods. See, okay, I'll make a statement, opinionated statement. See, everything that you will learn from now onwards will be methods, powerful algorithms that will somehow linearize the problem and do feature extraction for you. We live in the world where deep neural networks, deep learning is very hot. If you look at what deep learning does actually is that here we thought through and did feature extraction ourselves. When you can do it, you have a simple interpretable model that you can explain what is happening. We have a, how lovely it is that you interpret it as simply saying distance from the center of the river very intuitive but you don't have to if as you will learn when we use decision trees random forest gradient I mean boosting and support vector machines deep neural networks they all can solve this this problem is too simple they will solve it in the blink of an eye. But what will they lose? Decision tree still has some interpretability, but beyond that, the rest of them become black box, or black boxes, right? So that is it. But if you like what a decision tree does, for example, or what a support vector machine does, let me give these two as an example. When we do the theory of deep learning in that bootcamp that is coming up, deep learning bootcamp, you will realize that layer by layer, you are systematically extracting representations or in some sense features from the data and higher level features and higher level features or representations, right? In a systematic way. So a deep neural network layer by layer is a method to do the same thing, feature engine or feature extraction, except that you do it using machines because the problem domains are very hard to do. The same is true when we talk about, you you know whether it's true for like photographs and condolential networks of vision networks it's true also for transformers and so forth attention and for example when you apply an attention model to picture you ask this attention model that where is the we are looking for a building we are the buildings right so it will tell you that it found a building in the picture this is a building but then you look at the whole photo and you're not sure that this algorithm knows where the building is so you can use an attention model to say, or attention mechanism and say, show me where you suspect there is a building in this picture. And it will start highlighting those areas where it says, you should be searching for a building for, right? Because let's say that the building is just a way small, not quite visible or whatever it is. And the neural network is starting to pay attention to certain regions and it will then go produce a heat map for you showing that that's where the building is now if you think about it what happened you went through the neural network somehow went through a systematic process of feature extraction and learning what we are doing is we are learning the the feature engineering ourselves when you have tabular data and you can do feature engineering and solve the problem with simpler algorithms you have a gold standard because it's very interpretable you can explain it to customers and walk away with it okay so this is what we the complex models will do it automatically for us, but now the scope is for us to learn how we do it by ourselves. Yeah, how we do it by ourselves and also it has value. There are many, many situations, all these shiny algorithms, we will learn all of those. They may not be used for legal reasons and for a whole host of reasons you're forbidden or it was too dangerous to use them you are seeing what is happening with facial recognition for example it is a disaster it's become a tool for discrimination you should always search for interpretable simpler models to get the job done while complex models can do it for you you know you you lose a lot you have to sacrifice a lot the point within being able to predict and being able to interpret you want to have both and you won't have both if you use very complex models okay so I said just a quick question maybe maybe if I'm thinking it right if you go to the river again they were picture this one or the one here the one below that right this one just the way itself okay or the other one very midline I'm sorry the other one with the lining it yeah by the way as a best practice guys whenever you make your notebooks add a table of content to it you see this icon here it will help add a table of content and have a table of content on the left hand side so it is easy to navigate quite often you come across people's notebooks which are a mess you just keep scrolling up and down so yeah that's one here just want to validate if I'm thinking it right do you think if I normalize those blue point I will get a better middle line like a center line right i'm not sure if it's evenly distributed or not uh remember that when we were visual when we were doing the analysis we did normalize the point in the visualization we have not you can do that you'll get exactly the same graph okay uh you'll get the same but if you went back here it is a quality data transform this else and this polynomial expansion is a pipeline. You already normalize the . I can do this. So basic good hygiene, guys. Make sure you give a good descriptive title. Don't just leave the title accent. Give legends. Like, for example, this makes it clear what we're talking about. Model here. Do you see where my mouse is? We have a legend. Give legends. Label your axes and so on and so forth it is what true there is a school of thought like for example Edward theft like principle of least grid lines and so forth so I have made this with some places having grid lines and some not having grid lines all over the place. I think when I visualize the data, here also I have a grid line. So these days people are trying to remove clutter from diagrams or visualization so that you can see the information much more stocking okay so what I will do now is let me bring up the our version sorry one last question so when you made this middle like this midline right through the river so you separated out that river data using that T as the classifier? Yes, T is the target variable. Remember T is the target variable. It's always a target variable. Sir, in the next time, if we can have more visualization, like if we look at this picture below, you know, we can interpret that X2 variables are all over the place. below, you know, we can interpret that X2 variable all over the place. So it doesn't explain anything only X2 variable has inherited information. So if we can visualize those things that we can understand better. Yes, that's true. Okay, I'm trying to find where I kept my file for give me a moment guys I'll have to compile my latex file to generate the r version logistic regression support Thank you for telling me. Thank you. I'll fix it. I cleaned up. Okay, I'll do something. I should do something. Let me give you a preview of things going forward. Maybe we'll do it next time or will I do that? Partly because I can't find this note. Let me share the Oh, no, I shared the wrong one about the tree. One second. I want to show you something. This question that you can do better with more sophisticated algorithms I wanted to deal with that so Okay. So I'll give you a little bit of a walkthrough of what we are going to do in the coming weeks. We have just talked about trees. It may be worth asking how well do the trees do? So let me share that and how do I share it? Let me share the entire desk. Are you folks able to see my screen? I see one. Sure, please be a bit louder, please. Be able to share the notebook that you put today today is a little I haven't finished it yet the the Python Python when it said the is actually finished long ago the Python when I still have to do it I'm still adding a couple of things and the way I share it is that see if I shared the notebook itself in raw then you'll just copy paste the code so I'll make it harder for you I'll actually give you the chapter of my book that contains all of this so at least you'll have to key it in sure not but you'll get the idea so guys we are going to talk about trees let's look at the same data set are we seeing this data set guys oh by the way this is how it will look these are the chapters of my book so if you're looking this is the way you'll get it and I hope you agree that it is better to get it it is a well-written text rather than just a rough notebook nice looking yeah so this is the river data set. We use a decision tree that we learned about. This is just taking the data and bring basic statistics on it and visualization. Turns out there are many libraries in R that you can use to do it. We'll use one of them, please. And when you use trees, you come up with this kind of a let me zoom into this do you guys see this regions do you see how complex the decision bound boundaries are you know how complicated the regions are each of these rectangles is either zero or one would you agree that this looks very complicated doesn't those are extremely complicated extremely complicated then if you prune the tree and it becomes a little bit simpler but not very much compared to our interpretation using feature engineering or I mean this cannot even hold a candle to it yes it's a prune tree it's accurate but see it's still it is not very interpretable what it is trying to do more or less and if you look at the tree you know the decision tree I deliberately didn't put the labels here because it would be too hard to put it on a page this is the original decision tree on the left on the right and this is the pruned decision tree on the left still to me both of them look much more complex than what we are trying to do yes and you could do that and then obviously these are the metrics in art by the way this is once again there are tools in our that help you get your precision your recall your accuracy and so on and so forth the area under our OSE curve with decision tree also is good but if you look at our area under our OSE curve or we are actually look at this what is the area under ROC curve with the decision tree eighty eight point five percent isn't it guys yeah which is supposed to be hang on I'll zoom out eighty eight point five what is the accuracy or what is the area under ROC curve that we were getting? Let's go back and check. 90 plus. Roughly 95. Exactly right. So when we go and do that, we are getting an ROC area under ROC curve of 96%. Right. And the amazing thing is we used a very simple model you can't go simpler than logistic regression yeah right using a simple model we have just beaten what a much more complex model right and that is the beauty of feature engineering, guys. So let me go back to the tech. And you can zoom out. Let me zoom out. Yeah. So this is it. You can do the same thing using different libraries. Then you can use even more complex algorithms. We will talk about random forest, very popular in ensembles next Monday we'll do that when you use that right now random forest is almost the state-of-the-art for table data tabular data right and we would I wouldn't go into this by the way these are literally the notes that you'll get. It's still, even the state of the art algorithm just about comes close to our simple linear model. Isn't it? You see the area under ROC? 95%, we are at 96%, well close. But it took a state of the art black box model to achieve a comparable accuracy. So I hope I've convinced you of the point that feature engineering is a good thing, guys. Would you agree now? Or feel? Yes, of course. So that is that. Oh, I have a Python. So this chapter is ready actually. But now that I think about it, the decision tree chapter is ready. But and even this chapter was ready. Kate, you took this last time you took it. I did hand out both R and Python, isn't it? I have one in R, not sure if I have the one in Python. Have to look through Maya okay okay so your book yeah of my book chapter is it like a meep type concept me no it is not a book at this moment and now it's in the under publication against the leaves the chapter. Exactly as I'm writing. Yeah, as I'm writing this book, I'm making it available. 11.9 Python implementation of a decision tree and random forest for the river data. Yeah, I know but this feature engineering part. Yeah, I know, but this feature engineering part, that chapter is probably, if you look at it, maybe I did in Python, maybe I didn't. But I'll certainly add both of those. So any chapter will have implementations in both, guys. If you have ever written a book, you realize that writing a book is tedious. Good thing with writing a book is that things are clearly explained you have a reference right it is you notice that i use much more words than they are in the notebook notebooks are just i put descriptions but not as much in this as in this so you'll get the chapter in due time So you'll get the chapter in due time. Anything else guys? So I'm done. We have about 13 minutes. I'll take questions. I just had one question here. So when the deep learning workshop start, will it be on same day or parallel to the Deep learning is different Pradeep. It's a boot camp. The number of hours that you'll have to commit will be much more. So the way it will work is you'll give one evening, probably Wednesday or Thursday evening for theory. Just like this, we'll cover the theory, but then the lab work will cover the entire day of Saturday yeah that will be good because I have my some of the meetings so well I can be there these classes so that's why I wanted to check sell that it you know it can be on Thursday and workshop on Saturday that will work perfectly. Yeah, it will be like that. I'm not starting the boot camp yet. I thought let me make progress with this batch, train them enough because some of these people may also want to join the boot camp. I should probably start the registrations at least for the bootcamp are you going to take it are you guys planning to take it yes just just one question is it gonna be again the gcp based or deep learning part of deep learning part again it will be a choice you can do it in the you can do it in the cloud or you can do it on your local machines now most people don't have very powerful local machines yes so I would suggest that see by the time you get to deep learning rate the common man just does it in the cloud borrows the resources for a couple of hours that's it go because otherwise those machines are 30 40 thousand dollars just my workstation is like eight nine thousand dollars this one is seven thousand dollars the one in the white I have the black elephant here the white elephant in the office is more than ten thousand dollars so deep learning machines get very expensive the cluster you have in your office is that set up for deep learning no that is set up for big data and cloud like you really do stuff scaled out micro services and so forth right so yeah that and that is one massive like a six-figure cost I won't even quote it yeah another pay you than Google. Oh yeah, you can use it. Actually, that cluster is the cost of a house. That's what I'm thinking, right? I mean, for deep learning, if your setup is available, why not? Yeah, you could use it actually. So here's the thing. Obviously, the covet has hit support vectors hard enrollments are down but as it bounces back at their revenue stream continues lambda labs i was in the process of purchasing the latest lambda lab machine two reasons are weighted first is the covet situation the other was that i was waiting for the new gpus to come which lambda lab will catch up with it so we'll buy the next generation lambda lab server they pretty much put us back by about 60,000 then we can do all the labs locally but if I don't get it then we'll do it in the club that's that so guys yes I have a couple of requests to you the deep learning will start and then this workshop if you guys are liking this workshop please do spread the word out. At this moment, support vectors would like some enrollment. We are in a bit of a crisis. The enrollment this year because of COVID has been surprisingly low. Well, I would like to believe it's because of COVID and not because of my bad teaching. We like the class. Asif, you are fishing for compliments. You know and we all know what an excellent teacher you are. No, Kaliya, actually you will be surprised. One student who took the previous class, the ML100, because we are doing it virtually. His feedback was the class was completely useless. Oh, okay. So it isn't all positive. It is always a... No, no, no. He was taking virtual, right? He was probably watching something on the TV and then trying to follow the class. So you never know. Or I'll take it at face value. So, you know, there'll always be people who don't think it is so the fact is that there they exist at least one student who was frank enough to just say it was useless if he said it useless i wonder how many other people felt something we have to do this is you are the excellent teacher One of the best teachers I've ever met. I agree that I have really developed a lot of interest in this science. I started reading so much and that's why I came to this class again. Good. Thank you. Just continue that. Continue. The worry is that virtual is new for me i like to see people in the face you know see whether they are understanding or not or make sense of what is happening in the classroom so this whole thing is a new experience i'm myself learning to teach online so that's that i mean the advantage here is that the lectures immediately become available online as you know this is being live streamed so if anybody would like to set up some time to talk to you we can talk 15 minutes or so after this I have some time stop the recording and then we can talk anyway so I would be do send your friends if you like it do ahead. I have a small question. Well, as a coding? Yes, please go ahead. You know how when we're doing feature engineering, we're doing the vertical distance? Yes. How do you do distance from all directions? Because that's what I did on the Like, what is the nearest distance to the midline would be the perpendicular distance you know ideally you should have done that that's hard but why bother if just vertical distance is taking you all the way okay yeah no that's what i did on r but i was thinking about you know i want to make it more complicated you can can do that. You reach a point of diminishing return at some point. Okay. I'm stopping the recording, guys. One second.