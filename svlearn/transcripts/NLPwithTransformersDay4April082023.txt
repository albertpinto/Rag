 It's April the 8th, Saturday morning. A beautiful morning. It's more than 50 degrees, which is a really nice thing in California these days. This is our fourth session. The first session was sort of basic and introductory. The second two sessions, we did some real stuff. So I'm going to start by recapitulating what we have done so far. The first thing broadly is that we are in the world when it comes to natural language processing, the one statement that goes with it is it's transformers transformers all the way right just about anything you want there's a transformer for it for doing to recapitulate we did a search engine using ai semantic search engine and we noticed that we used a transformer architecture a very interesting architecture we used a siamese network and in in that Siamese network, we would pass in the way we would create semantic embeddings, train the semantic embeddings were by using a transformer. We will recap that and the whole thinking behind that. These transformers produce embeddings and those embeddings is what we use to do care nearest neighbors, and then we realized that we could do approximate care nearest neighbors and so forth. So we quick breezing stuff. We'll do it a little bit more in detail shortly. Then we went into the architecture of the original transformer, which is mentioned in the paper attention is all you need that paper is landmark paper and in a moment we'll go we'll review that paper too but basically it started the whole transformer revolution this architecture that was proposed in attention is all you need came out in december 2017 by the time people returned from their Christmas holidays and the New Year's holidays, people could sense that the world has changed effectively. Right after that, in very quick succession, a whole lot of things began to come out. They came the BERT, which is an architecture that we did in considerable detail. It's a bi-directional language model. It uses masking of words, random masking of words, and asking the transformer to infer what words have been masked. Again, we'll do it in a quick succession. Then came, a few months later, then came the sentence bird, which was literally the foundation of the whole AI-based semantic search. And it pretty much was the next big breakthrough in search engines. People have been doing linguistic search or keyword search for a very, very long time. The theory was really well established. Kiri was really well established. There are mature libraries like Lucene, Apache Lucene, as the core engine behind even larger open source projects like the Apache Solar and the Elasticsearch, which put even more layers of functionality on top of it and make it quite easy to use. They put a whole distributed application stack on it now then came the fact that you could do semantic embeddings and you could immediately see what a vast difference it makes in the quality of the responses instead of in the quality of the responses instead of being keywords driven this understands the intent what are you trying to say it can search for content that is very relevant but that contains none of the words that you actually mentioned in your text and it helps you do question answers and so forth so you could already see the beginnings of things like with these transformers of what we do. Chat GPT is a prompt. It's a question and answer, roughly speaking, framework today. You can see the beginnings of all those ideas with the coming in of transformers and with sentence word. One of the things we did in the lab is use sentence word for QA, question answers. One of the demos that Junaid made was to show how you can load a corpus of diabetes documents and a very short corpus, yet you could ask enlightening questions. What are some of the things you should do to avoid diabetes? And it comes up with very sensible answers. Things that were simply not possible with traditional search engines like Elasticsearch. So that, if anything, we started a course with that. The first lab we did was that. And the purpose was to show you how radically the world changes the moment you use transformers. Today, quite a few things have happened. First is these transformers and using transformers, the large language models have become bigger and bigger and bigger. As they become bigger, we find that their capabilities also get bigger. They start not only doing things with greater accuracy, but they develop what are called emergent abilities. For example, transformers at the scale of bird, bird base and bird large, they're not able to do arithmetic. You cannot say if I purchased 10 apples yesterday, and then today I ate two of the apples, but then my friend gave me six more apples, how many apples am I left with? It wouldn't be able to answer that question. But as the models get bigger, we notice that surprisingly, they are able to answer these questions. they are able to answer these questions. Now, sometimes they answer these questions somewhat wrongly, but they try to answer these questions. Then came other sort of small set of breakthroughs, a series of breakthroughs. There is the notion of the chain of thought, COT. What it does is it gives you a way to give little breadcrumbs to the transformers, to the large language models to reason through and when you give these little breadcrumbs using the chain of thought process then all of a sudden they're they're answering the arithmetic questions with fairly good accuracy accuracy. They're almost always right. And so with all of this, we realize a larger theme with transformers. We are realizing, especially in conversational transformers, which are becoming quite dominant these days, how or what input you give, the response seems to be dependent on that, obviously. But the quality of the response often depends on how you pose the question. So prompt engineering or prompt construction has become pretty much a cottage industry. On the web, you find a lot of websites literally devoted to the ways you can create prompts. I just ordered a couple scenes, a screen. What are we seeing? Are we seeing my drive? Yes. We are. So let's say that landscape visualization. So there is this paper which I would encourage you all to read. And perhaps given time, I would like to cover it. But I'll just give you a very quick preview of this paper. This paper brought out these beautiful diagrams of how the lost landscape looks and with a lot of pictures here, but what we are going to do is see it in real life. So there is this website called lostlandscape.com. I would suggest you go and look at it. And when you do look at it, you will see that these let me go to one of them oh we are not able to play with this. There was a way to play with this. Explore. All right, guys. So it is loading. Just to recap, what is a loss landscape? It shows you the loss with respect to the parameters of the model. Now you know that these deep learning models, they have hundreds, thousands, or millions, and now million looks too kiddish almost, kindergartenish. These days you're talking billions, and now trillion. But GPT-4 is supposedly a trillion parameter model right from what we know we we obviously uh don't know all the details about it but we do know that it is so now you realize that when you look at this there is a pr there is a clear minima do you see at the very bottom that pink place where the mouse is there is a very clear minima but at the same time the lost landscape has this um hills and valleys and if you look inside it and you imagine that you start somewhere up in the mountains a random point and you try to gradient descent it is quite likely that you might get stuck in one of those little ridges out there or in one of those little uh depressions ridges out there or in one of those little depressions, little valleys along the way, way up in the mountains still. And there is no guarantee that you will necessarily go to the minimum, the global minimum. You can get trapped in local minimum. And so this used to be considered a major problem with neural networks. Today, we don't think of it so much like that, because we have developed pretty good techniques to escape out of those local minima and make our way towards the global minima, or at least something that's close to the global minima. You don't have to be at the absolute global minimum, but you do get to something that is practically useful. But you see that these deep neural networks have a very fascinating and rich loss landscape. Now, with these landscapes, the next thing is the concept of attention. Before attention came, people used to do natural language processing, mostly with LSTMs, which are related to recurrent neural networks that are autoregressive or GRUs and so forth. Now, going back to what we said, so let's take the quintessential case. You want to translate from English to, let say french right or to hindi or something there are two phases to it you take the input and you encode it you encode the sentence into a hidden representation an abstract representation that abstract representation has a certain amount of memory associated to it and then the decoder decodes it has a certain amount of memory associated to it and then the decoder decodes it and forward generates the French representation or the Hindi representation and once you have an abstract representation a hidden representation you could do many things for example you could translate it into French you could translate it into Hindi but you could also do things that are similar but not probably so obvious you could translate it from plain text to poetry, right? To a sonnet or to a haiku. You could translate it into, I don't know, things like that. You could do it from one style to another. You could do a summarization, translate it into fewer words and so on and so forth. So you could do many things with a hidden representation. The trouble is when you use RNN style models, because data is input sequentially, the hidden state overwhelmingly represents what it has learned from the last few tokens. And it tends to forget what was learned from the earlier tokens. It sort of begins to erase. Very much like human beings, you realize that when we learn composition or diction in our schools, I don't know if it is taught. We'll ask the young man here, Abdullah, whether it's still true. One of the first things we are taught is to write short sentences for clarity. There has to be a reason to use long sentences. Why is it? There are people, for example, there was a great British writer, Matulay. His sentences were proverbially long. It could start from the beginning of a page and it could go to the end of the next page as one sentence. He was famous and yet apparently it was wonderful writing sometimes. We don't practice that anymore. One of the reasons is our attention spans are smaller, but more than that, it is far more likely that given our short amount of memory or attention span, by the time you reach the end of that sentence, you have forgotten exactly who it is that we are talking about. Right. So same thing happens with this recurrent neural network based models like LSTMs and so on and so forth. and neural network based models like LSTMs and so on and so forth. So the other problem with those architectures were that they could not be, they could not really benefit from the whole deep learning revolution. You couldn't create deep layers of recurrent networks. One or two layers is about as deep as you went. Like not all that was another. The third problem was in the computer vision space, already the people were benefiting from the accelerators, hardware accelerators, the graphic processing units, and the tensile processing units. So already you were having all these companies like Nvidia getting rich. So already you were having all these companies like Nvidia getting rich. But in the NLP world, there was no use because that parallelism couldn't be so well exploited. It was a recurrent neural network. You had to pass in one token at a time. The sequence of words had to be given sequentially. What transformer architecture did, or the big thing it brought is, it took the idea of attention, which had already been discovered, it was discovered in 2014. And people realize that attention is a useful concept. They were hybridizing the attention concept with recurrent neural networks and lstms and when they would hybridize the two ideas they would superimpose attention on those already they were seeing much better results right then so the general belief at the time was that yes uh rnns and lstms are the workhorses but there are ways to put it on steroids and attention is one way to put it on steroids. And then came the paper of 2017, which basically said, you know, surprisingly, you can do NLP, you can do an encoder decoder model architecture simply with attention heads you don't need rnns or lstms or grus at all that's a pretty remarkable statement because that was the choke point that was what forced you into a sequential way of thinking right and that is what prevented you in some sense from benefiting from hardware acceleration. So when you could put things through the attention head, it could all go in parallel. Not only that, you could have multiple attention heads. So you could look at that whole process of encoding from different perspectives in a way. One was more syntactically closer one was looking more i mean to to use common language though it's not really like that it's much more abstract but um sort of a rough intuition would be that one attention head is more grammatically sensitive another is more focusing on some other aspect of the input right and so they're working all in parallel all of these heads multi-head attention they are working all in parallel all of these heads multi-head attention they're all working in parallel and then of course you throw in the usual feed forward networks and a few residuals here and there right they're like peanut like i said one good way of thinking about them or i think about them somewhat wrong but or a bit tongue-in-cheek but it works for me is i think of the feed forward layers always as the peanut butter and jelly that you put in sandwiches right why do we need them just like you don't want to eat a sandwich without them in the morning you need them because obviously they are the ones that insert non-linearity. So if you want to have ultimately the universal approximator function behavior, you need feedforward networks to introduce your non-linearities. And you do that to create then better representation learning in the classic sense. They will take the input that goes into the feed forward and basically try to create higher order representations out of it so that the subsequent layer benefits from the higher order approximation right so that was the attention architect that was the architecture or the basic idea of the transformer which was the the original transformers are encoder decoder architecture, it has an encoder that produces a state that goes into the decoder the decoder is aggressive in some sense it takes the hidden state produces the first word, then that first word is fed back and along with the hidden state to produce the second and so on and so forth. And it goes on producing word after word after word, right. So, so remember these transformers are if you use the full auto the um the transformer the full architecture of the transformer you know that suppose you're generating text out of it whether you're doing a translation or whatever it is one word emitted at a time one word emitted at a time. You see that, right? You sort of generate one word at a time and you move with that. Just pause and think what it means. We are all so mesmerized with the latest developments of the large language models today. with the latest developments of the large language models today. People are, as you know, Microsoft wrote a paper called Sparks of Artificial General Intelligence. They claim they are seeing in GPT-4, which they have been playing around with a few. It's a pretty large paper. I believe it weighs in at 154 pages. Most of them are examples of prompts and their responses. It's a very easy read, but really enlightening read. But somehow it does make the researchers ask the question, are we seeing sparks of AGI? I don't think so, frankly. I feel that we are not seeing sparks of AGI but we are certainly in a land where we are seeing these large language models produce things surprisingly at the next level to the kind of task specific performance we were seeing so forth. If you look at the so-called call zero-shot learning that we do, there is always sort of a trick to it. For example, we notice how with sentence embeddings, we could create or with the classifiers, we could do, so zero-shot classifier, how does it work? You train a classifier to find some labels, ABC, how does it work you train a classifier to find some labels abc but then how do you do and then you say classify into pqr but what's that what's the what's the hidden trick we use we know that abc and pqr we can embed into a latent space and then instead of if we predict a we just find okay amongst pqr which is it nearest to, right? The statement, because the statement text is also going to go into the same embedding space. You go into the embedding space and instead of looking for proximity to ABC, you look for proximity to PQR, right? And that's a trick you use. So behind all this zero-shot learning, there's always a like there's always a trick like for example for the clip paper uh did i cover the clip paper in this one no so we'll do that maybe today we'll do that so what you do maybe today i'll use literally for clip so what we what they did is they said this is zero short learning we didn't need image and its labels to train to train a transformer to learn from images and its labels to train a transformer to learn from images. True, they didn't have to use an army of people to set and label images, but you know an army of people have already done that, because every time you create a web page, as you know as a web developer, good practices, you put an image and you put a caption there, alt, A-L-T, I-M-G, S-R-C is blah, A-L-T is blah, right? You're putting a caption there, alt, text to it. And so what Clip did is it just harvested all of these images with captions. And so it had label data. So when you, and they call it pre-processing or pre-training before you do the thing, right? But in reality, you end up with label data. So there's always a little bit of a trick. Now with this very large language model, so you are beginning to see emergent abilities, like the ability to do arithmetics and so forth. We are not really training for that. So it is a little bit of an extension, right? But we have to ask this question. Let's see, as I said, a transformer produces one word at a time in this autoregressive framework. And the next word it produces is conditional on the words it has produced and the context it was given the input. It is duly conditioned on both. So it is one word at a time machine. A one word at a time machine has not thought out the entirety of what it is going to say. And yet intelligent creatures, like for example, would you ever talk seriously right or perhaps to your boss or to your spouse right by first thinking one word and then thinking okay what should i say next and then the the thing you would get is watch what you're going to say next right so uh that's not intelligence has this thing that there is a there's a conceptualization and out of the conceptualization there is a verbalization of the concept with at this moment at least there is no evidence that it is a truly conceptualizing what it is going to say in one shot and then translating it into words you may argue that it is so but at least there's no clear evidence that it is to me it is still one word at a time machine right and that is why i'm skeptical that these things are agis or even have any but certainly they can fool us that They are far more intelligent than the transformers we are used to. And they are, like when they say they are sparks of AGI, well, not quite wrong either in the sense that it is impressing us. It's making us feel. And, for example, in artificial intelligence, there was this concept of the, what is it called, the Chinese room experiment, right? The evidence of weak. So first people thought that a strong intelligence, AGI, I mean, general intelligence has two categories, AI. The belief in strong intelligence means you have to show thinking. We don't even know what human thinking is. How can we conceptualize machines to think? There was historically, by the way, a company in Silicon Valley literally called Thinking Machines, a very high-flying machine with Minsky and the great AI gurus sitting there. Noble laureates came and worked there. That company is no more. It broke into two pieces. The hardware part became Sun. The software, the machine learning part went into Oracle. And then Oracle ended up eating up Sunny also. So now both of those groups are in Oracle except that they're different. The machine learning group is sort of, it is embedded, limited, it's embedded into the kernel, into the database kernel and Sun is still doing well, reasonably well. So the weaker form of intelligence that people put, AI, is that if you put a curtain and you ask questions, and if a human being is asking questions from one side of the curtain and cannot tell whether the other side of the curtain is a human or a machine, or is fooled into the belief that it's a human. Or you say that the machine has achieved a weak form of artificial intelligence. So we are in a very interesting place at this moment. So anyway, this is a small digression, and I'll move past this quickly. We're in a very interesting place. I don't know if you know that every single day, I think Amazon mentioned that their, what is that called, Alexa. Alexa receives dozens or hundreds of proposals, marriage proposals, right, from lonely people who are absolutely sure that these wonderful wise empathetic loving answers could only come from somebody with a beating heart with a large heart right yeah that's right now people have gone crazy with challenging That's right now people have gone crazy with chat gpt and they committed suicide Oh really Praveen is mentioning something very interesting for those of you are remote that similar behavior is happening with chat gpt to the extreme that people uh they're engaging with chat gpt and when they didn't get uh the same amount of a passionate response from chat gpt a a romantic response, they are committing suicide, right? So it speaks to how much these machines are fooling us into the belief that they are intelligent, right? So that whole question of weak intelligence, are we already getting them? The question is, who's asking the question? If you have a researcher ask the question, in two minutes, they come to the conclusion this thing is done, not even close. I remember when Chad GPT came out, I asked a question, which in turn there are a lot of other people asked. I asked a question, and actually somebody here in our audience, Chanda asked this question, not me. He asked this question, why is 7 not a prime number? And it immediately answered to Chanda that is 7 not a prime number? And it immediately answered to Chandler that 7 is not a prime number because, of course, a prime number is divisible by other numbers, is not divisible by other numbers except 1 itself. And 7 is, of course, divisible by 3 and 9, which is utter nonsense. So you see this one word at a time coming out, a sensible word. If you don't know arithmetic, if you don't know what frames are, it looks very logical, isn't it? But it was wrong. And now you ask the same question and it gets it right. So obviously there's a human in the loop somehow improving its training. No, more often they say it's memorizing. It sort of memorizes. That's the other. Yeah, because they ask even this one, the goat, grass, and that. Initially, first they bring out the right afterwards, they fixed it. They fixed it, yes. So, yeah, Prabin gave another beautiful example. Remember the story that a man has a goat, a lion, and some food. Now, it has to protect all three and cross a river. The goat will eat the grass, and the lion will eat the goat. So how do you cross it without either of the two mishaps happening? And there's a way to go sequence to go back and forth. When you give it a chat GPT, in the beginning, it got it wrong. It's a logical question. But after some time, it got it wrong. It's a logical question, but with, after some time it started giving the right answer. So two ways of looking at it. One is that somebody notices the wrong answer. There's a human in the loop. Now they're fed in the right answer as part of the training data, because it's going through continuous training. And it brings up actually a very deep question. Are these machines memorizing the answers? Because they have so many parameters, they have a trillion parameters. Can they effectively memorize? Now it turns out one can mathematically show that they don't memorize. At least they don't brute force memorize, but something else happens. See, if you train them on a certain kind of data right and they are making the condition on that the weights have shifted the parameters have learned from that what will happen if you ask a very similar question or that question it is predicting the next word at a time and the correct behavior should of it should be that given the weights, it should produce a response more or less like what it has been trained on, right? Because that would be the path of least error that would give you the least value of the last function. Isn't it? And so that brings up the whole question that given the way the weights are, effectively, for unique questions, if it produces exactly the answer that is correct, it looks like regurgitation. Has it regurgitated? Has it memorized? How can it regurgitate unless it has memorized? This again came out, I think one of you here were pointing out that somebody put in the source code of Samsung or something something a snippet of the source code and then it emitted out the rest of the source key isn't it like someone else i don't understand what was the input they basically like inputted the source code yeah and what i heard was um so instead of telling them that it leads somewhere else oh so in response to another person it regurgitated the source code yeah this is it right because source code is so highly structured that when you give long constructions of source code it has no choice but to adapt its ways, learn from it. But then when you give it any prompt that looks similar to that, it has no choice but to essentially try its best to reproduce it from its ways. Even though it hasn't memorized it, it has in some sense optimized itself in such a way that it will emit it out. Yeah. So that's why they're binding in some places, Walmart, they said they're binding in certain places. Yes, chat GPT. Yes, so companies, Praveen is mentioning, companies are banning it and the use of chat GPT in corporate environments. The last thing they want is people to feed it more and more of their secrets. So, Albert, go ahead. So you know, going back to your assignment, your original code, so you had this short sentence. So that's basically the embedding for representation. So is that similar to the attention also that we talked about? Or is it the same? You talked about attention. The those are the areas of attention or... Yes. Yeah. Albert's question is that we gave short sentences to these transformers and then it created these embeddings. How is that related to attention? The way it is related to attention is, attention was the mechanism used to see the relationship between the words, the semantic relationship. And because you're embedding it in a space where two sentences that are semantically similar should be nearby, right? So what does it mean that their attention heads should emit out a vector that are close to each other? And so attention is the mechanism that is used to get semantic understanding and project it into an embedding space close to each other. Right? All right, guys. So with all of that, we are in a place that transformers are on a roll at this particular moment, right? And for better or for worse, we are doing that. Now, what we did is I'll go through very quickly the theory that we went through we did the initial original transformer then we said that when you see transformers in the wild you notice an interesting pattern some transformer architectures they use only the encoder part of the classic the attention is all you need transformer the full encoder decoder architecture bird family in particular utilizes the encoder aspect of it is predominantly encoder based the gpt family is predominantly the decoder part of it are we together and that is why they are one word emitting at a time then the now what's the trade off between these two let's think of a trade-off uh between using and then of course there are architectures uh transformers that use both for example bard uses both not but but but brd uses not bird, Bart, B-A-R-T, uses, is it Bart? Or, Bart, yeah, uses Bart, and so on and so forth. So, there are many architectures, there's a mix. What's the trade-off between these two? I don't think I quite spoke on that, so I'll speak on that. See, when you use Bert, if you remember, how did we train Bert? When we reviewed the BERT paper, we noticed that the germ, the main crucial idea was, you take the encoder password, the transformer, and then, and of course you make it bigger, you have six heads, 12 multiheads and whatever, multi-attention heads and all of that. But at the end of it, the embedding was that. The word would get embodied, tokenized and embedded. It would go through the embedding was that the word would get embodied tokenized and embedded it would go through the emitting and even the embeddings were learned right you force the transformer to learn the embeddings then there was a segment embedding saying which of the two you you give two segments at a time two sentences at a time which of the two sentences a word belongs to and the third was position embedding what is the position of this word in the sentence so position embedding comes from the classic transformer idea now you you concatenate all of these three to create the full embedding of a token right the actual token embedding plus segment embedding this position embedding is the complete embedding for that token now you take a sentence with all its tokens you put a special token cls at the beginning and a special talking set between the two sentences right and then you shoot it through the butt right through the attention heads and some glue some peanut butter and jellies there feed forward networks there and then you get the output state now the weight now the whole question is how do you train it you need some task to train it so the task that they picked was a masked language what masked language model what it means is it's a really mouthful of a word for something quite simple. You would do what children are taught to do in schools, fill in the blank, right? What do we do? We know that the capital of California is blank. And now you have to guess what the capital of California is. We have all been through these exercises. And so it's like teaching that. It's a very effective educational exercises. And so it's like teaching that. It's a very effective educational methodology. And that's exactly what they did. They would take a vast corpus of documents and randomly suppress about 10%, 15% of the tokens. They would put a mask. And then they would say, ask the transformer, what was that? What was this? Guess which word was actually there. And for it to guess what word it is there, it's a classic thing that you can only do with attention because it needs context. Remember when we did the theory of attention, attention is based on what? Attention is, there is a context that forces you to put differential weights on differential tokens of the input. That is the gist of the attention. To recap again, when you are walking on a hike, you're enjoying the mountains, you're enjoying the stream going by and the trail and you're talking to your friend and all of a sudden you spy you hear a sound and all of a sudden the sky has disappeared and the stream has disappeared right and the only thing your attention is for where is that where is that predator isn't it so you know your whole visual field is narrowed to something else a search for something else so the context determines your attention where you where you put emphasis and that is attention that's the main idea of attention now when you when you put a mass word guessing it's a classic use of attention you have to pay attention to the rest of the words to a differential extent. You have to understand the semantics of the words in some sense to be able to tell what word was masked. So when you train it like that, what can you tell? This task is not something that you would actually use in real life. Maybe you would use, sometimes you do OCR and some words are gobbled and you may try to guess what word it is, but in the fact you do such a real thing says, is this document legal document or sports document? Right, or something like that. Use it for classification, use it for summarization, use it for all sorts of things. It turns out that when you train bird for this it is able to do very good classification for classification it's good because to classify text what do you need you need to pay attention to all the tokens that are fed in right or for entailment like what does one follow the other you need to pay attention to both the sentences and move forward with that so for that the encoder architecture works pretty good whereas if you're doing generative tasks like generating a passage of text right you realize that the autoregressive is better because you know you use the decoder part of it and it keeps on emitting word after word after word and keeps generating for you. So that is one way of separating the two parts out. See, when you're trying to generate one word after the other, you probably want to heavily emphasize the decoder because it will keep emitting word after word after word. It hasn't been, Word hasn't really been specifically trained for such tasks isn't it so that's the difference no this is this classification that's right but it focuses on the encoder part. See what happens is we talked about, and again, this is a recap. I mean, you can use GPD also for decoding, but I'm saying a bird, for example, is very good at finding mass width because it literally is what it's trained on, on whether these two sentences are similar. Similarity analysis, perfect for the encoder part of the architecture, because literally that's what you trained it on. Because what is the CLS token? What do you do? You feed it into a classifier. You can ask it a question, give it the probability that these two are similar or tell what is a mass word. It's literally very close to what it was trained on, sentiment analysis, isn't it? Whereas the decoder part, what is it well trained on? Translating from English to French, generating the text. Right? That is where decoders are good. So that's a very rough way of doing it. Of course, you can sort of hack one into the other and use it, but that's a simple dichotomy both uses transformer but they focus on only the encoder part because once they use transformer data both encoders no actually if you look at the bird architecture right the decoder is practically not there it's only the encoder but they use all transformers no they don't use the whole encoder decoder at all. Look at the bird, you'll see mostly is the encoder sitting there. See what do you do? You take this word segment and all of that pair, shoot it through the attention heads, throw in your feed forwards, maybe more attention heads feed forward, but ultimately sandwiches of those, right? And then at the top of it, what do you you get a hidden representation the cls and you feed the cls straight into the classifier that's it yeah so they don't use that they don't use the vanilla transform but there are certain transformers that use the whole of the transformer right but the most common ones that you hear about right they're actually using one or the other right so you can ask a quick question hey for the gpt don't they need some kind of encoding before they get to decoding part or oh yes yes the tokenizing and uh embedding of course you do uh and that is learned see remember that even in the decoder right decoder is nothing but the whole encoder right with two things the input is uh the input is not coming from uh generally like in the classic architecture input is not coming from the this part uh user it is actually coming from the hidden state and just a trigger you know just trigger the trigger will produce the first word you feed the first word back into the as input along with the hidden state and just a trigger, you know, just trigger. The trigger will produce the first word. You feed the first word back into the as input along with the hidden state and then it produces a second word. Now you take the first and second word feed it back into it, right? And like you keep moving forward like that. Now when you only use the decoder what you do is you actually it is nothing but the encoder but with the loop like whatever output it produces, you feed back in. Yeah, that makes sense. Because when I was reading the attention is all you need paper, I did not understand. Thank you for clarifying that because they've added on the output embedding, they've added a mass multi head retention. Yeah. I was thinking, why did they add a mast on this side? Right. To the multi head. So, yeah. Yeah. masked on this side right into the multi-head so yeah yeah is that why they have added it that is right the mass multi-head is simply to mask the words that it shouldn't pay attention to you know the future the words that are that are not yet you know the the uh like what it has translated is just one word so far right so that is it so that is your uh classic architectures now we went then why do we say that it's not encoder and decoder and only decoder heavy gpt why don't we do this let me do the gpt i've done but uh were you there in the bird session when we did that i was there so bird you saw it it's encoder heavy yes that i understood why don't i do this i just realized that i have not done the gpt architecture in detail why don't i do that because that will clarify all the doubts oh thank you so much let me do it and if we are planning to do that later in some other session that's also okay i don't want to detail this session it's okay we will do that either today or tomorrow or either tomorrow today or the next time. We'll definitely do that. Is generator and destructor the same as importer and decoder? Generator and? Destructors. Destructors. I've not heard of destructors. Oh, GAN. GAN are generators and discriminators. Yeah. So what happens is that, okay, so question here is, what's the relationship between the generator and the discriminant or discriminator of GAN to encoder-decoder? Actually, you don't think of it like that. You don't try to map one to the other. I'll explain to you what it means. See, in a GAN, this is a quick digression. A discriminator is just a classifier. Let's say that the basic quintessential idea is, suppose the generator is basically a counterfeiter right who has to counterfeit the currency of a country whose currency it has never seen let us say that you all of a sudden decide that you want to count it's a really good idea to counterfeit bhutani's currency right but you have never seen the bhutani's currency right So what happens is that you have a discriminator which is watching out, is the police in some sense. And when you give it in the beginning, you start training both of them. Now, let's say that even the cop is rather naive. So you first give it a genuine Bhutanese currency and it makes some random probability guess. it says it's a genuine or fake that produces a loss error function that law there's an error there and you back propagate the laws and so what will happen the discriminate with awaits will shift and learn something from it right what about the generator laws it also figures out that something is up. Right. It also shakes a little bit. But then the generator asks to produce a currency. It has no idea what Bhutanese currency looks like. It produces some random noise. It will make a picture like it will just give that and say this is it. Right. So, I mean, to make it a little bit more colorful not that it does that it produces random noise in the beginning but let's say that it puts a peacock on the on a paper and says this is butani's currency right then even that a naive cop knows that this cannot be put so it gives it a probability and it shoots it down now you you back prop the error, right? Now what happens? Suppose it said it's fake. The discriminator has now gotten a very small loss. It's happy. There isn't that much of things to propagate. It's sort of, it's feeling happy with the small losses, right? It says my weights are doing well for this fake thing. I caught the fake, right? And it's patting itself on the back. But what is the discriminator telling? I got it wrong. I mean, what the generator thinking, oops, I got caught. So it will change its weights. So next time it won't produce a peacock. Right? It will produce maybe a horse or something like that. I mean, it will slightly change. It will generate a different. But the amazing thing about gradient descent and backdrop is very quickly the generator without ever seeing Bhutanese currency will learn what a Bhutanese currency looks like and it will start producing it and giving it to the discriminator and the discriminator will also gradually smarten up because it's not so naive cop anymore. It has learned to tell the difference between the genuine one and the fake one. Why? Because it's not so naive cop anymore. It has learned to tell the difference between the genuine one and the fake one. Why? Because it's also getting genuine instances and the times that it gets right, it's told it's right. The times it gets wrong, it's told it's wrong. Right? And so forth. So it, and likewise for the fake ones, you randomly give genuine fake, genuine fake, and you train the discriminator to tell the difference apart. And so it's a cat and mouse game. The generator learns to produce more and more realistic fakes and the discriminator as a classifier gets better and better at telling them apart. So they have their own encoder and decoder feedback loop within them? Yeah, they have a feedback loop, but you don't think of it as encoder decoder because they're not like generator is not producing a hidden representation right when you encode you expect an encoded vector coming out of it in a way the generator does but then the the real data it is trying to imitate the real data rather than some encoded thing right so it's literally trying to imitate which is the purpose of GANs GANs can produce infinite infinitely many human faces that don't exist why because you have trained it to yeah you have trained the GAN to recognize human faces so if you give it a horse it will say no right so what happens gradually the generator begins to get smarter and it produces more and more human-like faces but once the gan has been trained what's that what's the whole point of a generator a generator it becomes a generator model that can produce infinitely many faces after that isn't it based on that it can go on producing faces and that's an amazing thing for something that started with just knowing nothing at all right and now when we talk about segment the meta paper the segment anything is that also based on that the which paper the segment the segment anything paper that meta just released. Oh, no, I have to read that. I'm sorry I haven't yet. But thank you for pointing actually let's post it to a slack. Network. I'll definitely read it tonight. That's okay. Okay, So so that's that's sort of a summary. Now let's go to semantic search. What do we do in semantic search how do we train to produce meaningful embeddings you deliberately take paired triplets right you take a corpus of triplets you take two two you take three sentences two sentences are genuinely together they they're similar, right? And the third sentence is deliberately some random sentence from the corpus, right? And so you say that the first sentence is a reference sentence, the second is similar, and the third is dissimilar, known to be dissimilar. Then what you do is you pass it through a transformer to create the hidden embedding, the embedding. And then what do you do? You look at the cosine distance between the reference and the similar statement. And you look at the cosine distance between the reference and the dissimilar state. And then you put a few classifier layers. And what do you want your classifier layers in the softmax to do? You want the classifier layers to minimize, I mean, to maximize the cosine similarity between the reference and the similar statement and minimize the cosine similarity between the reference and the dissimilar statement isn't it minimize the similarity cosine is a measure of similarity so maximize the distance minimize the similarity right and so you you can therefore easily construct the loss function associated with it and that is the basic idea you see how simple that idea is and yet that simple idea helps you create embeddings for sentences in such a way that basically you have in one moment it creates a revolution in AI search in fact the first big step forward since the days of keyword search right and that's the beauty of transformers like once you get the transformer idea right sometimes big ideas big changes can be done with what in hindsight looks like simple you know ways of looking at it now to complete that discussion of similarity uh so like semantic similarity we we took this idea that see isn't it ideal if we trained a bird to take two sentences two segments and pass it simultaneously together to have cross attention between them right it's a cross encoder effectively and let's say that you put a classifier the probability that you produce the larger potentials that you produce should be proportional to how similar these two statements are quite literally you can train it on that ideally so why don't we use that for similarity analysis so suppose i have a corpus of documents of sentences i get a query sentence why can't i look for similarity against all of this corpus of, let's say, a billion documents and find out which ones are the most similar, which ones produce the most? The problem is inference from a transformer is expensive. And for every query, if you have to do a billion comparisons through a transformer, you'll go bankrupt and the cloud providers will be smiling, right? So you don't want to do that. So the idea was the intermediate stage, and that is the whole point that the sentence bird does, it says that, no, what we do is by training the sentence bird like two sentence, by using this triplet siamese network we just create embeddings right embeddings that are semantically closer if sentences are closer and what we store are embeddings so now when i get a query i convert the query into an embedding and i'm not searching into a cartesian space i don't need a transformer for that i just need to do cosine distance comparisons which are way way cheaper than transformer inferences so that is good then we go one step further we know that when you do embedding something is lost i mean it is never as precise or accurate as doing direct inference through a transformer like that you lost something but you lose a little bit more because even to do vector comparisons against or cosine comparisons against a billion documents is not the wisest idea so then you bring then we brought in the idea of approximate nearest neighbors right which somehow bucketizes all the points or it partitions the embedding semantic space into sort of partitioned areas so that when you get a query, you go into that bucket and you just look for or you search for neighbors only in that bucket. The trouble with that, and we use all of these ideas, quantization of space and many, many ideas, we, resolution colors and quantizations and so forth. So there are many ways of doing it. And I believe I covered about five ways of doing ANNs. What ANNs bring you is speed. You don't have to do this massive, a billion comparisons. You do a far fewer number of comparisons and you get the neighbors but what do you lose if you look at search performance in terms of performance and recall i mean sorry precision and recall these two measures a precision being every single search result is really the relevant one and recall being every relevant search results that you know exists in the database in the corpus did show up in your result right so that's a measure that would be a measure of recall how how many of the most relevant ones genuinely relevant ones did show up in your results so one quick one so now the thing is you lo you lose both in ANN. Why? Because you're searching only a neighborhood of things. And so you miss out some things that are just beyond the boundary. And that may be very highly relevant, especially if your query happens to fall literally next to a boundary. Let's say that you happen to have a query that lands you in San Diego and you start looking for neighbors, but you can't cross over to Mexico. So all your neighbors will be Californians, including the guy living in Northern California, Eureka. Right? Whereas you missed out your neighbors just down the border. What's that city called? Tijuana. Tijuana. Tijuana. Tijuana. You missed your neighbors in Tijuana, who, by the way, are coming back and forth and helping you out with your, I don't know, backyard upgrade or your home improvements or whatever. It happens. or your home improvements or whatever. It happens. Or you cross over there all the time to get, I don't know, medicines, and US medicines is expensive. So whatever it is, you miss the neighbors. So that's the problem with ANN search. So there are two ways to address this issue. One is, and I use this metaphor of sand dollars and sand. If you're looking for sand dollars and sand, one way you can do is you can scoop a larger bucket of them. You want 10, just scoop enough sand that you expect at least 10 of those sand dollars to be there, but you also end up picking up a lot of sand. Now you have a problem. Two things, because you got scooped up more does it mean is it computationally more expensive a little bit but because anns are so cheap you can still go and scoop more now you scooped 50 results to get 10 really good ones 10 sand dollars problem is how do you sift through the sand now? Easy. Because now you can go back to the gold standard. What was the best, absolute best way of doing similarity analysis? Not by embedding, but by going back to the original transformer and doing cross-encoding, right? Two sentences at a time. So take a key query and every of the 50 results, pass it through, look at the probability, similarity measure, and then rank it. You re-rank the search results based on that. And then you pick the top 10, right? That's why you use a cross encoder for re-ranking. And when you do that, think about it. You got now, hopefully, the best of both worlds. You got speed. At the same time, you got your recall, hopefully, if you brought a big enough bucket, all the sand dollars are there, right? And a precision. Hopefully, when you re-rank, only things that are genuinely relevant were in the top 10. So both your precision and recall gets sort of sufficiently restored. And that is the whole architecture of putting the search engine, semantic search engine together. Now, one reason I started this workshop with the semantic search is observe a few points that now that you're familiar with it, that we should see. We had to put many ducks in a row to make it work, isn't it? Yet each of the ducks are things that in hindsight look simple. Now when I present it to you the way I presented it to you, I hope it almost looks intuitive that yes, that makes sense. would you agree guys yeah you would also observe the fact that it was multiple users of transformers we use a siamese network of transformers to create embeddings those embeddings we put it into a fast or an engine scan fast whatever in an engine in an database in an index retrieved it but then once again we use transformers to do cross encoding now this also opens up the path an interesting part by the way which i didn't cover see suppose the word is you just get one word embeddings like for example you get the word is, you just get one word embeddings, like, for example, you get the word cat. Right. Are you talking about computer aided tomography? Are you talking about cat, the cute animal? Right. So there's not enough context. So sometimes you can have a situation where AI search may underperform just keyword search. Great. And so one of the things people sometimes do is they use sort of a sparse vector presentation, or they just use ordinary keyword search results. They pipeline it. Remember I said you scoop a bucket of sand, but you also scoop a bucket of sand from elsewhere, from let's say your But you also scoop a bucket of sand from elsewhere, from let's say your keyword search engine. Because you know that ultimately you have the gold standard, the cross encoder sitting there as a gatekeeper, and it will re-rank it and it will make sure nonsense doesn't go through. So that is also sometimes done. Yeah, Albert, your question now. Can we take every piece of the puzzle, not every piece, I take that back. Quite often the crucial pieces are independent ideas using transformers. Like in this semantic search, the embedding is through transformers, the cross encoding is a transformer. So that transformers play roles in two places. The Siamese network as well as the cross encoder. The last thing I would say is, guys, people often tend to ignore the last step, the cross encoding and the use of ANN. A lot of vendors are giving you so-called semantic search engines or AI search engines and all they're doing is vector search right that is not the whole way why because you will have problems with precision and recall not to mention that if you're doing direct vector search you also throw performance out of the window right it will be slow all right guys so I'll stop with that and let's take how much of a break 15 minutes 20 minutes break 10 minutes okay let's take a 10 minutes break i'll have my coffee and we'll start after that so guys we did uh do a review of the theoretical landscape we covered. We said transformers, transformers all the way, that's our mantra. You can use it to solve lots of problems by creating a chain of transformers and other things that you know. We took as a classic example, we did the search. Now, in a subsequent lab, we are going to do more interesting examples, but we took a break from search. After doing search, I wanted us to become skilled with the tools we are using. In particular, the toolset that we want to use for transformers, and that has become a defective standard, is the Hugging Face library. Now, the Hugging Face library, interestingly, is written in Rust. Makes it pretty fast. As you know, Rust is a language that tries to have all the speed of C, but none of its security or memory leak issues security vulnerabilities so it's a it's a pretty robust library and what it does amongst other things is it gives you a uniform easy interface to transformers irrespective of whether they are written in pytTorch or TensorFlow. The two dominant deep learning libraries. A big thing from that. Now, people use different, they sort of bounce back and forth between TensorFlow and PyTorch. In my case, if you guys folks remember, this same workshop, long, long ago, I used to teach with TensorFlow. Right, but somewhere along the line I switched over to PyTorch simply because personally I find it to be more Pythonic, more research friendly if you want to do some experimentation and things, it is just nicer. TensorFlow is practically dead. Yeah. tensorflow has had a peculiar you know that anyway this is on records i shouldn't say it it has had an interesting history so apparently inside google so i'm told uh they they created a neural architecture that and the code base was rather uh let's say left a lot of scope for improvement so they said we're going to redo it and this time we're doing it in open source so they came out with tensorflow 1.0 except that perhaps the same people who created the previous thing created this it was very low level right so it is like you want to ride a bicycle here is the chain here here is are the wheels. Here is a seat. Here is a post. Go build it. And then right there, right there. Like Ikea. Like Ikea. Yeah, exactly. It became very popular. TensorFlow immediately took over the world. It was considered a necessary evil in many ways because it was really a very powerful, very good framework, but you had to do a lot of things by hand. So that created the open source community to step forward and create the Keras, which is basically a layer of sanity over TensorFlow 1.0. And it's a high level abstraction that did the simple thing simply. Then TensorFlow team adopted Keras as a de facto interface for 2.0. And TensorFlow was sort of two-phase. They would first make the execution graph and then you would run the execution graph. It made debugging harder. Like, you know, you can't put breakpoints and so forth. It wasn't either execution, Py touch is eager execution so then obviously a lot of us move to pi torch and then tensorflow also introduced eager execution try to catch up i don't know maybe a lot of people say that it's more production ready but i have never looked back from PyTorch. So what really happens is you take PyTorch, I mean, I'll tell you what I do. I take PyTorch models. I then forward generate into ONNX and TF, this thing serve, what is it called? The RT serve, what is this called? Titan, not Titan. The Nvidia server, the model server, inference server, there is this thing, I'll remember. the model server, inference server, there is this thing, I'll remember, Triton, Triton or something, you load it onto that. And you run it and it runs blazing fast. Right. So that's what my team does. So anyway, this question arose, which of the two to pick anyway, hugging faces is a layer overbought, It can work with both. Now the HuggingFace API has three core libraries. I would say four core libraries, four libraries, but technically three libraries. One is the datasets library.asets library is just wonderful. People produce data in all sorts of formats. This guy has CSV, that guy has JSON. It has this naming convention for the features and that for label, the target variable and this and that. In some sense, it brings a method to the madness. And it gives you a simple one-liner to get Datasets, load Datasets. We are going to learn about that data set library and see how convenient it is to use that library. Amongst its beautiful features are so you know if you have a library to deal with data sets, it should have the following good characteristics. First, there should be a large repository of data sets that you can access and we will see that it has access as of today morning i believe it was 29 000 data sets it could access those data sets included text they included audio they included video they included all sorts of things rich rich library for you to play with and we'll play with one of those libraries today one one of the data sets today. We will take the problem of classification and in classification, the simplest problem that are the quintessential poster child problem you solve is sentiment analysis. But we'll go beyond just positive and negative sentiment. We'll have a spectrum of six sentiments, sorrow, sadness, anger, so on and so forth. All of those emotions uh will have so for that we'll use the emotions data set right and we'll see how easy it is to use the data set okay so there's a rich thing secondly the api should be dead simple let us say that it is dead simple but a work in progress not everything is obvious i mean some things you have to work a little bit harder it makes common things dead simple but when you try to do something else you have to work a little bit longer or you know poke through the documentation the third advantage it has is reading from standard file format csv json dead simple. It is right, just like pandas library, and the fourth is, it should have interoperate operability with other libraries. So the data sets, for example, the most common data set loading framework that we know of in data sciences. The data frame right pandas data frame, And it is absolutely interoperable with it. And because Panda's data frame itself is interoperable with many things. For example, with Spark data frame. Now you can start chaining the thoughts together. Right? You could generate data in Spark, by Spark, and then gradually have a bridge over to this data set. And so you can do all that. Of course, you can write it as a file and then load it in data set, or you can directly with a couple of lines of code do a translation straight to the data set and save it. So those are the advantages of using the data set core library. Its internal representation of the data set is also pretty good. I believe it's arrow. It uses arrow as its native format, unless you give it something else. Arrow is a code based written in C++. It's a very efficient and high performance data format and gaining a lot of adoption. So one news that you may not be aware of is Pandas 2 came out. I don't know if you noticed that. Was it Pandas 2 or Pandas next version? I think it's Pandas 2. Pandas 2 came out and guess what? Pandas 2 has support for Arrow as a backend, makes it much faster and pretty much it sort of decreases the gap between uh the pandas and polar come again what is what is arrow so see when you represent data in memory what used to happen is pandas would typically use numpy arrays NumPy arrays is far better than using Python data types because Python data types less dictionaries are horrendously slow. It's an interpreted language. So one way that people used it is they know that it's very hard to beat Fortran. So the Fortran's basic linear algebra package, BLAST and all of that, Fortran arrays are super duper fast. NumPy is a Python library on Fortran and C. So when you create your data structure in NumPy, you're basically using Fortran and so forth. And obviously because of that, it was the defector standard. Python basically pandasame is a mindset that has pervaded statisticians and data scientists for ages, since antiquity. Antiquity being in computer science world, maybe two decades, right? Two, three decades. So it's been there. So NumPy though has certain limitations. Data, when you represent it in columnar format, you can do it in a much more compressed way. Because for example, taking project, one of the common things you do is you get a data set with many features, but you want to drop some features and keep only some. When you do it with a matrix, you literally have to have another matrix ready for it right whereas when your data is stored columnarily it's very easy to pick a few columns and you see that right and so columnar data structures have been known in the analytics world to be the most efficient format of data storage for analysis right? You see that for that reason, aggregation along columns, some totally sweet, isn't it? If data is stored along columns, you have the locality of reference, the whole thing can be loaded into into the CPU cache. And in one zip, you compute the sum or aggregation, or whatever things that you're doing on a feature. So for that reason, columnar formats are better arrow is a columnar format. Pandas 2.0 supports columnar format. I believe Dataset, correct me guys if I'm wrong, or Datasets to the best of my knowledge is native columnar, native columnar arrow format, right? Again, speaks to its efficiency. Now, so that is one of the core libraries of Hugging Face. The other core library of Hugging Face is the Tokenizers. It's part of the Transformer library, but we consider it as a core part of it, the Tokenizers. What in the world are Tokenizers? Who would enlighten us? What are Tokenizers? Say that again? did someone respond? what are tokenizers? similar yeah those are lexers parsers they do the tokenization see guys tokenization the word comes from um obviously uh if you are doing compilers etc lexing parsinging, the words. So let's put it this way. You have text. But if you look at text, there are infinitely many texts. But you need to break it down into constituent parts, a smaller vocabulary. Those smaller vocabulary components are called tokens. Now, for example, when you write a language, let's say a language like C, what does your compiler do? The first part that it does is there's a lexa parser. It will take your text, chop it, chop, chop, chop into tokens, the tokens that it understands because the grammar admits only so many words in the vocabulary and each of the tokens must belong to it. Otherwise it will throw a compiler error. And not only that, then there are syntactic rules on how you can put those things together into that. And that's the grammar of the language. Now in NLP, you don't really go so much into the grammar of the language. But you do still tokenize the text into constituent parts. The question is, what are those parts? And those parts are tokens. And now what is a token though? It turns out that the answer to that question is a little bit more interesting, more interesting more interesting than you think but you may think word you may think sentence is a token but there are languages that don't exactly admit the notion of a sentence right they don't quite have very strict definitions of sentence boundaries then well not that then what? Words? Well word looks like a logical way to break it up and it's somewhat. The trouble with words is that there are two problems with words. Words are many. The English vocabulary is almost 5 million or 3 million unique words, growing 5 million perhaps. Most of those words are rarely used. The most common words are about, I don't know, 10 to 20,000 words, isn't it? isn't it so you know guys that you can learn any language and be reasonably fluent with that language by the time you have learned the first 200 words right you can be reasonably conversant with it by the time you memorize only a thousand words you're doing very well you're pretty much at the high school level or so. And by the time you do 5,000 words, by the way, I remember that to come to graduate school, you had to take the GRE exam. And there is a guide to GRE called the Barons. And it used to have a vocabulary of 5,000 words. And if you really knew all those 5,000 words, it would absolutely ace your GRE verbal section. Right. So it just shows you that 5000 words puts you into the graduate school category. Isn't it in that language? So there is, in other words, a long tail distribution here most often use words are few so what do you want to do do you really want to use a vocabulary of 5000 words because in machine learning you have to do something called one hot encoding for categorical variables a word is a categorical variable it's not a measure it's not a number 3.65 now it is like you have to identify an animal, parrot, cat, dog, horse, for animals. It's a categorical. So any token is a word. Which word is a question you can ask. But if there are three million words or five million words, you will have to one-hot encode it into a vector with five million dimensions. What's wrong with five million dimensions what's wrong with five million dimensions if nothing else you'll be you'll be buying a lot of hard disk space and memory to load it isn't it it's wasteful and computationally is just terribly wasteful to do matrix computations with you know five five five hundred five million dimensions especially when that's such a sparse sparse matrix the it's not good but there is actually a further problem certain languages are what is called augmentative or there's a word for it in linguistics what it means is you you can you you have a concept in India, they use the word Sandhi. And Sanskrit is very common. So you can have an entire sentence made up of just one word. Because you have conjoined all the words together. Right. So then the reader is supposed to know where the boundaries of where to tokenize it, how to mentally tokenize it into word parts which individually have meaning. And that is a clue, right? Finland has the same issue. Japanese has the same issue. Right? You can go on concatenating words together into a word of arbitrary length right and i believe german has a similar issue oh okay nice right so um yeah pravin is mentioning that openai has a tool for tokenization that it will do. But today we'll focus on the Hugging Faces tokenizer library. Actually, why don't we take this as a homework? Guys, explore the OpenAI's tokenizing library also. So whenever you have an NLP library, they will always have tokenizers to break it up into pieces. Spacey, for example, has a tokenizer. Spacey was what we used in the last iteration of this course, a very popular, very good library, but now they are becoming, they are upgrading themselves to be transformer compatible. spaCy 3 is transformer integrated. So therefore it has value if you're using spaCy, very fast library. And so every library, NLTK has its tokenizers and so on and so every library nltk has this tokenizers and so on and so forth but we'll focus on the hugging face tokenizers very good exercise is to see how other tokenizing libraries work so tokenizer is the second piece but from this fact that words can be conjoined we get an idea that maybe the fundamental unit of the vocabulary shouldn't be word but the sort of atomic word pieces word segments or word parts that should be and i'll give you a further hint suppose you use the word just look at the word tokenizer itself there is token plus iser the iser word can be used used as a suffix to many other words. Can you think of what other words? Stabilizer. Yeah, and so forth. Each one of you can think of a word which ends in ISA. So you can really think that it's actually what does ISA do? The doer of it. The maker of tokens, the maker of stability. So you should really take them as two different words, token and ISA, word parts, right? And likewise, singular, plural. What if I add a S to it? Tokenizer, tokenizers, token, tokens, right? So the plural itself gives you a clue that you don't want the singular and the plural both to be sitting in the vocabulary. It's a waste. So you could have a word part that is a word segment that is just the suffix s and the suffix eiser. So that is beginning to give you a sense of how better to construct a vocabulary. We'll go through that exercise today. The third part of the Hugging Face core library is the transformer themselves, the transformers, the models. Now there is a rich variety of transformers there, but here is the interesting thing. Because a transformer, if you remember remember we said but you could use bird for classification you could use bird for figuring or fill in the blanks finding the missing the mass word you could use bird for entailment whether the second segment sentence actually makes sense as a follow-up to the first thing right for question answer things like that you can do but model for anything. So how come one transformer model can do all these things? And the answer to that, so if you look at cross encoder, for example, you give it two sentences and it produces the probability that they are similar, right? You give it a classification task, you give it something, and you ask, what is the sentiment? then you don't give it the second sentence you just pad it up with empty but you're asking the word to produce a probability for a emotion it is producing let's say a softmax probability for some emotion positive negative anger hate anger sadness joy whatever surprise whatever right how could you do that you're using the same transformer architecture for many things and so the way to think about it is this see what you do is and i'll give you the big picture you just look at the transformer without the head right so a headless transformer right now it's a little bit of a gory image. Don't think of headless people like the zombies walking around. Right? But it's somewhat like that. It's like a zombie walking around. It's very capable of something. But something is the head. You need to screw in a head, appropriate head, for it to do anything. So one very easy example that i think of is you know you get electric drills this called these electric drills can you do anything with the drill you can't do anything with the drill till you put a drill bit into it isn't it now based on what drill bit you put into it it will either you know you can use it for a whole variety of purposes do you see that so that for me that visual uh metaphor uh makes idiom and sort of make i don't know is it is it metaphor i think metaphor would be the right word isn't it ktm so that metaphor is what i carry in my head it's a drill without the bits right and you need to attach different bits to do different things bits right and you need to attach different bits to do different things so that different thing is called the head right so now how in the world does that make sense see what happens is that the transformer ultimately what did it do it created a hidden state representation all the tokens that go through the all of those multi heads and so forth take birth and what do they do they all create their vectors right the vectors get produced and of course the CLS token the special token also produces this vector so each token has a vector representation or you can call it a tensor representation but let's say vector representation to be intuitive now these vector representations are there. After that, let's say you're doing a classifier. You just take the vector coming out of the CLS and feed it into a classifier. Now, how do you do a classifier? Well, forget about transformers. How would you do a classifier? Maybe one example is you would just take a softmax, right? And into the softmax layer, you would feed in the input vector and output come because it's a softmax, it would produce probabilities for each of the cat probability is this, dog probability is this, horse probability is this, and whichever is the highest, you say that's it, it's a cat, isn't it? And softmax, of course course exaggerates a bigger number so that's it's a classifier is that now that is the simplest classifier you can put right softmax or if it is a binary classification you can just put a logistic classifier there right uh logistic or something now comes the next part you could do even better you could say i will take that and actually feed it through a few layers of my peanut butter and jelly what are those feed forward layers you may say that well let me learn some even higher orders of a representation from the output of the transformer body and then last layer is the sortness so you can you can games with it. You can do fancy stuff with it. And it's up to you how fancy you can get. And who determines what is the right way to do it? How do we determine which is the right way to do it? Like, should I just use the softmax? Or while you're at it, why not throw 10 more layers of feed forward and then put a softmax? How would you decide which one to go with? How would you decide which one to go with? Yeah the results speak. So what you have to do is you have to look at the measure, goodness of a model measure, something like either accuracy or F1 score or precision or recall based on the context you pick that measure and you basically the number the archetype the head the shape of the head how many layers there are whatever etc it is a hyper parameter of the model isn't it to put three layers or five layers to put no layers just a soft max one layer just softness that is a decision that is a hyper parameter of the model you have to do it and figure out what works best ultimately who decides the data decides that validation set decides which is the right approach to to deal with it and then again you can get fancy you can have residual links and so on and so forth you could do all of that up to you but the bottom line is i will just use the word in general classifier head to classifier similarity head let's say it's cosine similarity head to do cosine similar and so on and so forth you can apply a screener head to that so that is your transformer library with a catch each of these transformers what are they expecting they are expecting input vectors right embeddings themselves right uh they are not expecting words as in language text so what are they expecting token embeddings right tokens and embedded already nicely indexed and given to you. They don't expect the word CAT, they expect 49 hot encoded to whatever 49 becomes in the vocabulary, given the vocabulary. Assuming that the CAT word corresponds to the token 49, token ID 49, you usually call it the input ID 49. So that is the architecture. So it will be a sandwich of three things. Transformer will be the model will be in the center at the beginning. First, the text comes in, input doesn't go and hit the transformer. Input goes and hits the tokenizer. It gets tokenizer to input IDs and one one out encoding and so forth and further then it gets hit the transformer transformer takes that and further adds positional embeddings segment embedding and whatnot right and so forth it does that and the tokenizer has already added something small it has added attention actually segment embedding and attention layers, it's already added. Position embedding, the transformer will add, shoot it through the attention heads and its feed forwards and so forth. And out will come a hidden representation. I call it, I tend to call those words in a more general sense embeddings. Not many people would use that word embedding in such a general sense, but to me,'s embedding embedding is a vector in a latent space, right in a hidden space. It is a. These are vectors. Each word becomes a vector in its own space. Each token becomes, and that goes into the final head that you screw up for whatever purpose you want. question yes they are they are tokens and we'll see it right now so today is the practice day right so see guys now now let's get into the labs uh i give you the big picture because i wanted you to have this context in mind as we get into the labs now over these three core libraries namely the data set the tokenizers the the the models right uh you realize that last saturday we sort of danced away through the rose garden. We didn't have to deal with any of those. What did we use? We used pipeline. You remember, here is a pipeline for classification. Here is a pipeline for this sentiment analysis. Here is a pipeline to do a named entity resolution. Here is a pipeline to do whatever it is. There's so many. I believe the last I checked, there were almost 16 or 20 pre-cooked pipelines resolution here is a pipeline to do whatever it is there's so many I believe the last I checked there were almost 16 or 20 pre-cooked pipelines that hugging faces comes in so those are the common tasks so now that you look at it like that what what are those pipelines done when somebody has that pipeline what has he done somebody has sat down and put these things together for you somebody has sat down and put these things together for you. Isn't it? So last time we used the pipeline cycle. Now your first response should be, when you solve a practical problem, always baseline with a pipeline because it will give you a rough and ready answer of how much accuracy you are getting or whatever your measure is, accuracy, F1 score, precision, recall recall whatever matters how well it is doing start with a given pipeline right and now you can change the models you can try different models as arguments to the pipeline you can do that you will get some idea there is actually a significant degree of adaptability there flexibility there because in the pipeline remember we just gave the task name what is the task it's a sentiment analysis but if you look carefully at the pipeline API you will notice and that is what we are going to do today dig into the details you will notice that it also gives you the choice of specifying the tokenizer and specifying the model and the classifier right so all the pieces you can specify what it does is it glues them together for you properly because that's a bit of code you when you do it by hand you have to uh struggle with it right yes no no for everything all transformers sentence forget about sentence transform for the timing that was only for search i'm saying all transformers right for all tasks so pipelines is so they are god-given pipelines or rather hugging faces given pipelines and their pipelines we will learn to build our own pipeline today so with that what people have done as I said is they have contributed a lot of models that have been trained on something so when they give a model it is what is it it's a neural architecture but it is also the weights because ultimately when you train a neural network it's a fancy way of saying is I have magically determined the best set of weights that will get your job done. Some job done, isn't it? So it is no different from some fairy just moving a magic wand and telling you these are the best weights. Equivalent, so long as it works. But what you get are the weights for a model those a set of weights there is a more technically what you say it is a checkpoint the word checkpoint is often used as that you're downloading a checkpoint when you people say that i'm downloading a model in reality what they're doing is or more precise way of saying it is a bird is the model let's say but but checkpoint is bird uncased let us say the base uncased so what it means is i take the bird base model means bird has a larger model with a lot more. I believe BERT base has six attention heads of 12 and the BERT large has 12 or 24. I forget the exact numbers, but one is much bigger than the other. Lots of attention heads and layers. So the BERT base is the basic model or BERT large is the bigger architecture. basic model, or BERT large is the bigger architecture. But given the architecture, you have trained it on some data, right? When you say that I've trained it on a corpus, BERT base uncased, what does it mean? You have trained it with input in which everything has been turned to lowercase. Uncased means you don't distinguish between small and big cases in languages that have small and big cases, for example, Hindi doesn't have small and big cases. Right, and I think many languages don't have it right, for example, in my view, Hebrew doesn't have it. Middle Eastern languages, none of them have it. Sanskrit doesn't have it. Indian languages also tend not to have it. But European languages tend to have small and big cases, upper cases. So it'll bring it down. Now, typically when it says uncased, usually these models are also, like BERT, for example, that BERT, BASE, uncased. If you read the documentation, so associated with all these transformers, there's a convention. If you read the documentation, so associated with all these transformers, there's a convention. We call it the model card. A model card is a standard. Think of it as a billboard, right? Or a reference card that tells what the model is, right? And how it's been trained. So it will say we have used word Bayes, we have used Uncaged. And the corpus we trained it on is english corpus why because english is the dominant language in the data set that is accessible or publicly available other languages are far less represented right now somebody told me that actually french is more spoken in the world than english is that true that's what the french say Is that true? That's what the French say. Right. But for some reason, the French corpus is not, doesn't seem to be as big. And by the time you go to a corpus of like Tibetan, now you're getting into trouble. Smaller corpuses. How many of you in the room know Tibetan? No. okay. So you look at the model card, right? And what has happened is when you download, what you're effectively downloading is not just the architecture part, you're downloading actually the weights that has already been trained. That's why you call these models, these checkpoints, the checkpoints represents a word that we use, pre-trained. You download a pre-trained model. Why? Because training a model from scratch is horrendously expensive. So what I'm alluding to is the concept of transfer learning. So we'll start there today. So the top five sources of learning is Mandarin, Chinese, Spanish, English. We ended with, I'll just recap for a moment. We ended with visual QA. Let me remind you what we did. We gave it a, Let me remind you what we did. We said that transformers are capable of great things. Amongst the great things they're capable of, you can give it a picture like this and you can say, what is this animal? And it will tell you it's a snow leopard. Isn't that amazing? You can give it any picture. And how easy was it with the hugging face pipeline? Now think about this line, guys. After you do what we are going to do today, you realize that it's just powerful to invoke a model with one line and be able to use it, isn't it? Image classifier can immediately tell you what this image is and right away it tells you it's a snow leopard with 93 percent probability and it gives another five percent probability to just leopard between 93 and a half plus five almost six no it is almost 99 percent probability and And then jaguar, lynx, cheetah, which look like leopards, are far, far less. How very accurate it is. The other example we took is you can give it a picture and ask it questions about the picture. Look at this code. We are creating another pipeline, one line pipeline for visual question answer you give it an image and you ask what is the invoice number in the image right now the old way used to be that you would do some segmentation right you would zoom into that area put a box around it then do a specific recognition of that particular OCR or something like that right but now you're not giving it any hints you're saying what is the invoice number I don't know how many of you are impressed but I am that it can tell you that the invoice number is this us001 right why don't we run it and see let's do that let me run this and what is the name in the upside name pipeline i have to go to the top and run everything one second yeah i think the link to the photo has expired for the snow leopard when i ran that it had an issue why is it working for me hang on let me see oh no i have replaced it with that so i have to give you guys the new notebook okay i'll give you the new notebook so at this moment just point to any picture and you'll see okay so this is that now let me run this. Oh, it turns out by the way, I'm using a new machine. So I too am downloading it. And what would just happen here? Did it fail or do something weird? If you provide it without word boxes in the pipeline will but is it not? Oh, okay. On this machine guys, I have to install the libraries. Give me a moment. Why don't I do that? I have to go and install these libraries. So why don't I take a rain check on it because I need to go to the server and run this command.'ll show it to you after lunch i will do that this is the new machine by the way that sukpal helped me build so my thanks to sukpal by the way sukpal has also built a machine for abhijit really beautiful you should see his machine i'm envious now i want to exchange mine with his right so 40 90 is just yeah yeah and now he's beginning to hate his mac yeah it's very better it's very better right so yeah you can see what is the total amount and it says 154 dollars and 0.6 cents see what is the total amount and it says 154 dollars and 0.6 cents what is that like in the invoice what is the amount it's able to answer but do you notice that it's well if you look at the image if you look at the image there is no word there's an amount column but it is not obvious in the amount column there are many amounts isn't it so it takes some level of inference to know that this is the one that you have to answer right then you can ask what is the invoice date and the invoice date it gets wrong actually so is the limitation invoice date is 11 slash 02 slash 2019 and it interprets the slash as one. So there's a little bit of an error here, which speaks to the OCR library behind it. Then you say, who is it billed to? Now this is an invoice. Billed to is John Smith right here. And it figures that out. And you can go on asking questions. In the afternoon, I'll install the library and you can see. Who is serving the invoice? If you look at this, it's not a question. It right here and it figures that out and you can go on asking questions in the afternoon i'll install the library and you can see who is serving the invoice if you look at this it's not so obvious who is serving the invoice right there's a big invoice there's an address it doesn't say from right but it infers what it is from and we didn't train it i don't know if you're impressed i am like somebody repaint pre-trained yes somebody trained it yes yeah and so that speaks to the power of transfer learning you benefit from other people's hard work so in that case people's hard work. Go ahead. I was just saying that in that case, isn't that that this whole transformer or the model is going to go towards a global repository where you have models trained by and it's being trained by everybody. And that's where you Yeah, I think faces that you pause. Yeah, yeah, that is that and that's where yeah hugging face is that deposit yeah yeah that is that is literally what hugging face is is the right now we are downloading that right versus it's more towards those models are being hosted also so they're hosted no they're hosted so one of the things we will learn in this course is when you build your own perfect model how to contribute it back to hugging face so that others can benefit from it so see what happens is when you train your own model and you upload it your model becomes pre-trained model for someone else and they can do transfer learning they can use it and that is the hugging face hub right but i think i think what is the last, following up on that, so if I look at something like a chat GPT, you're not downloading that, right? It's kind of... No, that is the problem. Chat GPT, see, the problem that has happened is... Okay, so for those of you who are remote, let me repeat the question. The question is, chat GPT, we are not downloading. ChatGPT we are not downloading and it's a sad day that we have come to this. It started with GPT-3, the open AI all of a sudden realized that they're sitting upon a gold mine and they made frankly in my view lame excuses. They said, oh the model is so big it won't fit into the people's machines. Hey, let people decide whether they can fit it on their machines or not. But in any case, they didn't allow it. Chat GPT-4, at least Chat GPT-3, they sort of opened it and said how it was done. Chat GPT-4, they have written a paper that's more like a white paper. Technical details are rather sparse. So they're drawing the curtains on it. It's pretty terrible actually. And I would strongly say that given a choice between an open source model and one of this proprietary commercial models, always use the open source, at least encourage it, support, sponsor open source. If you have to give $100, if you have $100 to give, give it to the community. Don't give it to these sharks, frankly. So what I was going to ask is, I think, to Saran's point. So for Huggins Face, you download the model, you train it, and then we load it back again, right? And then somebody else can use it. If you want to, yeah. Somebody else can use it. In this open AI, it's kind of almost like this cloud so you basically like you are like training their own model no so what what they have done is they have pre-trained they have a pre-trained model when you use chat gpt you are just exercising the pre-trained model but if i load my data then i like i find tune it yes exactly so that's the second part we'll learn about the transfer level so then what happens is when you load your data, you get to have your private sort of variant, which has your fine tuning attached to it. So I had a question following up on that. So the fact that, like, for example, a source code got leaked so how does chat uh how does gpt3 work where if i put my data into like you know one chat will that transfer to someone else's chat or is it just strictly locked in yes if you're using the public chat gpt yeah almost surely so there is a lovely xkcd cartoon i saw i just love that one So it had a picture of some people having a conspiracy to overthrow the government, right? So they all say, let's meet at the docks. So the cop goes to this chat, this thing, and just says, we need to overthrow the government where are we going to meet and jack gpt says at the docks and he says gotcha right you see how it can be abused right so that is the that is i mean there are huge societal impact of these things and and that's the point that i think made in the past that the social and economic and political um aspects need to catch up they need to get real and catch up because nobody can stop this scientific movement it's moving very very very fast again i give you guys the background that when the steam engine was created, not many people understood how steam engines work, except that they knew how to make it work. Thermodynamics came much later, but it caused the industrial revolution, caused much benefit, cheap clothes, amongst other things, fast transport, steam engines, locomotives, but it also led to horrendous suffering. Children were slaved, chained to the machines, textile machines to work on them. The whole London was filled with smoke. People lived in how it's tiny little hole in the wall places. And there was tremendous suffering. And then came sort of the socialistic counter movement to reform the whole thing law and order was brought in and things improved we are in the same age unless we are careful it's an industrial capture those people who own these big models and who are not willing to share it anymore now everyone is realizing that these models have changed the world, put the world upside down, and they're going to make trillions. The rest of us will just be users with a begging bowl. Oh, can I please use your model? They say, okay, give us $1,000. Give us all you have. Reminds me of a joke. One guy, if I may, goes to a psychiatrist or a psychotherapist or somebody and says, doctor, I feel that everyone is after my money, right? I can't help it. I have this feeling all the time. He says, no problem. I'll cure you for that. He says, okay, how much will you charge? So the doctor says, how much do you have? says, how much do you have? We are entering that world now. Anyway, guys, so this is a recap of the last time and I'll make it work on the server. It's a new machine I set up. But let's go back to new territory now. Today, we are going to start with number, the concept of transfer learning. So transfer learning. So transfer learning is the basic idea and I'll dwell upon it. As you see there, there's nothing to do here, the code. So now the thinking is, if you want to solve a problem properly, what you do is you see if the problem has already been solved, right? If it has been solved, somebody has checkpointed a model, you download that model, model checkpoint, use it, right? You're done, isn't it? You're done. On the other hand, if you can't find your task, solution to your task, no. So even if it is done, you may say that maybe the performance can be improved because i have a special data set my own data set with which i can further train it make it better are we sure so then taking a model which already has some semantic understanding of problem and just it has already been trained through let's say a thousand epochs and then you take you but a different data set and you take your data set and then you train it for a few more epochs. The secondary part is called fine tuning. Implicit to fine tuning is you don't want to fine tune on the original data set. Because the assumption is that they were careful enough to reach a saturation point of performance before they checkpointed it. Usually is true true not always true but then but you take your own data set and then you train it so what what will the weights do they will go through some minor perturbations and they'll readjust and they'll get fine-tuned that's why the word you use is fine-tuning because you don't expect the weights to get radically transformed isn't it which is to completely look different you expect them to go through small perturbations the first one is called pre-training it's called pre-training pre-training is what you bring down yeah so transfer learning is made up of two steps pre-training and fine-tuning and now comes the interesting part of pre-training is the expensive part usually because you have to train it for a general problem with a vast corpus of data on usually a large cluster right right or you have to go to Sukhpal he'll build you a nice machine right so and then you can train it and then you will you can checkpoint it that's your pre-trained model. But fine-tuning is good because, see, you don't need so much data. Sometimes small data sets, for example, in the medical world, you're trying to classify a disease. How many cases will you get? Very few. You're trying to do a medicine compatibility with certain genomic types. You will get a data set of 100 patients, 100 gene blueprints, genomic print. So then it turns out it's enough, right? Because you're just fine tuning it at that particular moment. Of course, you can do small data augmentation strategies and do it, right? And you can get away with it. And we did that in the deep learning course, deep learning foundations, you'll see. You'll learn to tell the difference between a weeping willow and a pepper tree. But when you do that, you will take a model that can basically classify things. But it can tell the difference between a house and a tree and a truck and so forth, general set of problems. But then you will, and it will even be able to tell apart the pepper tree from the uh whipping below but with a certain accuracy but then you fine-tune it and when you fine-tune the accuracy goes up are we together or whatever metric you want to use precision record but in this case accuracy makes sense the accuracy will go up precision required but in this case accuracy makes sense the accuracy will go up right now in our today's labs what we'll do in the afternoon is we will take the example of a sentiment analysis so they yeah a sentiment analysis in sentiment analysis you will see that a pre-trained model will be able to classify at about 50-60 percent accuracy. Now 60 percent accuracy when you are looking at six classes is pretty good because randomized its accuracy should not have been that much. It should have been something like 17-18 percent accuracy at most. 18 percent accuracy at most right so from to go from 16 17 or so or approximately or 16 percent to go from there to about 60 plus transformer is doing a good job it's not a dummy it's not a dummy uh baseline classifier right it's not a nonsensically classifier it is doing something good but it is you can do better so how will we do better we will take the emotions data set and we'll do the fine tune so we'll do both stages of we'll trade with the pre-train and then we'll fine tune it right and if we can't finish it today then we'll finish it in the next lab right by the way guys we are moving a little slower than I had planned so as I said I'm adding two labs to two extra days to this course so that I we make sure that we cover all the territory that we want to cover so instead of six days now it's an eight-day workshop I see my question so like we talked to the media and they have this thing called limo and so they said we could actually take it and then uh say is that what they call like fine-tuning if that would be the final fine-tuning step if you give them a pre-trained model and say they have I don't know about Nemo that much but if they're holding on to specific domain specific data sets and they're not sharing it with you but they're saying we'll find you no no it's like um it's almost like a general purpose compute completely and so this one like you can basically then we use our own data in our own um you know cluster cluster and then train that uh oh that's that's fine too that's fine yeah so basically they're giving you a gpu cluster as a service you're basically saying no it's a model actually or it's a model it's like a speech recognition oh yeah so that is that so there is a pre-trained model and then you can fine-tune it with your data that's literally that and that is transfer learning literally so guys we got the idea of transfer learning right now let's move a bit faster we will take into let's go into one example now hugging face is something let's go to the hugging face i'll quickly go there to show you what the hugging face api looks like the and by now guys you know there are two things that i'll tell you the recipe for success in this field are two first is the statement of karpati he says really be friends with your data know your data well right and i say something similar i say that see uh when you uh in in amer, of course, we have an audience that probably didn't date, but in America there's a convention, most of the world is a convention that you find your better half through dating. So when you meet somebody on the first date, you're on the guard, you're just trying to make a positive impression. You don't really learn much about the other person. You just see like big red flags, then you don't go on the about the other person. You just see big red flags, then you don't go on the second date or something like that. You just get a gut feel for it. It's only after a long association that you know the person, isn't it? After many, many meetings. I think there's a human study that says it takes about 50 or 60 encounters before we genuinely trust somebody. Something to that effect. I don't know the exact stats or 50 60 hours of association before we trust somebody uh same is true for data think of data whenever you look at data think of it as a date right you're not likely to have a lot of success if you completely ignore your data. It's the same way. You want to know, you need to know the data really well, because it is the data that data science is about. All your prediction models, everything is about the data. So start by knowing data. it is a guiding light, it will tell you what to do and what not to do. Right. And you will notice that data can be you can learn through exploratory data analysis. But even more, when you build a model, you look at the way the model is making mistakes. By studying the mistakes you'll actually find, especially when you're dealing with label data, the mislabelings. Because every data has errors. You can use, sometimes you can trust the data, sometimes you can trust your model. And you can use your models to find out where the data has errors and fix the errors by hand. Now relabel it. So remember your models are also great relabeling tools or data correction tools. So do that. We'll do that. And of course, we have talked a lot about exploratory data. The second thing that you need to really be familiar with is if you want to do carpentry, what should you be most familiar with? The tools and the material, right? You should be able to look at a wood and feel and tell, is it softwood, is it hardwood, is it cross-grained? How am I going? The strength of the material. And you should know your tools, right? You, for example, you should know what the drill bits are, what each of the components in your library, in your sort of tool set are, not to know it. So for example, using the back of a drill to hammer a nail is a terribly bad idea. I have seen people do that. And silly as it looks, I have seen people do that in programming, using these libraries, they use something. Yes, you could use it, but really, right? So don't do that. Know the libraries. And today is about knowing the library. We'll go below the surface of just using a pipeline and getting a little bit deeper. So how do we do that first of all there is the data sets very good we can explore the data sets guys it has already been categorized by the tasks do you notice that like all of these tasks they have already been categorized by text classification you want a data set on text classification here we go text classification you can go and do that right and one of the things that i would suggest is we're sorted by most downloaded the glue and the super glue the imdb What in the world is the IMDB? Movie database. That's right. So Rotten Tomatoes is along the same lines, right? So the highly downloaded ones are a good place to start. Why do they call it tasks? Because these are things that you do, right? Classification is a kind of task, feature extraction, or, you know, question answering,, entailment and so on and so forth. These are the kinds of things that you do. The word often used is tasks. So you can pick the data sets. I will just take one particular data set in here. in here emo right and you notice that emotion data set when you go to the emotion data set do you notice that there is this data set dairy AI emotion 2.4 let's go and look into this data set. What does it look? So you preview the data, guys, so that you know what you're working with. Good thing with Hugging Face is instantly you get to preview the data. You see this string. You have a text and you have a label. And the label has meaning. Where is the meaning coming from? It is coming from the features, the description of the features. Right, so now notice that there is a data set card is one of the nice things conventions followed in open source that you cannot upload a data till you give some descriptions to it properly that's a practice so when you look at this. Right, you will see that it has a lot of things associated with it there's a paper you can go read the paper right and so on and so forth but i would get too much into it labels and they explain the labels do you see what does zero mean joy one love two anger three we have four right surprise and surprises this thing so remember guys zero one two three four are not degrees they are not numbers they shouldn't be treated as numbers they should be treated as categorical right even though they are written as numbers one minute even though they are written as numbers you should treat them as categorical right even though they are written as numbers one minute even though they are written as numbers you should treat them as categorical right and this is very common people in databases they store things as numbers why because it takes less space but then they have a dictionary to translate from the number back to a label remember categoricals must be treated as categoricals if you treat them as numbers all hell breaks loose albert what is the corpus here and what are the short sentences so the entire data set is your corpus and it is each document is one text and it's and text is the document right to go down yeah so it is text string so those are those are your document. Each document is essentially one text. Yeah. And then the label says what any good classifier should. It is very evident that it has to be used as a classifier, in particular classifying into emotional states yeah so it's a sentiment analyzer any kind of sentiment analyzer should be using this because it's label data right this is it use this data set you can do that edit data or train in auto train so one of the nice things is you can click on train with auto right so you'll have to tune in so i'll come to other aspects of it if you have a hugging face api like a account and i advise you to have a hugging face account it's very useful right if you're in this world you know you have to know where the meeting place is right so get familiar with it and so i won't at this moment go into it there's paper with code when you go to paper with code you will see that there's a emotional there's a and people have written some things to it i won't go into that the home page there'll be a paper it's always good to read the paper from which this data data set originally came like i said know your data and what other people have done like people create data because they're doing some research. You want to go and read that research paper just to get the reason, even though it's historic, go read it. We are going to deal with this particular data set. Let's go back now. And where were we? Like in the paper with the data set with the model? That we might have published or something? No, they give you just the data set and we'll come to the models in a moment. Where was I? Transfer learning we did. Oh, sorry. Give me a moment. Too many notebooks. NLP, Visual, QA transfer learning data sets and pre-trained transfer learning. I'll reopen it. So what we will do today is we will start with the data sets notebook. It's a worthy thing to do today. Actually I'll extend, we'll start a lunch a little bit later because we have been doing a lot of talking, which is good, but we need to cover some territory. So look at the simplicity of using the datasets. Make sure that the datasets is installed, pip install datasets, if you have not installed it. It's part of hugging faces. And the first function we'll use is list datasets. It will tell us how many datasets are where, on our local machine or... Hugging faces and the first function will use is list data sets. It will tell us how many data sets are where on our local machine or. In hugging face in the hub, right, so when you do that, as of today morning, there are twenty seven thousand nine hundred and eighty five data sets. So any one of you who are asking this question, I don't know where can I get data? You probably have an answer. Yes. Yes. So you have an answer, right? So this one is going to hugging face, like calling the data and then finding the number. Yes, it's finding the number. Isn't it lovely? And for you, it's just one line code. Now now let's say that you want to load a data look at the sheer simplicity of it give it the name what was the name of the data emotion right you load it and it goes and gets the emotion data set do you see that it gets the emotion data set do you see that it gets the emotion data set load emotion and this is the calling the API yeah and it's it's getting the date yeah it's there in the data sets so then you look at this data set dictionary which uh and so some data sets are given like pretty much they won't let you tamper with it it's there then you have the data set like if you just say down do you see it's downloading right and by the way i don't think i ran it on this machine so might as well run it see how quickly it should run through because this is a new machine so you and i will have the same experience this is it and let's see maybe the number has changed since morning like oh number has actually changed it's not 28 000. do you see how fast evolving hugging faces i literally ran it yesterday night. It was 27,000 something now it's 28,000. Let's download it. And of course, it downloaded it very quickly. It's a short data set. I hope instantly downloaded, you can run training. Now when you look at this data set, and you just say emotions, it will tell you that it is a dictionary containing three subsets of data it internally contains three data sets so a data set can have nested data sets inside why is that a good idea basic machine learning was the first thing you do for any supervised learning you want a split into train validation and testing right so this is it it already gives you the split how easy it is you don't even have to call the splits right and then let's take the training data which is 16 000 rows let's look at one row of the data now when you get the training data like this, you see that you're just looking into the dictionary and getting the training data. And after that, you can treat it like a list. When you say the first row of the data, it is basically a tiny little map. Label is equal to I didn't feel humiliated. Label is zero. Right. And so you can look at more rows. You can ask, what is this 0? What is this? So how would you know the answer? You can ask the data set to declare its features. It's telling you. So it's saying, ah, these are the names, sadness, joy, love, anger, fear, surprise. So 0 corresponds to, which is in this list, what is the 0th index? Sadness, right? zero corresponds to which is in this list what is the zeroth index sadness right so it is saying is this then you move forward and by the way it also comes the class label has a method it says id to string so it will convert a number into a string and you can go backwards you can say convert a string to id so you can give it sadness and it will convert it to id right we'll see that in a moment you look at these results here it is now you may say you know what let's add that textual label because remembering numbers is hard let's go and augment this data frame with this. Now, how do you do that? Most of us know how to augment using pandas. Pandas is easy, right? So very easy conversion to pandas. You just take that data set and you say set format pandas. Could it be easier? Well, it could have been easier. You could have said two pandas. But this is it. Now, once you get a partner's data frame then it's easy you just look at it here we go right and now you say okay let me add the emotion so when you add the emotion you have to be a little bit careful because what do you do for you first get the labels label will be a column of numbers right 0 1 0 3 whatever it is that particular column and then to that column you apply a function for each of the number you want to convert the number to a string you see this int to string is a feature it is something that comes built into the dataset api because it is calling a method of this class right remember training dataset dot features is what it gives you this in the feature is a map if you go to label you basically get an object of this type do you see that right you're directing yourself to this and on this class label there is a method called int to string and that into string method will convert it into a string and then when you do it here it is I just produce some random rows yeah more intuitive intuitive just to see what it all is because see numbers are hard to remember hard to remember it you'll keep looking at the dictionary, but when you see it, you can see let's look at it and see if we agree with it. You increase the size just by one control plus or two. Sure. Is it better. Yes, thank you. Okay. So it says, I have, I have made it through a week i just feel beaten now do you notice that it has first thing to observe is that actually these are not even grammatically correct or spelling spellings are not correct right these are very colloquial usages and yet i i mean somebody has annotated it with sadness. The question is, how will our transformers do? We'll find out. I feel this strategy is worthwhile. Somebody seems happy. The strategy worked. I feel so worthless and weak. What does he have? Et cetera, et cetera. Again, you notice mistakes here. I feel clever now. So this is annotated data. The other thing I wanted to point out is you can go from dataset to Pandas, but you can also come back, right? And when you come back, you don't have to come back right from this. Just to show that you can create a completely clean Pandas dataset and convert data frame and convert it to a data set i've created an example basically a silly example i took a x1 feature which is what a thousand points along the x-axis from zero to one right but then i created the another feature x2 which is the sine of x. I created another feature x3, which is log of 1 plus x1. And so I created a data frame. You would agree that this data frame pretty much, this is the description of the data frame. These are all standard Pandas operations. Would you all agree that this is how you would do it in Pandas? This is no brainer. Now, how do I convert this into hugging phase data you don't have to export it to csv and read back the csv which you can do by the way but i leave that as an exercise for you guys to explore you can just say a data set from pandas right this is it and you get this lovely isn't it right so this is so now i've just taught you the basics of the data set there's a little bit more guys huh so please take this as a homework in the lunch break try to explore go to the website and go to the where are we data frame this hugging phase now how do i explore the api let's go to the documentation we go to the documentation do you see these data sets guys on my screen go into that and it is really worth going through the tutorial guys go through the tutorials and how to guides believe me you need to know your tools right so um this is I suppose it's one o'clock it's time for us to take a segue uh take a lunch break but why don't we give this as a exercise, guys, do that. Guys, I'll just end with a joke. In Silicon Valley is where a lot of millionaires are made out of geeks, right? And one of the things geeks like to do quite often is buy cameras to do photography. You can always spot one. If you're into photography, you can always spot one if you're into photography, you can always spot one because if you go to one of the beaches, right, let's say Stinson Beach or Half Moon Bay or something, you can spot one of these geeks because they'll be carrying a really big camera, right, DSLR, right, mirrorless one with a long lens. And their family will be getting it would have a most annoyed look on their face and in broad daylight they'll ask them to pose on the beach while they're taking a picture right have you seen this anyone of you has seen this sort of experience or been guilty of it more of it right now what are some of the things wrong with it and if you ever go close to them and observe what's the camera setting it is set on auto right at auto a high-grade camera actually takes worse pictures than an iphone or a smartphone samsung or something actually takes much worse pictures it is not meant to be used in auto except in emergencies right so that is not knowing your tool you have all the expensive equipment but you don't know your tool many things wrong you don't never photograph people in broad daylight why they don't look their best they don't look because you have stark shadows sun is up there and it is casting dark shadows on the face it'll make people look the worst possible you look you take pictures in cloudy days or during sunrise or sunset when the light is coming the golden light is coming at you right anyway so those are the things but this is it guys it just as we geeks make a fool of ourselves the first time we buy a fancy camera and lots of lenses and everybody knows all the photographers know in the same way if you don't know your tools well hugging face tools well believe me in the nlp community everybody will know. Right? So don't be that. So today, learn. Let's start. We'll give this week to really learning the tools well. Guys, we could have put many tools. Hugging face is the dominant one. You have to pick one tool set properly. Like if I were to talk about cameras, it's a distinction between am I in the Canon club or am I in the Nikon club between am i in the canon club or am i in the nikon club or am i in the sony club it doesn't matter the ideas are the same you need to pick one and know that tool set well right because the foundational ideas the f stops the aperture the the shutter speed the depth of field, right? The color, this thing, balance, all those concepts remain invariant. All of these tools just do it slightly differently. In the menu items, it will be shown differently. But ultimately, the foundational concepts are the same and you need to now bridge between your concepts and how these tools implemented, isn't it? And that you have to do, you really have to do. Guys do that. I cannot, there is no way I can overemphasize this thing that you need to know your data well, but you also need to absolutely have details of the API of these tools. One API, pick one. You should not only know some at the basic level of Quickstart, you should know the APIs, the signature of the methods, almost reflexively from memory, you should be able to quote. You may not be able to quote all the arguments to a function, but at least the dominant ones that you use, you should be able to remember them. Are we together and with that comes power so take that please as a lesson and make yourself go through all for all of these go through the tutorials how to guides conceptual guides and finally the reference when you go to the reference let's say how did I know that the class label has that method into string and string to int? Why? Because I can go to it somewhere in here is a class label. Class. Why am I not able to find it? Control F. Yeah. So we see there's a class label here. Right, expand, where am I? Dataset up somewhere. Okay, so you keep searching for that. And some sooner or later, you'll hit upon it. Builder classes, loading classes. Okay. Yes, here we go. As a class label, right? So I can go into this. It sort of jumped somewhere. I don't know where it jumped, but it was supposed to jump to class labels. Hang on. Why did I not get there? Class labels. Okay. For some reason. Anyway, suffice is to say, I don't know why it is jumping around here. Again, you'll provide the Jupiter notebook. Yes. In fact, right now, I'll give some right now so you can play with it. Okay. Okay. Class, class label. Here we go. Class label. Here we go. Now, with some luck, class label data properly converted to tensors here here is this class label has names but let's drill into class labels and hopefully it will take us somewhere here we go it took us to class yeah here we go the class class table you see that it contains names it contains there are three ways now it will give you sorry it's running a bit slow ah parameters cast into yeah here we go do you see into string right and string to end so how do you remember it after a little while it becomes familiar to you, right? You know, you keep going through that. The other thing that I would suggest is there is a very good book. It's literally called Natural Language Processing with Transformers. It is a good textbook to have as a reference to this course. It's an O'Reilly book, Natural Language Processing with Transformers. Can you paste the link to the book please absolutely let me do that right now yeah yeah it used to be i'll bring it over you guys can look at it it's on my table in my office uh it's a very good book actually i like it it is completely hugging faces based, mostly natural language processing. Yeah, by the second one. The second one came just two, three months after the first. The distinction is its colours, its coloured pages. With transformers. Yeah. So you can see that it came out in August and I bought it in August. Right? So usually what happens is through the community, you know, through the community channels, you always know which books are coming out and when you end up pre-ordering them. So it's a very good book right i would highly encourage you to do that it's hugging face almost completely hugging face see when you think of transformers right open source hugging face is basically the word hugging face is synonymous with transformers. That is why the library itself is called transformers. So you say pip install transformers, right for hugging face. So guys, this is it. Let's break for lunch and where was I? Right, today's work, class work is actually go through all the tutorials, tutorials, how to guides, et cetera. Do it thoroughly because if you don't do it now, you're all busy. Remember your life has your boss, your workplace, right? And then your boss is at home, right? Your children, your your family your parents they all will be there and then you have to sleep and then you have to socialize and watch tv netflix right so any studies you don't do here the probability that you will get as much time at home is relatively low so do it here okay so guys we break for lunch and we meet at 3 17 3 17 okay got it let's pause the recording so welcome back from lunch folks we were making our way through the concept of transfer learning. And I think there's a reputation transfer learning and pre trained models are the same file, but we can verify. I have put all the files on the slack channel. So please see that it is there. Oh, yes, the you can ignore one of the files, because lesson 0 you can do in hugging faces is obviously the easiest thing you can do is use the pipeline API, isn't it? A pipeline to do this, boom, it's done. Give it input, output comes out. It can't get simpler than that. But the next simplest thing that you can do as you start peeling the onion is to use one of the so-called auto classes. Hugging face gives you a very simple framework. and these auto classes are best represented by this syntax they will be all auto some class or some task from pre-trained so let's break it down when you say from pre-trained you're loading it from a checkpoint isn't it so you give it the name of a checkpoint and it will load a transformer model model it will first look locally if you happen to have a local copy if you don't have have to happen to have a local cache of it then it will download it from the hugging face hub right so implicit to this is you can't be running this code if you're flying like 30 000 feet in the air because you might not have the internet connectivity so then in that case you hope that you have locally cached it so be aware of that that it reaches out to the hub to get it now we'll take a simple example we we have been dealing with tokenizers and we'll deal with the tokenizers in a moment uh did i cover tokenizers before lunch i have not yet right little bit okay we're going to do it a little bit more now so one example is auto tokenizer right so do you notice that the syntax is auto something from pre-trained so if you look at this syntax the syntax is auto now the the name of the class is very like that auto tokenizer from a checkpoint. A bird based uncased is the checkpoint right? And are we looking at this code folks? Is this making sense? So what it does is you get a tokenizer when you pass in a text to it. What do you expect? You expect tokens to come out. And it is as simple as that. If you look at the the element number six or line number six or input six, you notice that we send in a text which was and I deliberately sent in a text that is the tokenizer does tokenization. It does this to have fun with tokens. So there's a lot of redundancy here. Why did I give a word like that? Remember we talked about the word pieces and so forth. So it's a hint that there is more going on in the sentence, but we'll see that later. At this moment, we'll just look at the auto classes. You print the tokens and here we go. You get the input mass. Now, what are these numbers? Input IDs are what? These are indexes into the vocabulary that this tokenizer is using, right? And that is what it is. Now, what is token ID? And we'll come to this. Forget about the output here. We'll talk about tokenizers in depth. Likewise, if you want to do image processing, now you may say, oh, good grief. If you have done image processing and use the PIL and libraries and many things, you know that there are many, many lines of code you write to do just basic image processing and vectorizing the image. But how simple can it be when all you have to do is say auto image processor, an auto image processor from pre-trained, and you give it a proper transformer model into which you will feed in, Like what is an image transformer? VITs are obviously one of the classic image transformers, right, the original papers. You can use it for determining whether some image is a cat or a dog. But before you do it, you need to create it into an embedding vector that goes into the transformer, isn't it? That's what we talked in the morning, that they take input as an embedding vector. So you need to create the embedding for that. And then the transformer will take it and it will produce what it's supposed to produce. And remember, what does a transformer head in itself produce? It just produces hidden states or embedding latent vectors or or embeddings and i will use this word sort of interchangeably and this is it and here i won't go more into and show a full example but suffice is to say that you see that this code between tokenizing which is a nlp task and image processing which is a computer vision task how absolutely similar the API looks isn't it very easy guys very very easy and likewise you go to the next thing audio processing you have sound files mp3s you want to process them what do you need to do well auto feature extractor which extracts features from audio, is this. You can just use a proper transformer. This one does speech emotion recognition. And then lastly, you could go further. We live in the world of multimodal learning now. that you can have, we live in the world of multimodal learning now. So when you feed in data, nothing says that the data cannot come one parts from image and another parts from text, or one part from image and another part from sound, or you can mix and match and do all sorts of things. Right. And so, and by the way, multimodal is very big in the last two sessions of this workshop. We are going to focus on a lot of multimodal learning. If you think about it, all of these daily stable diffusion, etc., etc. These are all instances of multimodal learning, but it goes far beyond that. You can do very complicated things when you give input of text and sound and so on and so forth. You can do very complicated things when you give input of text and sound and so on and so forth. And we will see, and you can do the opposite, you know, sound synthesis and or transcription, which will do transcriptions for you. So we'll do a lot of multimodal stuff. They're part of natural language processing. But this is a this is a good first start. You take a LMV model, which is very well known in this space and you can load it it will it will do two things it is ready to do processing input processing of images as well as tokenization of text so think about it guys in one line you have put two domains which used to be separate in ai computer vision and text processing. And you can see how close they have come with this multimodal learning that you can now, there are checkpoints and models that you can load with just, and processes that you can load with just a line, isn't it? Now you can say, all right, that is for the processors or the preparing the data to go into the model what about models themselves that that too is very easy so we will take an example suppose you take a checkpoint distal bird case a distal bird base uncased now what was distal based to vet your recollection distal based is a trip it's sort of a smaller version of birth it's so it and how it learns we will how do you train a much smaller model to perform almost as well as the big model we will learn about this that's one of that's one of the loveliest papers so you how do you so what you do is you build a model you make it do predictions and then you train a student model and you treat the big model as the teacher and you try to make the student model not learn from the data but learn from the predictions of the teacher model right so it learns the way the prediction the teacher makes predictions and sort of catches up to it and so it has almost the accuracy of the much bigger model the stillbirth is a is a shining example of that and it does really uh give you how does it do that oh we'll learn about that we'll learn about that it's what the architecture is worth learning about so it's one of the papers we'll cover so but just assume it can do that patrick so i said for these multi-modal models so does it still follow the the KQ and B of attention where where like you break down you break down an image to like multiple pixels yeah so that's right so what happens is not at the level of pixels so there is a paper which I can be covered actually we will cover a lot of papers you'll see and at some point i'll start covering two three papers a day um it is called what is it 16 by 16 pixels is worth a thousand words or something like that so what they do is see what is a word made up of characters so they're saying that we'll take 16 by 16 pixel and consider it a word or a token 16 by 16 pixel and consider it a word or a token you got it right from there the moment you realize you get that insight you realize that the whole transformer architecture can now be applied doesn't it because it works on tokens sequence of tokens so you broke the image and chopped it up into tokens so it doesn't break away from the original transformer models that were based off of words exactly just you converted the signal into what fits into that so what do transformers need they need a sequence of tokens so how do you stare at a picture and say how do i make it into a sequence of tokens what is a token here you say okay i'll take 16 by 16 pixels and call it a token right because after all a word is made up of characters this is made up of pixels and that's it so they just go through the whole image and this is interesting yeah that's a basic intuition and now when we do the paper they do it a little bit smartly and we'll come to that good quick oh so for like going back to like tokens for words is it randomized as to like where they'll stop the token no no it's not at all not at all in fact that's a very next one so uh hold your breath right and you won't have to hold it for too long so here we go you can take a checkpoint and you can say auto model for what purpose let's say that you want to classify sequences sequences are what sentences text You want to classify text. A classic example is sentiment analysis. What sentiment is it, right? Or classifying what the topic is. Is it politics? Is it sports? Is it science? What is it? You can use it for that, right? So things like that. So for classification. So that would be sequence classification. So you can say automotor for sequence classification. Pretty much the way you would intuitively think of the sentence that's the name of the class on the other hand if you are using the same checkpoint because you can use the same transformer for different purposes remember we said that once you have trained a transformer it can do multiple things suppose you want to use it for just token classification. One example would be which part of speech is it? Is it a noun, a verb, an adjective? What is it? Right? So you could do auto model for token classification. Right? From pre-trained. Another way to think about it is, is it a named entity or not? Named entity resolution. So if I say Felix the cat jumped over the wall, right? Or jumped over the Berlin wall. Well, Berlin wall is fallen, but let's imagine, right? Felix is the named entity. It's a person. Well, I suppose. Cat is not, none of the words have to be ignored. And then you come to Berlin and you say location, it's an empty entity. So you can imagine that you can construct all sorts of things. So guys, I hope you realize that, excuse me, that auto classes are about the next simplest thing after the pipeline, isn't it? Very easy. So we peel one more layer of the onion and once you get familiar with it, just lovely. If you want to see how lovely it is, try writing this entire code in plain PyTorch. Or if you really want to trouble yourself write it in tensorflow right and see how many lines of code you write so oh gosh now that would be that would be like that that's in the in the bicycle analogy we use you have to take ore, you'll have to refine the ore, smelt it, smelt the iron, and build a bicycle from scratch. Okay, so the next one, we have been talking about tokenizers. Let's look into the tokenizers a little bit more in detail. So guys, are you following me? I want to go fast because this is easy stuff. By the way, all of these notebooks are on your Slack now. I hope you have taken them from there. So let's take an example. And we'll go into the tokenizer now in real detail, hands-on detail. So I took the same sentence. The tokenizer does tokenization. It does this to have fun with tokens. You'll see why i deliberately wrote that sentence so how can we do that we'll use the auto tokenizer something the auto one of the auto classes you give it a checkpoint bird base uncased and there you are you see how easy it was except that uh and by the way this is a trick I use. You know, when you print things out like this, sometimes it's not very, like, at least I find it a little harder to read, especially if there are many, these dictionaries have many keys and values are erased. So what I tend to do is, I use this trick, I convert it into a Pandas data frame and then read it. I find it a little easier to, I don't know and of course your your own experience may differ but i find it a little easier to read that these are the inputs right so now what has happened this sentence let's see how many words there are one two three four full stop is five six seven eight 7, 8, 9, 10, 11, 12, 13, and another full stop, 14. So you would have expected, including punctuations, 14 tokens. And yet we have, oh good grief, 19 tokens. So guess, where are the extra tokens coming from? No. Punctuating. We included that in the 14. Look at the sentence. Yes, start and end. Very good. You need to you need special token CLS for start, separate for the end of that. What else? Hint, look at the look at the text that I sent him. What does that text tell you so look at the results do you see 90 or 19 204 repeated a space yeah right and what does it correspond to 19204 seems to correspond to token the word token isn't it and so what do you why why has yeah so look at that the tokenizer so first is the 101 stands for cls as you can guess the second one stands probably for the and will verify whether it's true now now now use your deductive skills what will the next one stand for because it is repeated it cannot stand for the word tokenizer because tokenizer word happens only once and yet 19 204 is thrice now what is thrice in this sentence? The word piece token or the word segment token, token, token. It is there in tokenizer, it is there in tokenization, and it is there in tokens. So that gives you a clue why it is good to use word segments because now you can have a smaller vocabulary. We talked about it. So now let's see if our guess is right these 19 so i saw the mystery now look at it does it look right right yeah it got broken up into's a suffix. Isation is a suffix. Civilization, prioritization. There are many, many words that can end with this token, isn't it? And ends with s, the plural. Surely one word piece that is ubiquitously used is the plural part, isn't it? That is it. It starts with CLS and CEP. And it turns out that different tokenizers, they may have different sort of these special tokens. One may use this and you'll see that it differs. Right. So intuitively, without knowing all this, you would have expected this. So intuitively, without knowing all this, you would have expected this. The tokenizer does tokenization dot. But now you know that there is more going on here, right? So why is it a good idea? We have talked about it, but I'll just review it. Why is it a good idea for ISER tokenizer to take token away from the ISER part and keep ISER as a suffix, because we happen to know that there are many, many words that end with ISA. Right? A finalizer, finalizers, plural of it. Visualizer, visualizers, equalizer, equalizers, and so on and so forth. Right? And this is not an exhaustive list. You can go on thinking more and more right so we can do a little experiment let's put all of these i created a list of isers you know i created a string which is just made up of isers all sorts of isers right finalizer finalizers etc and let's pass these things through the whole process and see what comes out how How did it break up the word? Do you notice that final, eiser, final, eiser, and es, the plural. And do you notice that it did not create, like even though it created more tokens, it is from a smaller vocabulary, implicitly from a more condensed vocabulary. And that is the whole point of using word segments rather than words themselves. Because remember, English has 5 million words, and you could be in quite a bit of trouble if you try to use 5 million words. Now, one of the side benefits of using word segments is, remember, I said that for words that is not there in your vocabulary at all, you use a special token, like Bert will use UNK, right? UNK stands for unknown. And all the words that it doesn't find in the vocabulary, it will map to that. But what happens is that it's rather hard to construct a word which won't get chopped off into word pieces that are in the vocabulary right and so the number of mappings to unknown decreases because of that now it turns out so we realize that word segments are the building blocks of the vocabulary and so what happens here is your vocabulary. So imagine a little dictionary, not the Oxford dictionary, not the Webster's dictionary, but the Transformers dictionary, right? And its entries are the word pieces. And God knows how in the world would you give the meaning to ISA, but okay, some meaning is given. But what you do instead is for each of this token instead of meaning you give a number a sequential number because if you for example if you if nothing else alphabetically sort the tokens right then you can surely attach an index to it the array whatever the index index thing is, you could do that. Or you could just hand choose and pick some numbers. People apply some conventions, right? So they have done that. You associate a number. So there is a mapping between each token and a number. The number uniquely gives a token, right? So long as you maintain the map somewhere, you have your own private dictionary, the tokenizer a token, right? So long as you maintain the map somewhere, you have your own private dictionary, the tokenizer's dictionary, right? It is as good as Webster's dictionary, except that instead of the meaning of the word, you have a number for it. And once you have a number, what else can you do? Well, remember that words or tokens are categoricals. As I input, they are categoricals. As I input their categorical variables, they're not a measure. They're not 13.6. So actually that thing that is just the location, if you think of a large array of zeros, and suppose the token ID, the input ID of the token is, let's take an example, 101. So you go to the 101 index in the array, and only that bit you flip up. You set that flag, you make it one. All others are still zero. This is your one-hot encoding, right? This is called the one-hot encoding. You now get, for each word piece, a vector. What is the dimensionality of the vector? It is the cardinality of the vocabulary. If your vocabulary has a thousand words, then your one-hot encoding will have thousand dimensions. Actually, technically, it can have one less dimension, but you don't do that. When we do NLP, you keep it thousand. You may say that the one last one is redundant. Right? So for example, if everything is zero, you can say if none, if it is none of the words, it must be the word that I left out, the thousandth word or something like that. No, but you don't do that. You just make it the same size as the vocabulary right and you set a flag at the at the place that you would like to right and oh you're making notes here nice now it turns out that uh oh by the way if you write on this you have to erase also because otherwise i don't know there has to be a way to figure it out. Okay. So now there are many ways to do word pieces. It turns out that when you do multilingual words, then some of the presumptions of English do not apply. English, I said, you presume that, I don't know, words go in a particular way, right? From left to right and so on and so forth, you start interpreting. But then, yeah, lowercase, uppercase, yeah, these are assumptions of that. But if you're writing it in some other languages, you need to be very careful. You need to, for example, if you're using Mandarin, Mandarin, some of these assumptions don't work. I don't think there's an upper lower case in Mandarin is there Dennis. This is only one of those two languages that came from ancient people, like you know the hieroglyphics. Hieroglyphics. They're sort of like capital. Yeah. Like what was the actually writing the data, likeoglyphics. They're sort of like capital. Yeah. You know, it seems like whatever they actually buy in the day, they buy it. That's like all of this. OK. So it's pictographic. A house has a symbol, and that symbol is it. There's no upper, lower case. Yeah. Yeah, that's right. Yeah, nice. So there we go. So what happens need a different tokenizer that works better for multilingual models, multilingual text. And such a thing is sentence-based and we'll see it in a moment. Then GPT family of models, they tend to use yet another kind of tokenizer called the BPE tokenizer. So I will show you the result for one of them, I believe sentence piece and very easy homework for you is just use the bp and see what what output comes out right i'm sure you can do that just have to substitute bp there tokenizer so now when you give it the tokenizer now comes an interesting part Now comes the interesting part. You can give, suppose tokenizer giving it one data point, data instance or datum at a time, one sentence at a time is rather inefficient because the way these things run, especially on the GPU is they're tensors. You can pass a whole batch and it can do the whole matrix computation, forward computation at one go right so why give it one at a time so one easy way is you can give it a list here we are giving it a list and this is just to illustrate the point that it works just fine with this right so suppose you give it more i gave it two to r is human to divine, to learn is to live. And what does it produce? It produces these tokens, those identifiers. Remember those IDs, those numbers in our dictionary, in our tokens dictionary, the numbers that you are hugging face APIs, they call it the input IDs. IDs would have been fine. Input why? Why input IDs? Because they will go into the model. The presumption is tokenization is a pre-processing step to something else that will come after. So that's why you use the prefix input ID. Then comes token type ID I'll mention in a moment. Look at attention mask. Do you realize that of the two sentences, to live is to learn is a shorter sentence, isn't it? So if you make the encoding of equal length, so I've said padding is equal to true. So I'm saying just for matrix multiplication, just make them both of the same length. If I do that, what do I do about the fact that to learn is to live? And then what do I do? You pad it up with, like you put this special characters saying ignore, ignore, ignore. But you also do something. And this is where attention is beginning to kick in. Because you know that you are likely going to run it through a transformer, another transformer, you're saying, you're giving a hint to the transformer that don't even bother paying attention to these tokens. Make sure that nothing is paying attention to these tokens. So these are attention masks. You're hiding it from the attention mechanism. So when it is zero, you're hiding it. When it is one one you're not hiding it so that is the mystery that explains the mystery of this column so when i just give it one sentence of course you have to everything is open to attention isn't it that's why this attention mask here all the values are one do you see right now what about this middle column token type ids what does this signify this let me explain now going back to the bird architecture that we covered last week what do we give we give a pair of sentences isn't it two segments two sentences and so we need to tell that this word belongs to which sentence the first or the second isn't it and how did we did it that was the segment id we call it the segment embedding of the segment id we call it the segment embedding of the segment id and that is what this token type ids is it tells which of the sentences it belongs to and of course you start counting from zero so the first sentence right if you were to give it as a pair the first sentence of course has the id zero isn't it and that's what it is. That's why it's zero. And that's an explanation for that. Oh, what is the question? Sorry. NER is different for question answering than chat GPT-3 or sentiment analysis? Anyer, I ask this question as a prompt. As a logic, Anyer is different to sentiment analysis that was discussed before. Anyer is different for question answering that. Okay, let me explain about what Anyer is. Sorry, Asif, I was typing too quickly. I was also listening to you. So the thing you we were talking about sentiment and analysis. Then you brought in named and entity recognition. You bought bird and distal bird. So I guess my question should be framed as how is a bird or distal bird? Are they better or worse in terms of the specific which is named entity recognition um we have a sentiment analysis and we also have question answering which kind of chat gpt does i also apologize in advance for my grammar i was typing pretty quickly and apparently we can't edit um okay so i'll take i'll let you uh that was my question i'll leave it there so let us unpack it uh so we have a lot of uh things playing in that question first is chat gpt so let's keep chat gpt aside for a moment because it's a very large language model and there's more happening than just one transformer there so let's call chat gpt a composite thing and keep it aside now let's come to this question named entity recognition and sentiment analysis see named entity recognition and the other thing was BERT versus Distilbert versus GPT so let me answer these three things. See, we say that to do named entity recognition or to do sentiment analysis, these are tasks. These are different tasks, right? So when you are using a model for one, you explicit. So let's take anyone sentiment analysis internally you can imagine the following three things to be happening you are tokenizing text some input text you're running it through a transformer whether bird distal bird whatever and then you're taking some output either the average of all the tokens or all the all the hidden states but generally it's not necessary you just take the output of the cls token remember the word architecture and you feed it to a classifier and say what is it now you may do sentiment analysis as thumbs up thumbs down you may do it in a predict into a set of states sentimental states joy fear anger so on and so forth right so that is a example of sequence classification isn't it a good class that you a model that you can use is a model that has been trained on classifying sequence or a text a sentence on the other hand when you come to named entity recognition, things get more interesting. In its very simplest form, you are asking, is this token a named entity? Named entity means person, place or organization. A person is John, a place is Paris, and organization is World Health Organization. Or some company. So Support Vectors is an organization, the most famous of them. So then it is a classification. If you really think about it, what are you doing? You're looking at every token and asking, is it a person, place, or organization, or none of the above? Right? That is one way to pose the question. Or another way to pose the question is, is this token a named entity, like person, place, or organization? Or is it a verb, adjective, noun, blah, blah, blah? You can classify it into one of those, right? But in any case, it's at a token level classification, right? So those are two different tasks. Now- Asif, sorry to interrupt. Before we go into the other thing, would name entity recognition, I know we talked about sequence to sequence in another, and I think a class or two classes before. Do you think named entity recognition would still be valid for other languages like you said Hindi or other languages? Absolutely valid. Like for example, if you say Krishna Delhi ghum ke aaya. Krishna is a name, Delhi is a place, Delhi is a place, and so forth. So the named entity recognition is global. It's a language agnostic. You just need a named entity recognizer that has been trained on that a like a corpus of that language so long as it has been trained it does a very good job now recently there is also a movement to see if you could do some sort of a zero shot a few shot learning and whatnot right but let's not get into that but in its simplest form so long as you're you have been trained with the corpus and label data, it will work in any language. It would work. Next, I apologize, Shalini, I don't know Tamil. Otherwise, I would have given you an example in Tamil. As if you don't have to apologize, your Tamil is probably better than my. OK. All right. So the next, so that's a distinction between the two. Now comes the question of which model to use. See, there are two aspects to a model. Given the same data set, how, what is the accuracy of that? What is the performance of that model? Pick a performance measure. For simplicity, I'll use accuracy though i hesitate using the word accuracy because people get fixated on accuracy whereas in most real life practical situations it is either the precision or the recall or something else that is more valuable to illustrate the point if you have a screening test that checks whether somebody has name your cancer cancer, prostate cancer, breast cancer, these are two scary things for men and women. Right, so as a screening test, what do you want to do? You know that if it comes out, even if there's a slighter suspicion of of cancer, you would rather say yes, positive indication. Because why? Because if this person doesn't have cancer, the subsequent test will eliminate it, right? But the last thing you want to do, so the price of a false positive is, you'll scare this guy for a day, he'll go home and write his, you know, some emotional obitu's for himself or something like that right and two weeks later another blood test will show he's totally fine on the other hand if you don't do that if you are false negative the price is catastrophic because a lot of these cancers they're very aggressive and time is of the essence what stage of cancer it has and whether cancer has metastasized, determines the survival of this person, the way it exists. So the cost is very high if you miss it. The cost is relatively low if you have a false positive. There'll be a little bit of a drama, but that's about it. have a false positive, right? There'll be a little bit of a drama, but that's about it, right? So what do you want? Do you want to have high recall? You want to make sure that you don't miss a person at the cost of precision, right? A lot of people you declare you have cancer, right? They go to the subsequent test, half of them come out fine. So remember which measure you use should be determined by accuracy. And this is very odd, actually. Even with well-known, studied data sets, like the breast cancer data set, which is probably the hello world of classifiers. If you go and look on the wild, most of the Jupiter notebooks and articles that have been written, they will say, hey, here is my classifier to look at breast cancer. And by the way, see how accurate it is. Accuracy is meaningless. Do you realize that? The one measure you should look for is recall in a screening test. On the other hand, if your test is the one that comes right before the surgeon is standing with his operating room door opens and his big knife ready, he is looking at you. At that moment, you want to be absolutely sure that this person does have a tumor that is going to cut open and take out. So at that moment that particular test needs to have what a high precision you need to know that if it says you have cancer you you have cancer isn't it you're not sending in false positives right so situation Right? So situation determines which metric you should use. Remember that. So don't get fixated on it. So with that story aside now, let us take a measure, same data set for a particular task, take a task. Let's say that you take a task sentiment analysis or classification into subjects politics people against accuracy I'll take F1 score that's a composite of precision. industry people understand F1 score than they understand in academia. So can you kind of clarify what your stance is? So just so I can recalibrate what you mean by F1 score. See, when you do, you F1 score. So first is F1 score versus accuracy, why use F1 score? F1 score is, in my view view a better measure when there is no dire reason to use either precision or recall right because f1 is the harmonic mean of the two so you have arithmetic mean you have geometric mean f1 is simply the harmonic mean of the two are we together so what it means is that if either see remember as a harmonic mean it has a problem. If either the precision is too low, or the recall is too low, it will adversely affect the harmonic mean. Isn't it? So think about it. Let's say the precision is 0.1 but recall is 0.9 1 over 0.1 is 10 right and 1 over 0.9 is well 10 over 9 let's just call it what should we call it 10 over 9 is 1.1 let's say we call it 10 over 9 is 1.1 let's say right so you are seeing a 10 plus 1.1 right is 11 and so 1 11th what was the score so f1 becomes 1 11. what was the score leaning towards towards the worst number, right? Isn't it? So F1 score has one benefit. It sort of overweighs the worst quantity, right? Whichever of the two is worse. It sort of says that this is the, like it sort of gives emphasis to the worst of the two. And therefore it's a useful measure. Accuracy is not, I don't prefer accuracy whenever there is a class imbalance. And let me make this obvious from the same exam. See, actually, perhaps I'll tell a different story. You know, before modern medicine, to take on the task, let's say I take a task named anterior recognition. Then you ask this question, condition on this task, which model is better? Then comes, but that question also is conditioned because what corpus of data did you train it with? So suppose you train one, let's say you take Bert and you train Bert with a smaller corpus and you train Bert with a larger corpus, right? You train Bert with a corpus of just thousand documents. And let's say that the task is sentiment analysis and you take Bert and you train it with a hundred million or I don don't know 20,000 or 30,000 data points which do you expect to perform better obviously that particular bird the same architecture the same thing that has been trained on more data isn't it so we are we have to condition on two factors which is the task, which is the corpus they were trained on. Now, assuming that they were trained on both, both had the same task, that's a sentiment analysis, and they were trained on the same large corpus. Then the question is, which is better? I mean, BERT, distilled BERT, or let's say GPT-2 or something like that. So there, the answer to that is, generally, distilled bird, by its very nature, has learned from bird and approximates its performance. So it will always underperform bird, right? So Asif, I just want to ask. Just hold your thought. One second. Let me give it because a lot of people are waiting for the answer. Distilled BERT is trying to approximate BERT, so it will definitely have a little bit of a difficulty catching up to BERT, but there is more to it. So, it will be a little bit like. Now, the question is between BERT and GPT-2, let's say, one of the autoencoder, the decoder side of the models, which will do better. Now, if you take just GPT, so not GPT-2, if you just take GPT, clearly there's evidence that BERT does better than GPT. GPT-2, I don't know exactly what the answer is and how they are comparable, but you can look it up. You can look up what it is. Now there is one more sort of a nuance to it. The nuances, if your data set is smaller, then what should you train? Well, the thing is, if you take a small data set and you train a very large model, what will happen is you'll have massive overfitting. Right? So you don't want to do that. The bigger the model, the bigger the corpus you need to train that model and still not have overfitting, right? Because more data regularizes a model and complex model have a huge problem with overfitting. So you have to also look at the size of corpus available for you to train the model, right? That is another dimension that you have to take into view. And so with that things all being there shall need your question that and i've kept chat gpt away i'm just looking at these standard transformers i would say that a distal bird assuming sufficiently large corpus assuming that the task is let's say sentiment analysis uh a distal bird will slightly lag but not by much the bird especially and if you use a bigger bird, bigger bird will do better. A bird, a large will do better. If you compare that to just GPT, because GPT is one word at a time, sort of a, you know, one token at a time emission kind of thinking, generally, the, and bird is bidirectional. It pays attention to the whole sentence. It will tend to do better for this task. So tasks specific the performance difference. I'll come to you. The next part is ChatGPT. See what happens is ChatGPT is not a model. It is a composite. It's a pipeline of many things, right? Amongst other things is chat, is GPT-4. GPT-4 is a ginormous model of a trillion parameters, which has been trained on huge amounts of data. There is reason to believe that chat GPT at this moment seems to be for general purpose tasks, let's say the sentiment analysis. I don't, I have not seen any objective study, but people generally believe that Chad GPT beats BERT. Please do the experiment and look for that evidence, see if it is really true or not. And so task by task, you can go and see whether Chad GPT, I mean, I wouldn't use the word chat GPT, I would use GPT-4. Whether GPT-4 beats BERT or not, I would imagine that it does, right? Simply because it has been trained for far longer and it has been, it is a much larger model. Okay, so now let me take a question. So for Shalini's question, Shalini, go ahead with your question. No, I think you've explained correctly. Any other questions I'll post to Slack. But I think you've explained my precise question. So I think the summary is that you're saying that, you know, Distal BERT was trained on a larger model. Sorry, BERT is trained on a larger model, but distal BERT is trained on a smaller model, but it's the dataset that matters. You're looking at the F1 score and not accuracy, because accuracy is very dependent on the dataset. So that's kind of my summary of where we're taking it. That is it. That is right. So I get it that for a large model, you need large dataset, you need large data set. Otherwise you won't get it. But I think we talked about 5 trillion and 5 trillion large model, you only have small data set. So is that 5 trillion I don't get it? Very good question. Masmi has asked a very interesting question. I'll repeat it for all of you who are remote. She says, I made the statement that large models need a lot of data to regularize them to prevent overfitting. But what about the fact that I also said that for fine tuning a model, you don't need so much data, right? It can be to do that. So how do you explain this paradox? Anyone would like to take a stab at it before I answer it? How come you're fine-tuning a massive model, but in fine-tuning you don't have an overfitting problem? How did that happen? It doesn't shift the model too much because it recognized the weights are already set for a certain structure and a small sample size you know you're not drastically the way the instruction is made it just focuses on that data that. That is right. That's absolutely. So what Patrick said, I'll repeat, he said that in smaller datasets, because you're looking at a model that's been trained, the weights are more or less trained for an adjacent problem. And you're just doing small changes, very small changes to make it even better on this small dataset. So when you patch this, run a few epochs over this small data set, the weights will just slightly perturbate. But regularization has already been taken care of by the big data set that trained or pre-trained this model. Remember, fine tuning is the second half that follows the pre-training. Another way to look at it is that the total dataset that this model has seen in its full training journey is the vast dataset that pre-trained it plus this fine training. So the total dataset size is still vast. And so it is a regularized model from that. So how does it, for example, like if it has trained in this vast dataset, let's say for like for genetic, if we have a smaller data for our clinical trial set, we use it to find the model. How does the model keep it segregated and not dissipate everything else? else no no what happens is it's a very so long as the problem is the same i'll take an example suppose you and this is a real thing you trained a model to do sentiment analysis but we did this example actually in one of our previous labs to do sentiment analysis and it is pre-trained on general text but people in the financial world they speak a peculiar dialect of their own. They are bears and bulls and all sorts of things, derivatives and whatnot. So what you do is you need to just make it understand that little bit. And so what happens is when you take a small corpus of financial text and run it over that, it has already gotten the gist of the English language. Right. So, for example, if it encounters the word, I hate, hate, hate today. Right. So sentiment is pretty negative. Right. And financial terms are not, I mean, there's no financial term that will say, this is great right but you need to be uh be more right like you know that if you use the word this company is i think this stock is highly leveraged it's a negative you don't want to buy it no but in english you wouldn't know what leverage means right it's it's a if something is highly leveraged well what is it what leverage means right it's it's a if something is highly leveraged well what is it so you need to give it that little bit of weight change so that it starts recognizing this and when you that's why I use the word minor perturbations you don't see drastic changes you just see fine tuning people use the word fine tuning for that reason and the weight change will only be for your own usage for your own usage because the model that comes out is your model you started with somebody else's model and you tuned it now when you save the model it's your weights that's it okay guys so with that we move forward now to word segments and we talked about word segments we talked about the attention mask we now know all the three components of this so we uh to summarize we know what input IDs are. This is the index in the vocabulary. So imagine that there's Webster's dictionary, there's Oxford's dictionary, and then there is the tokenizer's dictionary, right? So where the word pieces have IDs as their meaning, right? So you just look up the ID, which you use for hot encoding, when hot encoding. Token type IDs is not terribly relevant for us here for the use cases we're looking, but it basically says that when you're passing it through a bird-like architecture, is it part of the first sentence or is it part of the second sentence? The third one, attention mask matters. Is this fair game for attention or is it just padding and you need to ignore attention from it? That's all it takes. so we understood the three parts attention no no no in the word vocabulary index never changes is the weights that change remember if it encounters a word that is not in your vocabulary it will just map it to you and you and K yes fix the the maximum set of a variety of tokens now is the vocabulary size that's it so you always every tokenizer starts with its decided vocabulary that's it gets map journal and it is not as terrible as it sounds why because the meaning of that word is inferred from the context that's the whole point of attention so all is not lost right Right? That is it. So let's say that the word zebra was never there. And the sentence is, the zebra ran with all its four hoofs. Right? Well, that sounds a lot like horse that's why that's a point so you notice that even if you give it a nonsensical word when you use word pieces like i used a jabberwocky statement jabberwocky is this beautiful poem that as kids we all love uh if you read it from it right and it says it starts with it was brilliant in the slithy toes the giant jimble in the way and mimsy with the borogos and so forth and when you read that it sounds very sensible except that when you think of the words none of the words make sense right so i took jabberwocky's first sentence and tokenize it and one of the interesting things is that when you do word pieces it will actually break it up into things it has so brillic is an unknown word but it broke it up into br because beer is the prefix to many words brother and whatnot and uh ill still bill hill bill hill mill so it got that and ig i don't know what what ends with egg twig pig twig oh yeah there we go so many words with that yes and a slithy again slit and the uh hy hy is many many many subject many many things have suffix and uh-E-S. Lots of word galore that ends with V-E-S, isn't it? You notice that none of them got mapped to unknown. I give it to you as an exercise to find a word which will get mapped to unknown. Take this as a homework and see if you can find one. Something that will get mapped to you and k. Okay, So now the the process of encoding or tokenizing the the converse of it is decoding. So when you decode it, you realize that when you decode this it gets to this. It was billing in the slithy toes sandwiched between the CLS and CEP. CLS is the special token that you'll use, for example, for classifier. CEP is the separator between the two statements for word. Now notice that everything has been lowercased. So it has been lowercase. But that is what this tokenizer does. So you realize that the tokenizer does more than just tokenize the words into pieces it does some pre-processing it does some post-processing isn't it so let's go into that a little bit more right so when you look at Jabberwocky and you look at the token now one more thing is there when you tokenize if you instead of using the tokenizer as a function itself callable function you invoke the tokenized method it will deliberately give it to you without the paddings without the CLS and subpaddings it's something to know you notice that it did not produce those things yeah or it's one way now you can convert tokens to ids and you can decode it it will give it back to you the thing as it is right now uh one more thing to do is to you know we use the auto class right auto tokenizer you don't have to if you're if you really insist you know what you're talking about you happen to know through private channel that bird based uncased is a checkpoint that used but how what's that private channel you go read the documentation right on on the hugging phase website so offline you know that it's a bird model so then you can explicitly say bird tokenizer you're tokenizing it for a bird model you can do that And pretty much the same thing will happen. Then you see, BERT model will tell you that BERT tokenizer will declare that it is, it tells you a lot about itself. It says, oh, BERT tokenizer uses a vocabulary of 30,000 words, pretty large vocabulary, frankly. And now in tokenizers there are two things there is a fast tokenizers let me leave that as a homework what is a fast tokenizer right it's a homework for you guys post the answers to Slack let's see who gets there first padding site means you pad it on the right. Like if your sentence is shorter, you pad it with extras on the right. Truncation site also right. So suppose you give a sentence that leads to more than 512 tokens. Chop off. Why are we chopping at 512? If you remember the BERT architecture, we said 512 is the token limit that it will take. Then if you need more tokens, use some other transform, use BigBert or something like that. So special tokens, what are the special tokens it uses? For unknown tokens, it uses ANK, separator, SEP, padding, PADAD and class token, the first one CLS and mask MSK, mask, right? That is if you're doing a masking job, mask language modification. So was it good for BERT? It's 512 input and then 512 output? No, no, no. Output is up to you. The hidden state for a typical bird is 768 the hidden state and we'll see that in a moment but at this moment focus only on the input and the tokenizer part so what is that what is it doing the tokenizer if you use for example the x xlm roberta base which is which is good quite commonly used for multilingual text right and by way, multilingual for me is a reality. I tend to, I used to have under me as a leader, many teams. I had a Ukrainian team. I had a Paris team. And I have a team in India. Right. And what were some very interesting things you learn. So I would do these daily meetings and people felt very free with me like they never really took me seriously so they would you know they would get heated conversations and so forth so with the ukrainians what would happen is when they had difficulty explaining they would suddenly uh transition into russian right and then they would look at it, just explained you, right, and I have no clue what they said, you know, and they wouldn't realize that they just spoke something in Russian, so they have a mix of English and Russian, and I had to sort of read between the lines to figure out what they must have said, or they would use a Russian word for something, so I literally had to use my own attention heads to figure out what they must have meant. When I would talk to, similar thing would happen with the French people. And then with India, it would be very interesting if two people are debating and they're both Telugu, before you know it, they're speaking Telugu. And if those two people are from Kannada they're speaking in that right and suddenly sometimes some peculiar things would happen one guy would suddenly launch into Tamil and then the Kannada would say you know I don't know Tamil in English right so the conversation would be actually multilingual in my meetings so well if you're parsing a multilingual in my meetings. So well, if you're parsing a multilingual conversation, you do need an auto tokenizer that's multilingual sensitive, right? So you use xlmrover.tabase. It does a pretty good job, but notice that the tokens it is using are, do you notice that? What is it using it for the start of a sentence yes right the html notation almost html markup notation and slashes which is again the end of sentence um that so that is it and then it says any word start is with an underscore right and if it doesn't start with the underscore then it probably is just a suffix right so every tokenizer will have a different convention i leave it to you as a homework to find out what sort of tokens the bpe will generate bpe tokenizer generates which gpt uses right so now i also said that see tokenizer itself is a pipeline there is more going on in the tokenizer than you think. Right. So there are 4 steps in the tokenizer. First is basic cleanup. Right. So what what's a basically like string white spaces, multiple white spaces you may want to condense into one or something like that. Remove accented characters, lowercase everything. Those are your normalization steps. So D-O-G capital would look the same as D-O-G small case and mix case and so forth. All of those things you do. Normalization is something that happens from other languages. I don't know what it is. If somebody can explain to me I'll be happy I mean no point in my regurgitating the definition of normalization but apparently the same thing can be written in multiple ways in I do know in Hindi and Sanskrit you have the word the there is a sort of a wobble or not wobble or something not a wobble actually but r right a half an r you attach now you can attach before the word which will be at the bottom of the character or above the word as a curly crescent and that will come after the word, right? And so there are these peculiarities. I'm assuming that normalization somehow has to do with that. I don't know, right? If somebody knows what it is, can enlighten. So you do all of that, cleanups and so forth. The second thing you do is you do pre-tokenization. Simplest way to think of pre-tokenization is you have this sentence. If you're for English, if you're breaking it up into words, breaking it up into white spaces, right? That's a pretty good way of thinking of pre-tokenization. Or you know what? If you use Java, for example, think of the tokenizer of Java. Word tokenizer of Java. Word break. How would it do? It would break it up into words, and then punctuations would get separated out. That's a classic example of what in NLP you would call a pre-tokenizer, because the real tokenizer comes afterwards, which is the one that is sort of learned and what it is doing is it is breaking it up into word segments or word pieces either sentence p you can use word piece sentence piece bp whatever but word segments i'll use the word word segments breaking it up into logical word parts that are part of the vocabulary so tokenizer is the only one that makes sure that everything is broken down into something in your uh in this a very well published a tokenizer's dictionary right not Oxford's dictionary of Webster's you came with the tokenizer's dictionary its vocabulary must contain it right so you should say page 13 there is this token described right and it has an index to it that sort of thing uh giving the metaphor of an actual dictionary with pages and post processing does what oh you need to still pad it up with something cls sep pad mask and so on and so forth all of those special tokens that need to be inserted that will be the post processing system so tokenizer pipe is a pipeline in itself it goes through four steps are we together guys and enough for tokenizers we'll take a 10 minutes break oh my goodness it's already getting to be five do you guys have have okay now if it is five then we should start the next topic next time go ahead please uh so when you're talking about the multi-language uh tokenizer so how does it differentiate between like a token from like english versus like a token from french or spanish it does it moves you cross train it on corpus that includes both the languages okay so is there like a way for you to specify that i'm going to use these two languages oh yeah yeah you can do that I'm going to use these two languages? Oh yeah, yeah, you can do that. But generally these things are pretty good at even sensing. So one of the NLP tasks is you see character, which language is it? Language detection. Right, and so they're very good at it. Language detection is generally fairly accurate. So that action is generally fairly accurate. Okay, guys, I wanted to cover one more thing. The classifier. We will get to the sentence. So what is the journey ahead? Our journey ahead is we are now going to feed this tokenization into a pre-trained classifier. We will stay with our running example of sentiments. We will see how it is that we can do it, not using the pipeline, but by hand doing the pieces, putting the tokenizer, taking its output, feeding it into the converting it into the PyTorch format, then feeding it into a PyTorch model, like, let's say, Bird or something like that, taking the output, then putting our own softmax or some classifier layers, running it through that, seeing the output then putting our own softmax or some classifier layers running it through that seeing the output then what happens is remember this is one part of the journey but we are used only pre-trained there is no fine-tuning here so the next thing we'll do is we will take the emotions data set and we will now fine-tune bird specifically this whole pipe will fine-tune the model to do a much better job on this particular task emotion detection right and see how well it can do and we will see so to give you a sense we will see that you'll go the pre-train will give you accuracies in the 60s, which is pretty good considering that if you're predicting in six classes, you're gone from somewhere like 16% to 60s, pretty huge lift, but you can do better. When you fine tune, you will go into the 90s,s. And when you go into the 90s, especially considering that there is a class imbalance, you will realize that that's a pretty good lift. And that argues in favor of the fact that you should never skip the fine-tuning stage. A lot of people, they just take a pre-trained model and say, I'm done. It's working good now. But it's worth fine-tuning model right that stage is worth doing the second thing we will learn is once we have fine-tuned the model we will look at those cases where the loss was high the prediction loss because see when you predict a number and you compare it with the answer. Your probability says, let's say for anger, it says 0.1. But the answer is anger. So the loss is high. Cross entropy loss is high there, isn't it? That's the whole point of the cross entropy loss. So what is cross entropy, guys? Going back to what I explained about classifiers and this, the cross entropy loss is a big word but all it means is it is the um the level of surprise you have right so one easy way to think about it is that think about this if you know that you think your model is good presumption model is worse or the radio typically is the weatherman the weatherman has said it's going to be the chances of rain are less than one percent and you go out and it's pouring are you surprised it. Assuming you believe the weatherman, which these days is hard to believe given the California weather, but let's say that you believe it. Are you surprised at the weatherman's prediction now? Right? So let's say that he said 1% chance of rain and it's pouring outside. How much is the surprise? Suppose he had said 99% chance that it rains. You went out and it's pouring rain. Are you surprised? You're not surprised. But when he said probability is 0.1 or 0.01, 1%, then you have a massive surprise. So you say, well, how much is the surprise? One way to look at it is to say, well, it must be the reciprocal of the probability. Think about it. If the probability is high, predicted probability is high, my surprise is less because it's rainy. If the predicted probability was low, now I'm really surprised at the model's prediction. Unpleasantly surprised, isn't it? So 1 over p may be a good way of thinking of surprise. So 1 over p may be a good way of thinking of surprise. But there is a problem with that. The 1 over p, if you think about it, what is the range of values it goes through? It goes from 1 to infinity. You may say that, well, when it said 100% chance of rain and it rained, why am I surprised? Why is the value even one? I would much prefer if the values was zero. It went from zero to infinity, right? And so there is an easy mathematical trick. Mathematicians will say, oh yeah, that's an easy one. Take the log of it, right? Log of one is zero. And log of infinity is still infinity right so log of reciprocal probability is a measure of surprise but you don't use the long word measure of surprise you instead use the word entropy entropy right it's your cross entropy and that is exactly what the cross entropy loss is. You got it? That's why cross entropy loss is the summation over all the probability predictions and how wrong they were. we didn't reach as far as we did from next time, we will have to move a little bit faster. I need to finish the second phrases and we have a lot of interesting examples. But the point is it is far better to understand deeply and move slowly than to rush ahead. Any other questions, guys? I'm totally open. We have 15 minutes. Go ahead, Sushil. Follow the phrase, right? Followed by this phrase. Follow the phrase, So, it's a follow-up phrase name, right? We have a tokenizer followed by this for the phrase name part of the statement. No, no, not sentence transformer. R transformer. Remember, sentence transformer is a special architecture that uses a transformer in a Siamian architecture, right? Network. Two of them put together. No, ignore sentence transformer. Just transformer. So, classifier. Yeah. So, classifier is therefore... right network two of them put together now ignore sentence transfer just transform yeah so a classifier is therefore so let me give you a preview of what it means uh what we'll come to and we'll repeat it next time uh so imagine that you know what a classifier is. You have a classifier, you give it an input vector, output will come up. The problem is we don't have an input vector, we have a text. So what is the journey that we need to go through to classify? First thing you need to do, you need to tokenize it and make hot encoding vector. But then you realize that if you feed it into a direct classifier, your performance will suck because it is not attention. It hasn't gotten the benefit of attention. The semantic awareness is not there. The context is not there. So what do you do? You sandwich and transform a body into it. And it's so turns out that these transformers that are posted on Hugging Pizzes, most of them, they're headless. Remember, they're zombies. So if you want to classify, you need to screen a classifier head on top of the transformer body. So the transformer is sandwiched between two things, tokenizer before it and classifier after it to make the complete pipeline text classification pipeline you got it right so look at the picture on my screen that's what it is on the code they're calling some pre-trained model there they're also because even the tokenizers have been trained right so in fact why don't i let you explore the question it's interesting go check it out the code of the tokenizer is available see how it is done okay so i'm trying to fix the yeah the pre-trained mark yeah the pre-train just means that somebody has already created the tokenizer for you and it's pre-trained in the sense that it is completely adapted to the for example if you get bird base uncased it is completely adapted to producing an output that can just feed into work that's what that would have assumed some track tokenism yeah and the the transformer assumes the tokenizer these two have to go together they have to go together yeah the reason for that is why why are the two broken up the reason they are broken up is one even though they are producing the same vector, embedding the same embedding to go into the transformer. Remember the fact, or not even embedding, just the hot encoding vector, one hot encoding. They're producing the same given BERT, let's say that we are taking BERT case, uncased base model, one tokenizer is good at English. Another tokenizer may be good at Mandarin. Another tokenizer may be good at multilingual. Another may be good at Tamil, right? And that is why you don't want to sandwich the tokenizer. You don't want to put the tokenizer. You don't want to put the two together. You have far more flexibility. If you let the transformer just be the transformer and let the right tokenizer bring it the same one hot encoded vectors that it expects. And it doesn't care how it got that vector because the different tokenizers are better at dealing with different languages right that is the now you got it right and you don't want to create one mega ginormous tokenizer that does everything perfectly right that is it so, see, you're used to Unix, right? What is the basic Unix philosophy? Simple commands, it does one thing well, and then you use the pipe. And you weave the pipes together and before you know it on that one command line, you can achieve what some commercial software does after charging you $99. dollars. That's a basic philosophy. So the whole idea is build pipelines out of reusable components. That is why tokenizers are interchangeable based on what data you have. Use the right tokenizer. Models are interchangeable. But of course, once you pick a tokenizer, you better use the model compatible with it. Right? that's it the name will be a giveaway the tokenizer name will start with like a bird case uh tells you clearly that i i i'm going to feed into bird right that's it generally these checkpoints right the names are such that if you use the same name for the tokenizer as well as for the model you're doing pretty well how do you no no so the documentation will say if you go to the the card of that thing the tokenizer it will say that it has been trained for english or for this or it's good tokenizer it will say that it has been trained for english or for this or it's good for this it will say that and that brings us again the same thing right people are posting a lot of things on hugging phase doing good diligence and writing good documentation means people will actually use your stuff if you don't document it properly nobody knows what it is for give Give sample code, document it. That brings me to one thing, guys. This file, the classifier, auto classifier, don't use it. You can use it, but the last element, I think has a bug at this moment. I need to fix because the things have changed a little bit. But now, this book, the book that I was recommending is there in this, Natural Language Processing with Transformers.