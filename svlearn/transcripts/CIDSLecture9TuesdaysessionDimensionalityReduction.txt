 No, wait a minute, actually, I'm still at the top. The topic for today's, today evening's discussion is something called dimensionality reduction. It is, we're going to not go too deep into it, we're going to, in this workshop limit ourselves to just a few methods, linear methods more or less. So, in particular, we are going to focus on something called principal component analysis. Now, why do we need to? So the first question is, why do we need to bother reducing the dimensionality of data? Would anyone like to volunteer? What purpose does it serve reducing the dimensionality of the data? What purpose does it serve? You can build simpler models. You can build simpler models, yes, that is a good question. Easy to explain. Explainability is, yes, a factor. Easy for visualization. The curse of dimensionality. The curse of dimensionality, very good. And what what did you say kaya uh it's easier to visualize these are main factors let's write it down because these these are some of the key factors that matter see we we talked about the curse of dimensionality Curse of dimensionality is one. The other one is visualization ease, which is a very important point. Remember that the world's most powerful inference engine is still the human brain and the human eye. The human eye has gone through 100 million years of training, learning, to the best of our knowledge. So if you can visualize it, our eyes will immediately discern a pattern. So visualization facilities. So visualization facilities. The next is, I think you mentioned that simpler model. And we can go on with a few. Does anybody want to add something else? The computational complexity, it's a lot simpler in terms of the computational load computationally less expensive determine which features matter most that is right and that is another thing That is right, and that is another thing. What the real underlying features are. What are the real underlying features? And we can go on. I think we have captured all the principal reasons why we want to do dimensionality reduction. So let me recap this. Curse of dimensionality is something we learned about last time and it was a pretty bad curse, like if you think of it. It always reminds dimensions, you lose the notion of distance becomes problematic. Most points in higher dimensional spaces are practically sitting on the periphery. In a fairly any reasonable distribution, you'll find that most points if you sit at a point and you try to reach other points you will have to go practically to the periphery to find them long distances you'll have to travel the other what it let me write it down, dimensionality reduction. So we'll go over point A, B, C, D, E. So A is the curse of dimensionality. What it means is in higher dimensions, one space to the periphery, periphery to find neighbor points. So that sort of makes a mockery. I mean, if you have to go so far to find neighbors, is it really worth it? And it follows that very sparse, these spaces become very sparse. There's just too many degrees of directions in which points can sort of diffuse out to right so that is a problem and it also manifests itself in a very interesting way if you look at the distance and you plot any reasonable distribution of the data and you try to plot how what is the distance to all the other points from a point you will see that for lower dimensional spaces for example in two dimensional spaces like for example the data distribution that kyle was just now showing in the quiz any reasonable data distribution and i might as well make it so suppose you have a data distribution like this i think this was the data distribution we were seeing something like this I think this was the data distribution we were seeing. Then if you plot the distance to the neighbor for any one point, let's say even if you take this point or this point, whatever point you take, and you plot the distance to the neighbors, you will find that they are all well-shaped, something like this. And those distances are short, quite short. Then, on the other hand, when you go to very high dimension, so this is low dimensional. When you go to very high dimension, what you find is that the variance of this, you know, the points, the variance of this distance, do you notice is very small so points that are so-called near you and the points that are so-called far from you on the grand scheme of things when you compare the total distance even to the nearest one this distance this distance delta feels very small, isn't it? Are we getting the point, guys? And we went over this when we talked about higher dimensional spaces. Remember, we did this thing that to get to 50% of the points in a uniformly distributed sphere, hypersphere, how far do you have to go? a distributed sphere, hypersphere, how far do you have to go? As as the dimensionality increased, it turned out that to just get half of them, we had to go 99.999% of the space, the distance to the boundary, to just meet the neighbors, to more than half the neighbors. So that is the problem. So what happens is that other half of the neighbors are all squooshed in a very tiny distance right relative distance from from you so that that is the curse of dimensionality what happens is your distance measure, your Euclidean distance or even Manhattan distance, they begin to fail, basically. Euclidean, so I'll just use the word Euclidean, to Euclidean gets into trouble. You can't do K-means clustering, you can't do any one of the distance-based algorithms effectively in very high dimensions. So that is the curse of dimensionality. Then the second point that you folks mentioned wisely is be visualization ease. We can plot one of the, if you notice that in this workshop, we have developed a lot of intuition about regression, about classification, about all sorts of things clustering because we took data in two dimensions and therefore we could visualize it. But when you take data in very high dimensionality, now you can't visualize it. You can take cross sections of the data, but there are too many permutations or too many combinations of X, Y axis to put on a page. And so it becomes, and we do make pair plots. If you remember for the auto dataset and for the breast cancer dataset, the California housing dataset, the feature spaces were quite big. What did we do? We took pair plots. The pair plots are instructive. You do get something out of it, but the number of pair plots goes up exponentially with the number of dimensions, isn't it? They order n, they order p squared, if p is the number of dimensions of the data, number of p is the number of features. All together is basic, the miniaturics, and p, p minus one over two. So many pair plots you can make. So it becomes hard,ization gets messy and hard. And ineffective. Because there may be a relationship between x, y and z, let's say three axis, x1, x2, x3, in such a way that you have to see them together, or four dimensions, you have to see them together. Now, just making pair plots won't help you in that situation. So if you could project the data down to a lower dimension, that would be wonderful be wonderful right the other reason is simpler model obviously that goes without saying so if you if you can build a model in a lesser number of features or extracted features derived features then you can explain it you You can, I mean, it simply goes without saying that a model in two or three features is far more interpretable than a model of, let's say 44 features or something like that, or a hundred features. So simplicity of the model, it hardly needs to be spoken. Computationally less expensive, which is another point. Excuse me. If you take data in very high dimensional spaces, everything, including distance, is hard to compute. Care nearest neighbors is hard and expensive to compute. But when your data is in much lower dimensions, you can do that much more rapidly. So that is it. And lastly, the real, I mean, in a way, the point of dimensionality reduction is not just to tame the beast of the data, but actually derive insights. It turns out that when you take data down to lower dimensions, you understand what are the real features that are involved. So now that is an interesting statement. I will qualify it. I'll just explain it. Let us say that, take a very simple case in which some dimensions are just noise dimensions. They don't matter. For example, I apologize today. I seem to have a sore throat after my walk in the cold evening today. So let's say that you're looking at anything. For example, you're measuring ducks, and you want to look at the relationship between their size, their age, their weight, and so forth. But somewhere in there, you have a feature which might not matter, which is that which, like at what distance from maybe the house where you set up your telescope to measure thing, the ducks. Look at the ducks without disturbing them or binocular or whatever it is, or measuring instrument. Let's say that you have a central point. Well, I'm contriving an example. Let's say that in real land, in a land there is a pond, and your observation center is here. From here, you decided which of the ducks to, which of the ducks to go and measure. And then you went to the ducks, and obviously you weighed them, you measured them, and you so on and so forth. But this was the point, or maybe this is the point where you brought back the data. And so you might have also computed the distance to each of the ducks. Now, you would agree that in most reasonable situations, probably this is an irrelevant factor, but let's say that you don't know. So now in your data, you have weight, you have size, you have age, and you have distance. So when you look at the data and you realize that distance doesn't matter. If you realize that distance doesn't matter. Have you learned something about the data? Have you learned something about ducks? You've learned something that doesn't matter. Yes, you have learned that certain things don't matter, that ducks, they don't they don't sort themselves by size based on distance, for example, or anything. Or for example, suppose there is another irrelevant factor, which could be maybe in a particular pond, or something to do with a beak some feature of the beak and it turns out that that feature of the beak is irrelevant right for whatever even if you look at the data you you find that it doesn't really matter so being able to take certain features out and know that their noise is a very important consideration. It gives you insight into the data. So there is an illustrative example I'll give. It's a very simple example. Imagine that you are measuring the... You have a spring and you have a mass. You must have done this little experiment. And what happens if you imagine that you don't remember your physics. If you pull the it and leave it? It will spring back and it will keep oscillating back and forth, isn't it? But let us say that you have a camera at an angle. So the spring is like this. From your point of view, from the camera's point of view our camera is pointed this and this camera can measure the could you please pause the recording for a moment so imagine that from a camera's reference frame you you're looking at, I'll just measure it as x, y. Or maybe to follow the convention that we have been following, that we give feature directions x numbers. This is oscillating back and forth. Let's say as it is oscillating, periodically the camera is taking account of where or taking an observation of where the location of this point is. Let me call this point P. So you will end up with things like X1, X2, and you will end up with a lot of measurements. Does that make sense guys? You'll end up with some location. Then another location, then another location, lots of locations of that point as it oscillates back and forth. Am I making sense? I hope this is quite simple. Feedback, please. Yes. Yes. Yeah, you have to keep giving me feedback. I can't see you so it's very hard for me to know whether we are getting it or not. So, if you plot these locations, and let's say that the camera has some error instrumentation error so it will more or less suppose this is the equilibrium point it will have lots of measurements around it and because it's making errors it will also it won't actually make measurements along just this point it will make little mistakes or maybe the spring is not as tight and linear as you would imagine it to be. So for whatever reason, you end up with data, something like this. When you look at this data, X1 and X2, obviously, if you recall your physics, you expect all the data to essentially have fallen along this line. Right? But data has not been there, it's sort of spread out like this. So you would say that, you know what, your camera wasn't positioned properly. You should have positioned the camera in such a way that you basically saw one axis, the real axis, isn't it? Let me call the real axis, what should we call it? Let me call it the x1 tilde, right? And let me call this perpendicular direction x2 tilde. So you would have said that if you can rotate your camera such that you're looking in the direction of x1 tilde, that seems more natural. Would it seem more natural? Yes. That would have been a more optimal way of doing it. Now, in some, and now comes a crucial thought. If you look at it, why did we get these lift along the x2 tilde direction? This direction, you know the points, they also have a little bit of a value along the x2 tilde direction. What do you think that represents? Noise? Yeah, that represents noise or instrumentation. That is the noise. I know, that's the noise. Let me write it this way. This is the direction of noise in the data. And this is the direction of noise in the data and this is the direction of the actual signal isn't it and so you would say that you know what the maybe maybe the real thing that matters is just to store x1 tilde values we shouldn't store the x2 tilde values isn't it and therefore the question comes how do you go from x1 x2 a coordinate system or let me just write it this way how do you go from this coordinate system x1 x2 to x1 tilde x2 tilde. Now let's think about it for a little bit. We observe a few things. First, we observe that you have rotated your coordinate axis, isn't it? We have to rotate our coordinate axis. And we also have to go to the center of the data. It's much better to have an axis right at the center of the data. So we need some transformation, some way of discovering what is that transformation, M. What is that M transformation or M transformation that will transform X1, X2 to not any other direction, but to that specific direction in which the signal and noise separate themselves. Right. So let me explain this with another way. See, in this way, when you look at this data, this data, and again, this data, let me put an envelope around it, right? This is your data. This data has a covariance matrix, right? What it will have is, it will have a variance, it will have a variance along, there will have a variance it will have a variance along there will be a variance you can talk about variance along the x1 direction variance along x2 direction you can talk about the covariance along x1 x2 of x1 x2 there are three things you can talk about in the original coordinate system. Would you agree, guys? This data, you can compute the variance along the X1 direction. Like, it varies from here to here. Let me repeat this data here. So actually, let us do a little thing. It is always good to go to at least, if nothing else, if not a full scaling, but to at least go to the center of the data so let me do this very simple scaling so this is x 1 x 2 now i go to another coordinate system in which the data has come become like this the same data is now spread out like this your spring data is what have i done i have just gone to the coordinate system so what is the what is the location of this this is x1 bar x2 bar this is a this is the center of this data isn't it the average value along both the directions. You would agree? So if I do this, x1 minus mu, well, okay. Let me just use mu because we have been using mu1, mu2. Mu1, mu2, mu1 is the new axis. And x2 minus mu2 is the new axis here, the data is centered. Now we are asking, if you look at this data and you plot the histogram of this data, would you agree that the histogram or the density plot of this data will look like this along the x-axis. And along the y-axis also, it will look something like two bell curves, right? Along x1, and this is along x2. Do you agree guys? If I project this data down to the x1 axis and the x2 axis, I will get two bell curves more or less as histograms, as frequency plots. We will get that. Right. So this is interesting. On the other hand, maybe I won't be at the same scale, forgive me. I hope there's a lot of dots on the page. Let's look at this data. And now think of a new coordinate system, which I'll color code it differently this data right now once again look at something interesting what would be the uh plot well okay let me use pink here again what was it pink was it okay yeah let me again use this along this direction you would have this distribution and along so this is along the x1 tilde direction x1 tilde let me use a different color just to illustrate this and along um this direction the the frequency plot would be like this right not even like this, or maybe a little bit closer. Raja Ayyanar?nilavarashivarotri.com density plot would be. Raja Ayyanar?nilavarashivarotri.com Like this. Raja Ayyanar?nilavarashivarotri.com Would you agree guys. Raja Ayyanar?nilavarashivar would you agree guys sure right now this is along the x2 tilde axis right so now what can you say looking at it observe something is the noise at all correlated with the signal right right? So for example, in this picture, do you, noise you expect is of unknown origin. It will have nothing to do with the signal, isn't it? So you would expect that the covariance of the noise, the covariance of x1 tilde, x2 tilde will be approximately zero. Isn't it? Should be zero. Signal is not related to noise. Right? Am I making sense? On the other hand, in this picture, the signal is there on both the directions along x1 and x2. So here the covariance matrix will have the component sigma x1, right, square. I mean, by the way, this should be square, but forgive me for just being sloppy with the notation. OK, let me make it sigma x2 square. And sigma x1, x2, sigma x1, x2. This is your covariance matrix. Do you remember that? So in other words, it's just a fancy way of saying there are three things that matter. The variance along the x1 direction, the variance along the x2 direction, and the covariance between the two x1, x2. That is what is captured here. Right. So on the other hand, when you do the, if you were to do it along this axis, you would say the covariance tilde in the new frame of reference should be, it should look more like this, sigma x1 tilde square sigma x2 tilde square 0, 0, approximately 0, 0. That is what you want. You want to find those directions in which you separate out the signal from the noise. Would you not want to do that? Would you not just removing this, putting all the noise along one axis and then forgetting about that axis altogether. You would have removed the noise to the first approximation, isn't it? So when you reduce the dimensionality of the data strategically, you end up reducing noise from the data, which is an important gain to get. You get insight into the data because the signal bubbles up. So far with me, guys? In the blue axis, how are we sure that the covariance is almost zero? the covariance is almost zero? The idea being that if error, if the deviation along X2 truly is noise, right? Noise has nothing to do with signal. Noise has a Gaussian distribution. It's instrumentation error, isn't it? So it cannot have a correlation with the signal itself. It's an underlying assumption that you have to think through. Like in this example of a camera, that's why I took this example of measuring, a camera trying to measure the location of this point. So the coordinates, so suppose I want to do the equation. So suppose I want to write the equation of where is X, the location, like if you want to do, do you want to write the equation of where is x, the location. Like, if you want to do, do you want to do it like this? X1, X2, some function of, right? Suppose you want to discover the real relationship. How does, with respect to time, for a given time, what is X1, X2? It's a lot harder, I hope you would agree, than writing it, the location here in this axis where you can say x1 tilde is equal to, basically, to the first approximation, you could write it as An oscillation, isn't it? In terms of, let's say, sine something. Sine t. Up to a constant. Let me just say sine omega t. This is the equation of its bouncing back and forth. So forth, right? Are you guys with me? If you remember, just bouncing back and forth, right? So what this equation would be far better to discover than some very complicated relationship, very complicated relationship in the original two-dimensional variables, two-dimensional locations, isn't it? You don't want to do that. And so you end up building a much simpler model. Let's say that at the end of the day, whenever you take the data, and obviously I have suppressed the time axis here. Maybe in the data, we can add the time axis, but let's just look at this here. This is what we want to discover. It becomes far easier to discover it when we put the data like this and the x2 axis is by construction the way we constructed our experiment uh we know that it is the camera's measurement error right and measurement errors we generally believe or we uh they tend to be gaussian in nature well curved in nature okay so in the yellow axis we see that when x1 increases x2 also increases but the blue axis when x1 increases there is no there right there is no correlation okay yes there is no correlation so in other words you can see for example when you do this thing when you do x2 tilde as some function of p you will not discover any function no function no relationship no relationship to time right so suppose your data is X1, X2. And now let's bring in the other dimension also, time. You will find that in the original axis, you can write X1 as a function of time. OK. X2 is a function of time that some other function of time, g. That's OK. Right. some other function of time g that's okay right but in the transformed one you will have x1 tilde is some function let me just put capital f of time and x2 tilde is basically is basically noise it has no relationship to time. Yeah. Would you agree? So that is the, what happens is when you do dimensionality reduction, especially this sort of technique, it helps you weed out the noise. There are simpler ways of weeding out the noise. The first is just take a subset of the features, the features that matter. Find out if, for example, the distance from the observation room to the duck mattered at all, right? You realize that it is just junk. Ducks are randomly distributed in the pond, right? It doesn't mean much. So you just throw away the feature itself. But on the other hand, in this method, none of the features did you throw away. What you did is you created new features, x1 tilde, x2 tilde, which are in some sense far better in separating the noise from the signal. Now, I give this example with just two dimensions. One dimension, because of the way I framed the experiment you knew was signal and the other dimension we knew was noise instrumentation noise in the camera but now let's generalize it to real life what happens is that data comes and you know that hidden in there are some real directions in which signals exist. So signal here exists in only blue dimension, but it could have been a plane or it could have been a hyperplane. Right. So let me mention this. I say that the data exists in p dimensions. So in other words, your feature space is p dimension. Another way of saying this is dimension feature space is equal to p. It's a p dimensional space what we are basically saying that the real signal part of the data the real crux of the data it lives in a lower dimension we assert that signal exists in a lower dimensional subspace of the original space, let me call this dimensional space of dim, let me just call k, dimension k, such that k is less than p. That is a basic assumption. Now, when you assert something like that, it may or may not be true. The data may not exist in a lower dimension. But you have to assert and then try it out, remember in modeling, we make a hypothesis, then we see whether that hypothesis was true, because you just got the data, isn't it? The question therefore is, what is is the value of k? Two, do we re-represent data in Rk instead of RP in P dimensional data, how do we represent this data in a lower dimension K? So this, to be able to answer this question is to do dimensionality reduction. That is the crux of dimensionality reduction. How would you know what dimension the data truly lives? So what is the intuition behind it? Intuition is, and now let me give you the intuition here. What we are saying is that, and look at it now geometrically, we have this data. Let me just put it here. We'll just take the center mean, assume center mean. That we have scaled the data properly. So assume we have centered our axis. So we are always looking at x minus mu x, right, for each of the directions, x1, x2, always. So with that assumption, data is somewhere like this. One geometric way to say this is to say that really data does not exist in two dimensions. It exists along this, just really along this axis, one line, isn't it? And this, maybe I'll add this. To the first approximation, basically the data is along a line. Would you say that? Yeah. We see that. And we can say, you know what, let's ignore the second one, anyway it's noise. So you have gone from one dimension to two dimension, but geometrically observe the the plane is wide but the data is very along a very narrow line a line is one dimensionality less than the plane isn't it so here k is equal to one for this? And original data was R2, right? What is the dimensionality of line? R1. So geometrically, you literally see that data was not fed all over the feature space. It was actually limited or localized to a lower dimensional uh surface right so now we'll generalize it from a line the first generalization is um it could be a hyperplane right if you go to if your data was in five dimensions or ten dimensions then there would be some hyperplane then there would be some hyperplane of dimensionality from one to nine, hopefully, in which the data is close to. So the way to think about it is that, let me take a sheet of paper. Okay, I don't have a sheet of paper here. So I'll take this book instead. This book is fat, so don't look at the fatness of it by the way this is a draft of the book that i'm writing so data is essentially on this plane real data is on this plane but some points are escaping the plane so you would say that in three-dimensional space a two-dimensional plane is enough to represent the data. I can ignore the orthogonal direction, which belongs to noise. Are we together, guys? So far? So that is a crucial thing. You have discovered a structure of the data. When you do this and you find that it works, what you find is data really exists. Data really existed in a lower dimensional hypersurface. And now this generalization, I said hyperplane, but what about hyperplane? When the relationships are linear, like in this example, the relationship was linear, but nothing says that it could not have been like this for example the data could have been very tightly along a quadratic curve all of it would you still say that the data is localized to a low dimension a low dimension the actual dimensionality of the data is low would it still be fair to say or not suppose this is the data yes and if you transform with a quadratic function yeah what you need to do is all you need to do is find some things that straightens this out, this curve. We know that there is a bend here, right? And so now we can't just use some linear rotations, et cetera. We need to find a transformation that will transform the data again back to one direction because data is actually only in one dimension. This is a nonlinear transformation, right? Whereas this movement to the red or just rotating and having your axis along the red line would be a linear transformation of the data, right? red line would be a linear transformation of the data. So when you do dimensionality reduction, there is a vast amount of literature. And in real life, there's a lot of non-linearity of data. And that is exactly what we talked about on Saturday, that there are kernel functions that will linearize non-linear surfaces surfaces isn't it so you can always go to a higher dimensional space where the data is linearized but then you can say now wait a minute the whole point was to go to a lower dimensional space not to a higher dimensional space to linearize it right so for nonlinear data the journey becomes more complicated what you do is you kernelize it to go to a high dimensional space where you find the very small low dimensional space, which in this particular case is still a line, right? And you project down back to a one dimensional line if it was this data, right? So you would take a kernel that will take you to a higher dimension. In this case, I presume, a kernel that will take you to a higher dimension in this case i presume for example a polynomial kernel should have done that would take you now to a slightly to a higher dimension and then project it down to a lower dimension from there right align because in that space you can do once again your principal component analysis right and so that is one technique, kernel PCA. We'll do a lab on that. But there are many more techniques. People have gone and brought in pretty heavy machinery from differential geometry and topology. So the area is called manifold learning and so forth. So out of that have come techniques which are the state of the art. I won't be explaining those techniques in this workshop. It's a little or maybe it's one of the Tuesday sessions. I will. Those are like called a Tsne. So let me just write the word t-s-n-e right umap those are very interesting highly non-linear techniques that can deal with non-linear data not always but sometimes sometimes even those techniques can't help you why because the data truly occupies the entire feature space in a genuine sense you can't do dimensionality reduction. You can do dimensionality reduction if truly the ground truth lives in a lower dimension, non-linear or linear. Now, today's topic, today's emphasis will be on one method, which is the one that we just did, and I did it for you without naming it. When you take this data, so look at this data, which is on the left-hand side here in the X1, X2 axis, and I created a rotated coordinate system such that we could separate the noise from the signal. Or it may not be noise. Now let's be softer about it. Let's say that X2 direction matters less than X1 direction. X1 direction really matters. X2 is not really. X2 tilde. X1 tilde really matters. X2 tilde doesn't matter. So you can go. You need not be very strict and say, well, x2 is noise. You can say, well, it doesn't matter that much, right? Or for example, if you're looking at, well, let's go back to our ducks. What would be something that slightly matters when ducks grow up, but not that much? be what would be something that slightly matters when ducks grow up but not that much maybe the coloration of the beak right the the um the wavelength the wavelength that represents the color of the beak right and you may see that the color of the beak doesn't quite change with the wavelength, doesn't quite change as the duck grows up. By the way, I'm not a biologist, so I may be entirely wrong. Anybody who has a better intuition with ducks? I know mallards, the male ducks have a nice bright color band on their neck and the females are more dull color more dull colored but how about their beaks are they the same color more or less okay guys again please pause the recording dennis is here let me open the door for him one second okay so we are asserting that if you look at the wavelength that you measure representing the color of the beak, the wavelength would show a very narrow variance in that direction. Let's say, the wavelength direction, most of the values will be very, very close to each other so that your data will be spread out. But this peak will be the error spectrum or the error variance will be very small in that direction. So what have you found? Well, it turns out that the color of the beak, at least in that species, is not significant or not a significant descriptor of the ducks size weight etc are more pronounced descriptors of the duck you would say right so that is what is meant that it need not be just pure noise it may actually be small variations that don't matter as much now so our goal is to find though create a new coordinate system by rotating the original coordinate system, such that we can separate out the directions of signal from the directions of either noise or, you know, things that matter much less. To do that, what do we need to do? See what have we done? We have created a rotation. That part looks obvious. We have rotated the coordinate system. We have gone to its center. But actually, we do more than that. And now I'll come to the main thing that we do. This is, you pretty much got the idea, but we'll do it again. Principal component analysis. Analysis, analysis, principal component analysis. It's one of the oldest techniques actually, and as many old techniques, it has many fathers. The original attribution at some point was given to a bunch of French people, then to other people, then to other people. It turned out, to the best of my knowledge, if you look at all the dates, the earliest that I could trace it was actually to an not so well-known Indian mathematician named Kusambi, who was from BHU, Benares itself. A remarkable fellow, and it's a family actually, father and son. Father made a very interesting discovery. He showed that if you just studied the coins of the different eras, you could say a lot about the history of the land. So he brought about the relevance of coin collection as a historical office to be something of historical significance. And the son, I believe, discovered this principle component analysis. He published it also, but in a journal, in a Western journal. But for some reason, he was not well recognized. In fact, I went to the department and I asked, can I see if there is a picture or something of Kwasambi? He was a great mathematician who did prolific work, but was mostly unknown. And much of his work, I find that is attributed to other people, right? Because they rediscovered it at some point or the other. So, and I went to the department and said, there must be something about Kusambi there. And there wasn't, nobody even knew who Kusambi was. So it just shows you how geniuses fade into obscurity. But anyway, he was at least one of the earliest discoverers of principal component analysis, if not the earliest, the first discoverer. So what is this principal component analysis? It says, and I will, it says exactly what we did. Find these directions, this new x1 tilde direction and x2 tilde direction these are called principal component one principal axis one and principle access to the two principal axes right and where does it come from the the two principal axes, right? And where does it come from, the word principal axes? They are, intuitively, they are the axes that matter more. First axis matters more than the second axis matters more than the third axis. That's the basic intuition. But there's another way to look at it, and that is what is significant. See, going back again to our quintessential data that is like this this explanation suppose you have the data that is like this imagine that you're shrink wrapping the data would you agree in your imagination if you shrink wrap the data would you agree in your imagination if you shrink wrap the data it would become an ellipse ellipse and in higher dimension i'll just call it ellipsoid it's a high high dimensional ellipsoid but let's keep it to simple things you know what let's talk ellipse. You can shrink up this data into an ellipse. Now in the language of ellipses, these are common terminology. We call this the major axis. This the minor axis. This is, make sense? Do we remember this in coordinate geometry way back in high school? You must have done this major and minor axis of an ellipse. Yeah. That's what it is. Now the question therefore is how do I go and discover without knowing it without seeing the data? How do I go and discover this principal axes are major the principal axis. So in this language, it will be principal axis one principal axis two, how do I go and discover it? So to do that, it turns out that the information is there in the data. And the key thing is the covariance matrix itself. If you really think about it, once you have the mu of x, original x, mu of x2, you date what the center of the data is at the same time if you have the spread of this data uh you know along each of the axis so in the original axis of course it is like let's say it is this so you will have uh you will have a bell curve uh well this is terrible drawing overwriting each other let me do a better job well we went over it let's just say that you need to know the variance along x1 variance along x2 these were important things isn't it but was that enough to describe the data you also needed what will variance along x1 x2 do it will show how spread out it is when you project it down to the axis. But what is most essential is how correlated or how much covariance there is. Because if the data is like this, is the covariance along in this picture in data set A? Zero. Covariance would be zero. Right. And in this situation, on the other extreme, 0. Covariance would be 0. In this situation on the other extreme, the covariance would be very high. In fact, correlation would be 1. Covariance depends upon how the data is scaled. In the unscaled form, what the units of measurement are in B. And then the reality will be, most reality, like our reality is somewhere in between. What distinguishes these three realities, A, B, C, what distinguishes it is sigma x1, x2 covariance, right, sigma x1, x2, which is the same thing as covariance x1, x2. Covariance informs us in a sense how strong the relationship between x1 and x2 is. That is why the word covariance, how do they vary together? Remember, we talked about it a long time ago. So now comes an interesting situation. For data sets, which are where the relationships underlying relationship is linear you don't know it yet, but the relationship truly is linear like for example, look at this data. You started with this data and you went to the blue coordinate system in the blue coordinate system x one mattered, x two doesn't. But the main point is the relationship in your mind if you were to draw a line to capture the essence of the relationship it would be a straight line right so here comes and i'll just make a statement if if data has a linear relationship, right? Data has a linear relationship, right? And one, two, the density plots are Gaussian, especially along error. Well, basically you have to assume that. So let me leave it as that. So suppose you have Gaussian, like bell curve. The data has a bell curve representation, right? And in particular along the target variable, let's say if X2 is the target variable, then it is like this if these two conditions are fulfilled then the mu 1 mu 2 mu x 1 mu x 2 see this is the mu vector right let me not keep on saying mu this is the mu vector isn't it mu along x 1 mu along x2 is the mu vector. So let me just refer to it as mu vector. Mu vector, the variance vector sigma, in fact, the covariance vector, these three things, sigma, if you're in a matrix, sigma x1 squared, sigma x2 squared, sigma x1 sigma x1 x2 remember this is it's a symmetric matrix because there are only really three things in the matrix this this and this and this is just copied here but it makes it a nice two by two matrix in our particular thing so there is formally a statement that if these conditions are true, and I think, did I miss a condition? I hope not. Okay. So then these two, then mu sigma, or the word is sufficient statistic. What is sufficient statistic? It's a formalism in statistical theory. You say that it has captured the essence of the data, right? It is enough to capture the essence of the data, or in other words, in some sense, to be able to essence of the data or in other words in some sense to be able to roughly recreate the data if all you knew is the mu vector and the sigma the covariance matrix then you can in a sense regenerate the data or something that looks like the original data not exactly the data but something that looks like the original data which i hope is geometrically obvious to you from what we are doing. So far, so good, guys. Anybody still here? Yeah, yeah. OK, so if you get this, now comes a beautiful fact you can ask what is that how do i find those directions and the answer it turns out is in this sufficient let's use mu and sigma itself first thing is just saying a mu's value is value is that it helps us center the data. So once you center the data, suppose you come up with x prime, y prime, right? Which is x, well, sorry, I wouldn't write x prime, y prime. Let me just write it as vectors. x prime y prime let me just write it as vectors x prime and uh which is equal to the original x minus mu what would be the what would be the mu of this what would be the mean of this what would be x prime bar or mu zero yeah you would agree that the mean of this by definition is zero, right? Because it will be equal to mu of the x vector, right? Minus, well, mu itself, the mu vector itself. And this is nothing but mu minus mu is zero, right? So in other words, mu, if you center your data around mu, you have extracted all the information that you could from the mu, if you center your data around mu, you have extracted all the information that you could from the mu, from the mean. Now, we do know that sigma contains the rest of the information. How does it contain the rest of the information? It is actually very beautiful the way it does that. See, and I'll first give you the answer and then explain how does it work. See, remember I said that the data is ultimately along an ellipse. Well, my ellipse is terrible. It's more like a more like an eggplant than an ellipse. But OK, so imagine that your data is like this. And now you have already centered yourself. Well, that too is not a center okay well can i be withdrawing yeah okay let me try again now this looks better okay suppose i have centered the data now and let me not call it x prime y prime i'll just assume i'll just give it x x 1 x 2 assume the data comes to a center so i don't want to put prime and tilde and everything on top of things so just if it helps you put a prime okay for the time let's put up suppose you get the data here you realize that you have to create this new axis and the and another new axis here right x1 tilde x2 tilde now you realize that what is the unit vector along x1 axis what is the unit vector it is 1 0 right simply because the unit vector is just of unit length along the x1 axis along x2 axis it is 0 1 now what we are doing is if we are going to a new coordinate system, we do need a new unit vector. Would you agree? We need a new unit vector. That is really fat. Let's try something less. Yes. E1, let me just call it E1 unit vector. So here the unit vectors were, what was it? This is often called I hat and J hat. You remember I, J in basic high school, you call them as I vector and the J vector along the X1 and X2 axis. You usually call it XY axis. Let's call this vector here, this is 2-hat, which color did I use? This one. So this coordinate system, call it e1, e2-hat, right? So we have two vectors, e1 along the first major axis, major axis and E2 along the in this case minor axis. If you think of it as just a two-dimensional ellipse. We can have unit vectors and of course you would agree that e1 e2 the dot product would be zero in other words orthogonal remember coordinate systems we still want to draw perpendicular coordinate systems right and the word higher dimension, you use the word not perpendicular, but orthogonal. And if their sizes are one, then it is orthonormal. Normal means, in this case, unit sizes, orthonormal. So far, so good, guys. Just as here, you would agree that the following conditions still hold. i.i is what? good guys just as here you would agree that the following conditions still hold i dot i is what i dot i is one j dot j is one i dot j is what is i dot j dot product of the i vector and the j vector zero zero. So the same relationship holds here. The E, E one, E one is equal to one. E two, E two is equal to one. E one, E two is equal to zero. It just says that we have a nice orthonormal coordinate system as we should. All right. So you know that E1, E2 is a rotation. If I take this vector, let's take this vector, this vector, and this vector. E1 vector, e1 vector e2 vector right in the original coordinate system if you look at the original coordinate system each of these and here the unit vector is i and this unit vector is j right you would agree that E1 can be represented. It has some coordinates. A unit vector has some coordinates along the X1 direction and the unit vector here. This is the X2 direction. So E1 would be represented in terms of some amount of X1 direction and x2 direction, isn't it? So let me just call it E1 1 or E1 2, isn't it? Some value. This little bit is E1 1. This little bit is E. I'm just taking the first, this quantity is E2, isn't it? This vector can be represented in the original coordinate system, like any other point. A vector is a point. That point has some coordinates. I can write it like this, isn't it? Then, Then likewise, I can represent E2 as E21, E22, right? Some values. Now, I won't draw E2 here, but you can assume that that also will have some value, right? So for example, actually, let me do that. This, suppose this is here, E2. This one is E21, and this one is E22. So far, so good, guys? Right. Now, there are names to these vectors. These two vectors, E1 and E2, they are called eigenvectors. That is their formal name, eigenvectors. All right. They're called eigenvectors. And if you now, there is something called eigenvalue, and I'll explain what that is. See, if you look at a point along the eigenve, let's say, E1, right? In that direction, in this direction, E1 direction. Any point in the X1, X2 feature space, take a point, and then do this. Apply your covariance matrix, your matrix, to that point. See, this is a matrix, right? So a matrix will rotate it. So this is a little bit of a matrix let me write it out in this sigma x 1 sigma x 2 square square sigma x 1 x 2 sigma x 1 x 2. this is what it's a matrix right if i apply to the to any point along this direction right let me just call it a point X1, X2. If you do this, something very interesting will happen. It will just become lambda, some stretching factor. It will just get stretched out. It will just get stretched out. Of one by one, right? of y1 right so you can say that sigma of x y y well i i shouldn't use the word x y sorry x1 x2 my apologies i keep mixing the notation but yeah x2 x1 x2 right then well am i am i saying it with the covariance matrix diagonal v transpose okay let's just say it is something like this x1 x2 is it with the diagonal or the eigenvalue matrix okay we'll leave it as this for the time being when x x1 x2 are along e1 the same thing happens when you do this x1 x2 along for a for a point along e2 direction it will become lambda some value lambda 2 it will get stretched by a certain factor x1 x2 however any other point absolutely any other point that you take and you you take any other point let's say you take point there are these points you take this point and you apply this point will have a certain coordinates when you apply this matrix it will actually go through two things it will actually get rotated and it will get stretched so when it goes here and you look at this you will find that it has gotten maybe this doesn't look the stretching so much, let me make it more obvious, it goes to here, right? So what happens? This point has been partly rotated, partly stretched, right? So, and here's the way to look at it, that any other point to which you apply this matrix will undergo that sort of a behavior. There is also something very interesting that if you take unit circle of noise or Gaussian noise, apply sigma to it, and guess what? It will become the shape of your data. Your data is like this if you apply sigma on this data on this it will become this so sigma is the transformation that will take a unit i mean a basically gaussian noise and shape it along your data that is why you say that it is sufficient statistics, right? Covariance matrix was enough to basically take Gaussian noise, which has no relationship whatsoever between X1, X2. And out of that, manufacture, rotate it, and shape it into a relationship, right? What did sigma do? It shaped it into a relationship. So you could see the shape of your data. Are we together? So that's why mu sigma are important. And these directions are important. So now what can we do? So let's summarize the lessons we learned. When you take data in high dimensional space, x1, xp, original space, then we can represent the data into a lower dimensional xk. Lower dimensional space by ignoring xk plus 1 all the way to xp because this matrix is this much. But what you do is you say, hey, you know what? These ones don't matter. Let's set them to zero. And so if the coordinates of every point is x1 to xk followed by zero, this thing you can ignore. You have a lower dimensional vector space. You have a lower dimensional vector space. In other words, you have new features x1, x2, tilde, and so on and so forth. These features, they exist in a lower dimension, dimensional space, which is RK. And you are basically, you're saying, let's describe the data in this space. The other ones probably represent noise. Now the question comes, how do you know? How do you know that they are noise? So, and before I do that, I'll tell you how to. First, you look at the sigma matrix. Now there's another beautiful thing that comes up. It turns out that there is a process, a mathematical process, a linear algebra process, it's called eigenvalue decomposition. It's a very decomposition. What eigenvalue decomposition does is that when you do the eigenvalue decomposition of a matrix, a well-behaved matrix, a symmetric one like that, you will find that it decomposes into three things, just as a number divides into prime numbers. In the same way, this sigma can be written as a product of three matrices. What are those matrices? This matrix V is nothing but E1, E2. You realize that this is a two by two matrix, right? Why? Because E1 is 2 by 1 matrix. E2 also is a 2 cross 1 matrix. So this is a 2 cross 2 matrix, right? So this is V. And these are literally your principal components or principal directions, principal axes, E1 and want any to and the D is the diagonal in fact magically you'll find that this is exactly this the amount of stretching the the points undergo when you take a when you take, for example, this noise. R. Vijay Mohanaraman, Ph.D.: I said, Nice and you apply the covariance matrix to it, you will notice that the amount of stretching. and you apply the covariance matrix to it, you will notice that the amount of stretching, right, that the points along the eigen directions go through is exactly equal to this. It's a diagonal matrix. So it's beautiful, actually, if you think about it. What you're seeing is sigma is internally made up of E1, E2, the amount of stretching lambda 1, lambda 2, right? And then, of course, the transpose of this E1, E2. The same thing containing no new information, just the transpose, the structure is that, is the transpose. And it's quite beautiful that it can, it is like that. Geometrically, it is like that, right? So what you do is, when you take data in very high dimensions, let's say lambda one, lambda two, lambda three, you will find that. You can compute, you can look at the variances along all of these new axis directions and you will find that. The signal directions will have high variance, you know, you see that you see a lot of. The signal directions will have high variance. You see that. You see a lot of variance here, big variance. And the irrelevant dimensions will have smaller, hopefully smaller variances. And then there is a whole technique called proportion of variance explained. What it says is that if I take all the coordinates, then how much does it explain the target? Let's see if you're doing regression. How much does it explain why? But if I take only, I throw away the least significant principal component, how much is it then? How much is it done? Right? And so what will happen is that if you throw away or suppose you do the other way around, actually let's do the other way around. If you throw away the most important component, you will notice that you'll have a significant drop in the variance. So let's simply put it the way any model that you build without the first principal component would basically suck quite badly. You take the second principal component, maybe a little bit less. Then let's say that after third component, more. The variance explained is this. Something like this. Let me make it like this. something like this let me make it like this but then after that it is like the as you take more and more principal components so what do you see if you were you look at this just intuitively how many principal components seem to matter in this picture maybe three three three see at three you get the elbow. Yeah. After that, you reach a principal point of diminishing returns. You're not losing much by throwing away the rest of the things. You could do made the other way around. How much variance, like if you add only the first component, how much of the real information you have captured? Then the second one is this, then the third one is this. After that, it plateaus off. Small, very small increases. So you know that you have reached an elbow somewhere here. And so you should keep, actually it's better, I think this diagram is usually made the other way around, this way. So you say, okay, I got my ELWO, so three components are what matters. So we'll do it in the lab, we'll do it on the data, and you'll see these plots. And the components are each a mix of the original features. And the components are mixed, because what are they? They are, they all have a coordinate system explanation you see that they are all a mix of the original coordinate system to different degrees right so then it means that every point in the x1 space, so suppose you have x1 space, it can be represented in x1, some value, phi 1, right? x1, 1, 1, plus phi 1, 2, x2, right? It can be written like that. Now, what happens is that these are often called the loadings, loadings of x1, x2, that to get the first principal axis, how relevant was the first feature, how relevant was the second feature and so forth. That's all. So there is, when you do it in reality, you find two things. Principal component analysis, when it meets these two criteria of linearity of the data, underlying relationship, and a Gaussian noise or Gaussian in the features, then it is extremely effective. It helps you discover a lower dimension space in which you can represent data. Or another way to say it is that it distinguishes between what is principally the signal and what is mostly the noise right it it creates helps you get to a better signal to noise ratio um sir i have question yes go ahead go ahead so in order to find the principal components, the data has to satisfy two conditions which you mentioned earlier. Right. And so in that case, if it satisfies the condition, every data point has to be multiplied with the covariance matrix uh is that correct you don't do that i just explain what it does all i'm saying is that you you find the co you just compute the covariance matrix so step one one find sigma how can you find sigma this is just a computation you know you find variance along the first axis you find the variance along the second axis you find variance along the first axis, you find the variance along the second axis, you find variance along first and covariance along this. So you can create this, find sigma, find mu, mu is just for centering the data, which we do even before doing that. So once you have done that, then step two, eigenvalue decompose, decompose sigma. So you will end up with VD, V transpose, right? Now VD, so the columns of V, columns of V are eigenvectors. Eigenvectors, right. So these are the two steps. Eigenvectors, right. So these are the two steps. That's it, EK, right? And all the way to EP, but you want to throw away some of these, right? Then you want to keep only the main ones. And the diagonal matrix will give you the eigenvalue made of eigenvalues right which you don't really care that much in this particular case you just need to know the eigenvectors how do i rotate it to that direction but it helps you find what matters yeah based on the value of say lambdas if they are insignificant we can directly reduce remove them sir yeah you can do that or people also use this they go back and they say what is the proportion of variance explained and so forth there's a little you can get a little bit more involved but if you can literally see some of the lambdas are tiny right Just zero it out and you have reduced the space. Okay, so the first point is finding that whether there are significant signals in the data itself. That is right, that is right. See, the point Pradeep is, it is, you must do this. If linearity condition is fulfilled, you must go with the hypothesis that surely the data cannot be like all the features cannot possibly i mean they may all matter but the data truly doesn't exist in the whole of the p-dimensional space because if it did if it did it would basically be noise. Think about it this way. This, if I write this, what is this? Noise. Noise. And what is this? That signal. Quintessence of signal. This is a lower dimensional on the right-hand side. This is occupying the entire two-dimensional feature space. So the point is, if you really, if the data is meaningful, you first assume, it may or may not be true you assume that there is linearity and there is a gaussian shape and you go ahead and try principal component analysis if you luck out if the relationship underlying is linear then your pca will succeed if it is almost linear it will still succeed partially then your PCA will succeed. If it is almost linear, it will still succeed partially. Right. Yeah, that's why I was little confused that when you were explaining that you started with that it's noise and then it becomes linear. I mean, there is principal components come into becomes a signal. No, no, it doesn't. See, look at this this data what you are seeing is e1 is the direction of signal e2 is the direction of noise right if you remember the the example we took let's go back and review this we took the example of something oscillating you have a spring and there is a mass that is oscillating back and forth right so you will end up and there's a mass that is oscillating back and forth. So you will end up, and there's a camera which is making imperfect measurements of the location of the points at different time intervals. So if you ignore the time dimension, you look at the x1, x2, you will find that this box has been at all of these locations at various points in time. You agree with that i i joined pretty late sir so i joined late okay so that that explains it okay let me recap what i said see imagine that you have a spring like this right you have a spring you remember in physics doing basic experiments here is a wall here is a ideal spring here's a frictionless surface and then here is a mass and this mass and then you stretch the mass right around the center and this is your x right the displacement x what happens x is a function of p right it oscillates back and forth right now imagine that you have a camera not looking at it like this, but looking at it at an angle on the surface, right? It is looking at the X, Y plane. And when the camera looks, it sees the spring at an angle in the plane, right? And so this point is oscillating back and forth. When it is oscillating back and forth, it is producing data that looks like this along the x1 x2 axis because it has instrumentation errors when you when you consider this you know that actually the point was really oscillating along the pink line the x2 direction doesn't matter right so this is a idealized case in which truly the other direction didn't matter. It was just pure noise. Right? X2 direction, X2 tilde direction is the direction of noise. X1 tilde direction is the pink direction is the direction of the signal. Right? Correct. That is it. That is it. And so if you take this intuition, realize that we can if we could create a rotated system x1 tilde and x2 tilde let me mark that till the x2 the two is gotten shifted ah x2 the tilde associated with it the x2 direction, this is X to today direction, then one direction represents signal another direction represents noise, the only question is how do we find these directions, because if we can find it, we can just ignore X to we can throw away X to. R. Vijay Mohanaraman, Ph.D.: Like we can sit to the first approximation let's it is sufficient to know only X one X two is is not contributing, it's just adding to the clutter. Isn't it? And that is the intuition. So how do we do that? It turns out that if you create the sufficient statistics is, if you have the sigma, the covariance matrix, which is made up of the variance and the covariance. And you know the mu, there's a point I mentioned. If you know sigma and mu, these two quantities, they are sufficient statistics. Why are they sufficient statistics in such situations? Mu is just to help you center the data, go to the center. Can I stop you here, sir? Like on this diagram, the one above pink, where there is this circle, right, with the noise. So what I wanted to say the diagram a cannot convert to b or c yes in fact here i'm illustrating two situations they are entirely different data there are they are different data sets so the point i was saying is, if the covariance is 0, then it is spread out. Yeah, there is no principal component in that. If the covariance is very high, you will get a straight line kind of a relationship, right? Correct. In correlation 1, right? That is the point. So there are three different realities. Now, coming back to this picture, you realize that if I am looking at this picture, X1 tilde, X2 tilde, and I want to make the covariance matrix in these new directions, is there a covariance between these two new of the data in these two new axes? X2 is just noise direction, right? Right. Gaussian noise. It cannot possibly have a relationship to sigma, right? So you would agree that if you make the covariance matrix for this guy, I think I said it somewhere. If you make the covariance matrix for this guy, sigma or tilde, would be what? Some value here, sigma x1 tilde. Well, okay, this is very cluttered. Let me put it somewhere here. Here, you would agree that, you would agree that sigma tilde is equal to sigma x1 tilde square sigma x2 tilde square. You can find a covariance along the new coordinates, right? They will be significant. You would agree that the variance here, sorry, this is the variance. This is the variance along x2. along x2 it's not covariance sorry variance along x1 direction and x2 tilde direction this would have the greatest variance just looking at the picture you can see that the variance of the data is huge right and variance along x2 the x2 tilde is less but what about the covariance between the two zero zero exactly and that is one of the beautiful things you have found you have essentially found directions of a new way a new coordinate system in which the covariances are close to zero right that is the point so that is nice no to know yeah the whole thing it summarizes in just this one exactly and so you want to find those coordinate system which essentially does that those that coordinate system and its axes are called principal component and principal axes the principal eigenvectors and i can you know the principal axes are the eigenvectors that is it that is what i said in this so this is it guys then i will stop here we have been going for now almost two hours any questions did you all get it it was was a straightforward idea. Knowing a little bit of linear algebra helps, but I hope I explained it. Explain the necessary linear algebra as we went along. Basic idea is that data can exist in lower dimensional space. There is a method. You don't have to know how to do eigenvalue decomposition the the psychic library will do it for you but you should know the intuition that magically it finds the eigen eigen directions right eigenvectors and reprojects the data in terms of new axes, new derived features. What are those axes? They are derived features, right? And what happens in reality is, especially many fields like medical field, people then develop intuition. So what happened, let's say that the data was really cluttered, but you find that you can represent it using only two in the X1 tilde x2 tilde right people end up understanding that oh this is made up of it's a mixture of x1 x2 of course mixture of x1 x2 coordinates because it can be the eigenvector. Eigenvector 1 can be represented in the old coordinate system. But people realize, first of all, they know something from real world that this axis represents. Quite often, the moment they see that the data has separated itself like that, people in many fields, especially medical, they give nice scientific names to these axes the newly discovered features they will give names and they'll develop intuition of what it really is right and so that is all that is dimensionality reduction guys dimensionality reduction is finding the lower dimensional for for principal components finding the lower dimensional hyperplane on which the data lives and in a more general in a more general case finding the lower dimensional embedded hyper surface on which data lives go ahead but. But this one is very, it has some limitation, a lot of limitation in that way, sir, because it is going to find only for the data point, which has that assumption of correlation. Yes, so that is why I said that these two conditions must be fulfilled. Did you notice that? If then. Right. Having said that, see see data is rarely linear but often approximately linear so doing a principle component analysis is still worthwhile because you develop some intuition before you go and try more sophisticated things try more sophisticated things. It has been the workhorse of the industry. I would say it is perhaps one of the most often used methods in many, many scientific disciplines and medical disciplines. So even though people don't realize that there is correlation there does exist correlations and this can find yeah it can find that exactly it can find it all right guys any more questions was that easy to understand guys i got a lot of silence i didn't get many questions from people basic intuition you covered earlier when you did the uh spectral exactly exactly and it just shows how powerful this method is so spectral data is highly non-linear but we we linearized it using the graph theory arguments Raja Ayyanar?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati?nilavarapati? dead k-means or whatever it is in lower dimensions because there in lower dimensions all those distance-based algorithms work because distance still has meaning in high dimensions it gets into trouble euclidean distances etc get into trouble i think was there a measure here in terms of the contribution of each factor yes so what happened that is the load factor. So when you write the value, a proportion of value, that is the proportion this diagram, right? Look at this diagram, actually, I should not have given the first one that is less expressed. This is the more common, the first principal axis, it is called PvE, proportion of variance explained. One, two, three. So proportion of variance explained is basic intuition is it is the amount of signal that each of the principal axes captures. And is that directly correlated with the lambda values? values? It is, let me just say that lambda, the lambdas will be bigger for that proportion. Okay. Because the main directions is where the signal is, not the off directions. The reason I was hesitant is it is, see, lambda, so I'll give you an example. Suppose you, a point is left alone, nothing happens. But on the other hand, the lambda is either shrinking it huge and doing it. So just magnitude isn't enough in the sense that what if it shrinks you down to zero or something like that? Right? Well, it won't you down to zero or something like that right well it won't shrink down to zero it will stretch you the big lambdas will stretch you out it will say that these are the directions in which most of the information is the information could yeah i mean the it could have been a negative contributor also in the sense it explains uh what am i trying to say here like let's say something is measuring the height of something right there could be a variable that's a significant contributor but it's actually going to work negative in terms of the height which is also fine yes i should mention a point see guys there is a problem with this algorithm. One more problem. First is that we expect the noise to be Gaussian. But we also expect, so suppose you have x1 all the way to xp variables. One thing that can screw all linear methods, including this. This is a linear method. What happens if there's collinearity? In other words, some xi, x xi xj right are highly correlated right in other words you can write xj as some lambda of let me not use lambda let me say you can write it as some theta times x1 xi then what happens they capture the same information right it's like suppose one variable is the duck's size in a centimeter cube and the other one x2 is the same thing in inches cube right or weight then what happens there'll be perfect correlation between one and the other. You would agree, isn't it? More or less near perfect correlation between x2 and x1. When you have situations like this, you have to watch out. You have to remove those co-linearities in the data. But we won't get into it here for now. That's another limitation. Gaussian shape of that data and so there are quite a few stringent conditions. These were true for linear regression also. Linear regression wouldn't work if there was multicollinearity in the data. Do you remember that, guys? Actually, I was reading an article. There's a very lovely article that I was reading today afternoon itself on this. Somebody wrote in 2003, there's an article here explaining the same thing i was looking for one article that is just at the right level of you know depth to explain it it's called a tutorial on principal component analysis i'm going to post the pdf for you so if you can read that you'll find that it's a review in in our textbook this topic is not discussed in as great detail but i think you already did sir i i took a look of that few days no that was a video that somebody posted of eigenvalue decomposition uh the pdf sir um you did a pdf also i did a pdf on principica i don't remember that ah yeah just a few days back, I'm just trying to- I might have done that, but I don't remember it at all. Okay, but I'll post this one because this one I was just studying. In fact, I found the way he explained so nice, I was highlighting a few aspects of it. I'll be posting it for you guys today. So you also read it. He explains it well, right right and in a way i his notation is slightly different from mine but you'll still get it all right guys any other questions see if you don't understand this is set with the any of our tas they will help explain it to you help you by explaining it again to you these things take a little bit of time to sink in but the idea is basically this how do you rotate the coordinates and in such a way that it's pointing in the right directions the coordinate system coordinate system. And therefore you discover a subspace. You throw away the noise directions and you keep the signal directions. All right. The one you did was on curse of dimensionality, not on the . I did it on what? Curse of Dimensionality. Curse of Dimensionality. Yes, yes. That is the article. That's a beautifully written article. He quotes a lot of historical references and people quotes, etc. I like that paper very much. It makes many interesting observations. But on this one, I'll put a tutorial. Somebody has written a very good tutorial actually and you can take it from there.