 In our previous session on Tuesday, we covered agglomerative hierarchical clustering. If you remember, there we started by saying, let each point be its own cluster. These are called singleton clusters. Then we need an agglomeration criteria. How do you do that? And what you do is you start merging a point to its nearest neighbors. The moment you talk about nearest neighbors, of course, you are talking about, well, if it is just a point, you might be talking about one of the distance measures, Euclidean distance, any one of the Minkowski norms, and so on and so forth. Once you do that, then you end up with these mini clusters, which may be built out of two points, three points, four points. And if you look at this figure here, for example, oh, I'm not, indeed, I apologize. To cure this problem that I don't know when I'm not sharing, I'm going to put a TV there to see what is it that I'm sharing. I'm very absent-minded, I apologize. All right, so here we go. If we look at this diagram, and let me just zoom into this diagram a little bit. Oh, where am I? Yes. Still not sharing. Why is it? You're not sharing the best part. The zoom is good. Goodness. Okay, I apologize again. Thanks for catching this. Any better this time? Okay. All right. So if you look at this diagram, let me zoom into it. What do you notice? We have a cluster which is just made up of A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, NOP, right, and so forth. So now comes the question, how do you, now that you have many clusters and those points are gone, what is the next step? You coalesce those clusters, right, by looking at the distances between them. But the moment you talk about distance between clusters, now comes a rich set of questions. What is your definition of distance between clusters? Even if you say, all right, I'll go with Euclidean norm. Still, it leaves open the point, what exactly is the distance between, let us say, this cluster and this cluster? Is it the shortest distance between these two? Is it the farthest distance between these two? Or maybe farthest would be here or something like this. Well, let me take this. This distance between these two? Or is it the distance between the centers? Or is it the average of all the pairwise distances between the clusters? We don't know. Each of these definitions are valid and they lead to different results. Now, your book talks about all of these, quite a few of these, and these are called linkage functions, linkages. How do you link two clusters together in this dendrogram as you go up, right? Now, there is one more method, the Watts method, which I will cover in the lab, in one of the labs. You'll see how to do that. Roughly speaking, what it does is whenever you coalesce, you basically say, how do I coalesce in such a way that two things together still look like a cluster? They don't look like very disjoint things. So you look at this thing, you go out, look at the distances from the center to each of these points, you look at this distance. And then you see if I merge these two, and now if I look at the distances, how much of a degradation do I get in my... the same thing, sort of something like the WSS itself, you can say roughly. You can think of it like that. And so that is the basis of the word, but we'll go into the details a little bit later. Today, I want to focus on a new topic which is density based density based clustering so before I start any other questions guys from agglomerative clustering So, the notion of distance here is critical to your view of the cluster, right? Absolutely. It's very, very critical. And so if I come up with, say, two, three notions of distance, right? I'm trying to slice and dice the cluster in different ways. The profile of the cluster will look differently in different views. May look different, may look differently. So how do I interpret at that point? How do I kind of... So I'm changing the distance, I'm not seeing the order of the points. I'm seeing my notion of distances. I have three, four notions of distance. See, what happens in reality is when you cluster, when you have data, you will end up with clusters. You look at the data points that belong to a cluster, some points that belong to this cluster, some points that belongs to an entirely different cluster. And you ask yourself, as you talk to a domain expert and you ask, do these things really go together? And are they different from these other things which also really go together? And are they different from these other things which also should go together? And if the domain expert comes back and says, yes, they are different, you have a good clustering. If he says no, actually you mixed it up. So for example, in your cluster, you have half the people who have, I don't know, some disease and the other one has, yeah. I look at some things related to sugar-related pieces. I see a set of people over here, and then I look at some pili-related stuff. I see a different cluster of people. But if there are certain people who work together, then I will know that they are... That is right. It's a degree of noise. ...between those two guys. Absolutely. And that brings us to the whole question that see if we know the labels, if there is a label associated with it, it is far easier to know whether my clustering is good or not. So quite literally there are, so we'll go into this. The moment you know the labels effectively then to know the quality of clustering is very easy the problem is in real life many many situations you don't have labels it is truly an unsupervised learning problem a pattern recognition problem and so you have to ask yourself is this a good cluster or not and so you have to answer that question in the absence of uh absence of that measures that are derived the absence of that measures that are derived from the labels. Right? And it's hard. Generally it's hard. And the way to do that is, or the way I do that is, first of all, there are certain measures like Silhouette and the WSS and so on and so forth. You try all of them. Generally in the field of clustering, we understand that there are no good methods. At this moment, all of them generally in the field of clustering we understand that there are no good methods at this moment every all of them are imperfect right there are no perfect clustering methods it's again the no free lunch theorem that no one clustering will always work for all data sets so what you have to do is you have to keep trying variations till you hit upon two criteria first of all the quantitative metrics look good you're still whoet and then your wss and all of those things begin to look good and then the second thing is you the domain experts begin to agree with you so the specific data is looking at the spare data the cdc is a corona thing and they have a huge bucket over there unexplained internal organ failures condition right and so the regular one they have put like normal ones like diabetes and then there's this huge group that i didn't nobody knows what what it means but they know there's something wrong absolutely now you have to ask different domain experts say do you see a correlation between kidney failure and this over here right because things have got messed up yes Because the individual domain experts only know that part of their stuff. So it's kind of weird. But this data will show, right? Yes. See, let's put it this way. See, when you're on a journey of exploration with data, especially in the unknown, COVID is a very new, like the research in it is very, very nascent. We don't understand everything about COVID even today. Right? Typically it takes researchers 10, 15 years to get the handle on this. Even though this has been running on, this has been one of the most remarkable global collaborations to understand this virus. It's still there a lot of unknowns. It took us a long time to realize that it was, it was a problem that is more cardiovascular than lungs. This was the long-term problem. Yeah, that is right. So today, so one thing I can say, whenever you do research, and you do it all the time, that's where all the fun is, that every single analysis, clustering, classification, all of these you keep doing, they are necessary but not sufficient. Because you don't know when you'll come upon the breakthrough. string classification, all of these you keep doing, they are necessary but not sufficient, because you don't know when you'll come upon the breakthrough. All you can do is if you don't try, you won't end up on a breakthrough. So you have to try every method. And from everything, you learn a little bit, but you don't necessarily come upon a breakthrough, you keep learning a little bit at a time. And gradually, the picture emerges right most of the good problems they don't crack open immediately they take a lot of hunting here and there and bits of pieces come together and you keep gleaning insights and then one fine day the all the pieces of the puzzle fit together and you have a breakthrough so i think with corbett we are still far from it. Like for example, it is true that a lot of the deaths that have happened in COVID are unexplained. We don't understand the biochemistry of it, what happened, what triggered it and so on and so forth. All we know is that it's a virus where the body is completely unprepared in many people. So should we do clustering? Coming back to this, definitely, you must. You must do every single possible thing. Because if you fail to do it, it's like that. Surely, breakthrough will be in that one thing that you didn't try. It's always like that. Yeah. All right, guys. So any other questions? So Google actually introduced a new architecture to address that problem a couple of days back what is that it's called like the pathways the pathways okay so it is like multi-modal uh multi-modal output like that so it's like it can take like multiple inputs get strained onto like one architecture but it can do like multiple inferences they just release it like okay but there is not much information about it they just released a youtube video okay yeah so for those of you who are remote uh anil here mentioned that google has released a new a framework called the pathways yes and he's going to post details on the Slack. Let's all explore what it is. It seems promising. The question that Sachin asked, without getting into COVID, COVID seems very complex in the way that the inference is hard to do for domain experts. Let's take a simpler one. Now I was looking for your thoughts on, let's say marketing. Let's say,'m i want to pick the project that i did when i was in these groups trying to understand how kitkat and uh what was it then uh perk work in the market the minds of people so we were doing a little market research that are connected back to clustering is i think they study on the basis of people's perception of what is a good chocolate and then you get them into buckets which is basically you've done some kind of clustering for the buckets ultimately the goal for a market here is to figure out those buckets are they different segments and if they're different segments target them with different ad campaigns or different strategies so So once those clusters identified what are I was thinking about that time was basically, there will be some kind of principle component analysis to understand across certain variables, what were those contributions for each of those, and hence be able to say, this is what defines this cluster, that's what defines this cluster, then you give it a name, a yuppie versus an old adult and all that. What are your thoughts from what you see in the work that you do? How do you try to attribute meaning to clusters? Pramjit's question. Pramjit is asking that when he did marketing in business school, they would look at chocolates and different flavors of chocolates and see, look at the data and then look for groupings of people who had sort of one kind of taste and maybe then, or clusters. Then those clusters, by inspecting what the records are in the cluster, instances are in the cluster, you try to infer, you try to give a name like the yuppies, the old people and so on and so forth. And the question is, what do I think about it? And there's also dimensionality reduction. So the thing is, it is on the mark. So in marketing, the right way to do segment analysis is through clustering. Amongst the many ways, it's usually segmenting is very, very effective. The other is obviously you can directly do classification, which is also effective if you know exactly what you're looking for. So once you have identified or created labels, generally you cluster, like you said, you find clusters and you give it a name, you label it. Once you have labeled it, now other things like classification can come in, which are quite effective. That is more or less a given way that much of the marketing works, even today. Yeah. Yeah. Great. The second question, the second part of the question that you said is, where is dimensionality reduction? Hold that thought in your mind, because that is one of the core topics for today. Any other questions before we start? All right. So density-based clustering. This in my view, and certainly as a person with a physicist background, I would say that it is to me the most intuitive. If you think of each data instance in a feature space to be a particle of dust or a grain of sand, then you would agree that you can visualize as clusters are regions of high density. Does that intuition make sense? Imagine that in the feature space. So just imagine that you have a page. On the page, you have sprinkled sand and the sands form clusters. What can you tell about clusters or the center of the clusters? Those clusters are regions of relatively higher density of sand as compared to the space between the clusters. Does that make sense? So that's a very simple intuition and it turns out that when you build upon that intuition you can come up with very powerful clustering algorithms. Now, before I go into that, you might ask, we did k-means clustering, we did agglomerative clustering. Now, why in the world should we go on learning more and more clustering algorithms? It's all right, the field has many, many. Why should we do that? In practice, actually, I find that i use k-means and agglomerative sometimes but by and large for most problems i find density-based clustering far more effective it's a little bit computationally intensive but it is far more effective and it is really worth learning and also it very directly agrees with a human intuition that clusters are regions of high density if you look look at the astrophysics, if you look into the sky, then what do you know? I mean, you would notice if you were to look at the telescope and you look at the images, the space between galaxies is a very tenuous, very, very tenuous intergalactic cloud, very low density intergalactic cloud. Really, the amount of atoms that you may find, or molecules that you may find, in even a square mile would be almost close to zero. Very tenuous space. But the moment you come close to the galaxy, your density increases. You start finding more and more matter there. The cores, the centers are dense regions. So that is the end galaxies are my intuition of clusters, a very basic intuition. And the space between the galaxies, of course, the more or less empty vacuum, the intergalactic space. the more or less empty vacuum, the intergalactic space. And in fact, it is the intergalactic cloud, but it's so tenuous, it's hard to see the even. So we'll carry that intuition forward. Now, when we do a density-based clustering, there are two considerations that come in. First, one of the simpler methods I'll mention. It is called a dbScan. And the other that we'll talk about, which is not mentioned in your book, but actually is much more high performance is DENCLU. These are two different methods too. Now, dbScan has a slight improvement. It is called the optics. And I will talk in passing about that. It's a technical optimization on that. Now, let's get started with dbScan. In dbScan, you say the following. So let's again draw out clusters. I will draw out the cluster says. And then let us say that I have another cluster here. And maybe some outside points. Are we together? So how many clusters do we see here? Two. We see two clusters. Now, the way this dbScan builds itself up is a very elegant mathematical argument. It takes a formal approach. It says that imagine a concept. The concept is that of epsilon neighborhood. Epsilon neighborhood of a point and there is another concept, endpoints. These are the two important things we'll take. Given a point, let xi be a point. Then you create a radius and using whatever metric, Minkowski metric or distance metric, any kind of a distance measure, so long as it agrees with the definition of distance. Remember the distance measures we said, the inequality and the symmetric and this one and so forth. So long as it is, this is xi and the radius is epsilon, right? And remember, I made this circle because it's Euclidean, it's simple to visualize, but there is no such restriction. You could have whatever notion of distance that you so choose. Are we together? Right. Now, what you do is once you have that, you say this region is the epsilon neighborhood. The interior of this disk. And in higher dimensional space, what will the disk become a sphere a hypersphere right of of this hypersphere hypersphere right which are actually it's a solid sphere so look at this hypersphere then you you look at all the points that are inside it so suppose there is a point xk outside. Are we together? We have these points xi inside xj. Now, if it is inside the disk, you would agree that the distance from xi to xj, by definition, is less than epsilon. Would you agree? Right? Yes. So the distance between xj and xi, d xj xi, is less than equal to epsilon for all points inside the desk right so we call these all of these points as directly reachable all such x j are directly reachable You use the word directly, directly reachable. Are we together? And you look at the set of, let me just say it, you look at the set Xj such that dxj Xi set associated with I is less than equal to epsilon. This would be a set of points in the data. There'll be many instances of your data that will fall in the interior of this neighborhood, epsilon neighborhood, inside of this sphere. I hope I'm saying something. So I will assume that you're understanding it. If you don't, please do speak up. So we look at this site and we ask how many? The cardinality, the word is cardinality, how many are there? Cardinality of this set. Let me just call this set SI. So the cardinality would be, or well, let me write this word call it SI. People often write it as SI. This is equal to the number of points. Right? This is the, okay, let me not write the number of points here. Okay. I apologize. Okay. This is the number of, what is SI with the bars around it? It is the number of points here. Okay, I apologize. Okay, this is the number of, what is SI with the bars around it? It is the number of points that you actually find in the epsilon neighborhood of Xi. So far, so good? All right, now let's a matter of definition. Let us say that you have another point x x i j k l xl now is xi is xl directly reachable from xi directly reachable from xi? Did I hear something? Yes. No, it is not. It is not directly reachable. It's not in the epsilon neighborhood of xi. But is it directly reachable from xj? If you make a circle around xj, then I't know something like this is it directly reachable from xj looks like it yeah epsilon neighborhood of xj and so you can say that i hope you would agree with this statement that excel is directly reachable from xi right and j sorry xj sorry thank you and excel is directly reachable from XJ. Isn't it? And so you say that there is a transitive quality to it. You say that it's not directly reachable, but you say that X, the association. Now, I'll just, for the sake of Excel is reachable from X, J is reachable from x j is reachable from xi so you say that any point that is reachable not directly reachable just reachable from xi through intermediaries through a chain of directly reachable links do we understand the definition of reachable reachable is it is not within the epsilon neighborhood of xi but there is some point inside and from that point maybe there is some other point and there is some other points of example if i have another point X, yes, so you say Xm. And any such points, these points are all reachable from Xi, even though they are not directly reachable from Xi. So now you wonder why we are creating this all sorts of interesting geometric definitions. So the reason is very interesting. And actually the paper that brought about DB scan, if I remember right, it got the best paper award in its area, I think in machine learning or something like that, in that particular group or that journal or whatever in 1998 or maybe got the best paper at conference or whatever forget the exact detail it was considered a very very interesting uh discovery and the next year they improved upon it and created optics and that paper also got awarded literally so obviously this idea is very elegant and now let's see what it means for us. So I'm going to recreate, now look at this cluster carefully. Suppose I give you this point, randomly pick a point in this cluster. So let us say that I pick a random point, this as my x1 and i ask and i have a certain definition of radius let's say my radius is this much this is my epsilon neighborhood right so i ask this question that if you start with x1, start acquiring all the points here, maybe I'll telescope this a little bit longer, bigger, this cluster. So let me make this a little bit bigger now. Suppose I have points, this, this, this. Let me do that. Old fashioned way. I want to also do this. And then suppose there is a point here. And suppose there is a point here. And let us say that about on the screen, my scale of epsilon neighborhood is, let's say this much, right? And I put one more point here, right? This is my epsilon. Now, let us say that I happen to pick this one as a point, x1. What points are directly reachable from it? If you look at your epsilon, is this point directly reachable? Yes. This is directly reachable. This is almost, yeah, it is still within the bounds of that. Now what happens? These are directly reachable. But now from these points, what other points are reachable? This is reachable. This directly reachable but now from these points what other points are reachable this is reachable this is reachable this is reachable now from this this is reachable and from this this is reachable do you see how it grows from this this is reachable and from this this is reachable this is reachable this is reachable this is reachable, this is reachable, this is reachable, this is reachable, and this is reachable. Pretty good. But are there any points here? Is any one of these points reachable on this side? Too far. Too far, right? Because if this is my epsilon, then it is too far. then it is too far. What about this guy here? Nothing. So you say, you have one interesting definition. You say that given any point, if you find, so you pick a number, let's say that here I'll take four or three, NPTS3. If a point has three directly reachable points or more, you call it greater than three. If a point has greater than equal to NPTS, directly reachable neighbors, i'll just say neighbors directly reachable the table reachable i.e within its epsilon neighborhood we call it an interior point are we together so this is one definition number one interior point we just defined interior now the second point is if it is reachable from an interior point but is not itself an interior point, but not itself and interior point, interior point, we call it a boundary point. It's the definition here. So can we identify looking at these pictures? Now look at this point. Would these points be the boundary points? Look at how many neighbors. Do they actually have three neighbors? They don't. Right? They fail to have three directly reachable neighbors. One, no. Well, this one is not an interior point. Yeah. So I believe we got all the interior points. This. Because even though this one has this it has only two neighbors and the third one seems to be a little bit far well i don't know maybe not maybe this is also not an interior point i take that back yeah so this is also not an interior point which one am i saying this one i'm putting a question mark you realize that this also not an interior point. Which one am I saying? This one. I'm putting a question mark. You realize that this is not an interior point because I can do this. Three neighbors, isn't it? So now I have come up with boundary points and interior points. So it's an interesting definition of interior points and boundary points and then everything else excuse me i apologize every no this is hardly visible and Is it more readable? Everything else is an outlier here right so any point that is not reachable at all from an interior point is an outlier so now what do we do let's say that call it exterior point uh no it will we don't usually use the word exterior we just call it a outlier oh okay right and so now look at this we have these points and then you again start with another point here. Let's say here. And once again, you can start making your well, let me cheat a little bit and add a few extra points so that these things don't look outliers okay so now we got this isn't it and here And obviously you can interconnect all sorts of ways. So you have, what do you end up with? When you look at the interior points, they span out and capture the cluster. Isn't it? They span out. I mean, you can span out from an interior point and every reachable point, if you include, you are looking at a cluster. For example, would you agree that this is a cluster? This on the left hand side is cluster one. Cluster number one. And this is cluster number two and this is an outlier and maybe i should throw in a few more outliers this is an outlier this is an outlier so let's throw those in because they are not reachable outliers these are outliers they are not reachable outliers these are outliers one second i'll take questions in a moment out liars let me finish the arguments guys outliers outliers so if you think like this one of the interesting aspects of dbScan is it gives you a very natural definition of what an outlier is, not just a way to detect outliers, but a definition of outliers within its framework, dbScan framework, isn't it? Which is a remarkable feat because one of the questions that always vexes you is as you go to higher dimensions, how do you detect outliers and what is an outlier itself? That keeps vexing you. So I'll summarize before I take questions. So this entire argument started with saying, let us have a very simple notion of epsilon neighborhood given a point we'll draw a hypersphere around it a bubble around it very intuitively a bubble around it all points in the bubble are directly reachable right now if you have more than a threshold of points let's say three points or whatever neighbors directly reachable neighbors or within your bubble you are called an interior point now from the interior point you keep reaching out to all other points right through a reachability so long as you are directly reachable to i is reachable to j j is directly reachable to k k is directly reachable to l and so forth you create the chain and when you have exhausted the chain you have the set of all you have a cluster you have a reachability graph in which every two points in the cluster can be reachable from from each other isn't it through some a path in the some, any two points in the cluster can be reached. Or are reachable. No, no, arbitrary many. You can have all sorts of paths from A to this. Let's take this example. I can go, let me call it, see, let me mark some paths. I could go from here to here directly, or I could go from here to here and then here. You see that, right? So obviously these paths, there are many, many paths, but so long as there is a path there exists a path from these things these are all reachable points once you have a reachable point you call the set of all reachable points a cluster now look at the remaining points once again you will end up with either points which which have no neighbors right or we don't, like basically don't have neighbors or are not reachable from any of these other interior points at all. Those are your outliers. Then you go and find another interior point and from there span out and you'll discover another cluster. And you can keep doing it and you'll keep discovering clusters, right? Whatever remains are outliers questions such as so the boundary point doesn't the npd has to be one uh not well boundary points are points that just have below the threshold not necessarily is really one also boundary points right the other ones that have two neighbors because they have three neighbors that's why no no oh yeah this one has two yes you're right so this one also should be that's a good catch actually this is also a boundary point yes that too is a boundary point uh where we go yes let me mark it this too is a boundary point where we go yes let me mark it this too is a boundary point no it has three neighbors oh this also is a yeah this also is your boundary yes so you can do that yes you're right these are also boundary points so any points if you don't have enough number of neighbors right you could keep doing it and you will have a boundary point so what happens is that the notion of boundary that you have here by the way this this is not a boundary point because you can go like this right so you i think we have exhausted the boundary points here sir uh I have a question here uh you just draw a boundary point right Sachin pointed out the one that the where there are three points can't we connect that point to the top top again actually hang on this is a good point what is the let me see my Epsilon I keep forgetting what the size of my Epsilon is you're right actually okay you got the idea. I am not very good at measuring distances, obviously. So this is not a boundary point. You're right. This is not a boundary point because from here, this distance can be reached, right? This can be reached. So there are a lot of interconnectivity. Things can be reached from here to there. And this already is not a boundary point. You can connect. So let me look at this big. That's a pretty big distance. This can be connected. Once we get to the fusion, the concept would be, let's say you're a donut shape. Yes. Then there would be something around. What happens in the center of the donut? No, there is no confusion. There is no confusion there is no confusion see this is one of the interesting things as uh as you learn so far as the donut is concerned you will get a perfectly good you will get points on the boundary and points in the inside of the donut if your epsilon is correct there there happens to be empty space in the donut in the feature space that's perfectly fine yeah so we'll come to that asif how do we calculate the the epsilon no epsilon is not calculated and so that is the problem with this algorithm um see db scan is is, so first let me talk about this, the points of it. With dbScan advantages, with a judicious choice, and this is the crucial word, judicious choice of epsilon, number of points that you want to consider in the neighborhood these are the hyper parameters right hyper parameters of this model db scan works works very well, actually, very well most of the time. Many of the times. Often works very well. In machine learning, you have to be careful using words because you can always produce infinitely many data sets where it won't work well. So often works well. Works very often. Often. Let me just say often. This is enough. It works very well often. But the problem with this is two things. It has some disadvantages. Disadvantages. The disadvantages are, first is number one, computationally intensive. for large data sets. Number two, which is a problem, is very sensitive to the choice of epsilon and NPTS. These two hyperparameters, what is your epsilon? One inch, half inch? What is it? How many points? Three, ten, whatever it is. So I'll give you an example. See, suppose you have data like dumbbell. And obviously, this software is playing games. So suppose you have data that is and I'll just now you should get the intuition you would agree that based on what your epsilon npts is you can have how many clusters do you see right basically two or one yeah two so the, based on this, you may have one cluster or you may have like two clusters. Are we together? So that is the essential problem that you would like to see two clusters or one cluster. It totally depends upon how you choose your epsilon and points and it's terribly sensitive to it. Right? And there are two hyper parameters to manage. So this is it. So then came the question that can we sort of improve upon this situation? There is an improvement which is brought about by the optics algorithm. What it does is by the same guys who created the db scan, they noticed that, see, you can have a hierarchy of how far you are from the center of the cluster, like how dense you are or region you are. And if you use this, then it turns out that you could relax, I think one of the constraints, either the epsilon or the endpoints, I have to check, it's escaping me, the situation gets better. But it still remains computationally heavy, it still remains, it improves the situation, doesn't cure it, right, in finding the clusters. So to review, we understood what ourability argument to identify clusters. And what remains are outliers. Its key strengths are the fact that first, you don't have to tell how many clusters there are. It will automatically discover the right number of clusters. You just have to give the epsilon size. See, what is epsilon? It is the granularity of the data. At what level do you want to see data as clusters? So that is that. And then it takes over and does that. It finds outliers very naturally, finds the number of clusters very naturally. Its main problem is computationally intensive and very sensitive to the choice of epsilon and n points. Optics improves upon it, but I'll next explain an algorithm which, in my view, removes a lot of these disadvantages. So before I do that, let's take a 10-minute a 10 minute break now before that let's take any questions any questions that's all there is to density based clustering I mean sorry db scan and optics any questions on this yes go ahead, Albert. cluster one. No, no, no it's an interior point it has three neighbors anything that has more than two is yeah more than two is a interior point what is the main difference yeah so the main difference uh is that uh So the main difference is that it uses a fact that you can create circles of sort of neighborhood, how far you are or how dense you are. So regions which are really dense, which have a number of points is very high. It counts like in your epsilon neighborhood how many points there were. If it was high, it's sort of an inner circle. If it is less, it's a slightly outer circle. You see the point, right? So, the idea is that our clusters are dense in the center and they radiate outwards, become less and less dense. So, for the clustering is done based on epsilon and then sub-clustering is done based on output? outwards become less and less tense so for three clusters no no i'm saying that if you don't like db scan optics improves upon it optics is an algorithm by the same authors that improves upon it by saying that this whole dependence on epsilon can be somewhat uh you know mitigated you know the sensitivity on epsilon can be somewhat mitigated. The sensitivity to epsilon can be mitigated by looking at, given a neighborhood, how many points there are inside it, how dense it is. So you have to read the paper. It's an argument that they make. And that is all there is to it. Also for optics, are they looking at the marginal like for each new circle with the change in the number of points is is that what it is yes yes exactly it's a it sort of radiates out to less and less dense and then yeah so optics is in ordering points to identify the testing structure so they use the ordering like internal ordering. That is right. And it's based upon how dense regions you are in. That's all. How many neighbors that you have. OK. And I think, did I answer your question? Yeah. So here's the thing, guys. At this moment, you notice that I'm being not too deep into optics. The reason is, let's understand one first. DB scan properly optics is a minor improvement about that right but make sure you got you got your db scan right so it doesn't matter where we uh like choose the first point from. It doesn't matter. It doesn't matter. So in other words, its virtue is it's predictable. Every time, you'll come back to the same result. Whereas in k-means clustering, you realize that every time you run it, you'll get slightly different clusters or sometimes radically different clusters. that is another advantage actually any other questions before the break guys if not we have been going on for an hour let's take a 10 minute break and we'll regroup with a new topic