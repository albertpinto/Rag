 Today's topic is clustering. This is one of the many very important things we do in machine learning. So far, you have learned about classification and regression. In both of those algorithms, the goal was to predict something. For example, in regression you predict a number. When we talked of the entrepreneur trying to open an ice cream shop on the beach, the input was all the weather data. The output that you were trying to predict whether something is a cow or a duck. It's a process of identification and that identification, to the extent that you have, you know, you're trying to tell which class it is, categorical variable is the formal word you use is it a cow or a duck or more practically whether to give this person a credit alone or not how much is the risk and based on that you make a decision to give this person a loan if you're a banker to give this person a loan or not and or as a friend some friend comes by and says i want to take give me a loan of one lakh rupees or something like that and you would want to you would want to know whether it is safe to loan to this person whether there's a reasonable chance of it's coming back and if so do you give him the loan or not that's a classification problem right or trying to for example if i show you a character any character and or or some text and you're trying to determine which of the languages the text belongs to is it in the indian family of let's say not indian family of, they have Nagri script base. Is it one of the, for example, is it Tamil or is it Tibetan and so forth? So when you try to do that, you are again classifying, you're looking at the input and you're trying to speak to our estimate, which looks the most likely answer. That's classification. But nonetheless, in both these situations, the output is a prediction. There is an output. And in a sense, you're building a predictive model, a machine that can make predictions based on inputs that generalizes from data, learns from data, generalizes from that to be able to make predictions on unseen data. This form of learning is called supervised learning. Supervised part being the training part where you learn from training data. We saw that in the lab for California housing. Today, we're going to deal with another area of machine learning altogether. Its purpose is not to make predictions. The purpose here is to just find interesting patterns in the data. purpose here is to just find interesting patterns in the data. When you discover these patterns, you learn something about the data and the forces that might have produced the data. Now there are many things that you can recognize in a data pattern. The simplest of them is the cluster. You may find that data comes to you in not uniformly distributed in the feature space, but is clustered around some points in the feature space. It does happen. Now the question is how often does it happen? We'll come to that in a moment. But to the extent that there may be clustering of data, or the data may be bunched together in the feature space, it is an exercise to discover those clusters if they exist. A different kind of example of unsupervised learning, so this is different, pattern recognition could be, for example, you look at data and you build, you understand what people, mathematicians, use a technical word, a probability distribution, the probability distribution of the data in the feature space. If you can determine the probability distribution of the data quite clearly, it leads to a very powerful effect. You end up with a generative model. Now you can generate data at will. You know, you can, what you have done is you have taken data, learned from it, and you have a generative model. You can go on generating new data. And these generative models these days are extremely effective. They are quite the sensation. For example, all of these things, I won't go into those algorithms like variational autoencoders and things like that. But at the end of it, what happens is that they often become powerful models that can generate new data and they're uncannily good data. Sometimes the line between supervised and unsupervised learning gets blurred. For example, some models, it's hard to tell, they lean more towards supervised learning and still they do pattern recognition or they build generative models and so forth. So there's a bit of a continuum between supervised, semi-supervised and unsupervised learning as the term goes. So I tend to use different words. I tend to use the word predictive modeling, generative models, and pattern recognition and things like that. And examples of that is, for example, of the generative kind of models are like, suppose you have a whole lot of pictures. I give you your picture and suppose I take another reference picture. Let's say you take a picture of whosoever is the latest Indian film star. Could somebody give me a name? Somebody who's very popular these days. Amitabh, I suppose. Amitabh is still going strong, is it? Yeah. Goodness, isn't he a geriatric fellow? Doddering old age, not yet, eh? Okay, so let's take Amitabh. He was a guy of our generation my generation i'm impressed still going strong okay so let's take amitabh bachchan so suppose you you take your reference face and amitabh bachchan's face what we can do what a generative model could do is it could generate all sorts of faces in between you and Amitabh Bachchan that make you look progressively like Amitabh Bachchan. And those faces will be very, very realistic. You won't be able to tell that these are not real faces. So these are called fakes, the fake images. And the fake images are all the rage. And you can do not just images, you you can have and you can do not just images you can do videos and you can do audios one of the videos I'll post to the our discussions like discussion is a video that was that was introduced in the MIT machine learning course. It is of a president, a president of United States, former president Obama, who is very popular in the US despite the fact that he is no more the president, giving a speech. The entire speech was never given by Obama, but you can't tell that was never given by Obama, but you can't tell that because it is generated by, it is machine learning that's producing that. You can have a situation in which you are speaking, but you can have every word that you speak come out of the mouth of Amitabh Bachchan, with Amitabh Bachchan's expression, with his accent, right? And you will never be able, it's very, very hard, actually, near impossible to tell that it is not Amitabh Bachchan speaking of what you are speaking. So that is the power and, of course, the danger of generative models. These days it's become a huge issue with power comes abuse and it is being abused these days. So people create all sorts of fake images and fake videos and so on and so forth. These are called deep fakes. They're so utterly realistic that even machine learning algorithms, powerful machine learning algorithms cannot tell the difference between them and reality. So it's a completely different world we are entering. And so after doing the predictive models, I thought it was time for us to get into this other two kinds of models, generatives and the pattern recognition. But today I'll start with obviously just clustering. There's a third form of pattern recognition. I mean, some more of them. There are many, many things in this space, but I'll talk about one more thing. This is dimensionality reduction. Sometimes, you know, you get a lot of features. People are trying to determine, for example, factors that cause diabetes. For example, factors that cause diabetes. So you may get all sorts of factors, environmental and life science and genetic factors. Each of them is very responsible for diabetes to a different extent. But then let's take one class of people, type 2 diabetes, that is adult onset diabetes and you the question is which action that you can take will lead to the person getting or managing his or her diabetes better so towards this end you may think of okay there's so many things a person can do. The person can, for example, go for a walk. The person can do, I don't know, pranayama. The person can reduce the portion sizes of food, eat less sugar, and so many, many factors you can do. And you can enumerate the factors. There are hundreds of actions that can do. Each of them in small measures will benefit the person. Right? So when you try to make a recommendation or think about what will help this diabetic, you may be inclined to give a very complicated recommendation. But what you can do instead is look at the underlying theme behind those recommendations. And so let us say that plausibly you come to the team that you tell the the patient, I think more abstract. You don't give an action, but you instead say do do two things or let's say three things. First, increase your activity level, do more physical activity, walk more, sweat for at least half an hour. Whatever way you do it, whether walking, swimming, running, jogging, tennis, or whatsoever, just make sure that you have about half an hour of sweating in the process, vigorous exercise in whatever form you like. And you should cut down your portion sizes in your sugar by a significant amount, don't exceed this threshold. And you can give her probably a third advice and say, make sure that you sleep at least eight hours a day, eight hours a night. And then, and you have derived these recommendations by looking at the data. But now you're making recommendations along only three axes, exercise, certain amount of exercise, certain amount of diet control, and a certain amount of necessary sleep, which is a far better thing to do. It goes to the core of the issue lifestyle issues compared to making small recommendations or very specific recommendations that you must walk and you must swim and you must do this and you must do that but some of which may not be feasible for example if you look for swimming there may not be a pond or a swimming pool nearby for the person or a sea nearby. So in a sense, what you're saying is get to the heart of the issue. And in some sense, the core factors, the underlying latent factors that are behind this feature, you can discover somehow and they are fewer in number. So while there are multitudes of factors affecting a certain behavior or certain data set, underneath them all are some few core latent, the word latent is a Latin for hidden factors, hidden factors, and those factors, those few factors are the real things you should go after. To do so, to discover those sorts of factors is called dimensionality reduction. Now the literature of machine learning has a lot of techniques, beautiful techniques to do dimensionality reduction. It has a lot of techniques, beautiful techniques to do dimensionality reduction. It has a lot of techniques and vast amount of literature for generative models. It also has a lot of literature to discover clusters and data. Each of these are vast fields. Each of them have produced like quite literally tens of thousands of PhDs, and researchers are working on those areas. So these are worlds in themselves and machine learning is now a vast, vast field as probably you must be getting a sense of. It is as big as the field of medicine, say, or any other discipline, vast discipline. But today we can't get introduced to all of that and we are going to take an hour, hour and a half. We will focus on one thing, which is the detection of clusters in data. This process is often called clustering and the algorithms that find clusters are called clusters. So let us start with the very basic principles and I'm going to do that now. We'll walk our way through, write and walk our way through this whole process. Let us do that. So suppose we have data. I'll take an example. Let's go back to the example that I keep quoting. Let's say cows and ducks. Suppose you look at the weight of the cow and the size of the animal, you would agree that all the ducks are lightweight. So let's say that this is X1 and this is X2, the size. And then so would you agree that most of the ducks data would be clustered here because they are small and lightweight creatures they are flying the birds right whereas the cows are big and heavy so they will be clustered somewhere around here so folks So folks, is this looking pretty obvious? If you look in the world, this particular feature space, which in this case is a two dimensional feature space, I hope it is very obvious that you should observe some behavior. A data distribution would look like this, right? So far, so good, guys? Right? It's a simple fact that you would observe something like that. Obviously, I don't have actual weight and duck data with me. I mean, sort of weight and size data for cows and ducks with me. So, but I'm sort of giving a motivating example, and I hope reality is not too far from this. Now, what you observe is that you have in the data clusters. This is a cluster and this is a cluster. Now the question is how do you discover clusters in the data? So the human eye of course will immediately notice the cluster. The human eye and the human brain is the most fantastic computational machine and a learning engine. If you were to think of it as a learning machine, it is the most amazing learning machine that we have. And not just humans, if you go and look at your, or any of the animal species, vision is one of the greatest marvels in the animal kingdom. I don't know about plants, but certainly in the animal kingdom. I don't know about plants, but certainly in the animal kingdom, vision develops pretty early from very rudimentary forms of visions, which may be just sensory to full-blown vision. And someone once, I read from a researcher once that the vision evolved close to 100 to 300 million years ago in the species and the evolution of animals. So what has happened is if you think from a machine learning sense, the brain is a machine and the eyes as learning machines. These machines have had absorbed data that spans close to 100 to, I don't know, I just use the word 100 million in the order of 100 million, in the order of 100 million years. Right? So it has been learning and looking at data for 100 million years. The end result of that is the learning that has come from that is so super efficient that when we look at a picture like this, the human eye recognizes the clusters, perhaps even before you utter the word cluster. It has seen it just in the blink of an eye, it has seen it instant, it sees it. But the question then arises that that is fine for the human eye, but can a machine detect it? Can you write code that will detect clusters in data? So there has been a lot of effort in this and a lot of history. See, when you look for clusters in data, if you really think in an AI way, you can say, well, you know, if I have programming background, I can search for things, I can sort data, why can't I discover clusters in data? It must be easy. But no matter what you do or how you try, you will soon realize that the straightforward programming cannot solve this problem. We know today that this problem is at least NP-hard, right? Or maybe NP-complete in the sense that programs cannot do it in non-polynomial times. And yet, the human eye or any animal eye detects the presence of clusters immediately. So if you have a lion and you show a herd of ducks, a herd of cows, it wouldn't take the lion more than a blink of an eye to see the herd of cows and start running towards it. So there is no ambiguity in the minds of the lions. So we do know that clusters can be discovered somehow. The question is how? And that's where machine learning comes in. Machine learning looks at this problem. And the answer is, you know what, there is a reason why you can't discover clusters immediately. It has to do with the definition of the clusters. Like, for example, is this red thing the cluster? Or is this more dense area inside that is the cluster? You see the ambiguity. If you look at the dense area that I have, where I have perhaps more points. So which of these would you call the cluster? There is an interpretational aspect to it. There is also an aspect of granularity. At what level do you want to see the cluster? Do you want to go even further? And let us say that you have points very close here. Do you want to go further and call this this little thing the cluster? Right? So what is the cluster? And what notions do you need to define a cluster? Those issues come up. And you have to address those issues then only you can discover clusters, which is why it is not a problem in normal programming, it is a machine learning problem. Are we together? But it, okay, so this is, this is about the process of discovering clusters, but even before we go here, there is an outstanding problem. Given data, do you expect to see clusters? How often would you find clusters in data? Suppose you imagine that you're taking a broad sweep and surveying all sorts of data, data sets that you have been given, that you find something coming from mechanical engineering and aeroplane flying, some medical data, some financial data, some all sorts of data, you know, your bugs, your economics, econometric data, social psychology data, everything that you can consider. There's a wide variety of data in the world, of course, and an infinite amount of data sets that one can have. It is a question worth asking. How often would we find clusters in this data? So let me throw that problem to you. If I were to ask you, in your just gut feel what proportion of those data will have the presence of clusters one percent half a percent 0.1 in thousand one in ten thousand or something like that let's see what I got feeling says would somebody like to answer speak up guys, speak up. Anybody still listening? Yes sir. Yes, don't make a guess. Yes sir. Yes sir. Yes sir. Yes sir. Yes sir. Yes sir. Yes sir. Yes sir. Yes sir. Yes sir. Yes, so make a guess. Yes, yes, yes. Yeah, so throw a guess guys, throw a guess. You'll probably see it one in thousand, one in hundred. How often would you see the presence of clusters in data? Sir, one in thousand. One in thousand. That's it. Thank you for stepping forward and making a guess, Marisa. Anybody else? Except for the repeat students, of course. How often do you think you'll see it? Let me start taking names since you guys are not stepping forward. Rahul? Yes, sir. yes sir it's like uh uh in hundred we can see we usually see the cluster how many times in a hundred one in half is it yeah one in hundred uh anybody else who likes to have a different answer? What about you, Srikanth? One in thousand. One in thousand. And what about you, Kumar? Same, sir. One in thousand. One in thousand. All right. So Rajeshwari? I think she just joined. Hello? Yes, go ahead, Rajeshwari. Yeah, no, sir. I just joined due to network issues. So that's it. No problem. How about you, sir? S it no problem how about you sir we always yeah as if we always see a cluster um so i don't know how one one in thousand or one in hundred how much um that would be a realistic but we always see a cluster in the given data that's right uh did you attend the previous lecture of mine on this? I think you did, isn't it? I've talked about this. Or is it your speaking experience? No, no, I didn't attend any session. It's a very good answer, actually. Surprisingly, it turns out that you very often see a cluster. I would see that clustering in data is all greater than 80 to 90% of the time. If you have data, you're much more likely, even more, I would say that 95% of the time, you always have clusters in the data. And more than even 95 would be actually, now the more I think about it in the examples, very rarely do you see an absence of clusters. And the reasons are quite interesting actually. It has to do, I mean, I could argue from a theory called the kiosk theory and nonlinear dynamics, but I'll explain it in a simpler way. See, have you ever gone to a wedding reception in India? Those are big events. When you go there, suppose you're looking from the balcony to people in the room, the main room where all the guests are sitting. Do you see the guests uniformly distributed, or do you see them in clusters? Do you see the guests uniformly distributed or do you see them in clusters? Clusters. Form clusters. Congregate into small groups and talk and things like that and they are forming. You go and look at a schoolyard, do you see children just randomly distributed everywhere, uniformly distributed? Or you see them in little bunches, little clusters or groups? You always see them in groups, right? You stand on a highway, you know, now you have highways in Bangalore and in India. So you stand on a flyover, it is called an overpass. Do you have overpasses over your highway? Like can you stand on a bridge over the highway and look down at traffic flowing? Yes. Yeah. If you ever do that, you will observe something quite interesting. Even when there is no traffic congestion, especially when there is no traffic congestion especially when there is no traffic congestion when the traffic is congestion congested there are no clusters it's one uniform sea of cars and automobiles stuck and motorcycles and everything stuck there on the other hand when the traffic is free flowing there is no congestion if you stand on such a bridge and overpass and you look at the traffic flowing, you will observe that the cars don't, or the automobiles, they are not growing uniformly separated out or more or less uniformly separated out. They sort of come in bunches. They come in waves. One big wave of cars and automobiles will go, followed by a little gap and then more automobiles will come. And it is worth wondering, so what is happening is that on the highway they are clustered. They're going in little bunches together, whereas ideally they should perhaps be uniformly spaced out. It doesn't happen for some reason. It leads to, and the reasons are to do with non-linear dynamics at kiosk theory and things like that it's very interesting that it happens more than that if you just look at something very basic look at the the males coming from the postal mails coming from your coming to your house or sometimes the packages coming to your house and so forth you will realize that someday you don't get much mail, one or two mails. Some days it seems that the whole world is remembering you, right? You get a lot of mails, but you don't get a steady state of mail. It isn't that every day you'll get exactly three mails, the average number of mail to your postal, you know, envelopes to your house. Have you noticed this, guys, that some days you get much more and some days you get much less. Now, when you think about it, each of these letters are coming from completely independent sources, they are not in collusion. And yet coming from completely independent sources, nonetheless, when they reach you, they reach you in bunches, right? They reach you in clusters and clustered in time. A lot of them come on one day, another day that you wouldn't get as much and so on and so forth. So they come in waves. So when you look around everywhere, you will see that actually data is clustered. Things, events, happenings are clustered. Are we getting the sense? There was a very interesting thing you can do. Even if you make people, like let's say enter a room. I'll take this thing. And this is coming from one of the first episodes of a lovely TV show, which I encourage you all to see. One of the few TV shows which are centered around machine learning, it is called Numbers. B. The three is reversed, I believe. Numbers, right? It may be visible in Netflix, I think. I hope so. Or in one of the streaming services that you folks have in India. Check it out. It's an old TV show, but I think it's still very good because each episode introduces, it's a mystery, they'll typically be murder mystery or something or the other. But the solution of that will come through machine learning, through the application of mathematics to the problem. So one of the first episodes had something similar to this. The mathematician was trying to show that data is clustered and when data is not clustered it means somebody is deliberately trying very hard to prevent a pattern from being visible. So what he give us an experiment, he asked everybody to enter a room and stand somewhere. And when they stood somewhere, they automatically began to form clusters. Right, they were not uniformly spread all over the room. Uniformly spread all over the room. It is natural for data, for people, for everything to form clusters. So an absence of clusters usually shows something unusual. There are absences of clusters. For example, if you go to the beach and you look at it, all you'll find is sand. Now you may say that sand too is clustered in some sense, but sand is more uniform. You probably wouldn't see very well-defined clusters. But between the sand, you may find clusters of bushes, like some green bush or something like that. But the sand itself is more or less through a lot of erosion and so forth in the sea activity. It has become a uniform bed of sand. So there are situations where you don't see clustering of data, but it is much more likely that you will see that. And to me, the most evocative proof of this is to just lie down on summer days and look up at the sky. I don't know if it is still true in India, but when I was growing up, that was 30 years ago, 30, 40 years, actually 40 years ago, it was common in India because load shedding used to be common or because in India the houses get extremely hot in summer you don't feel like sleeping indoors you didn't in those days so you would sleep outdoors when you would sleep outdoors you would you would be you know look sleeping in your courtyard or you'll be sleeping on the roof and you'll be looking up at the sky when you look up at the sky on in the night what do you notice you see stars there now are in the night, what do you notice? You see stars there. Now, are the stars uniformly distributed? Like, do you see a sky filled with random and uniform distribution of stars? You don't see that. What do you see? Anybody? Clusters, I'm sorry. Yes, the stars are clustered, right? And especially if you have a telescope, it becomes much more apparent. We have galaxies, then we have constellations of galaxies. And the universe is not a uniform distribution of stars, isn't it? So that's again an evidence of clustering. So these are all just to give you a variety of data that exhibits clustering. So clustering is a fact. Therefore, the question remains, though, how do we discover those clusters? And that is the context of today's algorithms. The clustering, people call these are clustering algorithms or clusters. You will see clustering algorithms much more commonly used. Clusterers is more intuitive, right? But I suppose it's less commonly used. Actually you find clusters used paradoxically in more formal literature. So research papers and so forth, you'll see the word cluster mentioned. Anyway, so today we are going to learn about three kinds of clusters in quick succession. We will learn about partition based and I'll just call it. Okay, let me take the example. hierarchical, and we will do density based. So today would be a bit of a long session because we'll give some time to doing each of them. It may perhaps be the longest of the sessions we have done. So if you feel that we are going too long, we can of course break it up into parts and do the second part next time. So I will start with the first one, which is the simplest of them. And the most ubiquitous, any way you look, you'll find somebody or the other has applied K-means clustering. Its strength is that it is simple. Its weakness is that it can be unstable and it may create clusters which are unrealistic. Those are its limitations. Now K-means, the version that I'm going to explain here, it is a very simple version. Usually, you use a bit more sophisticated version of it to speed it up, to make it run faster. But we don't want to get into the complications. We want to get to the core of the idea. And to get to the core of the idea, we will avoid all sorts of optimizations. we will avoid all sorts of optimizations. So for those of you who are technically inclined, you will see that you use data structures like metrics, like certain things like metric spaces and metric trees and sort of KD tree is quite common and so forth, cover ball, cover spaces, O trees, old ball trees and so on. There are all sorts of data structures that people do bring into the space, but I won't go and talk about all of that because it would be a distraction from the core idea itself that we will use. So let us go back and bring back the problem of our cows and ducks. So suppose you have data point again and I'll just make this ducks here and then ducks and then this cows. So this algorithm would basically say is that it solves half the problem. See, with clustering, there are two main problems to solve. Clustering has two parts. A, how many clusters are there? B, where are the clusters? So discover the clusters. First is decide how many clusters are there. Find out how many clusters are there. Now, when you look at this diagram in the feature space, which is X belongs to here, it's a two dimensional plane. And I'll just keep it abstract instead of weight and size. but you can think of weight and size as a motivating example. When you look at this, I ask you, how many clusters do you see? Two, sir. Two clusters, right. So now K-means does not answer the question how many clusters it says you tell me how many clusters to find and i will find you those many clusters no matter what so for example if you ask it to find 10 clusters it will go find 10 clusters in this data so it leaves half the problem to you and solves only the other half, the part B of the problem, of the thing. It says, tell me how many clusters you want to see. So let's take a, let's say that you make the happy guess of two clusters. So that is what the K stands for. K, K is equal to number of clusters. So let us say that you say k is equal to 2. How will we do it? Let's figure it out. Now, so I'll explain the word means here, that is where it will come to. So the way you do that is, it is made up of a few steps. And these are very sort of intuitive steps. It says first step one, steps, pick any two points randomly as the initial centroid, centroid means center of gravity pick any two points so let us say that you pick if you have if and of course i'm getting to the intuition first so let us pick some point that is like uh which one should i put? Let's say if you pick randomly, you ended up picking this point, right? And also randomly, let us say you end up picking which point, this, maybe this point. These are two random points that you picked up, right? So then let me just call it 1 and 2, the second point. Then the algorithm says 1, 2, assign every point to the nearest centroid. to the nearest centroid. Here, initially you said centroids are 1, 2. So if you take every point, so for example, if I were to take this point, which centroid is it closer to? Is it closer to 1 or is it closer to 2? 1. 1, right? So you say it belongs to one. So in other words, every point votes or becomes the constituency of one of these two centroids. When you have done this, you would agree that the entire data set would have bifurcated into two sets one belonging to centroid one the other belonging to the other centroid isn't it so so with that being true would you agree that this would all these points would belong here and all these points would begin to belong to centroid two. Does that make sense, guys? Then it says that the algorithm basically says that, aha, that's pretty lovely and you're pretty much done. The third step is find the centroid. And so when you assign each point to the nearest centroid, i.e. build the two clusters, right? You build the two clusters. Now the third step is find the centroid of each cluster. So if you look at the centroid, what is centroid of a cluster? The center of gravity, the center of mass or the center of the center. Basically it is the central, the mean point, the mean value of x1 and the mean value of x2, of x1, x2 for each cluster. So if I look at this picture, let us draw the mean. Would it be fair to say that the mean would be, let's say, somewhere here? This is the mean or the center of gravity or the center of this cluster. And the mean of this perhaps is somewhere here. Would that be reasonable? Guys? Yes, Paul. It would be reasonable. So what happens is that you call these the new centroids. Find the centroids of each cluster and these become the new centroid the new centroids right and then what you do and here is the part that is interesting, you keep repeating this cycle between two and three. The two-step part, first assign all the points to the nearest centroid, and then find the centroid for each of the clusters. You keep doing this, do this, repeat till, till you feel like stopping. Well, that's a very vague word to use, feel like stopping well that's a very vague word to use feel like stopping machines don't feel but i'm just using an intuitive word feel like stopping for example you can have many stop criteria stop criteria could be criterion could be one that the centroids are not moving do not move much or it could be that a points are very few points are changing loyalties. Like very few, very few points are moving from one centroid to another, right? From one centroid, from one cluster, I should say perhaps, from one centroid, from one cluster, I should say, perhaps, from one cluster to another. Are we together? So all you could do, or just arbitrarily pick a number, or arbitrarily really arbitrarily this is butchering the spelling arbitrarily pick the number of iterations if you're pretty sure how long it will take right iterations is equal to i mean whatever you can say five ten whatever whatever suits you and now you say well if you pick the number of iterations what if you're not done it is possible generally k-means class converges very fast like for example in this case you went from this to this and from a random guess to the correct answer quite literally in one step isn't it guys if you repeat it you're not going to change your position or your clusters very much so this is a happy example where the data is clean your initial guesses were good right and because your initial guesses were good you quickly move to the centroid. Now, there is another, but it may not be true. Let us now look at the same data, but with bad guesses and see if it still works. So suppose I take this, I'll redraw this diagram and see what happens if the data is not what we think it is. Or I mean, data is what data, of course, I apologize. But the initial guesses are terrible. That's a really unfortunate in making the initial guesses. So this is, I hope, a close approximation to the original data. Suppose you have this. OK, this is. Suppose you were unlucky and your initial guess was. Just by misfortune happens to be, let us say these two points, one and two. Well, now what? You realize that you have a problem here. These two are very close to each other as guesses. The question therefore is, will this work? Let's find out whether it works or not. So what we again do is we associate all of these to its points. When you do that, you will initially discover that your first cluster is like this. these points will belong to one cluster and these points will belong to the second cluster. Can you see that? If you assign all the points to the nearest centroid, given one and two as the initial guess of centroid, this is the cluster you'll come up with. Does this make sense, guys? You agree with that. So now comes the next thing. Let's find the centroid of the second cluster. The centroid of the second cluster would be the center of gravity here. Would it be reasonable to say that it will be somewhere here too? Guys, is that reasonable? Yes sir. Yeah, that would be reasonable. Now comes the tricky part for one. Where would it be? Would it be reasonable to say that the centroid would be maybe in this empty space somewhere somewhere here as a guess would this be a reasonable answer guys yeah but that looks reasonable but we don't have any data points there that is right we don't have any data points but that's perfectly okay our algorithm didn't say that you can't put the centroid where there's no point okay this is one of the things we allow for it looks it looks very odd the the most empty space we are calling a centroid but so be it we do that right so this is the second step now let's use a different color let's make the let's now that the centroid has moved here let us redo this let's redo the clusters so it is clear that let's say that this point if you let's make the cluster for two you would agree that most of these points would have gone to two. And most of these, the rest of these points and all of this would go to one. Are we together, guys? So the journey has been, the centroid has moved, the two has moved to this, one has moved to this, and now the pink or red color, whatever this is, is your new clusters, the two new clusters. So far so good here so here actually the one can move to either to the uh second centroid or it can come even to the first centroid no sir there will be a two option for that in that case because uh in the first group so we have taken the complete thing so which is it can come to both the center i feel i don't know i just take a sing uh i don't know see the center of gravity is the mean is the mean of points the location of points so in the beginning you have a lot of points here and a lot of points here so it the so let me just call this ducks right this was the ducks let us say and these were the cows you cannot have one move all the way to the ducks in one step say and these were the cows you cannot have one move all the way to the ducks in one step right in the beginning because see look at this yellow points you started very close to each other so in the beginning one the yellow one will end up owning half the cows and the right but this but and it will own all the ducks, all the cows and half, I mean, half the cows and all the ducks. But then what happens is when you find the centroid, one moves to this pink colored one, right? The centroid is now here. Manisa, are you getting that? Sir, actually, no, sir. So actually what you explained is like, I don't know whether I am correct. So here the centroid for the first one is the one which you have mentioned in the yellow colour. I mean the one. So because both the things are close to each other, so the first centroid can be either to the one which you have placed in the centre and it can either be to the one which is next to each other. So the first centroid can be either to the one which you have placed in the center and it can either be to the one which is next to it only. It can be both, right? So it cannot be only to one, I guess. Let us redraw. Let us redo the entire thing over again. I'll erase this and I'll redo the whole thing. Let me see if I can redo the whole thing. Suppose you take these two. Unfortunately, these two are your initial guesses. Centroid guesses. So this is an unfortunate pick because, of course, if you look with your eyes, they look as bad a choice as they can be. Isn't it but our rule is we still have to give the points to one every point has to belong to one centroid or the other so let us take this point where my mouse is which centroid do you think that belongs to this point let me just call it a where does a belong to which centroid does it belong to one or two two what about this point the one very close to it where my mouse is this belongs to two two so how about this guy it belongs to two it also belongs to two what about, so let me just call this C. What about this point? D. Which centroid does this belong to? One. One, exactly. So you have switched over. So how do you decide who belongs to that one easy way geometrically is to take the perpendicular bisector of the line connecting to the these two centroids take the perpendicular bisector and you would realize that everything to the left of this line belongs to one and everything to the right of this line belongs to two belongs to one and everything to the right of this line belongs to two. Would you agree? So, for example, if you take this point, which is it likely to be closer to? It's closer to one, isn't it? Than to two. So suppose you look at this point E, which cluster does it belong, which centroid does it belong to? One. And continuing that argument forward forward here is your perpendicular bisector these all of these points these duck points which cluster do they belong to which centroid do they belong to one or two which are they closer to are two one one one so are you agreeing with that uh yes sir now i got it now you got it right and so the first time around your cluster would be oh sorry your cluster would be something like this uh let me bring back those white points your cluster would be something like this thing this entire thing is your cluster one cluster even though it has a lot of empty space and this part is the other cluster right now what is the center so now i'll remove this abcd uh maybe they're cluttered maybe i'll leave it let's leave it there now ask this question of of the centroid 2 and the cluster 2 so let me just give it a name cluster 2 and this is cluster 1 tell me where is the centroid for cluster two? Would you agree that the centroid for cluster two would be somewhere around this point? Is that where the center of gravity of those those points are does this look reasonable guys yes sir yeah so two has moved here what about the centroid for one the one the the cluster one owns half the cows and all the ducks so so far cluster one is a rather rich farmer has a lot of cows and all the ducks. Cluster two has only half the cows. So now where's the center of gravity of cluster one? The centroid, where would you put it? Would you say that- In the middle. Yeah, somewhere here. Let's say that it's somewhere here, right? Would it be reasonable to put it here? Yes, sir. So now that it is here, let us repeat the process. Let us assign all the points to one cluster or the other. So we realize that when all the ducks will certainly go back to one, they will remain with one. What about two? In two, you will see that loyalties of some cows are changing right what will happen is if i look at the perpendicular bisector of this the perpendicular bisector is going along this line right so now these many cows all belong to two. More cows now belong to two. Do you see that? Some extra cows flipped over loyalty to two. And these many, this is the cluster for, let me just call it cluster one and this is cluster two are we together guys would you agree with this so what do you notice that cluster two is picking up points cluster one is losing the cows what now let's go over to the let me use another color now I'll use blue let's say let us find the centroid of 2 cluster 2 the centroid of cluster 2 perhaps could be this point would it be reasonable to say that to say that? This is 2. And what about the centroid of 1? Would you agree that it would have shifted to near this? This is 1. Isn't it? And now once again, you find the cluster for 2. So when you do the cluster for two, right? So when you do the cluster for two, let's join the two with the line and do a perpendicular bisector here. And you realize that what happens now, two ends up owning all the cows. Do you notice now two owns all the cows? Would you agree and one ends up actually it owns this empty space also but i won't focus on that it ends up uh owning all the ducks at the bottom so this now is your uh where am i So this now is your, where am I? This is now your cluster one, and this is now your cluster two. And now let's find the center of gravity of, let me use one more color. What could be it? Maybe green. I'll take green. Let's see. Green. This is it. OK. So now what is the center of gravity of this? It would be this. Would you agree that this is the center of gravity of one? And the center of gravity of this cluster is is perhaps somewhere here, or somewhere here, too. Does this look reasonable, guys? Yes, sir. And you know that you're pretty much reaching the end. Those cluster, it's the, now the points will not be changing loyalties, right? They will all stay within the cluster. And so the center of gravity in the next or the centroid in the next iteration is not likely to move. And therefore you have detected the clusters. You can say, I can stop now. I have two clusters and my clusters are these clusters here and the clusters here. These are my final clusters, right? This region is one cluster and this region is my it's my other cluster right that's how you do it or actually more precisely what it does is it also divides the empty space so i join these two line between the two i will have a perpendicular bisector so you would say that everything on this side, though we don't, even though there are no points, like logically speaking, we are doing something like this. We are dividing the space into two parts. One belongs to cows and one belongs to ducks. That is why these algorithms are also called space partitioning. Feature space partitioning algorithms. People don't use the word features prefix so often. Partitioning algorithms. K-means. K-means is an algorithm that clusters by dividing the feature space into regions. One region for cows and one for ducks. Are we together here? So it's a beautiful, the K-means is a very simple and elegant argument for finding clusters in data. You notice that even when you make very unfortunate choices, you can still discover the clusters, isn't it? You have a journey. Points move. For example, if you look at the movement of one, one move from here to here to here and from here to here. You see how it migrated, the centroid of one migrated. The centroid of two didn't migrate that much. It went from here to here and from here to here and from here to here, not much movement in two. But centroid one moved quite a bit. But at the end of the day, you settle down at a point at which the centroids have been discovered. So that is a power or the beauty of K-means. Now, there are a few limitations. One is, do you notice that if you make bad choices, it takes more number of times to find the answer, isn't it? For example, in the first one, where we made good choices of the guesses, good guesses of the centroids in the beginning, we converged much faster. On the other hand, when we made not so good guesses, it took a while for us to converge to the centroids. So the runtime of your algorithm therefore becomes proportional to a random process, random choice of guesses. That's one limitation. There is another limitation which is pretty actually bad, which is, and I won't go into that. Sometimes you end up with cluster centroid choices in the beginning, guesses so bad that it ends up sitting on the perpendicular bisector of the real clusters. And so, as you can argue, if you're in the perpendicular bisector, you just get to a very unstable situation in which the clusters that you discovered are bad clusters. They're not good clusters. So, for example, if you end up discovering a cluster halfway like this, this is one cluster and this is another cluster, the lower part. Now, what happens is when you do that, your cluster centroids don't move. In both the cases, the centroids would be somewhere like here and here. So this will become like one and two in that situation. And when it becomes that, there is no further movement, but those centroids, as you can see, they are terrible. They hardly, I mean, those clusters are not real clusters, just one look at it and will tell you that this somehow is not right. Right? You you have picked up half of one cluster and half of another in one cluster. Right. And in the second cluster, also, you have done the same thing. So these cause instabilities. So people have found ways to solve this problem. Many clever ways people have found. So one clever way they can argue is, so I'll just erase this part, this bad clusters that I drew, let me just erase it. So one way that you can do is whenever, Let me just erase it. So one way that you can do is whenever, or maybe I'll do it in the next part here. Let me draw this again. I think the previous drawings have become cluttered. Suppose I have this, again, I draw these points. And I draw these points here. And so let us say that I randomly pick a point, pick a point, let's say somewhere. Let's say that I pick this point as one. One suggestion that often helps is pick the second point which is as far away from the first point as possible so what would that be it cannot be one of the ducks you'll end up picking maybe a cow this one will become your two would you agree agree? This seems to be the furthest point. This cow seems to be the furthest point from 1. Would you agree with this? Yes. Then what happens is, as you can imagine, that this will converge very fast. Because in one hop, you will end up declaring this as I let me use the same color you will end up using this as your first cluster and this was a point and this as your second cluster and then in one hop you would get to the right answer which is that one will move to this guy and two will move to this guy and the clusters will remain the same do you notice this right so it's a clever trick you can sometimes it works it doesn't always work sometimes it works there's no guarantee always work. Sometimes it works. There's no guarantee that this process itself will work. But it does seem to work quite often and sometimes people use this. Then sometimes people will do other tricks. Like for example, they'll say, you know what, I need a way to measure the quality of a cluster, how good the clusters are. And so there are mathematical measures. One of them is called within cluster sum squared distance. What it means is, suppose you're given points, four points or five points. How many did I make? Six points. Find every pairwise sort of a relationship. I don't know. Did I miss something? I'm sure I missed something. Yeah. Missed this. I missed something. Yeah, I missed this. So you see you have a lot of interconnections between the six points. For each of them, find their distance. Let me say d1, d2, all of these distances. And take the average, or take the, with some square distances, just add d1 squared plus d2 squared Etc. Etc. And sometimes you take the average you divide it by total number of points. Let's say 1 6th, right? so you take this distances between all the points and then What you find is and do it for the other cluster also, so it's a summation in mathematical notation. It will be a summation over each of the clusters and it will be a summation over every pair of points, distance between every pair of points, d i j belonging to a particular cluster, square, d i j square belonging to whichever cluster it belongs to. You do that. This is, why is this important? The reason it's important is suppose you make the mistake and you pick, suppose the points are here, let me just make a few points, not too many points to illustrate this. Suppose your points are here. I'll deliberately put them in a line for the time being. Let's say that this little bit like this. Suppose you made bad clusters. Suppose you discovered clusters like this and this. Let's look at the cluster 1. You would notice that if I try to do pair distances between every two points in the cluster, you will end up with these big lines. Do you notice that? How many long big lines I'm drawing compared to the other one and we can keep going that so you realize that these lines are much much longer than in this situation when the clusters are clean so your wss here will be be large will be large, right? And so if what you do is you pick the WSS cluster, minimize WSS. How can you minimize WSS? What you do is you keep on doing the clustering a few times. Let's say that you do 10 tries. So you build 10 random clusters, and then you pick the minimum WSS. Now, in practice, what will happen is the probability that you'll make a blunder like this is very, very low. So what will happen is at least nine times out of 10, you will pick the right clusters. So nine values will be very close to each other, and the 10th value may be off if you're unlucky right it is very very very unlikely that all 10 uh choices or tries would lead to very unfortunate clustering results so this is the way you you can do by increasing the number of tries right and so when you use it in scikit-learn the python library that we are going to use for clustering in the next session you will notice that there is a parameter it will ask you k means will ask you how many clusters should i find in data and the second question that it will ask you is how many times should i try to find clusters right Right? So that it will look into the WSS and try to pick the best and so on and so forth. So number of tries matters and so are we together? So that is for K-means clustering. I believe, oh, all right. It's already one and a half hours. So we probably cannot cover all of this in one day. So you know what, today I'll stop with one algorithm because each of these are conceptually big. How about this, we repeat this and I will early release the code for this clustering so that even before the next time's lab, you guys can review the code, try to read it on your own and then I'll walk you through this code next time. But clustering is a beautiful thing. It's a beautiful algorithm, clustering. To find clusters in data has amazing, amazing scientific value. It is heavily used in medicine, in medical fields, medical research. It's heavily used. I mean, I can't think of any field that does not extensively use it nowadays and so we learned one particular algorithm which is the k-means clustering. So any questions guys? Hello sir, so here about the clustering what you told us so you mean to tell the number of tries will give us the more accurate clustering model so like because you told that uh when we are working on this clustering it will ask that like how many clustering we need to find so for example if you press something like 10 i'm not sure so that shows that there are 10 clustering and i mean the result will be more accurate no no no no see you mix up the number of tries with K. Don't do that. K is asking how many clusters should I find? Let us say that you always want to find two clusters. But you realize that when you're trying to find two clusters, you might end up doing very well one time and not doing so well the next time, right? So what happens is you need a way to find out how good was your clustering. And you can use it to just, you know, try it a few times to make sure, maybe two, three times to make sure and pick the best clustering that you achieve for k is equal to two. But the question still remains, what is k? What is the correct number of clusters? This algorithm, k-means clustering, is silent on that question. It doesn't answer that question. It says you have to tell me how many clusters there really are in the data. If you tell me how many clusters there are, I'll find it for you. And if you make a mistake, if you say, suppose there are two clusters, cows and ducks, right? So so here suppose you give me an answer like five or three i will find you three clusters and the best possible three clusters in the data and then don't come blame me because you said three or you said five but i'll give you the best five clusters that's that's what k means clustering is saying means clustering is saying so if we have data like uh in circular pedifiller i mean two into uh uh circular ways the data is distributed right so in that way this game is able to find the on like cluster yes yes very well so it's a good point you raised when when the data what you call circular you can uh the people use fancier terms when the data has a convex well-defined convex hull or when people have a data is very convex or I use a simple term globular, secular like, right? For example, an ellipse will do just fine, isn't it Rahul? Doesn't have to be a circle, an ellipse could be. So more general definitions are globular, roundish, some form of roundish or convex shape. When it is convex, K-means clustering works wonderfully. It does a very good job. The trouble with K-means clustering works wonderfully. It does a very good job. The trouble with K-means happens when the data is not globular or when the two clusters overlap. So with cows and ducks, it's easy. But what if we are talking about cows and buffaloes? You realize that there would be a certain degree of overlap, right, between the two clusters. Or cows and lions. So those are the situations where it becomes a little harder to tell whether a data point you should put, which cluster it should go into and so forth. So those are some of the limitations of k-means. And the biggest limitation is it answers half the question. You have to tell it how many clusters to find. Once you tell it that is the k, then it will go find it for you. But it's amazingly simple. Actually, if you think about it, I wrote down just three steps or two real steps, a two and three here, assign each point to the nearest centroid and then find the cluster associated with that centroid and then repeat the process. It can't get simpler than that in machine learning. This is perhaps one of the simplest algorithms and yet also an extremely powerful algorithm. What simple and powerful. It is one reason you find this used extensively in the industry, very, very commonly used in all sorts of problems in the industry. And as I said, there will be clusters in your data most likely. And this answers that important question. How do you find those? So you can say that K-means is a cluster. K-means cluster is what we studied. The process of finding clusters is called clustering. One more doubt I have. You told that depending on the model, for example, if I tell I have found some three clustering, then if my friend, she tells that she has some five clustering, so then how will come to know that it is a correct model? So like we don't know the accurate cluster clustering, right, in particular model, because my guessing may be three and her guessing may be two. And finally, what will be declared a correct cluster in that case? The answer to that is, and that is something I'll tell you about. Okay, maybe I should do that. It's a little techie. I mean, if you get it, it's all right. If you don't get it, it doesn't matter. Remember, I told you this WSS measure. What you do is, this method is called the Elbow method. Elbow method. What it says is for each of the value k is equal to 2, k is equal to, even k is equal to 1, k is equal to 2, k is equal to, you know, 3, 3, 4. You can keep going, right? 5, 6, 7, 8, 9, 10. You can keep. What you do is you find the WSS, end. You can keep, what you do is you find the WSS, this measure that we talk about. And then when you do that, then you end up with a plot, like for one, you'll get a certain value. For two, you'll get a certain value. For three, you'll get a certain value. For four, you'll get a certain value. For five, you will now get a certain value. Six, a certain value. So what happens is these values will still go down, keep going down, down, down. But if you notice something imagine that this is your shoulder. And this is your hand. What is this? This is your elbow. Isn't it? If you imagine your hand being like this, I don't know, are you getting the impression? So imagine, I won't make such a big hand. Let's say that this is don't know are you getting the impression so imagine I won't make such a big hand let's say that this is it these are your fingers these are your fingers I'm not a bit I'm not a great artist imagine that this is your hand with your muscles and so forth. What is this point? It's your elbow, isn't it? Guys, are we seeing the intuition here? Yes, sir. Yeah, so what happens is if you look at the WSS plot, you will usually find an elbow at some point. And that elbow, that that is that elbow is let's say it's here is pointing to four so you would say that the best number of clusters to find in the data is four right so there is a this elbow method is a more formal name mathematical name uh scree plot right you can you remember the Scree plot if you want to, or more intuitively, just think of it, it is called the ELBO method in common language. In more formal literature, you call it the Scree plot, right? So this is it. And you can plot it and just look for the ELBO. And that will tell you between you and your friend, which is the better answer to pick three or five that you gave the example does that answer your question yes sir thank you that's that all right guys so that is it if you have any questions more, any more questions, you can ask me, otherwise we'll call it a day. I hope you found this topic interesting. It's a very beautiful topic, clustering, and the algorithms here are very, very nice. Well, just remarkably powerful and simple algorithms. So in the previous session, you had given us some homework regarding that one hot coding. So we need to find what exactly is that. Yeah, one hot coding. And did you do that? Actually, I was not sure with that, but I think it is something related to binary number zero and one, where all the coding will be taken to zero, two and one, one being the higher field. No, okay. One hot encoding. So, all right. Let me answer your question. One hot encoding. So suppose you know there is a feature XI whose values are, let's say that you're looking at the ice cream, you know, go back to your problem of sale of ice cream on the beach. Then your features are temperature. This is your X1. X2 is wind speed but x3 is day of the week day of week right so your numbers would be here would be a number for example this is 21 degrees wind speed is 17 miles an hour, but this could be like Tuesday, Wednesday, Sunday, right, then maybe Monday, something like that, the values. So what do you notice? And then you have to predict here the Y is ice cream sale, sale of ice cream. Are we together? This is also a number, let's say 150 buckets, right? 152 buckets. Sunday will be 300 buckets. Monday is 140 buckets or things like that. So now the problem is when you try to put it into your algorithm your algorithms will protest because it will say well you know what we don't deal with textual you know this is these values are not okay these are strings these are characters you know these are not numbers uh we don't we only math only deals with numbers So how do we convert these things into numbers? We have a problem. So what you do is you actually do what is called one-hot encoding. So this X3 is problematic. Why? Because at this moment, X1 belongs to real, X2 belongs to real x2 belongs to real but x3 does not belong to or does not belong to does not belong to real so what the way you solve this problem is you say all right i'm going to create binary features i'll say is sunday is mond, is Monday, is Tuesday, and so on and so forth, is Saturday. I created these seven features. And so what will happen is for any given data, let's say that this is data one, two, three. Let's look at the first row of data. Which of these values will be true? This is Tuesday. I put a one here. Zero, zero, zero, zero. Right. And then let us say for the third data set I would have. It is a Sunday. Right. So it would be one second data set would be Wednesday. So here we go, is Wednesday. Wednesday, right? So here one zero zero zero. For the third data set it seems to be Sunday one zero zero zero zero, etc. You get the idea, right? So now what has happened is this three has become multiple variables. Let me call it X3, X4, X5, X6, X7, right? And so forth and so on and so forth. Right now. Are now these values are the numbers. They certainly are numbers, right? There's zero and one. Do you see that? Yes, sir. Yeah. And this is called one-hot encoding because now instead of a three-dimensional space of input, the third one breaks up into seven parts. So it becomes seven plus two. So you will end up with the input space now x vector belongs to a nine dimensional space two of the dimensions being temperature and wind speed and the other seven dimension being dimensions being the one hot encoding is sundays mondays tuesdays wednesdays thursdays fridays saturday seven of those and so this now is a number. And now you can continue on with your machine learning. So that is the meaning of one hot encoding. Thank you.