 So welcome to this deep learning workshop. This is probably the fourth time that I'm giving this workshop in as many years. It's a very interesting workshop to give. This subject is so fast moving that every time I give this workshop, I feel that 80% of the content has changed. Long ago, when I started giving this workshop for the first time, we used to cover a few handful of architectures. We used to do RNNs, LSTMs, these are all buzzwords for you at this moment, Congelational Networks for Computer Vision, encoded the auto encoders and GANs. Today, when we do it, literally in the last couple of days, the last couple of years, the landscape has completely changed. Deep learning is a lot about transformers and geometric neural networks, graph neural networks. So when we do this workshop this time, we will do it from a different perspective. We'll be far more into the latest stuff, which is the graph neural networks will also be heavily based on transformers. Now, what do these words mean? We will deal with later, but I thought I'll let those people who are repeating it know that they are practically going through a new course. words mean we will deal with later but i thought i'll let those people who are repeating it know that they are practically going through a new course so don't just assume that it's a repeat you'll be learning a lot of new things now when we talk about deep learning deep learning sounds rather deep right it's an interesting word or sort of a marketing word. What it really means is a deep neural network. Now, what are neural networks and what is deep about some neural networks is something we'll learn shortly. It refers to many, many layers in a neural network. But all of those are buzzwords. I just thought I'll clarify one thing. What is the relationship of deep neural networks or deep learning to artificial intelligence, to machine learning, to data science? People often use these words interchangeably. So here is how it all starts. There are two big fields. One field is artificial intelligence. Ever since computers were built or even before, people have been dreaming of creating intelligence. You hear culturally many stories, for example, Aladdin and his magic lamp from which the genie comes and the genie is intelligent and can do all sorts of things then in the hebrew tradition there is the concept of the golem which is that you make a human being out of clay a little doll out of clay and then you somehow impute it with uh intelligence interestingly in the tradition it says that you must be extremely careful how you use the golem. Right. And so you can see, for thousands of years, people have been aware that if you endow intelligence into a machine or into something that you create, it is fraught with danger. And that is very much true today. We see artificial intelligence doing amazing things it is powering the search it is powering the social media it's powering the vacuum cleaner it's powering just about everything is there in your car it's everywhere but artificial intelligence also has the potential to tear apart the entire fabric of society for example you, and many people attribute it to quite an extent, that the rise of dictatorship, authoritarianism, and the decline of democracies is rather closely tied with the fact that artificial intelligence makes a creation of fake news and fake things, a very easy enticing fake news that will get you interested very easily. And so it's become harder and harder to distinguish between truth and fiction. One of the regions that I'll show you, and it is there in your course portal, is of Obama giving a speech in typical Obama accent, a speech that typical Obama accent, a speech that you never gave. Today, these are called deepfakes. We can take very little information and then make you say whatever the AI wants or somebody behind the AI wants. So that's the danger of artificial intelligence. Now, this field of artificial intelligence, if you look at Charles Babbage who made the first computer, actually didn't make it because mechanical engineering wasn't advanced enough for his designs to work. Today, as you know, if you go to San Jose Museum, you see that the Charles Babbage machine has been implemented. It's a true computer. But was he trying to make a calculator, a smart calculator? No. If you, Charles Babbage's machine has been implemented. It's a true computer. But was he trying to make a calculator, a smart calculator? No. Charles Babbage was trying to make artificial intelligence. That was his aim. If you look at, and those of you who have watched the movie, The Imitation Games, what Alan Turing was again trying to do is create sort of universal machines and endow machines with intelligence. It's the one core pursuit that has been, whose history is longer than the subject itself. Within this broad field, there is a theoretical core which is called machine learning, a different word, machine learning. Now what is the difference between AI and machine learning, a different word, machine learning. Now, what is the difference between AI and machine learning? Simply put, AI, machine learning is a subset of artificial intelligence, is the theoretical core. Robotics, for example, is an application of machine learning. Robotics too is artificial intelligence, but you apply machine learning to get things to do something, robots to do something. Now, there are other areas of artificial intelligence will come to it. When people tried to create intelligent machines, there was actually the history of Silicon Valley is there used to be a company here called Thinking Machines. Many legendary scientists and engineers, Minsky and others, worked in that company. They sort of tried to create intelligence. The trouble that happened is, we don't know what thinking is. If you were to just pause a moment and ask yourself this question, do we understand what it is to think? Can we quantify it? Science is about measurable things. Can I measure one bucket of thinking versus half a bucket of thinking? To use it in a figurative way, you can't. We cannot do that. We cannot tell the quality of thinking. So it's a very tough pursuit to equate intelligence with thinking we don't even know whether our dog thinks or not or to what extent the dog thinks right where does thinking begin do tadpoles think do amoebas think we don't know somewhere along the line we claim thinking has emerged and intelligence has emerged which is all fine and futuristic but to engineering and science about tangibles it's about measurable empirically measurable quantities you have to have equations in them so if we can't quantify thinking and intelligence what is it that we can quantify it turns out that the thing that you can quantify is learning, where learning is defined as making less mistakes by some measure. So take a task. For example, if you take a child to a zoo and imagine a sort of a very simplified zoo or a meadow, and in that meadow, all you have are cows and ducks. And let's say as a parent, you're saying, oh, look at that big animal with a swishy tail and horns. That's a cow. Then you come to a feathery duck and you say, oh, look at that feathery little thing that can fly, and it's a duck. Let's say the child is just two years old and is grasping some of it. After a little while, how would you know if your child has learned about cows and ducks? You would point to an arbitrary animal and say, what is it? If the child has not learned, not gotten the essence of what is a cow and the essence of what is a duck the child is likely to make a lot of errors because she would be guessing on the other hand if you notice that the error rate is coming down you keep on asking the child every time she makes a mistake you explain some more teach some more and after a little while you you notice that the child is making fewer mistakes. What do you infer from that? You infer that the child has learned what a cow is or what a duck is. It doesn't necessarily mean that the child will be perfect always, but even if the child gets it right, let's say 80, 90% of the time, clear evidence that the child is beginning to get the hang of cows and ducks are we together so this is measurable you can say initially when there was no learning this was the error rate right half the time the child was wrong right on the other hand you notice that now the child is right 90% of the time, only 10% are right. That is an empirical evidence of learning. From that, one can infer that the child has some concept of a cow now. It may not be perfect, but she does have a concept of a cow. At the same time, she has some concept of a duck and therefore the word learning the the thing we that we that we should associate with learning is in some quantifiable way making less errors or doing a task better take a task t and have a measure of performance and error is a pretty good measure of performance. And if you see a reduction in errors, you can therefore declare that the child has learned. Now, in this comes an interesting observation. It need not be a child, right? It could be anything. If in any machine, if you're behind a curtain and you're talking to something, you don't know whether it's a human being, whether it's a cow or duck or it's a machine, and you nonetheless see a measurable decrease in errors, would you infer that whatever it is that is behind the curtain is learning? Would that be a sensible inference? It would be. And therefore, learning transcends human beings. It is a quality, it's a measurable quality that theoretically can belong to anything, including machines. It so turns out that machines do learn and can learn. And in fact, this entire field of artificial intelligence, the core of it is machine learning right it is once again to summarize what i said machines learn how to do a task any task by making fewer and fewer mistakes that's the evidence that it's learning now how it learns is the fascinating subject that we are going to learn right Now, within the field of machine learning, then it's a vast and wonderful field. You can be in this field for many, many years, I've been in it for now 2025 years. Long ago, before this subject was very popular. At one time, even the computer science people, if you told them that you're doing a learning machine learning they would say why are you doing a project in the mechanical engineering department the understanding was so little about machine learning today everybody knows about machine learning artificial intelligence and so forth so uh even after studying it for so many years, I can honestly say that there is more that I don't know than what I know. It's a vast subject, but I can say that I know the core of it. Not all the things that people have done. Beautiful, vast subject. Many, many areas that are within it. Now, within this vast field of machine learning there are many contributing forces so depend if you ask for the history of machine learning it depends upon who you ask you'll get a different answer for example if you ask statisticians they will say machine learning is statistics wearing a marketing hat they like to believe that it is just another way of doing statistics right and that is a perspective and i don't agree with that perspective i will take you through the historical roots of this subject which seems very new but it started with actually the son of a coal miner in 1800s, who was discovered to be a prodigy by a schoolmaster, patronized and taken to the right places, who later on flowered to be a great mathematician. Today, he's often called the Prince of Mathematics. Anyone would like to venture his name? would like to get venture his name? Not Ramanujan. Goss. Yes, Carl Frederick Goss. So Carl Frederick Goss was given, actually the legend is, I don't know how true it is, the way I heard the stories, he was told or requested to solve a problem in chemistry, chemical reactions was told or requested to solve a problem in chemistry, chemical reactions by a friend. And in the process of solving that problem, he came upon one of the most fundamental principles, which is called well, today, it's called the principle of least squares. He came upon that, he was quite about it. And then there was some challenge thrown in Europe to predict where some planetoid or some heavenly body would show up next. And you couldn't tell. The sky is vast if you lie down and look up. It's a vast sky. You don't know where a planetoid would suddenly appear after many, many years, 40, 50 years, and on what day it would appear. And you had to predict both. So many, it was a grand challenge. Europe had a tradition of patronizing mathematicians in science by throwing grand challenges. So many people tried, many mathematicians and physicists tried. It turns out that they were all wildly wrong, but Gauss was uncannily close to the right answer. Where he pointed out, it was there. And so he wrote the paper about it, but when you read the paper, you don't see the principle of least squares very clearly stated, right? Then a few years later, it was rediscovered, but when it was rediscovered, I think it was Laplace, when it was not, I'll remember that, when it was rediscovered, he says, "'No, hey, wait a minute, I'll remember that. When it was rediscovered, he says, no, hey, wait a minute, I did it. I did it first. So a very interesting history of this. So we will talk about it today. Remember when I said that you quantify errors, least square, some squared error is one thing. So most of you have been through some level of machine learning, if you have not, but I'll talk about it more when we get into the nitty-gritty details. But to give you a high-level picture, so today is a different lecture, it's mostly high-level. So you have errors, but how do you minimize error? How do you learn? That's an interesting question. And that field, there is a vast field around that. It is the field of optimization. Again, like any other field that you encounter, you find that people have been working on it for decades. It has yielded some very good results. The ones that we will focus on in deep learning is something called gradient descent. What it simply means is that if you're hiking in the hills and the shortest path down a hill is not to carefully take the road down the hill, but just to tumble down the hill. We all agree right we try our very best not to take the shortest part down but you would agree that if you take the steepest path the steepest part is the shortest part down to the bottom that is the simple intuition now what hill are we talking about? The only difference is while what is applicable to real hills and valleys is also applicable to something called the error surface or the loss surface. It's a mathematical term that we are going to learn about as we make progress in this course. In fact, today itself we'll learn about that. And you'll realize that in the abstract loss surface, which is hills and valleys, the that in the abstract lost surface which is hills and valleys the shortest part to the bottom which is the point of least error is just to tumble down the hill as quickly as you can right so that is called gradient descent or what i call tumbling down right so we will learn a little bit about that now within machine learning one area of machine learning in this vast areas of machine learning one area that today is very hot is deep neural networks or neural networks people often call it neural networks simply or deep neural networks or deep learning you will find all of these words. You will also find an abuse of the term. Sometimes when people say AI, especially business people, they often conflate it with neural networks. So the academics are surprised because to academics, AI is a full field of which machine learning is a theoretical core. And neural network is a full field of which machine learning is the theoretical core. And neural network is a small part of machine learning, a very hot part, but a small part of machine learning. But the way the business people these days talk with sort of odd way of looking at it, they think neural networks is AI. And what about all the other algorithms? They say, well, that was machine learning, this is AI. And what about all the other algorithms? They say, well, that was machine learning, this is AI. So you will find this abusive term in the industry. Just be aware of it. Now, with that background, this course is all about deep learning. But before we do deep learning, I would like to give you a very fast breeze, quick survey of the core ideas in machine learning itself, because after all, deep learning is machine learning. So, two ideas. Machine learning walks on two legs. One is, you need to quantify what is it that you want to improve? Are we together? Think of it as error or something. It is, some people call it the error function, some people call it the loss function, some people used to call it the objective function in statistics. Many words are used. In the interest of simplicity, we'll stick to one word, we'll call it the loss function. Loss is a measure of think of it of how wrong you are, in some sense, roughly speaking, it's more than that. Loss surface is not just the error, but you add more mathematical terms to it. Those are called regularization terms and so on and so forth. There's a reason why you add extra bits to it. But broadly speaking, it's a measure of how wrong your algorithm is. So I will now draw it out for you and we will learn. Will the best way to learn all of this concept is with an example. So let me take that. Is my screen visible, a black screen visible? Okay, so suppose... Not to the remote participants. It is not to the participants. Okay, that's a good catch. Let me take care of it. And any better? So suppose we take now a very simplified world in which data presents itself like this. Very simple data. And what could this data be? Let's say that you, to frame it in a practical situation, imagine that you are a young entrepreneur. You just opened a little shack, ice cream shack on the beach. Now you wonder how much ice cream to buy every day from the wholesaler. If you buy too much, it will go waste. And wholesalers don't take back once they sell you ice cream. wholesalers don't take back once they sell you ice cream if you buy too little you'll have the misfortune of just sitting there quietly and seeing your competitor get all the customers so you want to predict how much ice cream will be sold so that you can get it from the wholesaler make sense so we'll use this sort of a hypothetical situation. And let us say that the entrepreneur then starts gathering data and observes a relationship on the beach. Let us say that this is temperature. The X axis is temperature. The Y axis is ice cream sold, consumed by your clients, right? So this is why. And this is obviously a simplified world. We are taking only one predictor. You realize that on a weekend, more ice cream sell than on the weekdays and other things. But we will fix it. All other factors being the same, we will say that at least if the temperature is anywhere between 40 degrees and let's say 80 degrees or 90 degrees that's a fairly pleasant temperature on the beach this range ice cream more ice cream will sell the higher the temperature because higher temperatures will attract people to come to the beach and you'll sell so you have this relationship you want to capture this relationship. You want to find why is, and let me use a slightly bolder color or maybe this bolder. Is it clearly visible? You realize that there is some underlying relationship effects, isn't it? And up to an error error term because there's always measurement error you know you can't quite measure exactly the ice cream some goes wasted and this and that there's some measurement error measurement error we always write as epsilon right it's sort of a thing you can't get rid of now this epsilon captures not just measurement error, it's called the irreducible error. It is also because of the fact that at the same temperature, on a weekday, you might sell less ice cream, on a weekend, you might sell more ice cream, isn't it? It also captures that which you don't know, or that which your model has not accounted for, your data has not accounted for, right? It acknowledges or it is a reflection of that reality. That's where some of those errors come from. But with that thing being there, you look at this and you say you need to model, you need to find a function that says given a temperature, given a temperature, how much ice cream to buy? How much ice cream to buy? That's what you're looking for here. So what happens in reality is, you'll never know what effects really is. For example, the underlying reality could be like this. You don't know the exact relationship, but what you are saying in machine learning is, I want to find out y is equal to, well, this is not a good color. Let me take this one. Y is equal to some function of, some function g, a simple function g, the simplest g that I can find, which is a good fit to the data. so you're not trying to fit to a function that you don't know about because there is no way you can find out about that function all you have is the data and you can you can fit all sorts of g's to this data but in machine learning there's a occamor principle, which says that of all the explanations for this data, you consider the simplest one that works as the one that you will move forward with. Now, is that G correct? So one of the questions that people ask in machine learning is, is this model right? And the answer is very interesting what could the answer be you know it yeah so the answer is see you can never know because you can't see f ever so any g you can go on creating g's and you never know which one is right. But the point in machine learning is you don't search for right, because right cannot be known. What you can search for is an effective model or a useful model to get your job done. For example, the entrepreneur does, does the entrepreneur care to the fifth decimal point how much ice cream she should go and buy? No. What she cares about is roughly how many pounds of ice cream do I need to buy? Isn't it? So effective model. And therefore there is a famous statement by a great data scientist, Box, who says, all models are wrong, but some are useful. And this is, by the way, true about much of science. For example, is the gravitation law true? Newton's law of gravity, is it true? No, it is not true. Today we know it's not true, but it is a fairly good approximation. It works. It is superseded by Einstein's gravitational equation. Now, is Einstein true? We happen to know that there's something fundamentally amiss in physics because Einstein's theory of relativity does not sort of mesh with quantum theory. So either one of them or both are wrong, right? And there's yet another theory beyond them. But are they useful? We have thousands of satellites up there and they wouldn't be up there if we didn't take Einstein's theory of relativity seriously. We just discovered a black hole in pictures, as you know, at the center of our galaxy, Sagittarius A star. Completely agreeing with Einstein's theory so far right so in this field a theory is an effective model in effect that makes good predictions and that is what we'll care about right you need a model that works that is useful right so the statement was all models are wrong, but some are useful. So you're never in pursuit of F. You're in pursuit of a simplest G that will fit your model, right? But it must fit model and make good predictions. So now if you look at this data and if I were to redo this, now let's ask the question, how would I know what is wrong with this data? So I'll make the data here again for a moment. And this time I won't make so many points. You can assume that there are lots of points here. So suppose I make a further assumption, and this is called making a parametric model. A model is a hypothesis. You can look at this data and would it be reasonable to make a hypothesis that the relationship of y to x is linear? There could be a straight line that sort of goes through the data. Literally, the visualization invites you to draw a straight line, isn't it? Now, with this model, how would you know that this is right? Suppose you can't see it. How would a machine know that this is the model? What about some other line? How would a machine know that this is the model? What about some other line? Let's say, what about this line or this line or this line? There are many, many lines that you can draw. So how would you know which is the best line? So what you could do is you need to be able to measure the amount of mistakes it makes. What is a mistake? Let's look at this one. Sorry. Let's look at this line and this line, B. We'll compare these two. For any given point, let's say, let me make it, yeah, big. So for any given point of X, let us see what prediction a makes a is making this prediction right data is data is let's say this and red is making this prediction now which which of the models is more accurate for this point the red line is more accurate for this point. The red line is more accurate. But likewise, you can go and look at another point. Let's say I look at this point and then we ask for this point, what happens? Oh, this model predicts that you'll sell a negative amount of ice cream, right, hypothetically. And the red model predicts much closer so that gives us an insight what we need to do is see what is the gap between prediction of your model and the reality that gap is called the residual error or simply error so you can say that for any given point, the error is y, which is the y, the actual prediction. Let's say this is the xi point, right? And then this is the yi predicted. The real yi and yi hat. So this is yi hat and yi. The reality is yi and the prediction is y i hat and y i the reality is y i and the prediction is y i hat now it is a convention in this field to make predictions wear a hat did you notice this hat symbol at the top of it in this field we always put a hat over predictions to distinguish it from reality there is another convention that sort of is followed that generally things that are parts of your model, you make them Greek and things that are part of reality, you use Roman characters for. But in deep neural networks, you don't do that. People use weights. W is special. Weights are Roman, but we still use it for the model. But leave that aside. So now once you have gotten an error error you need a way to add up all the errors the trouble is errors can be positive and negative to make sure that you're taking the magnitude of the errors quite often what you do is you take the total error as epsilon i squared sum over all of them that is y minus y i hat square and you can say, which model would be the best? The model that has the least amount of error. Would you agree? Right? And what does that depend upon? The equation of a line, as we all know, is y is equal to, let me just, in the language of deep neural networks, I would say a bias term plus, is this the equation? B is the intercept, W is the slope of the line, right? This is the equation of a line. So you can say, you can write this equation by saying, this is yi minus b minus w xi squared. So in other words, this is a function of b and w. When the reality is given, xi, yi are given, the only thing you can change is the slope and the intercept of a line that you are making a hypothesis. So now think about it. You quantified the error, the one leg of machine learning. You have some way of quantifying error. Now comes the question, how would you reduce it? You could say, well, that's easy. Draw lots and lots of lines on the page in the data. And for each hypothesis, for each line, figure out how much error it's making and then pick the line that makes the least error be wrong with that? See, what happens is in a plane how many lines can we draw? You can draw infinitely many lines, they're far off lines, this, that. So if you start randomly drawing lines, picking random values of this and testing it, you'll be busy for a very long time. Even if you made 100 lines, the best of the 100 lines, there is no guarantee that it's the red line, isn't it? So that is the main problem. You can't mathematically prove that the model that you picked as your best is really the solution, right? Because if you had done the experiments more times, a few million times, you might have a different solution, which is why you don't do that. You instead do a process called gradient descent. Now, a gradient descent basically says that if you look at this error surface, it looks in a certain plane, like this plane. If this is B and this is W, this is the way it looks. How do we know it? It's a quadratic equation. If you look at this, it is quadratic in B and quadratic in W, the energy function. How do quadratic equations look like? It so happens that mathematically, they look like a bowl, right? In this particular case. Why? It is positive. You can see it's a square, so it can only be positive or zero. And so it's a bowl pointing up, right? Somewhere in there, this maps to some point, which is, I'll call it B tilde b tilde w tilde that is our best solution right that is the solution we are taking so suppose you start with some random value let's say that you started with this value that happens to be this point now what is the shortest journey from this point i'll call it a to b From this point, I'll call it A to B. The shortest journey is that somehow you sort of steeply, you take this path. Would you agree? Somehow you take this path, and that part would take you from here to here in this place. And that is the steepest path. You don't want to go around in circles around the bowl before you go to the minima you don't want to be wandering around in random directions so there is a mathematics that takes you there and that mathematics is called a gradient descent and we will devote quite some time to reviewing gradient descent and based on the gradient descent idea there's more in deep neural networks. We do momentum-based gradient descent and so on and so forth. And we'll come to that. Secondly, you can minimize the error efficiently. You don't have to randomly make guesses. There is a process that will quickly help you minimize the error. So the minimization of the error is learning of machine learning. Are we together? And that specifically, in most practical practical terms it comes down to especially for supervised learning it comes down to a gradient descent right tumble down the hill because your error surface your lost surface will have certain shape you'll be up somewhere randomly if you pick a point you'll be up somewhere in the hypothesis space and you just have to tumble down. Now, those of you who have, of course, taken courses with me in the past, a lot of it may look like review. But for those of you who are new, I'm putting this idea because this is all you need to do the Deep Neural Network course at this particular moment. Now, given that these two things, what is the journey of learning? We will do that. See what happens is that suppose you have a model M. So you come up with some model M. You take an input. It will produce your Y hat isn't it this is often in the language of deep learning you call it the forward pass but then y hat may not be the right that if you have data so suppose you know exactly like for this xi you happen to know that what the real, at that temperature, what was the amount of ice cream sold. So you can now, from these two together, you can compute the error function or the loss function, right, which is a function of Y, right, YI between this. And then you add up the mistakes from all the data samples that you have and you'll get the total error are we together y hat right so i will uh you have this is called the loss function you need to get to a loss function you need to be able to quantify the error so you made a prediction in the forward pass that gave you then you found the loss function you need to be able to quantify the error so you made a prediction in the forward pass that gave you then you found the loss function now what do we do with this loss function this is what we do gradient descent step on so first you need to do two more step you need to find the gradient right now a gradient is represented in this world by an interesting thing you take a triangle and put it upside down right it's a symbol for gradient it's a mathematical symbol we'll get more into it later so if you take the gradient of this, then you can say, well, now I'm there because all I need to do is gradient descent. It means go and there's a bit of mathematics. I can improve by doing a gradient descent step, which is W minus this. Some weight loss. The next W, I should move in the space, in this space, in this direction. From here, I need to go here, right? I need to go here, here. And in this error surface space, I need to keep making steps like this. Are we together? So that is the basic idea. And this pass from the loss function to get the gradient and then doing the gradient descent step, people often refer to it in the neural network literature as the sort of the backward step, right? You're taking a backward step. Now, the backward step can happen for frankly, every machine learning algorithm, but in the case of neural network, something is special. What happens is that it's a little bit like domino. You touch one and there's a cascading effect. So what happens in a neural network is there are many, many layers of these machines that can learn. These are called nodes. Then the learning, the last gradient, you compute on the last layer, then the layer before that, and the layer before that, and so forth. So that is called back propagation. That, again, we will deal with in considerable detail as we make progress with this workshop. But at this moment, the big ideas. So there will be a while this is universal to all learning all predictive models all predictive models learn by this learn like this in the case of neural, what happens is this box for neural networks, it turns out that the box is rather complicated. It has all of these many, many layers. So it is made up of these atomic units. These are called, in the world of neural networks, we call it literally neurons, or more specifically, just a node of a neural network, node of a neural network. So when you see the word neural, you often wonder, does it have something to do with nerves? Are we recreating the brain? Now, if you were to believe the science fiction people and also the marketing people of the AI company, they will all assure you that artificial intelligence neural networks are modeled after the human brain. There's only one problem to it can you guess what that is nobody has the foggiest idea how the neurons learn inside our brains right for one thing there is a so they will always make this little diagram of dendrites and so forth and they say this is the neurons um here it is and maybe something and they say this is the signal transduction on the neuron. And see, we are making our neural network node these things like that. And so when you take a whole bunch of nodes, what you're recreating is something like the human brain. There's fundamentally many problems. We know exactly how a neuron conducts neuroscience. We'll tell you how it conducts the signals. The first thing you learn about a neuron is the signal conduction is unidirectional. Right. So there is no back. There's only a forward pass. There is no back pass or back propagation, what it is called. So what machines do is what we have found, learn to do and it works. It manages to create learning systems. But for us to claim that it somehow mimics what the human brain does, absolutely not. Especially because we don't know how the human brain thinks yet. But the dream has been there. People have thought that we can learn by looking at the brain. And have we learned by looking at the brain? We have actually. It turns out that one neural network architecture called convolational neural networks, that's a big mouthful of a word, convolutional neural networks, CNNs they're called, they were inspired by looking at how the cat's brains work. Right? By seeing the areas of the brain that light up when cats look at things. Right? So neuroscience has contributed with ideas to neural networks. So it is good that we have the illusion that we are trying to create the brain because a lot of the ideas can be borrowed. So that is it. So now what are neural networks? Let me explain it using a simple idea. But before that, I'll take a simple analogy and take it in a sort of as an analogy, not as a literal truth, how many of you know what a rheostat is? Anybody knows what a rheostat is? Okay, so here's the thing. Suppose you have a voltage and you have a resistor. Can you tell how much current will go through it? It so happens that they will tell you that the current is V over R, the resistance of the wire. For example, air has practically infinite resistance. So you don't have, you know, the live wires are not shocking you if they are at a distance. This is a current. Now, there is such a thing as a rheostat. What it does is it's called a variable resistor. People often also call it a variable resistor. They have a line here, a contact surface. So depending upon if you keep it close to this, close to this, you go to the current goes to less amount of material. And if you slide it to the right, it goes to a more amount of material and so more resistance. to the right it goes through more amount of material and so more resistance you can change the resistance are we together now suppose i ask you that your i you are expected to produce a certain amount of current for a given voltage so what will you do you'll randomly put the resistor somewhere like the the slider somewhere and you look at the current and you realize that oh my goodness i'm producing too much current so what will you do next you'll slide it back isn't it and you'll systematically slide it back and forth right not randomly guess or based on whether you're over producing current or under producing current you'll move in the right direction and you'll be able to therefore get to a point where you're over producing current or under producing current you'll move in the right direction and you'll be able to therefore get to a point where you're producing practically the right amount of current right expected for different voltage values as best as you can right that is essentially you learning to put the resistor at the right place and what you just made is a good model a good model for the relationship between voltage and the output current that you expect you see how real all of these things are now comes an interesting thing there is a concept we look at it as in this world of neural networks let's look at a neuron or a node in a neural network it's simple like this it is made up of two parts so suppose you get now let's complicate the problem a little bit instead of just one, we say that the entrepreneur just realizes that if it is windy, less ice creams will sell, right? It also depends on the wind speed. So you're giving it a temperature and you're giving it the wind speed, temperature, wind speed, how windy it is. Now you need to build a model of that. So you would say that how much does temperature matter? How sensitive? Would it be a reasonable thing to say? How sensitive is the sale of ice cream to temperature? And how sensitive is it to change in wind? You would agree that these would be different things. The sensitivity to temperature would be a parameter, let's call it W1. The sensitivity to wind would be a parameter called W2, let's say. And there may be a baseline amount of ice cream you would sell anyway because there may be some diehards who come to the beach every day and eat ice cream, right? Some kids who live in the house next door so you may have a bias term so you can say that see the output is the amount of ice cream produced would be x1 w1 plus x2 w2 plus the bias term the baseline amount of ice cream that gets sold every day anyway. Does this look reasonable, guys? In a little bit more fancier mathematical notation, you would say that it is x vector times dot product with the w vector. And if this notation is not familiar to you, just pick up a basic high school book on vectors. Just pick up up to dot products. That should be enough for you to get the hang of. Where a vector is, what is a vector? This vector, x vector, is essentially, and it is often written as, in this language, it's often written as a column vector, x2, and w is equal to w1, w2 as column vectors. And x dot w means pairwise you multiply it. In matrix notation, you could also say that you take the transpose of this, x transpose w in other words you do x1 x2 times w1 w2 and which is also the same as doing the opposite w transpose x is the same thing which would be w1 w2 multiplied by x1 x2 there's a few who are familiar with the matrix notation that's what it would mean right so now you say that all right what did we learn you can say that they use this y is equal to w dot x right plus b right so i may forget to put the vector notation on it but assume that it is there this is it but now suppose the data is not linear. If the data is linear, you're done. You just have to find a best fit model for this. But sometimes reality is not as good. You keep on increasing the temperature, guess what will happen? Once it crosses 110 degrees, the beaches would be empty, isn't it? So what happens is that you would say that your ice cream relationship is, and maybe it starts a downward trajectory. So this is no more a linear relationship. It's not a line that best fits the data. Would you agree? It is. line that best fits the data would you agree it is it is this so you ask this question oh boy what what about this you have computed w dot x plus b here but that is not sufficient to fit to this data what else do you need you need a distorting function a distortion function You need a distorting function, a distortion function, isn't it? Something that will bend the line in a way that it begins to look like this curve. Some distorting function you need. Now, let's call that distorting function. It is often called, well, it has many names. I will just write it as sigma. Don't confuse it with sigmoid, guys, because people often use sigma for sigmoid. Okay, some distortion function is there. This distortion function now acts on w dot x plus b. And you hope that whatever distortion function you have will now fit into this. Now, if you're lucky, and if you pick a good distortion function you have will now fit into this now if you are lucky and if you pick a good distortion function it will fit into this but then comes an interesting question what if you are unlucky right and no matter what you try what distortion function you take to bend the line it doesn't for all you care seem to fit this so what people have done is instead of being very creative in finding these distortion functions they have taken a small set of very simple distortion functions these functions are called activation functions distortion functions. These functions are called activation functions. Activation function, right? And there are two kinds of activation sigmoids the other set of distortion function is almost linear now you say why linear doesn't it defeat the whole purpose because if you just do a linear transform of a point you'll again a straight line may be bent or something like that it will still remain a straight line but the crucial word is almost and we'll see what that means almost linear so the almost linear came later examples are re lu etc those of you who are familiar with the word but i'll just go with sigmoids for the time being. But even before I do these two things, suppose you take a simple function. It brings some non-linearity. So what is the purpose of this distortion function? Distortion is non-linearity, isn't it? You're bending. It's non-linear, right? So the purpose of the activation function, and now I'll use the word more often used in this literature, activation function is to introduce nonlinearity. non-linearity. But suppose you take some simple distortion function and you bend it, you bend the line a little bit. Now the question is, it may not be the bending that you're looking for, isn't it? It might not be what you're looking for. So what do you do now? Well, there is a trick and that's where the big insight comes in what you can do is you can take the same input let's say x1 x2 same input and you can say this particular node one node i let's say node i and this is node two node one and node two yeah you can take two neurons or two nodes and you can compute it has its own w11 w1 w21 and then this can have the same x1 x2 sorry let me just draw it out here the The same x1 goes here also, and it feeds into this also. And this one would be x12, and this would be x, wait here, would be xw22. The second part is the node that it is going to. This is node number two. This is node number one. So these subscripts, I hope hope are not confusing we're just keeping track of which w we are talking about and what is it connecting are we are we together so now what happens is suppose you apply sigmoid here sigmoid here what will happen is this will distort it some way and this will also distort it some way. And then what you hope is these two distortions put together, you get the bend that you want. And what you can do is you can let it, you can fudge around with the weights. What are the things that you can change? You can change the weights and biases, B1, B2, right? Usually the activation function is fixed. You pick something, you stick to it, right? Some you pick something you stick to it right some simple function you stick to it so that simple function will bring about a simple bend distortion but what you do is you you then start turning the knobs on the weights and biases so that to see if you can make those curves wiggle around in such a way that it adapts to the data. Are we together? And then now the thing is, how do we know that it will work? We certainly know that one node is not going to capture it, two, may or may not capture it, three, who knows, greater chance that it will capture the flexible, the bend. But how do we know that if we continue this exercise, it will adapt to it? So there comes a remarkable result in this field. It is called, and we will see this in lab, everything that we are saying, we are going to do the lab and we are going to see it. This is a fundamental result. It is called the universal approximator theorem now it's a theorem which means theorems are always true there is never a v2 version of a theorem right if a theorem so pythagoras theorem there's no v2 v2 product releases of that right so it's a dundee so it's a theorem and so you can it is completely true in other words it's a theorem. And so you can, it is completely true, in other words, and forever true. It says that if you, with certain class of activation functions, sigmoid is an example, and then ReLU is this, good activation functions. They are, the neural networks are universal. What we just created is a neural network, right? Many nodes put wired together is a neural network. Many nodes put wired together is a neural network. Now, they are universal approximators. They will adapt to any relationship, any function. Now think about it, the sheer implication of this statement. See the relationship between input and response, input and output could be highly complicated isn't it so for example if you're trying to predict the weather today based on as you know whether models are based on thousands of parameters right what you're saying is whatever the very complicated relationship there is right a neural network will capture it right or let's know whether it's a different because it's very time sensitive and so forth. Let's take something else. If there is information there, if a function is there, y is equal to some relationship of x, let's say, whatever complicated relationship it is, you take a layer layer of neurons and interesting thing is you need only one layer of neurons to do it the statement is with activation function you just need to take enough number of them and your your input x1 x2 and now i'll generalize it to xn right suppose there are n features here and then all of them i won't draw all the lines. As you can imagine, they are very, very complicated. Each of them is feeding into each of the neurons. And from there, you go to the output. A single layer neural network, and this layer, the one that does the real magic, remember this was the W dot X plus B, let's call it z z is equal to this and the activation of z this set of neurons is called in this world a hidden layer why is it called the hidden layer because we can all see input it is data we can see output it is prediction see input, it is data. We can see output, it is prediction. But what you can't see typically once a model is trained are the hidden neurons and so on and so forth. So you call it the hidden layer, the layer that you can't see or whose action you can't see, whose output you can't see. So it's called the hidden layer of a neural network. Now the universal approximation theorem is very universal. Approximating theorem guarantees that a sufficiently deep, sufficiently wide neural network layer, will approximate any relationship. And if you ponder about it, that's a very remarkable statement. It says that however complicated a function that you draw, and this is just one dimension, when you go to higher dimensions, like their n features, a very complicated, a curve becomes now a surface, isn't it? It becomes a hypersurface. However corrugated or complicated that hyper surface is, the real relationship, this will approximate it. And remember, we are looking for effective approximations. Remember, machine learning is the search for G, an effective approximation to the underlying reality. The underlying reality from our data is unknown, right? reality from a data is unknown right and so it can approximate it we will see this in the lab and therein comes the power of deep of neural networks there is a problem though because this neural network is a very if you just make a single layer with lots of neurons it turns out that from a practical perspective you end up using up a lot of neurons it is far more effective to break it up into a few layers so one layer containing some and then another layer containing some more and then with lots of relationships in between right and what we did is let's look at this suppose you have two layers and i'll make it here. Suppose you have layer one, two, three in the first layer. Let's say two in the second layer. So if you make it like this, layer one, layer two, these are both hidden layers. And then of course, there's the input layer, input Xn, the X vector. And the output is, let's say that this is the let's say that you're doing binary classification or just regression this is your output layer okay so do you notice something interesting every element of a layer and this you can say layer three every element of a layer connects to every element of the next layer. Do you see that none of the edges are missed? This together is people often in graph theory, they call it a bipartite graph and so forth. But it's a fully connected layer. All nodes between two layers are connected. But no two nodes in the same layer are connected. Those are the rules, isn't it? So for example, A is not connected to B, that's not allowed. They are all independent decision makers. But A is connected to A prime and B prime, right? To every. So these things, this is, it turns out that this is one neural network architecture this architecture is called fully connected neural net it's also called feed forward word neural net now let's think what happens input comes in the first layer will compute its partial its own results you know it will compute its own um z's the w dot x's and then it will distort it and it will produce partial results here. They own different results. And those results, it will feed it to the next layer. Does that intuition make sense? Right? And the next layer will not know that it is getting something from the previous layer. To the next layer, it is the input. Isn't it? So this layer does not know what it is getting is not x 1 x 2 x 3 it need not know all it needs is what does it do for the input that it receives right and the input it receives is the output of the previous layer and it will act upon that and it will distort it a little bit further so when you layer it it turns out that it is more efficient and so this is your feed forward network you may choose to have only one layer you can choose to have multiple hidden layers but you must have at least one hidden layer right so this architecture is feed forward now comes an interesting fact and now we will step back we have all the necessary ingredients to understand deep neural networks right what is deep about deep neural networks it's a little hard to say but in a very intuitive way and mostly correct way a deep neural network contains lots of layers right so you say that layer one, layer two, layer three, keep on going, many, many layers long typically. And how many layers depends on who you talk to. Somebody will say five layers is already deep. Somebody will say, no, no, no, hundreds. And there are architectures that have hundreds of layers or thousands of layers, huge architecture. Now comes another fact. What I just showed you is actually one way of wiring the neurons together. As you can imagine, given these neurons, you could wire them completely differently, isn't it? And how you wire the neurons to make this big picture, neurons to make this big picture that each particular way of wiring it together there's a lot of theory behind it and why you should do it why you should not do it and so on and so forth and they lead to beautiful effects completely different effects those are called neural network architectures right now in this particular workshop, the many neural network architectures and every day new ones are springing. It's every architecture that comes out typically is considered a revolution. It has a lot of implication on this field, and this field is very, very fast moving, very fast moving. So I'll give you the name of some architectures. One, of course, we did the feed and then we'll take a break. Feed forward neural network that you just saw, right? It is also called fully connected. Then the second way is something called convolutional neural net. What are these? They are used for processing image data predominantly, but can be used for other purposes also for example it can be used for text processing and other things also one window convolutional neural nets but basically this is inspired by the the vision of a cat cat's brain the third neural network architecture are called recurrent neural nets fourth we have auto encoders what are auto encoders they they create summarization of data to a lower dimensional space like suppose you know if somebody has been imagine it like this suppose data comes to you as a vector, a three-dimensional vector, right? Now, it is about the wind speed, but it turns out that the wind is not going up. It's not going there. The only thing it's going is, in this particular neighborhood, wind only goes from west to east, always. So what can you do about the other axes? You just need one axis, isn't it? You can just forget about the other two axes. Given X, Y, Z, you can keep just the X axis. Forget about the Y and the Z axis. What you just did is from the information, you realize what is the essential part that you need to keep. That's a very trivial example of reducing data to or summarizing data representation. Coming up with a summary representation in a lower dimensional space. But autoencoders are a little bit fancier, but we'll learn about it in this course. The fifth thing, which is the bane of modern world, and the thing that tends to topple or play mischief with elections, are called generative adversarial network. Generative adversarial network are interesting architectures, and I'll give you a one-minute summary of it. There is a cop it there is a cop and there is a counterfeiter so the relation and i'll just the relationship is very interesting the counterfeiter imagine is trying to create counterfeit currencies us dollars right now we will but the counterfeiter has never seen a US dollar. And there is a cop whose job is to tell is this counterfeit or not. So first, the cop is served either a real dollar, so cops also need to be trained. So how do you train the cop? You show the cop a real dollar and make sure that he says it's real. And then you show the cop a fake dollar, you make sure that he says it's real. And then you show the cop a fake dollar, you just have squiggly marks on a page. And you say, What is this? And he should say it's fake. Right? So in the beginning, what will happen is the counterfeiter will lose hands down because counterfeiter has no idea what a dollar bill looks like, isn't it? So for all you know know you'll make a cat's picture onto a page and then try to pass it off as a dollar bill but the cop will say no this is fake now you feed back that error right a vote to the cop and to the counterfeiter you congratulate the cop you got it right and then you tell the counterfeiter that no that didn't work now what the counterfeiter does is changes the drawing a little bit. Right. And again, tries to pass it off. And the cop also doesn't know what is what he's getting counterfeit or fake, because randomly he's either given a counterfeit or fake a real dollar bill. And he has to. So he will also occasionally make mistakes. Now what happens is gradually over millions of cycles as you run through it, the counterfeiter gradually figures out how to fool the cop. So he starts making drawings, surprisingly, that look amazingly like the real dollar bill, without ever seeing a real dollar bill. You see how interesting it is. And then comes a situation where COP begins to find it harder and harder to distinguish between the real and the counterfeit. So there is an adversarial relationship between the counterfeiter and the cop, isn't it? So this kind of a neural architecture, you have one neural architecture, which is the counterfeiter. You have another neural network, which is the copfeiter. You have another neural network, which is the cop, and the two are playing an adversarial game, and they both get trained. So cop becomes a better cop, and the counterfeiter becomes a better counterfeiter. Right? And this is called generative adversarial network. Why the generative? Because now the counterfeiter can generate dollar bills. Fake dollar bills, but how would you know? Isn't it? So adversarial, of course, the relationship is adversarial between the two networks. So those are generative adversarial networks. Let me write it down now in this workshop i will focus a lot on two more architectures which are quite central and that have sort of emerged just in the last three four. One of them is transformers. Now, when you think transformers, don't think of that movie Transformers. It's a little different, but let's just say that it has had as big an effect as that destructive transformer had over the landscape. It's a huge bit of mischief. It's created. Transformers are based on a concept called self attention. So when they showed up, I believe the first paper provocatively and famously called attention is all you need. Right? It is it is one of the most celebrated research papers in this field and we will go over that research paper literally line by line in this workshop at some point but when it came it was published in december 2017 by 2018 early 2018 everybody knew that the world has changed. Literally knew that the world has changed. And then came all sorts of transformers. There is the BERT. There is the then BERT derivative. BERT has become a whole BERTology. Literally, there are all sorts of BERTs. There is Roberta, there is Excel BERT, there is this B. There is this bird that word and Bart. So there's a lot of things that have come about. Then another kind of transformers which are auto-regressive are the like. The examples are famously Gpt. You may have heard the word Gpt. generation of GPT is so uncannily smart that actually even the second generation of GPT was so smart that the people who made it OpenAI initiative, they actually were very hesitant letting people know what the architecture of GPT-2 was because it could write poetry, solve equations, do arithmetic, God knows what. And then came GPT-3, which is one step further today. And this is a reality, guys, to give you a measure of how smart these things are. And if you haven't seen that, try out the code pilot. Right copilot. And there are many, many tools that are emerging in which in your visual like VS code or in your favorite editor. So Microsoft has one coming out GitHub has another coming out and so forth. You just write the comment. Right? You say up, up, creates this function takes this and produces that. And guess guess what for all the standard languages like javascript java etc it immediately writes perfect beautiful code right you so much as describe what the ui should be like and guess what this thing creates a beautiful react based ui with all the widgets placed, right? And you did not say that put this widget to the left or to the right or something. You just say, I need a form to take in a user's profile or something like that, capture the user profile. Profile has these fields. The moment you do that, UI comes up and code gets written. When you actually see these things in action, you realize how far AI has come. And you wake up to the fact that AI genuinely is eating the world for lunch. I don't think programmers as we know it, this vast holds of programmers will be doing what they are doing today, because much of that tedious programming will be taken over by AI. So it is again, and I'll just pause it, that it is great that you're learning AI, because after a little while, AI is pretty much will be the dominant thing around. The traditional programming that people have been doing will get more and more marginalized, more and more trivialized because AI systems will be able to do that. GPT-3 is already a year or so old. GPT-4, we don't know when it comes out, but whenever it comes out, just imagine how powerful it will be. Now, these models, now those of you who have taken machine learning, ML 100 and 200, for example, in the equation of a line, we had two parameters, intercept and slope. In the case of a surface, like a plane, there'll be more parameters. But these models have billions of parameters. Not even millions, there are billions of parameters. Can you imagine doing a gradient descent in a multi-billion dollar dimensional space, parameter space? And the very fact that we can even do it, or that we even dare to think you can solve a problem in such very, very high dimensional space is just utterly revolutionary just completely revolutionary what it means is that this model which is so highly adaptive and flexible because the more the parameters the more knobs you can turn right the more you know distortions you can bring and adapt to whatever decision surface or whatever your surface is, right? Then the fact that you can use it to capture just about anything is just absolutely amazing. And that speaks to the tremendous power of neural networks. So we'll do that. Now, there is one more thing that is completely changing the world, graph neural networks. So we'll do that. Now there is one more thing that is completely changing the world, graph neural networks, or I should say graph neural networks, more generally geometric neural networks. It turns out that many things that we were doing, for example in computer vision using convolutional neural net when you reformulate the problem as a graph you get much better results right so nowadays what is happening is people are looking at transformers and geometric neural networks and the most common of them is the graph neural networks and they're reapplying it to traditional problems the problems they were using other architectures to solve and what they are finding is literally off the bat they lead to state-of-the-art performance right so like i talked about gpt i talked about bird etc they are all transformers they are all transformer architectures and now graph neural networks are emerging on the scene so in this course we are going to focus a lot on transformers and we'll i'll introduce it early on one of the earlier introductions to it right so so you you get a sense of why neural networks are so hot these days. They seem to be able to do just about everything, including slicing bread for all we know, right? They're really smart, to the extent that people have gotten carried away and they feel that with neural networks there, there is no need for any other machine learning, right? uh that's obviously not true because it turns out that just as there's a universal approximated theorem that says neural networks will work there is yet another theorem in this field called the no free lunch theorem it's genuinely a theorem called no free lunch it has a frivolous name but it is actually one of the most profound results in in the in computer science and contribution to the theory of human knowledge. What it says is no one algorithm will, on average, outperform other algorithm for all sorts of data sets. So ultimately, for each data set, some algorithm will work best. For other data set, some other algorithm will work best. So remember that no free lunch theorem. So remember that no freelance theorem. It's a very important theorem. So you shouldn't say that, okay, forget about the rest of machine learning. I'll only do deep neural networks. Having said that, deep neural networks are very, very hot, and they are solving a large class of problems that couldn't be solved and surprising results. For example, your ophthalmologist would look into your eye annually at your retina and see, for example, it's called the optoderm, the picture of that, to see if there is any nerve damage or if there's any blood vessel damage and so forth. Somebody trained that, those images, lots of those images to predict whether you have diabetes whether you have heart disease and guess what it is all written there and neural networks could find it right and the interesting thing is those signals are so subtle that we can't see it right but these neural networks can see that so people often say when often say, when will the artificial intelligence outperform us? The fact is, there is no general purpose artificial intelligence. We are general purpose intelligence machines. We can drive a car and write poetry. But the AI models, they are good for one task but usually at that task they beat us they generally beat us at it right and so with that with that introduction and this is sort of an introduction of this field i will let's take a break and then regroup and now we'll go one level deeper into the into slightly technical things now before i break I break, I must also say, we will meet on Wednesday, but I believe next Monday is a break. There's some holiday next Monday. Or the Monday after that. Memorial Day. When is that? It's not the coming Monday, but the Monday after that. Okay, yeah, sure. So that's good. So we'll be continuing as normal. The way the course is structured, Mondays we'll do theory, Wednesdays we'll do lab. So we'll do theory, lab, theory, lab. Now, the nature of this field is that theory will lag lab. So in the lab, I'll get you introduced to things that we haven't caught up in theory and theory will take some time to catch up. in theory and theory will take some time to catch up right but we need to do it like that labs will like you'll you'll be using techniques or things that you haven't yet learned right so be mentally prepared for it now there are certain textbooks that is there on the course portal get those textbooks the framework we will use is py torchorch. Programming framework we'll use is PyTorch. The language we'll use is Python. For deep learning, it's pretty much the lingua franca. Everyone uses Python more or less. You can do it in other languages, but you're asking for trouble basically at this moment. So the alternative to Python in PyTorch offered, there's another framework called TensorFlow. I will introduce TensorFlow also and give you solutions. But whether you do your labs in TensorFlow or not is optional. PyTorch is necessary. This particular workshop has four legs to it. Five legs to it actually. It's a five-legged table. One leg is theory, what we are going through now. I'll explain things, you listen. Then the application of the theory is the lab then to check whether you have learned this we have quizzes i invite you in this break to go and take quiz zero you will see on the course portal the quiz zero please take it immediately it will give you a reality check on how well versed you are in the basics of machine learning. So if you get a perfect zero, let me know. But if you get anything better than that, you're well on your way. Right? So and don't worry, this workshop and again, speaking to the mechanics of the workshop, see, three four teaching assistants they are very dedicated dennis is there kate is here kyle is remote so you you practically have 24 hour coverage whether it's day or night you can reach out to them all the time over slack and then you can set up one-on-one sessions they'll always help you they've been very very helpful now i i would go so far as to say that they will be far more important to your learning than me because i would be giving the theory and the labs but with them you can have a personal relationship like you can have a much closer explain it to me and this and that and um you would testify to that isn't it right so she has been benefiting from the tears so that's how it is please use that year so that is it uh we'll have quizzes then we have homeworks lab homeworks so there'll be guided labs in which i'll walk you through the solution i'll explain each line of code and how it works and so forth but then we'll also create homeworks for you to do little bits of code for you to do and that will give you practice because this field is a lot of practices needed with this code and the last leg of the courses which is purely optional is that we do research paper reading like i talked about this attention is all you need research paper. So when are we going to do it? We won't get time in these theory sessions, but I do optional sessions and I'll announce what time it is in which we cover those influential research papers in this field. There are many influential research papers. For example, in computer vision, when transformers came in, right? There was the so-called visual transformers came in, but there is a paper for that, that made the big breakthrough, right? In the same way, when neural networks impacted computer vision and they showed that they can get better results and they impacted natural language processing and they could do many things, there are paper for that. BERT itself, there is a paper for it architect when bird architecture came in so we will cover all the landmark papers but it is optional because paper reading it will give you practice in reading research papers i'll explain every line of that paper and then you'll begin to feel comfortable why is that important this field is very fast moving. So if the only thing you rely on is textbooks, you'll be at least two, three years behind, right? And two, three years in this deep learning space is ages. It's literally ages, right? Things move that fast. Are we together? It really moves that fast. So you need some practice in that. So that's a five-legged journey. Remember, the whole point of this workshop is we are there to help you, right? We're there to help in the learning process. So stay engaged. All you need to do is stay engaged. You know, there's a saying that says that half of success is just showing up, right? So that's what it is. Just be engaged with the workshop and I think you'll do well. If you don't know Python at all or you need some refresh in Python, reach out to the teaching assistants. We can do some extra sessions. We'll walk you through and get you started. If you have difficulty setting up your environment, again, reach out to us, we'll help you. Before you come next time uh please uh and in this break we are going to take about a 20 minute break please go and take quiz zero now these quizzes are for you you can keep taking it there will be 20 quizzes in this workshop there are eight sessions there there'll be at least 20 more likely 24 or 25 quizzes. Each of these quizzes you'll find are, I would like to believe, very carefully thought through. I've made those questions very carefully. And you learn a lot. The previous batches have said that they have benefited hugely from the quizzes. Especially, it makes them interview very well when they go and interview with the top-tier companies. In fact, I have not heard any one of the people who have taken the quizzes come back and say, oh, you know, I found a Google machine learning interview hard. In fact, they found it pretty easy. Any one of these interviews easy. They always come back and say that was a good preparation that really helped prepare for all of that. So from that perspective, drill, it's a good drill. And remember, you can keep taking the quiz over and over again. So it just tests your current state of knowledge. A few weeks later, come back and take the quiz again and see how well you do. You'll be surprised that you will go up in your score. And three, four weeks later, you'll come down. Right. And you'll get a reality check and you'll have to go back and review. Right. So that is the format of the workshop guys. And so reach out to us stay engaged. There's a slack channel, engage with it actively. Right. And that's it. And despite the fact that there is a no freelance theorem, there is no sorry, could you say that again? Oh, i'm having trouble hearing you this is my watch all right so um there is fortunately there is no no snack theorem so let's go snack i'm hungry at least and let's regroup in 20 minutes but please do take that quiz please log on to the course portal and take the quiz quiz 0. course portal and take the quiz quiz zero uh i'll say one question that's the buy so n plus one you're right because the bias term is there is getting n plus one inputs so so by default it takes only uh the linear thing no the input is coming directly to it no then the first layer is going to do apply the distortion is going to activate after it has applied the weights right so remember the processes it's like this so imagine that it is let me simplify down to a line suppose there is a single input x the temperature on the beach right what is this first neuron doing it is doing w dot x plus b z is equal to right so z is equal to w dot x plus bias right then it is activating it the output here is sigmoid of z do you see that right so this is your output from the first neuron now suppose it's going to the second neuron then that is it now if you include wind speed also then we need to be cleverer w1 plus w2 x2 right and then here is wind speed x2 coming in so now there are two neurons so how many parameters there are there is a weight w1 w2 plus there is the bias term okay that was the second second layer it is uh whatever so second layer so let's look at this this example second layer in this second layer every node is receiving how many inputs is receiving three inputs right even though the first layer may be receiving let's say let's say that this is 30 dimensions 30 features the first layer every node is reach receiving 30 inputs but the second layer in the second layer each of the node is receiving only three inputs plus the bias term of course isn't it you see that yeah only three three nodes here that are feed feeding forward into that node into the nodes of the second layer that's why the word feed forward is used in this so uh if neurons equal number of neurons are taken then two layers give better result than one layer see um that is an interesting question the more complicated so that brings us to a topic which we will cover in detail the bias way instead of depends on the underlying reality see you don't know what the underlying reality is when you don't know you can start with a reasonable number of layers and either you can decrease or increase it and in fact next time the first exercise we will do actually why next time um we may do it right after the break you'll see how we solve a problem and decide how many layers how many nodes per layer make sense the answer is it depends on the data data decides everything all right just because you add more layers doesn't mean that you'll solve the problem better. In fact, you may get into the problem of overfitting. Right. And we'll talk about that. Does that answer your question? Yeah. Yeah. So the basic answer is it depends and data rules, you have to experiment, that is why you say that the number of layers and the number of nodes in each layer is a hyper parameter of the model means something else has to help you understand that. Alright, just take a break and we'll catch up in a few minutes. So all right, let's take a break and we'll catch up in a few moments. All right folks, today we are going to play around with something. It's called the playground of tensorflow. Here we will build all sorts of networks to solve certain classification problems. Classification means we have to tell the orange dots from the blue dots, right? So for example, let's take this one. There's a cluster of orange dots and there's a cluster of blue dots. What we will do is I'm going to remove all of these neurons for the time being and reduce the number of layers and ask can you solve this problem separating your problem is to draw a line or a curve that separates the blue dots from the orange dots and let's see what happens the inputs are x1 and x2. In other words, the x1 axis and x2. In this world, y is reserved for target variables, what you are trying to predict. And all the features are called x1, x2, x3. In other words, a feature vector. So x1, what you think of as y-axis is x2. And the game is, what is the curve you can draw that best or optimally separates the orange from the blue ones? So this is the simplest network you can create, just one neuron. Let's see if it works. This now, when you train it, and you will learn about things. I just talked about activation function. You can have a whole, and we'll have a whole session on activation functions in a bit. But the choices are ReLU, DynEdge, Sigmoid, Linear. Many activation functions exist. This is not the exhaustive list, but these are good ones, representative ones to pick. We will pick one. Let's say we'll pick tarnage for what it is. Regularization is something, if you have taken ML100 with me, machine learning, then you would know what regularization is. But let's just say that regularization is a way to prevent a machine from learning from the data too enthusiastically like overfitting to the data reading more into it than you should or another way to put it more precisely is how do you ensure that the machine learns the signal but not the noise right it doesn't start seeing patterns that don't exist that are just wrong. That process is called regularization for historic reasons. And so you can pick some regularization, it doesn't matter. Learning rate is important in gradient descent. It's how fast you learn. Generally the smaller the steps that you take, the better. What it is we're going to do shortly. Today, time permitting, we'll do gradient descent in a little bit. So this is the learning rate. Generally, the rule is the slower you learn, the better. But what happens is in the beginning, you may be very far from the solution. So you tend to learn fast in the beginning, and then you slow down as you come nearer to the solution. So you tend to learn fast in the beginning and then you slow down as you come to this nearer to the solution because the last thing you want to do is hop beyond the valley. So imagine that you have a very small valley and then you hop literally past it. You don't want to do that. So learning rate is an important thing but we'll take some default value. is an important thing but we'll take some default value this is a classification problem classification means tell them apart right tell them apart is a very common word for that so now we are going to epoch means how many times you wanted to run through the data one complete journey learning from each piece of the data once is called an epoch When you have run through each piece of the data, you have completed one epoch. And then you start all over again and you run through many epochs. So let's see how well it does. Let's see. Here we go. Do you notice that it immediately learned do you think it's making a good decision boundary the region size blue so it's a power of it now why did it do that those of you who remember the logistic regression classifier what you're looking at is literally the logistic regression classifier, what you're looking at is literally the logistic regression classifier. Tanh is very similar to sigmoid. We can do it with the sigmoid and instantly it will do that. Sigmoid learns a little bit slowly compared to tanh. In the neural network world, it's much more common to use tanh as a function. It doesn't matter. Logistic function. Sigmoid is actually abusive term. It's a logistic regression function. Sigmoid is actually abusive term. It's a logistic regression function. You could use another one which will go even faster instantly. It'll pick up, reloop. So these are various activation functions. But do you notice that irrespective of this, they all tend to learn, right? And you can compare it with test data. It still tends to learn right and you can compare it with test data it still tends to run the loss do you notice that the test loss and training loss for how many mistakes it's making zero mistakes let's try something complicated we will try this guy now intuitively you see that dots are around a circle you can imagine a circle here that separates the blue dots from the orange dots isn't it let's see if just one neuron can do it what do you think will it be able to do it it will probably not be able to bend it enough It will probably not be able to bend it enough. And what happens? It fails. Right? So now what do you do? Well, let's go add another neuron and try it again. And first of all, let's take a distortion function. I'll just take the N-H for the fun of it. Let's see. Any better? Well, not quite. Let's try one more. Let's add more distortions and see. Three did the trick, isn't it? Where is the weight? The weight says, weights are not visible here it is the how much or how sensitive is this see this is w dot x the weights are associated with these edges but we are not changing weights right like that no no that is the learning part it is internally learning and it's not showing it here it doesn't show what the weights are right so i wish it did but it doesn't show but in the lab that we will do we will see the weights also next week yeah so now tell me does it make sense to add more more layers for example what if i add one more layer and reduce it by one? Is it necessary? Well, what do you notice? It actually is a little counterproductive. It takes longer to learn. Did it take longer to learn, guys? Right? So this is the nature of the beast, that you can add more layers the question is what is necessary and how much is enough right when the loss has already minimized you stop this hopefully answers this question one of you were asking how do you decide how many layers to put now let's go back to one more thing i said that there's a universal approximator theorem that says one layer is enough for every situation. Let's see if that is true. I'll get rid of the second layer and hide the second layer entirely and now let's see. Three is enough, isn't it? It does seem to hold up the universal approximator now suppose we take more complicated this one what about this guys let's try three what see you fairly good right will two do no two is not two is doing very poorly. Then three does well. Oh, sorry. Why did I add? I ended up adding. I wanted to add a neuron here. Pretty good, isn't it? But you notice that in your mind, whatever hypothesis you have, the way it should look, it tends to get a little bit better. This in your mind is probably a little bit more right or more fitting to your intuition, but it tends to do that. Now, I could have done four layers here, four neurons in a single layer, or what could I have done? I could have done four layers here, four neurons in a single layer, or what could I have done? I could have done less neurons per layer, but tried more layers. And let's try this. And you can see that you can play around with it and do it. After a little while, you notice that there's instability that comes in. And this thing you find when you make your model a bit more complicated than it should be quite often. So this doesn't quite deal with it. And in fact, look at this. If I remove this layer altogether, it is actually better. So remember, more is not better. In fact, in science, we have a principle called the Occam's razor principle. We say that theory we consider to be the right one because there is no absolute truth as the effective or the right model which is the simplest explanation of reality so always try to create the simplest neural network that will get things done now let's try something more complicated you say well that how about this this should be really complicated wouldn't you agree? Yeah. Right. So let's see. Will one neuron do? No chance. What about two neurons? Will this do? Not quite. This is not doing any better. What about three neurons? That didn't do either. So let's go on adding. Oops, sorry, not layers. First let's verify the universal approximator theorem. Eight neurons is maximum is letting us do. And it's learning a bit slowly. It will take time. Just have patience. Do you see that it's beginning to fit now? patients do you see that is beginning to fit now still learning yes the loss is still going down the error rates you see this loss here going down but because it doesn't let you add more neurons in this we can't do but you have to trust me that if you added far more neurons it will get better and better but it has already improved quite a bit isn't it if you go to 32 or 16 it will it will match it but what would be a more effective way of solving it let's add another layer and now let's try this. Four. I've just added five neurons. And maybe you don't need so many here. Let's try this. So do you see the learning take place? The errors are falling. The way it's coloring the page, orange versus blue, is different. Now, it will eventually catch up. At some point it will break and then add more. But let me just add a few more layers this two layers now what happens and i'll change the activation function to redo it's a little bit faster for reasons so do you see how it starts from a nearly hopeless situation and begins to improve upon that? Isn't it? Now, as you can see, it isn't still far from learning that the error rate is still pretty high. It's about decreasing. It's about 25% at this moment. Let's see how far it goes. Who would like to bet whether it would learn or not? It will capture it or not. And a little bit more. So the error rate is about 20% at this moment. But you notice something. When the training loss and the test loss, they begin to diverge quite a bit, you realize that it is not a good sign. After that, things don't look too good for it. So let's try more. Let's add one more hidden layer. And so plus. Now let's try a look do you see how quickly it picks up yeah right so what could have happened is we could have made a single layer very, very sort of a wide layer or we can it's more efficient to have many layers. With a smaller number of nodes and it quickly picks it up right now there's a whole theory behind why this happens will gradually learn it so that we are going to do this course is we're going to peel the onion. First, the big picture, then gradually we'll move to more detail. We'll drill down into the details. Now, this is true for classification. You could do instead a regression problem. Let's see, this is regression. How is this regression? Which dataset do you want to use? How is this regression? I data set do you want to use? How is this regression? I don't understand, but it will... Now I don't know. This looks a bit odd. Okay, it's trying to create a prediction model based on, again, the blue dots and the red dots. These are values. These are continuous values. But anyway, let's leave that aside. So this is it. You would imagine. Now, why is this spiral an interesting case? If you think about any of the linear classifiers that you have learned, it would be a very hard problem for it to solve. This sort of problem can only be solved by nonlinear problems, nonlinear solvers, nonlinear algorithms or models. Nonlinearity in neural network comes from what gives it nonlinearity? The activation function. So I will start the discussion now with an act what are activation functions. Anupam Sharma, Ph.D.: That will be the topic for today, and I invite you, I will introduce only two three activation functions, but I invite you as a homework to go discover all sorts of activation functions that are used. Anupam Sharma, Ph.D.: Alright, so. Anupam Sharma, Ph.D.: Let me now go back to the writing mode. Once again, the point of activation function is to introduce the non-linearity. So the bending, the ability to bend the line so that it can adapt to the data. The intuition is as simple as that right now let us say that okay so there are two kinds of activation functions i would say nearly linear or sigmoid. What is sigmoid? Now, let's pay attention to sigmoid. And this term is, by the way, abused. Quite often people, when they say sigmoid function, especially in the deep neural network literature, they mean the logistic function that you have introduced to. But you shouldn't do that. Sigmoid is more general than that. So write an S. This looks like an S. Now stretch it in this direction and stretch this in this direction so that it begins to look... Is this better? Right? So a function that looks like this so which goes let me make it big bold which goes like this so that the area the where you see change pretty much looks like a stretched out s. Would you call this a stretched out s? All such functions are called sigmoids. It turns out that sigmoids are all over. It is, again, a function. It's a transcendental function. It means you can't represent it, these sort of shapes, using just algebra. It's not a polynomial. Things that can't be represented as polynomials are called transcendental functions. The early transcendentals that you encounter are sine, cosine, tan, right? Because if you expand sine in terms of polynomials, it's an infinite series. So sigmoids are like that they're transcendental functions now the interesting thing about transcendental function says if you just as we have english as the alphabet letters of english in the english alphabet a to z it turns out and this is sort of a subjective way a poetic way of looking at it if you look at the language in which much of the universe is written, nature is written, you will see that it is written in the language of transcendental functions. Literally they're everywhere. You throw a pebble in the water, there'll be ripples, right, and so forth. The hanging of a cable is that. And so this sigmoid is another transcendental function that is pretty ubiquitous. It's there in neuroscience. It's there in electronics. It's there in computer sciences. Everywhere. What are the qualities of this? The good qualities of this are following. That's what makes it special. One, it's a continuous function. Would you agree that there is no break? let me just put the x y axis here let's say this is it and the x axis you can put here or in the middle doesn't matter x axis y roughly speaking right would you all agree that it's a continuous function right there's no what does continuous function mean there are no breaks to it the second thing we have is that and this is where so continuous function is a necessity if you don't have continuous functions we don't want to use it as activation right because we don't want a function that is quiet about certain values it says i don't know about certain values it says i don't know it must be smooth the word people use is it's a differentiable function differentiable means to the first basic intuition is a smooth function right so would you say that it is differentiable everywhere? Would you say that it is smooth everywhere? Right, rate of changes can be told at any given moment. You can find the slope at any given moment. The third quality is, that is very good about it is, it is monotonic, monotonically increasing monotonically increasing means it shows only one type of change it increases it doesn't do like this is not monotonic why because it goes up it comes down it goes up, it comes down, it goes up, it comes down, and so forth. But something that just rises or just falls, they are monotonic functions. In this particular case, it's a monotonically increasing function. Are we together? Now, another quality that it has is, this particular sigmoids have, is that they have two asymptotes two it's our saturation points saturation values what it means if you go to X more and more negative x, suppose this is 0 and this is 1, at x minus 1000, let's say, what value of y is there? Close to 0, isn't it? So you can't go below 0. You gradually reach down to 0 or nearly 0. So you say that this is the lower asymptote. And this is the upper asymptote. These are saturation points, right? This is it. And it turns out that at one point, people thought that in neural networks, that's a good thing. Today we are not so sure. We actually prefer activation functions that don't have saturation points, right, at least in one direction. So that's that, but this is it. Now, how many functions have this property? It turns out that there are many functions functions the most common is the one that you encountered the logistic function logistic function what does the logistic function look like it looks a little scary it is y of x is e to the minus x right now if you have taken machine learning courses with me before you would say ah this is of course a familiar beast right it's not a scary monster at all right but if you're not then you say what is this really so complicated right and you can convince yourself i'll let you convince yourself at x is equal to minus infinity what happens to this term minus of minus infinity is plus infinity e to the plus infinity is infinity and so what what does one over infinity become zero which is this point. It goes to 0. What happens when x goes to plus infinity? e to the minus infinity is 0. And so the denominator goes to, well, it goes to e to the 0. Yeah. So this term goes away, and then this this remains and it goes to one right so it's suffices for this purpose this term now there are other ones there is tan h tan these two are the most commonly used functions tan h is and now i hope i don't screw it up e to the x oh gosh i will so you guys correct me please look up the tanh e to the minus x e to the x i i believe it's minus at the top plus e to the minus x something like this can somebody please verify if i'm making a mistake off the top of my head it's right okay so this is tanh it turns out that you can convince yourself this will go from minus one to plus one so it has a slightly bigger curve or bend, a region of bend, which is why in neural networks, TANH, there's almost never a reason to use sigmoid. I mean, sorry, the logistic function, you should always go with TANH. Whenever you feel tempted to use the logistic, use TANH as the activation function, except as a last layer of the neural network sometimes for binary classification it's a logistic unit as you learned in the previous classes because it has one side uh the tanh has a wider spread from minus one to plus one instead of zero to one right so it gives you more steepness more more area of change a region of change. So which is why, and if you go around, go to the playground and try it out. Sigmoid, see there's an abusive term. They use sigmoid, but they mean logistic. In neural network literature, they often confuse the two. When you pick the sigmoid, which is really logistic, it learns slowly, with TANH it learns faster. Yes. For that reason. Now, these are the qualities of it. So there are many such functions, right? In fact, even bell curve, the integral of the bell curve is the error function. That too is a sigmoid. It also behaves like this, but I'll let you figure out more sigmoid functions. So could you guys all please take it as a homework today? Discover more sigmoid functions, post it to Slack. See how many sigmoid functions you folks can discover. Then comes the nearly linear, what I call nearly linear. By the way, this is my term. You won't find the word used in the textbook. What they are, and they're quite surprising actually. So suppose this is the x-axis and this is the y-axis. The simplest of them is called rectified linear unit. This is a 45 degree line that is y is equal to x if x is greater than 0 and it is equal to 0 if x is less than 0. Right. So what it means is it looks like this. Now what happens at this point? Is it continuous? Yes. Is it differentiable? Is it smooth?, is it differentiable is it smooth. No, there is a wrinkle here. isn't it and that's not allowed, so that is one limitation people thought well, you know what this is not good it's not a differentiable function generally mathematicians love smooth. Smooth continuous functions, so it took a while. And so this ReLU was actually discovered much later and it solved many problems. So there's a whole problem of vanishing gradients and so on and so forth. We'll get into that at some point. But ReLUs are relatively recent additions in the last couple of decades. And then these are called relo re lo what does it stand for rectified linear unit now linear is this part is fine what about the rectified rectified comes from the word from electronics and so forth see a diode takes current in only one direction. So when you pass a current through a sinusoidal current AC current through a diode, what happens? Only the top half, the positive half remains, negative half flattens out. You say that you have rectified it, right? There's no negative voltage anymore, right? So this also has gotten rectified. There is no negative value anymore. Only zero or positive isn't it that's why it's called the rectified linear unit it turns out to be surprisingly effective now one of the problems it has is that you notice that for large values also it keeps changing right it doesn't saturate it turned out uh surprisingly that not saturating is actually a good thing and the universal approximation theorem applies to this also right to all of this now people have come up with from relo they have gone to leaky reload so a lot of people were saying you know i don't like the fact that there is a there is a non-differentiability here so what about this we will take this function bend it have a smooth bend here right so that is leaky relu leaky yeah and then there is more there's there is c-lue and there is more and more and i invite you to discover all the activation functions that is important today guys and so we will learn today on your own discover all activation functions guys this is these are the building blocks of this subject. Activation function, it may look today that, oh, you know, all right, there is the sigmoids and then there is the relu and so forth, the big deal. But actually it was a big deal when relu were discovered and found to be working for the situations and solved a slew of problems. And people have really, there's a cottage industry. People keep trying to come up with better and better activation functions. But generally, those functions are not horrendously complicated. Generally, they're simple functions. Because you get to adapt to very curved surfaces or curved, I mean bent, complicated shapes by using lots of neurons, not by using complicated activation functions. So that is about activation function. So far so good, guys? And now I will talk about, I'll end today by talking about gradient descent, tumbling down the hill. I'll give you guys the intuition for it we won't do a full-blown mathematics so whatever i say is mathematically correct but i'll deliberately use sloppy language you know everyday language right so imagine that you're in the in this uh hills that are behind that are behind the our next to our valley here on east bay hills or first let's start with this suppose you have a function like this and you randomly you you're supposed to find the minima of this function, right? So this is some function fx. You have to find y is equal to fx. You have to find the minima of this. So you can say, hey, just take the derivative and set it to zero. That's how you do optimization of functions, isn't it? Right? Except that you don't know the function. The whole point is you're trying to discover this function. You don't know it at this particular moment. You don't know it. So what you do, well, not discover it. This is the loss function. So this is the error function, loss function. But forget about the loss function. Take any function. Assume that you can't just take the derivative and set it to zero. Now, what can you do? Assume that you can't just take the derivative and set it to zero. Now, what can you do? You could locally take the derivative because locally you know the structure of the function, what the slope is. Because every thing locally, if you look, it looks smooth. So to it, look at the globe. We all live on the earth. For the longest time, people believed that we live on a flat earth. Why did they believe? Because we are tiny little beings, and in our neighborhood, the earth is flat, isn't it? But globally, it is not flat. So that is the nature of it. Any complicated surface, so long as it is smooth and continuous, nature of it any complicated surface so long as it is smooth and different smooth and continuous locally it will look you can draw the tangents or the tangent surfaces you can find out the slope but in one dimension at any given point let me just call this point a and this is a prime i have the slope right what is slope considered i'll explain what the word slope means and i'll take another point b and i have a slope here slope geometr explain what the word slope means. And I'll take another point B, and I have a slope here. Slope geometrically is the tangent to the curve. And you call it the derivative. Slope and derivative are the same. What derivative stands for, slope stands for, is if I make a unit step in the positive direction, a tiny step in the positive direction, direction a tiny step in the positive direction how much will the y change right the ratio of the two is called the slope right so suppose i make a very small change from here to here delta x a then did did y increase or decrease decrease because i went from here to here i fell down so the change in y is negative right delta y is smaller than zero isn't it the final y minus initial y is negative right on the other hand so we will say that slope is slope. Slope here is is positive or negative. Negative. That's what it means if we take a tiny step forward along the X axis. Did you raise or fall? If you fell along that curve, the slope is negative. If you rose, slope is positive. And the ratio, the delta y, how much you rose or fell for every small step of x, is essentially the definition. It is called the derivative or the slope. It is also the slope geometrically. Now, at this point, at b, if you take a little step forward, delta XB, what happens? Do you rise up or do you fall? Rise. So here, delta Y is greater than 0. Does this make sense? So slope is positive. Now, suppose your goal is to find the minima, the point on the x-axis where this function achieves its minima, right? So you could have started with any point. Randomly, you could have started from a or you could have started from any point randomly you could have started from a or you could have started from b you don't know because initially you just start somewhere and you say now what do i do where do i go right so it turns out that you can do this if if you are at a so let's take a if you take a positive step, the slope, if the slope is negative, should you want to make a positive step? So suppose at XA, do you want to move in this direction or in this direction? To the right. Now, let's say that the next position, me call x tilde a how would you based on the previous position of a right it will be the previous position of a now plus now how how do you find that you say that you need to add a positive quantity but if you look at the derivative the slope you notice that now you realize that the steeper it is, the more bigger step you can take. Because for a little bit of a fall, you moved here. Now, this is it. So now what you can do is you say that, well, slope is negative. So if I add it, it will be a disaster because I'll move back. Isn't it? So what happens if I do this? Now am I moving in the right direction? Because slope is negative, and negative of a negative would be positive, and I'm moving in the right direction. Except that what happens is these derivatives can be big numbers, and you need to dampen it down. So what you do is you put a learning, there's an alpha, it's called the learning rate. How big a step are you willing to take? You don't want to be hijacked by the slope, because it may make you take really big step. Right. So you you take tiny step. You let that slope inform you. But you take a tiny step in the direction. This size of the step that that controlling is called the learning rate right and it's typically 0.001 or something like that small amount you take one percent of it or one thousandth of it or whatever it is of the slope now now let's go at point b at xb you don't want to move in the positive direction where do you want to move you want to move in the negative direction right so you want to subtract something from it you have this but how do you subtract your slope is positive you need to put a minus sign here right and also apply a learning rate, dx at b. Are we together? Now, what do you notice? These two equations, do they look different? They're exactly the same, isn't it? Surprisingly, they're exactly the same equation. So now we learn a lesson. We say, irrespective of where you are, you can say that your next step, your next location should be your previous location minus alpha. And now let me make the language a bit more rigorous. When the delta y and delta x are infinitely small, they're called infinitesimal steps, means the tiniest little step that the little ant can take, right? Then you use it, you don't use delta symbol, you use the d symbol, dy, sorry, dy, d, not dy, dy, dy, dx. Are we together? So this is called a gradient descent in one dimension. One dimension. Now let's generalize it to more dimensions right if you are not in a curve but if you are in a surface right like this what you would do is your next step component wise x x1 axis x2 axis you can do the same thing x1 in higher dimensions dimensions you can say x1 is the previous value of x1 minus alpha now this function is dy partial derivative with respect to x1 means if if i change along this axis a little bit then how much does the y change because y is a a function of, let's say, x2 also, in the simplest case, x2, right? It's the same thing. Now you're taking little, you said that, okay, if I move a little bit along x1 axis, how much does y change? Well, that gives me a gradient descent part of it. So the next value of this is the x vector and this is the x tilde vector right and so you can say x tilde as a vector is equal to x vector minus alpha now there has to be a symbol for this weird thing uh d y weird thing uh dy uh dy dx1 and dy dx2 what in the world is it's called the nabla or the grad. People, these are the words that people use for this symbol, grad or nabla. Right. So this is your gradient descent equation. Now, this is true for general. But what is the why that we care for in machine learning? Remember, what are we trying to minimize? The error function, the loss surface, the loss function. So all we need to do is say, but the loss function is a function of what? Those knobs, those parameters, those weights in the neural network, isn't it? What the value of the weight should be. So you can say that any given weight, and by the way, weight can belong. So now think of the weight should be so you can say that any given weight and by the way a weight can belong so now think of the neurons this is layer one layer two layer three so given any weight let's take this weight it is coming from a node one two three one two two, one, one. So it is coming from a node in the previous layer to a node in the next layer, right? This is what the weight is. So you say that this is one, one of layer, right? Of layer L of layer two, right? Two. And let's say that you have the input X1, X2 are the inputs and they are feeding into this, right? You treat the input also as so-called the input neurons, basically, right? So you treat them symmetrically. So here, this will be your layer zero, let's say. Actually, you call it layer one. In this language, you you call it layer one. In this language, you can call them layer one. This is layer two, layer two, layer one, layer two. So you can now start writing the weights, right? So at any given weight, you need to know which layer that weight is associated with two neurons, isn't it? It's connecting. It is associated with an edge. Are we together? Every edge has a weight parameter. Make sense? It connects two neurons belonging to two layers, adjacent layers, isn't it? So this would be layer L, layer L plus one, right? This is the, layer L, layer L plus one, right? This is the, or actually, let me say L minus one, better, and L connecting to layer L. So the weight here, because this weight's usually you associate with the target one, this weight is, and now in this layer, it is the ith node, right? In this layer, and this layer, it is the jth node, right? So you say that this is. And this layer, it is the jth node. So you say that this is i j of layer L. This is the notation you use. There's a whole lot of indices and you say, what? But you get used to it after some time. Remember, a weight is associated with an edge. Edge is between two neurons. Look at the target neuron, which layer it is in. That is the L. Which is the originating node? Where is it in that layer? And which is the target node, ij? So that is the proper indexing or the address of a weight. But weight is weight. At the end of the day, you can say that weight, that weight, ij, its next value should be what? Ultimately, it's a weight. It's, derivative of the loss function with respect to WijL, isn't it? Does this make sense, guys? I'm saying that this could be just a parameter. So the way is, if this looks scary, think of it as just beta or theta. Some parameter, just write it as theta. Think of it as one weight or just w. That's what I think. Whenever I'm thinking my way through, I forget the fact that, you know, there are lots and lots of edges and lots and lots of... I'll just write it as w, right? And say w next is w minus alpha. And because now we are talking of vectors, because vector laws with respect to W, which is the same thing written in simple form. Are we together? And this is the celebrated gradient descent equation. equation. This is essentially the learning of machine learning. The learning in machine learning is some variant of this generally in practical terms. I mean, there's more to it. I shouldn't put it, this is oversimplifying the situation but for regression models and classification models etc this this is definitely true this is what you do gradient descent to learn the best predictive model the best parameters right so here is one way to look at it suppose a musician is singing and you who's the guy who you know the mixer have you ever seen a mixer And you who's the guy who, you know the mixer? Have you ever seen a mixer? Sound mixer? How many knobs does it have? Lots and lots of knobs, isn't it? And most of us are like, I have no idea what this does. Right? And sometimes there's professional musicians will have a whole bank of these knobs in front of them and they know exactly what it is. But suppose you didn't know. And here is a musician singing you know he's a good singer you have to produce the best possible sound right they got the best possible sound you know that there is some combination of the knobs that will get the right sound out isn't it because you have to mix it with some other music and something else and whatnot you have to dampen the bass or whatever the musicians do they do they in other words they shape the sound to make it better but to shape the sound you have to turn all the knobs but if you didn't know it what will you have to do you have to turn all of them a little bit in a systematic way to get to the optimal sound isn't it So think of these weights or those knobs and gradient descent as the proper way, the most efficient way to turn the knobs so that quickly you can find what is the best way to configure this mixer for this particular musician. Right? That is a gradient descent. And these parameters are knobs. Think of them as that you have gotten a big machine with lots of knobs and you just have to turn them to fit the data, right? To make good predictions or produce good music, given input sound. That's one way to sort of a metaphorical way of looking at it. So that is gradient descent. Are we together? Now, this is the simplest form of gradient descent. There are many layers of complication I had, for example, and I'll just give you, like peel the sheet a little bit and show you what's underneath it. How much data do you give to this to learn? Do you give it all the data? Do you give it all the data? But when you say all the data these days, you may be the training data, maybe millions. For example, millions of images may go into training this neural network. So will all of it go together or will you take little batches of it? Or will you take just one image at a time to train it? One data at a time to train it one data at a time to train it and there are words for it like for example when you take all of it together it's called batch gradient design the problem with that is to run through the whole batch may take forever so you may see improvement after waiting for an hour or two days so what people do they use mini batches but there are other reasons to use mini batches. When we come to that, I'll talk about it. You take little batches and you learn from the data, small batches of data. The other extreme is you learn from one point at a time. When you learn from one data at a time, because the data contains both signal and noise, and you can't distinguish it. So your learning is rather haphazard it's like you sort of meander your way haphazardly towards the optimal solution you're getting there but you zigzag zigzag your way towards the optimal solution on the other hand if you use batch then yes you have a lot of data to pass through each cycle of learning, but you have to wait a lot. So the best practice is to use mini batch. And also, the size of the batch has a lot of effect in basically leading to better models. It's sort of a regularizer in some form. It does that. Also, the memory of the practical reality is the hardware memory will usually not be able to capture the entire data set right so you have to send it in batches there's a graphic card onto which you run it nvidia cards they typically the memory they're very very expensive right and so they have limited memory you want to squeeze your data through and you can't send big batches of data you have to send small batches of data so those are called mini batches right so that is one complication. The other problem with this is that what happens is in this is when I made a surface like this, see beautifully I made a surface like this, right? These surfaces are called convex. Now, what is convex about this, you take any two points any two points that said, these two points and if you were to connect it sorry. This is not the line let's say if you were to connect these two points any two arbitrary points in B, you notice that the curve all the points in the curve they stay on one side of the line they never intersect the line isn't it on the other hand if i take a surface like this and i take this and i connect it to this point what happens the intersections isn't? The part of the curve between A and B intersects the line segment twice, right? Sometimes it will do it once, like for example, if I connect C and D, they'll intersect once. So in other words, if there are segments which can be intersected by the curve itself, right? You see, this is non-convex. Now, for the longest time in machine learning, people believed that the loss function should be convex. It should be nice round like a bowl, right? Like this. But, and for the, there's a lot of theory about a convex optimizations and so forth. But in neural network we realize that actually that's not a necessity. Neural network loss functions are inherently non convex. In fact, they're beautiful visualizations will go through in the lab of this loss functions, and when you look at it those loss functions it's just absolutely stunning and beautiful of what they look like but the one thing is they're made up of lots of hills and valleys right so the thing is the problem that happens is suppose you have a situation like this right do you want to get caught up here? Suppose you start from here. Is this your best solution? No. So what do you do is think in real time. In real life, what happens is if you drop a marble from the top, it will gradient descent descent to this but will it stop here it will overshoot it right why because it has momentum right the momentum will make it overshoot this and then it will be happily on its way down here so that is one of the things we learn that when you do gradient descent incorporate momentum when you come to a local minima don't just stop there if it is truly a global minima then what will happen you over sheet you come back and finally you will stop there but if it is not a global minima with luck you will shoot past the local little local minimas. Are we together? That is why you add a momentum term to the gradient descent. But this equation is the foundational equation. And now you add things to it. And it creates a whole variety of optimizers. So that is the world of optimizers. At this moment, without going into the theory, because we'll go into the theory at some point, but today I want to end now. The one optimizer that you should use just by default is Adam. And in code, when you do that, it's a pretty popular optimizer. It incorporates the best, many best good ideas in doing the gradient descent right it's called the atom it's a momentum based it has some other good qualities and so on and so forth and we'll use that but optimizer again is a topic just like activation function right we'll deal with that another day i will take last 10 minutes i'll take questions if anybody has questions so the answer that you put over there, right? Why is the same across all the dimensions? You could have different. For reasons of simplicity. But you could have different. You could have except that you would not know how much to put in each of the directions. See what happens is that experience has taught that it doesn't really help you much. We start, but what people do is they evolve alpha in time. In other words, as the learning continues in more and more epochs, they slow down the alpha. And there's all this theory about doing, letting the alpha learn its way through. A lot of things are there. But aren't you dragged by some one-dimension kind of polygons and dimensions? Hold that thought. When I come to lost surfaces, you'll see it in a very visual way. So it's the learning rate, but the whole, there's all this theory about one-shot and this and that will come to that later. Any other questions, guys? If not, then we'll pause the recording here.