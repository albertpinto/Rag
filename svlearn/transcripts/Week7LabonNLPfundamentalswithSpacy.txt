 All right, folks. Welcome back to the NLP workshop. This is session two. This is our first lab session, and so we'll be focusing on the project we talked about. Let me recapitulate what we said the last time about doing the lab. The way we are going to do it is as a project. We will devote every Monday to learning new things, to theory, and we have a lot of interesting theory to cover, but this workshop is predominantly guided labs. What it means is I will be giving you sample code or starter code with a lot of fill in the blanks to get you started. And the end goal is that in four to five weeks, you will be able to create a fully functional project. Now, this sounds daunting. It is not actually because as you will see, I will take you through every step of the way, it will prove quite easy actually to do that. Now what are the phases of a full-fledged application? You do various activities, you create a UI, then you create a REST microservice, the REST application in the backend to handle requests from the UI. Inside the REST application, you will load an AI model, an NLP model and exercise it for inference. That NLP model you'll have to train, which will be the core of this project. You'll train a lot of NLP models for various NLP tasks, such as translation, sentiment analysis, classification and continuation and so on and so forth. So we'll do a lot of NLP tasks, we'll become familiar by the end of this project with most aspects of NLP activity in the workplace. So that is one. Then we'll take it to production. These days, most companies go to production in the cloud. We will use the Google Cloud Platform. No particular reason to choose Google Cloud Platform, except that we need to standardize on one. Those of you who are very familiar with Amazon, you may want to stick to Amazon. That is if your entire team agrees with that or with Microsoft or with Oracle or pick your cloud, whatever you want to do. But for the code that I will release, I would have tested it out and ensured that it works on the Google Cloud platform. So once you have your app working on your machine, we need to take it to production. We will use something called Kubernetes, which is pretty much the gold standard for scaling out and deploying your application. standard for scaling out and deploying your application says that it elastically takes load it scales out with load and shrinks back after that it would be a good exercise some of you if you are really motivated may want to do it as an app engine and so on and so forth but we won't for the sake for this particular workshop because we want to move fast we'll keep to one solution and we'll move fast with that now i posted a write-up to the slack group and to the course web page did you folks get a chance to go over it anybody who has not seen that that pdf document right if you haven't the break would be an excellent time for you to go go read that document actually let me answer the question the converse way how many of you have read through the document just started scanning through it yeah yeah i have read the document Yeah, part two. Yeah. I have read the document. Just got it. Those of you who read the document, did you find it clear? Does it articulate the project clearly? Yes. Yes, also. Nice. So, do please read the doc. Maybe, so since most of you have not read the doc, it means that we should give some time today to go over parts of the document which we will do then the other thing we will do is we will spacey and some of its features today was traditional machine learning you know the sort of things the traditional natural language processing some of the things that you can do without bringing in the transformers from that you can do without bringing in the transformers. From next week we'll start bringing in the transformers and we'll learn about all sorts of transformers. So just setting out the scope of what we are going to do. Now, Hannah are you here in this meeting? I don't see her. OK. We need to reach out to her. All right. So I noticed that everybody here was there the last time also. Is there anybody who is joining just today? So we were all there. Just to recapitulate, the format of our workshop will remain the same. Mondays will be theory, Wednesdays will be guided labs, and then Saturday will be a quick quiz review. I will be releasing the quiz quite likely tomorrow morning so that you have two, three days to do the quiz. The topic of this week's quiz will be the natural language processing without using transformers or attention models. In other words, a lot of the things we'll do is with spaCy and so forth. The old version of spaCy, the new version of spaCy has an integration with transformers, of course. But the current version. We'll do that. We'll also include some of the things that we learned about NLP in the ML400, in the fundamentals workshop, the part one. So that is the context of the quiz. Sunday, we do paper reading. We take a latest breakthrough in the field and read through it. So this week's paper reading is something called Performa. It is a very recent breakthrough from Google. It speeds up the transformer from quadratic performance to linear performance. And it is one of those small breakthroughs that means a lot in this field. I think can mean a lot, I think, in this field. So it's an important thing to review. We are going to review that paper. I'll post that paper to the course webpage. If you get some time, please do try to review the paper and give it a read. On Sunday at noon, we will study that paper in greater detail. Are we together? Now, having said that, I have not received a lot of the team formation part. So today, because team formation is so important, I will actually require that you guys use some part of today to just right here collaborate and form your teams and let me know what the teams are. I asked you guys to post onto the Slack your team name and team membership. It's important to make progress with that. And I haven't seen much traction. So please do that. Now, with those words, I would like to do two things first is from this machine today unfortunately i'm not able to connect to the my went to server which means i'll physically have to do the code walkthrough from that machine so you won't be able to see my face in the second half So you won't be able to see my face in the second half while I talk, but you'll of course be able to hear me. So with that in place, let's go to the course webpage. I'll share my screen and we will look through this document that I talked about. So guys, use this to go over to your Hangouts and Solutions. When you go here, you'll find that I have described the project in a PDF document. This document you can actually... Yes. So this is the document, you'll see it. Now in this document, let me just give you an understanding. We will talk about the user experience, like how do you expect the users to come and engage with your application. And behind that user experience will be microservices. There'll be machine learning libraries and of course, then, so these are the phases of your project, the milestones that I've mentioned. So let's go and look at this. So we will pretend for the duration of this project that we are doing a startup, right? And the name of the startup I chose for reasons that will become sort of evident to you in due course of time, I chose it to be Eloquent Transformers because you'll use a lot of the transformers. You guys can of course choose a different name for your projects and for your startup, but the goal is that together we will build this. Imagine that you're building a company and a product. So you have a product, and I'll sort of read the first part of the rest of it you can read. Consider that we are an exciting AI and NLP-based startup named Eloquent Transformers. AI and NLP based startup named Eloquent Transformers. We aim to build a fantastic new product which will intuitively change the world as all startups do. Furthermore, we will do it all in four short weeks. The Eloquent Transformers, a cloud based natural language processing application, will harness the power of artificial intelligence and cloud resources. It aims to deliver state-of-the-art content analysis and annotation. Given any arbitrary text content, and these are the three types of contents, textual, speech, or video, the Eloquent Transformer will do each of the following. First is part of speech tagging. So you take the first sentence in the text and for the sake of this project, first sentence is enough. You give the part of speech of every word. So you say, this is a verb, this is a noun, this is an object, noun object, noun subject, noun object. This is an adverb, this is an object noun object noun subject noun object this is a adverb this is an objective and so on and so forth so this is a preposition so sort of like that you'll have to do part of speech tagging of each of the words in the sentence next you'll have to do something called dependency graph and do it only for the first sentence. The reason I put this as part of the project is a dependency graph is a very useful tool. It shows you the structural relationship of the words in a sentence as you study it from a natural language perspective, from a linguistic perspective. What does the sentence, the structure of the sentence look like? Then we will do sentiment analysis. What is sentiment analysis? It's just, is the statement expressing a positive or negative sentiment? Sentiment analysis is particularly relevant when you look at product reviews or service reviews like you go to a restaurant and their reviews of the restaurant of the movie and so forth so we will use multiple databases we will use movie reviews and we'll use yelp reviews as some of our benchmarks to train our algorithms then language detect detection. If I give you arbitrary text or speech and so forth, can you detect the language of the text? Again, in NLP text. And then how do you translate it to another language? Right? So suppose you get the text and it is in English, how would you translate it to Spanish or to French and so forth? Next other thing you will do is summarization of the text. Can you take the long piece of text or YouTube video or a podcast and can you summarize it in let's say 100 words or 200 words, a very short summary, 150 words, and what would your summary say in that case. Next is lemmatization. So if you have the sentences, sentences made up of words and the words, can you lemmatize the words? Lemmatization is the process of taking a word and going to its root word, like for example, run, ran, running ran running they all the root word would be run so things like that uh can you can you uncover the root word uh so i say so does it mean that like we remove any context around that word and get to the root word? Yes. Limitization is context unaware. OK, then you have keyword graph. Keyword graph is obviously you take all the keywords and you show the relationship of those keywords in a particular subject. And so at this moment, I'm itemizing them at the high level. But as we make progress, each of them are mini projects. Week by week, as we make progress with them, I'll be releasing skeleton code and you have to sort of fill in the blanks and use it. So that would be that. Then subject and topic detection. So I'll give you a list of about 15 to 20 subjects and you have to do a detection of what subjects or topics the content seems to be about, whether it's a video, audio or text. You'll have to detect that. Now name entity recognition. This is important. Can you recognize the organizations being talked about, the locations, and so on and so forth? So that is the entities. Another important thing we'll do is in this project and is hate speech. Can you detect the presence of hate in the speech? You know, racial slurs or hate in the speech and give it a score that what's the probability that it has some hate speech in it. Next we'll do is continuation. Continuation is the ability to generate the next paragraph of the text if you can. See if you can generate a few more sentences and how good those sentences are. We like to see that. So that is it. Next is a readability score. Now, each of these things can be done in a very simple way and can be done in a very sophisticated way. For example, let's take any one of them, readability. You can do readability without using deep learning or deep neural networks or transformers. You could do it just through basic statistical analysis and techniques have existed for the last almost 60, 70, 80 years to do a linguistic analysis just based on the statistical properties of the text. When you are starting out your project, use the most inexpensive manner so that you can bring about your project and make it stand up quickly. This is part of the art of building a product. You put a minimal viable something out there and then you iterate over and you improve upon it. And you gradually replace it with a more and more mature version of you. Likewise, when you do sentiment analysis or any one of these things, in the beginning, you can just use transfer learning, you can take one of the pre trained models, often a transformer meant for that purpose, and just plug it in. Then later on, as you have time, then you can fine-tune that transformer or do something. And if you have even more time, then you can train a few more layers and a few more things in the transformer so on and so forth right so you can progressively make it more and more advanced so always take the approach that for the first time or in the first part put what is a minimal viable solution and the winnable viable solution will be fairly dilute. Its effectiveness would be weak, but still it is worth doing that just to put your end-to-end product in place. So that should be your approach to doing it. So in the spirit of us doing the minimal needed, let's talk about the user experience. So in the user experience, how can you do it? The range is, you may just treat the Jupyter Notebook as your user interface, right? Just create a Jupyter Notebook. So many of you don't have experience with Python, with HTML and CSS and JavaScript. You're not familiar with React and so forth. So the simplest user interface you can create is literally the Jupyter Notebook, which all of you are familiar with. So you can start with that, then gradually, you take that you put a little REST service behind it. And today, I will show you the code for the REST service. And you'll be quite amused at how absolutely easy it is the reason i mention it is some of you said you don't have experience with creating rest applications and you'll see it is a matter of almost two three lines of code to create a rest application so you can create a rest application and put it get it afloat with just your Jupyter Notebook as a front-end. Right? Do you see how easy it can be? Then the next thing that you can do is start writing a simple HTML and CSS, just a HTML form. And creating an HTML form is a five minute activity for something, take one functionality, let's say translation. For translation, how much effort does it take to create an HTML form? It is literally five minutes if you handwrite it and maybe 30 seconds if you use any of the HTML designers that is available on the web, web page designers. You'll have to know a little bit about HTML or these things in the process. So you can create it and then you can get more and more sophisticated. As you build out these functionalities, you can then actually draw out a wireframe in your team. You can build something that is useful, that can be used to, I mean, that's pretty, that sort of is pleasing and has a good user experience, not just pretty, I suppose, but with a good user experience, aim to be a startup. At the end of it, it should look and feel like a app that you are yourself used to. One of the stretch goals that I'll give you is, see if you can also, so you can do it in HTML. If you're using React, you have the possibility that you can use React Native, so you can create native apps for your smartphones and tablets. For your teams, I'll leave that as a sort of a stretch goal. Hint, it is not hard at all. Within half an hour, you can do that. If you can do that, all is good. I mean, that is assuming a simple app. If you're making a very sophisticated and really polished app, of course, it will take more time. It may take a day or two to create a UI. Then the microservices. How should you go about creating the microservice? First of all, just create a REST endpoint. Once you have created a simple REST endpoint in which we will learn how to do today, then we will load a model, load your model to it. Assume that you have a model ready, load the model and use it for inference when the user request comes. So once again, start with something very simple. You can run a REST endpoint literally from the command line on your laptop. Then when it works, then as a second step, think about making the code a little bit more polished, more rigorous with logging and tracing and other things. And then when you are quite satisfied with your code, you have good exception handling and everything, then take it to the next step. Put it into Kubernetes engine of Google and now deploy it. Once you deploy the code, obviously at that moment, you're done basically. And now you can keep iterating. You can have some form of build and deploy framework so that you keep updating your code. Then machine learning libraries, when it comes, oh, by the way, when I come to talk about microservices, there are only two things that I would be able to support in the python world i'll be releasing examples in flask and fast api fast api is significantly faster in python because if they're underneath it has c libraries the implementation is in c so obviously you get better performance in the same way in the Java world, there are two particular implementations. Just as in the Python world, if you use Flask, it is implemented in Python itself and is a bit slow. In the Java world, you have two implementations. You can do it with Spring Boot or with Quarkus. Generally, Quarkus is a more modern, more minimalistic framework and it has it is lightning fast. Not to mention that it has a absolutely native integration with Kubernetes and therefore deploying it to Google Cloud would be a pleasure for you. Now one of the questions that you may ask is how can I and this question keeps coming there's so many people who come to me and say okay I've done my entire model in python how do I run it in a web service that's written in java like in the cloud and so forth how do I take it to production where much of the production code happens to be written in java so one of the things we'll do in this project is learn how to do that. So that is that. And so if you're familiar with Spring Boot, go use Spring Boot if you prefer. If you're just starting out, I would always say start with something newer. So that would be a Quarkus. Any other, so in each of these worlds, the world of Python and the world of Java, there are countless frameworks to do REST applications and REST microservices. But if you choose to use something else, then remember you will have to debug. If you get stuck, you wouldn't be able to ask me because I only know these things right of course I know Jack in Java I know Jack Saris the underlying the code and so if you're doing it directly Jack Saris over tomcat once again I'll be able to help you but not beyond that like if you use some very different libraries and I wouldn't have the time to go learn the library and help you work with that. The same is true for machine learning. We have been doing right from the beginning, the libraries that we have used are scikit-learn, numpy, you know, scipy and so forth. And then for deep learning, we have used PyTorch predominantly. I would strongly advise to continue using this. Supplement with libraries that are very compatible with them. Like for example, the Transformers library of Hugging Face is a no-brainer. Use Spacey, use other libraries that sort of work smoothly with it. But stay with that. Now, one of the things you should do is, and I hope you guys will do it as part of the discipline of doing the project. At the end of it, you'll write a white paper, team's white paper that sort of describes the architecture, the work that you have done, why you have done it and so forth. And then you'll create a presentation. The final presentation, you'll make a video of it and we'll all watch the video, but the intermediate presentations, of course, you'll make to each other, you know, various teams will make to all the participants. Those would be your progress report. Next, we talk about the collaboration teams. Now, remember I said that you should collaborate because the project, even though it is made up of small, relatively easy pieces, there are a lot of pieces and there are a lot of moving parts. It is more fun when you do it with somebody. Those of you who really are driven to doing end to end yourself, remember, you're looking at 15 to 20 hours a week. If you're not willing to give that kind of time to it, then you have to think about it. So first thing you need to do for the club collaborative team is the following. You must have a distinct name and mascot. I have not received that guys. I've received it only from Nisarg as far as I know. If others have sent it for some reason, I didn't notice it. So please do do send it post it to the slack group i would like to see the team in fact today i will i would rather give the last half an hour to making sure that you all we all take care of this the second thing is your team sizes should not be greater than four should not be greater than four. Now a few tips on team formation. Pick team members of similar ability. This will prevent asymmetric effort where one team member contributes so much quicker that there's little left for others to do except to sit back, or stuck. You don't want that. You don't want a guy who's so good in your team that before you even think of how to do something, the project is over because that will be counterproductive to your learning. Pick people at more or less similar ability so that there is more or less a symmetric sharing of effort. Then you'll all learn. The other thing is each week the project and lap will consume about 15 to 20 hours of effort besides the time spent in the sessions of the instructor so besides a classroom time which we do together it will still take 15 to 20 hours of effort so now 15 to 20 hours is a rather hard or a tall order if you have a day job, if you have family, and not to mention the work, the pressures of Netflix and Prime movies and whatnot. So it's hard, right? With the social life and everything. So what you do, I mean, these are realistic things. We are all going through a COVID pandemic. Life gets boring. These are the ways that we are all coping with it. So at the end of it, after a long, tiring day and taking care of your family and social life and so forth, there's only a limited number of hours you can give without getting completely sleepless. Find out what number of hours you can give in a given week. Be pragmatic about it. Be realistic. Don't be aspirational. If you're aspirational and say, I'm going to give 25 hours or 30 hours every week. Look yourself in the mirror and see, is that pragmatic? If it is, then do give that. If not, think about it. So suppose you say that you can contribute 10 hours a week to the project. Then what do you need? The remaining 10 hours a week, you need to compensate with one or two members. Somebody else may want to put 10 hours. Or you may find two people who may be willing to contribute five hours a week. Aritya, please mute yourself. So take a general sense and divide the project amongst yourself. The next is set honest expectations with your team. This is important, guys. To be upfront and say this is all I can do is far better than to promise high and not be able to deliver Satish Penmetsa, Ph.D.: Because then it may it may it might make you feel embarrassed later on. So don't do that. It's all have a very pleasant experience towards that set an honest expectations with your team. such an honest expectations with your team right and let people know that this week is a busy patch for you the other week you'll be more free and so on and so forth so if you do it like that the whole experience is likely to be much more pleasant next is obviously divide and conquer but don't divide and conquer to such an extent that one person like on any one piece only one person is working the minimal rule in software development is pair programming. At least two people should be talking their way through code as you write code. If you write code all alone, you risk getting demotivated, introducing more bugs and so on and so forth. There are people who with a lot of experience who can just quietly write a ton of code, but they're actually far fewer than we think and even they benefit from pair programming right so so that is that and this is also giving you practice for uh whatever for jobs or workplaces that's how things work uh then set up if at all possible it's a tip set a fixed schedule to the extent possible. Your project is much more likely to succeed if you start early and make deliberate and sustained progress on a day-to-day basis. It's very important. Procrastinating the project till the end will inevitably overwhelm you and you are more likely to defer it to an unspecified time afterwards visually when the workshop is over. When the workshop is over, you think you'll have time. Now, new work, new responsibilities will come and crowd it. You won't get time. Such a time will likely never materialize when you will come around and finish your project. So don't wait. Don't wait. I'm tempted to quote a line of poetry from my native, from my hometown, from a poet named Kabir. I'll just give the first line of it for those of you who are Hindi speaking. Kal kare so aaj kar, aaj kare so aap and so forth. So in other words, don't procrastinate for the future. Get started early. Set up a, then also amongst yourself have a regular check-in and have a check-in with me periodically so I can guide you through the project. See guys, by now you must have realized that, I'll stop for one second. By now you must have realized that this part two is very project centric. We will learn new things, lot of new transformers and theory also, but you'll miss out a lot if you don't do the coding. So here's the thing. So, you know, when you do a project, it is good to set milestones. I sort of enumerated the milestones for you. And give me a few minutes, guys, and then I'll field the questions. So the first milestone is set up a project plan. These days, agile methodologies is the norm. That is what people do in the workplaces. is the norm. That is what people do in the workplaces. And it sort of works. I wouldn't say that it works perfectly, but it is about more or less a standard practice. So break up your work in sprints. We have about four to five weeks to do this project. Four weeks is what I would ideally like or less, because we have a lot to cover, other topics to cover, image processing, image processing anomaly detection time series and so forth so that means four sprints right four springs or three springs plan out each sprint take your task break it up i mean take your project work it out into tasks and then create a backlog of those tasks and then every sprint decide how many of the tasks you guys will do. The sprints are usually themed around some achievement, the UI, the this, the that getting done, some functionality getting done. You're saying that maybe the text classification and this and that you'll do end to end as a team, right? Or something like that. Here are some milestones. And by the way, these milestones need not be done one after the other. You should typically make a progress at a few of them in parallel. So the sketch out the ui design. The simplest ui you can design is no ui at all. You just do it in Jupyter as the ui. The next level is you can hand sketch a few drawings on paper in your team and share it using your iPhone, you know, take pictures with your smartphone and share it with each other on Slack, if you wish. Or there are softwares that can help you mock and design UI. So whatever is your comfort zone, just sketch out a UI so that all of you agree that this is how it will look. As a minimum, you'll have just two pages. One is the form in which you'll just either give the URL of the YouTube video or you literally paste the text, right? Or give the URL of a sound bite or of a podcast. Or the other is literally the text. Start with text. Text is simple. What if the text is given to you right away? So do the UI design. And the second page is once you submit it to your models, your model will respond and you need to create a page to show the responses, the translations, the predictions and all of that, you need another page to show that. That is a simple two page system, whichever way you can do that. And as I said, guys, it does not take fancy web development abilities. It is it is something that you can pick up in HTML, CSS, HTML world, and you don't even have to prettify it. You don't have to know CSS or anything so long as you can produce the result it is good of course then once you have the minimal viable product in place and now enrich it do a bit more CSS will beautify it improve the look and feel and usability of the application and do things to it then. Likewise your REST application, we do the bare bones in the beginning and the bare bones are not hard as you will see. You do that once you have done the bare bones as I said later on you can deploy to Kubernetes engine again fairly straightforward. Likewise for natural language processing milestones. Each of these tasks that we expect this engine to do, this application to do, amongst yourself, set it up. In the beginning, you may just use transfer learning or use a library that does it. It does not have very high quality, but it produces results. Then the next stage is to fine tune a pre-trained model and the next stage is to pre-train, like fine-tune a pre-trained model and the next stage is to train it from scratch if needed or to create a custom architecture to do, to just get state-of-the-art results, right, and publishable results. If you think really deeply you can come up with your own architecture, neural architecture, and that is the high bar. You can do that. architect. And that is the high bar. I can do that. Integrate your NLP models with the microservice and UI. So then comes the integration phase. You have the backend trained model sitting somewhere. You have your basic microservice and you have a UI. So you need to do three-way integration. You need to integrate the UI with the microservices and you need to integrate the UI with the microservices, and you need to integrate your models so that the microservices will use it at runtime for inference. So those are the two integrations you have to do. And the final milestone is, and it is the one that you should not skip, but in my experience from previous workshops and bootcamps, is the one one for some reason engineers find it very, very difficult to do, which is to write a small technical documentation. That document need not be like 30 pages or something like that, a very short article, let's say four pages or so, about a couple of thousand words, two, three thousand words, with diagrams and so on and so forth, explaining what is it that you have built, and also talk about the narrative aspect, the human dimensions. You talk about the team experiences, the roadblocks, the conquests and so forth. And believe me me these sort of documents make for a very entertaining reading and often it can be a strong motivator for people startups and companies to pick you because they see a human dimension to your efforts so do that and finally create a youtube video as a team right a simple youtube, which in the bare minimum can be just a presentation, slide presentation with voice capture would do. And then from there, see if you can improve upon it. And always talk about your experience, guys, because learning is a very human experience. Doing a project and learning from it is one of the most fascinating experiences you can have and it leads to very fascinating projects rather than dull projects which are repetitive so talk about that i will of course be releasing helper code samples all along and these helper code samples you are used to now onwards it will be sort of fill in the blanks. I'll give you one example and you can follow that example to do other pieces and so forth and then to flesh it out and make it deeper. So this is it. So a few final thoughts on this project. I don't know how you guys are feeling. In case you're feeling intimidated or something. So support vectors carefully crafted the project to be done in about 30 hours of cumulative effort if one is familiar with the libraries. Of course, since we are just learning, it is likely to take us much longer. However, with actually it's not well, without the learning involved, it should still be possible for almost all teams to finish within 60 to 80 hours of effort. My own feeling is it will take you somewhere between 30 to 50 hours total cumulative over a month, which I hope you can put even as individual effort, you can do that. People who are highly motivated may be able to finish the entire project end to end in a couple of weeks. As the ancient saying goes, the journey of a thousand miles begins with a single step. So that's that. Now I'll take questions. I do know that there were quite a few raised hands. Where should I look for questions? Give me a moment. Participants. Yes, I believe Sukpal, you had a question. Sukpal, are you there? Yeah, I'm here. It was not questioned. That was the poetry you were saying. You didn't say the second line, sir? Yes. . Yes, sir. Nice. Yeah, I mean, I apologize to all non-Hindi speakers. It's from a poet in India, from my birthplace, actually, named Kabir, who is very well respected. He's sort of the Thuro of what Thuro is to US. He is to India. Any questions? Any other questions, guys? So, so far, so good, guys. I hope this looks straightforward. Is there any feedback? Guys, this is the time to get engaged with me. Tell me, how do you feel about this project? Yeah, Ashif, question. We're learning the new concepts at the same time doing the project. Yes. So how do we plan the project when we do not know the topic? Actually, I will be explaining, like say every Monday, I'll be explaining a new concept or thing like that. You can do that. See, there are two aspects to it. To do a minimal viable version of the product, you actually don't need to know much theory. Okay. Libraries are there, you just use the libraries. Right? And from what we did in ML 100, I mean, sorry, in the part one, the ML 400 part one, remember the code samples that are there. If you just go and browse that, you will realize that 80, 90% of what you need to do this project is there. I have not seen that one because I did not attend the first part. That's why you should form a team quickly. So that they'll sort of walk you over the code and I can walk you over that code. It's there. And so create a minimal viable product, which doesn't have to be great, which doesn't have to even use transformers in some cases, just use basic stuff and get it done. Once the application is up and standing, then you go and flesh it out and enrich it, make it better. enrich it make it better by the way guys are you comfortable with this style of making progress in part two anybody has a feedback or would you prefer the old traditional style this is better approach sir i i think and of course it will be the same thing i'll still be giving out a lot of code but it is all the only difference is it is having a cumulative effect at the end of it you would have a full-fledged application end-to-end right that is education into it that's right so there are uh even though the parts related to full-fledged applications are applications are small uh but i guess many of us do not know that so it's i think it will be very enriching experience to just go through the entire process holistically nice any other feedback shanka what do you think Right now, I'm only one person. So probably I'll stick to one small aspect of it, like transformers and focus, I will do the entity recognition, just the ML part. Okay. Okay. But have you are you going to partner do it alone? Your team is solo or with somebody? Right now I'm solo. All right. So guys, do please form your partnerships. And if you're going solo, all power to you. This is very good. This project actually can be done if you put in effort. Well, I don't want to trivialize it and say two weeks, but certainly can be done by one person very comfortably in a month uh anybody else has a feedback i said can you hear me yes i can so if we can get these pre-reqs sort of quickly out, you know, a session on those, then that will be helpful. I'm sorry, if we can get? The pre-reqs, right, the setting up the microservice and then maybe invoking it from the HTML and then from microservice invoking a model, you know, if we can get those quickly sorted out, then we can start. Yes, yes, yes. In fact, today I'll give you the REST application, a starter code for REST application. And when you see it, you'll laugh at how simple it is. We'll do that. Let's take a small break. I need to move over to the other machine guys, or at least try to fix the networking issue that I'm having. Let me see what's happening. So let's take an early break today for 10 minutes and then let's regather and we'll do that i'll stop the recording until then i said i wanted to ask a question go ahead so in the classifying tweets whether uh you know they are like sarcasm or non-sarcastic tweet. Will it be kind of sentiment analysis or it falls into different category? It's a classification, Pradeep. No, I mean, in classification, is it like a sentiment analysis? Does it come in sentiment analysis part or something else? It overlaps with it. What happens is that sentiment analysis has been notoriously bad at catching irony and sarcasm. So people are now trying to create, and a sentiment analysis is a text classification problem. Right. People are trying to now train, are doing a whole lot of work to train machines that can better catch those things those nuances it's a it's really good question and it's working actually uh so we are talking in this project you'll become aware both of the power of nLP, which of course the idea is that we become very good at it and we feel good about it, but you'll also hopefully learn about its limitations and the frustrating aspect of NLP. By no means we are in a way, you know, we can pat ourselves in the back and say what a marvelous thing NLP is. In many ways it is is but in many ways it's still in its infancy and this is true in general of ai itself machine learning itself some days you wake up and you'll feel what amazing things ai can do and some days you wake up in frustration and say my goodness maybe in another hundred years it'll be a bit smarter. And you go back and forth between the two. There is an excellent book that has come out. Some of you may be interested in it. It is called The Alignment Problem. And if you get a chance, read that book. We talk about word to work, for example. It's very common in word embeddings and LP to say, oh, king minus man plus woman is queen. And that is quite impressive. But not so impressive things are stereotypical biases that the AI has learned. For example, shop, what was it? Shopkeeper or shopping or shopping, minus man plus woman is housewife. Now that doesn't quite look right and there are many such other statements that are embedded in this. So these are the alignment problems in AI. Sometimes it works, sometimes it doesn't work. Today, for example, as you know, this week's big news is, I don't know how many of you heard that big news is um tesla seems to have solved the problem of the so-called local minima right they are using they have made a fairly good breakthrough so that the new beta of their self-driven car or full of their automation is pretty good there are lots of videos if you go to youtube recently showing how it is able to make turns and drive through streets and do all sorts of things uh and yet yet there are things that it cannot do so and those are hard problems so we'll see problems so we'll see thank you sir any any other feedback guys otherwise we'll launch for this week we have given considerable amount of time just setting up the framework for our uh workshop but now we'll move much faster it won't go so slow we'll we'll really have to back up we'll learn a lot next week is is very concept heavy, lots of new things to learn, lots of new things to do. The project will make a lot of progress and so forth. So all right guys, I will share the screen and then I'll shift. Did I start the recording? So guys, I have given you the starter code. By now you all must be familiar with the Deep Learning Workshop project. It contains, and I'll let you increase the font size, it contains svlearn, where we write our Python scripts, the Python code, dominantly, and we have Jupyter notebooks where we engage with them. So what I would like to do today is two things. First is to show you how to create a very simple web page that will interact with a back end and a very simple rest endpoint that it engages with. That would be exercise number one. What the page will do is it will give you a place to enter some text. And when you enter the text, it will give you a translation of the text. Then in the REST service, I'll walk you through the code and you will see how we do the translation and how do you create a REST service. Once we are done with this, the second part of the lab that I would like to do is just as a quick review, walk through spaCy. So just in case some of you didn't get a chance to study spaCy in the last two days, I will just give you a quick introduction to spaCy. spaCy for your traditional non-transformer based machine learning, at least for the time being. Now spaCy has transformer integrations. You can do that and it's certainly a core feature of spaCy 3, but spaCy 3 is not efficient. So I'll just use the standard spaCy. All of these are straightforward things. So let us go into the notebook. And the first thing that I would like to show you is, let's go to the NLP. How do you create a page to accept the input? So look at this page. I created the simplest possible page. It just says a simple starter page for translation. Enter some English text. We'll enter some English text. Hello world. And we will submit it. And does it take submitting okay from this machine is something because it's a within the Jupiter framework, something is happening. Let me actually look at it from the other machine where everything is running the server is running. Oh, because it's going to the local host rather than to matter. So we will look at this code. Before I do that, let's look at the page source. Do we notice that this is the HTML? Let me talk about it. Actually better still, let me do it from Ubuntu. This machine does not have the whole environment set up. So we will go and do it from the Ubuntu machine. That would be better. Give me one second. I'll log in from there. Directly from the server so I can show you things. All right, so I'll again expand the font so you can see it clearly. Once again, you remember we started out with this. So I wanted to show you how simple it is to write an HTML page. So let me literally show you the code I wrote. And I wrote this literally in the break. It just takes one minute to write the HTML code. And perhaps this also needs some basic explanation. Let me expand the font size and go over it. See, oh, in fact, I did not even bother to finish the HTML segment. So what it is, is HTML is a very forgiving language and a very simple one. You can make mistakes and it will actually be pretty forgiving about your mistakes. Like for example, I'll explain what these things are. This line says we are writing an HTML document starting with HTML tag. Everything is between these tag brackets, angle brackets, and the corresponding what is line one, the corresponding end of it is at line 18. Likewise, we have the body tag and generally it is customary to occasionally sort of a tablet things that are inside the body to tap it and so forth, just to make it a little bit more readable. HTML file, HTML page, HTML means hypertext markup language. It is essentially a webpage. This is your webpage. And in this webpage, you are center aligning everything. Center just does that. Then this is a heading. In markup or in Jupyter Notebook, this is the equivalent of two hash marks. You're creating a form. In the form, there is a label, enter some text in English, some English text. And then there is a text area where the user can enter the text. We are saying give 10 rows of empty space and make it 100 columns wide and name this text area. Every element inside a form has a name. You're giving it a name called text. Then that's all it is. And then you have a little bit of a break. And then you have a submit button. And this is a form. Typical form that you see on the web is like this. So for those of you who are new to HTML, whenever you create a form, usually you create a form to send some chunk of data, some structured bit of data, first name, last name, so on and so forth, address and so on and so forth. That would be a user registration form. In this case, we'll just send a text and we'll expect a backend service to translate it. So this is it. Do you see guys how simple it is, this code? And obviously on the internet, there are tons of generators that will help you write this code if you're not familiar with HTML. You can just drag and drop and your thing will be there. The only important thing to know is where is the server? What is the rest endpoint that you're calling? We are calling this rest endpoint and we are calling it with the method called post. In HTTP protocol, you have get, post, head, and put, and so forth. But roughly speaking, when you have structured data to send, you should, or large amount of data to send, you should always use post. So that is that, right? So what does it look like on the webpage? So this is the source code. When you actually look at it, it looks like this. Oh, we did some glitch here. Form input bracket. See, occasionally you'll end up screwing up a little bit. I seem to have done that. You don't have the end tags for, okay. Yeah, so suppose I say hello world. Now I did not take care of the UTF encoding. So it will come back as a translation but most likely to look garbled up. Let's see. So we have a service so let me show you the code now what is the code that is handling this the code here is this you import the library flask there's some things you import actually j75 we don't we are not using it yeah yeah we are using it you're returning adjacent as a question so let me walk through the code um okay so i don't know if you're if you guys are able to see the code which is very simple you just need to create a this this is boilerplate time for a flask cap you create a this this is boilerplate for a flask app you create a one line code right and then you create something called the routes what it means is after the server name what is the uri right what is the resource that you are trying to get to. Here we say translate. The HTTP method we respond to is POST. We are defining a function a normal way. We are defining a function. In the function we say that if the method is POST means handle it only if you are responding to post, otherwise don't respond to it. What are we doing? We expect that the post contains a form text and from that there is an input field called text. Do we have that? We can see that it is there, right? You see this name is equal to text in the html form we have this so in the request there is a form and in the form there is a text field and so that is the text field we are referring to in this code where is the code okay in this code so does that explain this now guys? Yes. Yeah. And this is something you're familiar with from our introduction in the ML fundamentals, 400 fundamentals. This is just a library that will give us a simple translation of the text. So what we do is we use the library over the text to create a blob. And then we say, we take the blob and we say convert it to Hindi, to language Hindi. Hindi is because it's at least something I can understand. I don't understand many languages. If it is French or Spanish, I wouldn't understand. So now you end up with a translated thing, which is in Hindi. Then all I need to do is convert it to JSON. Do you notice that I have converted it to JSON, right? I just created JSON. JSON is just key value pair thing. Sorry. A key value pair to uh sorry a key value pair can translate text whatever the original text is and whatever the translation is and so you do that here and when you do that you can run this you know this application when you fire it up you can fire it up in multiple ways. For example, if you go and look at the, I created a sample, a page for you on how to do that. So here it is. You can give from the command line, you can give this argument. you can give whatever text you want that you want translated and you can do it so this is even if you don't create a web page you can do it like this right and you what will happen is in response you will get a hindi translation which you can't see in jupiter because it is a utf encoded unicode encoded but it is a translation of that i think in the command line it is a UTF encoded, Unicode encoded, but it is a translation of that. I think in the command line, it is a little bit more easy to see. Let us try a look. The same thing, I'll put it in the command line. And let's try a look here. And let's try a luck here. Let me open a clean tab. Are you guys able to see this? Yes, you're a obedient computer. Yeah. Somebody's typing pretty hard. And so what happens is that you will get back a response like this. Right? Now there is a way to set this UTF encoding to somewhere there must be an encoding so that we can see the text in actual Hindi glory. Where is it console and profiles? default. So there is a way to get to the change the encoding advanced shift miscellaneous encoding. Yeah, it is Western Europe. You have to select Dev Nagri, sir. What's that? It's Dev Nagri, I believe, D-E-V-N something for Hindi. Yes, maybe I'll have to change to that. Yes, I think so. But you can do that. You can do that too, PyCharm is also easy. Little bit louder, Sukhbar. Sir, in PyCharm, you can also do it easily as well pie charm has a lot of years in pie charm it runs like uh it runs very well actually uh it will come out the way you expect it where's my fight yeah you must have font installed though if you have devanagari fonts you need hindi fonts If you have Dev Nagri fonts, you need Hindi fonts. No, here also. Yeah, you have to install the fonts. I don't have Dev Nagri fonts on either of my machines installed. So it's not showing through properly. But you can trust me that this is a translation into Hindi. Right? So it can come out with some Hindi text. But the point to note is, let's going back to that. Do you notice guys that creating the web page. Where is the index page gone? Index page. If you look at the source code of this view source body no this is not it uh the source code is here this source code is very simple and i said you were saying isn't it that you haven't done web programming so does it look very easy now yeah but where is this written asset in notepad yeah i just i'm well i use linux so there is an editor called kate and kate is a very good editor actually a very powerful editor emacs gate these are the things you use typically in linux think of it as the equivalent of notepad obviously if you have sublime text which i do but i didn't want to fire up the whole sublime text for something so simple. And the code, the actual code, again, is very simple. The actual code is here it is. I know here. Where am I? Go back to that and as we learn, if you go to NLP, okay, things are for some reason running a bit weird, spacey NLP lives, okay, and NLP, Okay. And NMP. Something very slow happening here for some reason. Might as well just open the file right here. So is PyCharm looking very small small guys is the font looking very small here yeah it's looking yeah very small okay all right so i'll just open it up in a console using and so we have a simple pie. Let's try and look here. Is this very readable guys? Yes, good to know. I'm such a good editor. Come again, Kate. Good to know. I'm a a good editor. Come again, Kate? Good to know I'm a handy editor too. Oh yes, you have many functions. So there we go, translate. So guys, is this code pretty much self-evident? Do you realize that one line fires up a web server and you don't even need to give these arguments. I give it because I choose a custom port. Debug means on the command line it will produce a lot of messages and this is it. And you just run this Python file and your REST endpoint is up and alive and running as a REST application. Easy, isn't it? So suppose you want to do something else. You want to do route, let's say. Another one is sentiment. And the only methods you may want to support is again, let's say that for sentiments you may even have short sentences. So you support both get input. And what do you want to do for that? Let us define a function. you take a sentiment and actually let's keep it to post suppose I mean just to keep it simple for the time being and then what do you do you can say pretty much the same thing I mean these two things can remain the same. And then you can just bring up the sentiment analysis part. But I'm going to cheat here. I'm going to always just say return positive, which is cheating. But do you see how I just added one more functionality to this app? So it is as simple as that guys. To create an application in the beginning is very, very simple. Then you can make it more complicated after that. And as you notice that here we are using a actually using a nlp library and by the way here this is text blob it could have been a transformer and you have the code for transformer and everything and you can just stick it in here it is quite literally as simple as that right is it looking easy guys to you Is it looking easy guys to you? Yes, I'll say. So what I would like to do now is go back and review a little bit of, now I'm't done today, I'll just take you through basics of natural language processing from a hands on perspective. So we'll wait for this page to come up. Yeah. So how do you install SpaceCy? From your command line, just go and give this command. By the way, all of this code and everything is already posted on your website. There you have it. So you go and say pip install spaCy. It's a pretty good NLP library. The reason to prefer this over NLTK etc is, the performance is blazing fast. It's meant for production, production use. NLTK etc can be a bit slow. Though they're very feature rich, they can be a bit slow. Spacey is quite fast. It's written in Python, a C version of Python, a C extension of Python. a C extension of Python. Now, we use spaCy. There are two aspects we will use spaCy. spaCy also comes with a visualization build on called displacing. Go ahead. One quick question, Asif. With this exercise that you're talking about for spaCy, is this OK to do on a typical desktop, or this needs to be done only on GCP because the need for computing is very lightweight okay when we when you start needing hardware I'll tell you about it okay these all of these things you can do in a fairly modest laptop okay so there we go so this is it you import now comes one interesting thing when you look on the web and when you look in the textbook or they talk about something called encode typically you'll see this small all the code samples on their website will use something called a small English model. Let me talk a little bit about models. What are models? See, a model, a language model is roughly speaking the artificial intelligence machines understanding of a natural language. It is a sense of what the words are and what the relationship between words are. That is the intuitive meaning of a model. A more technical meaning is a language model is needed to finish the next word in a sentence for prediction purposes but it's a narrow meaning. Let's take it in this more broadly. Think of a model as something that a machine learning has deduced after you have trained it with a lot of data and it has figured out the, it has somehow figured out the language or understood a natural language. So when you have a model like that, the models can be of various sizes. In particular for the english language there are three models that you can use with spaCy they are the small medium and large so most of the examples on the spaCy website they use the small model don't use that so the first thing i would say is don't use that. If you have reasonably good desk space on your machine and a laptop which has at least 16 gigs of RAM or so forth, just go straight to the large model. The reason for that is a lot of the things that we will do, the high-end stuff that we will do, and especially even very high-end stuff, even basic stuff like word embeddings. They are not available in the small model. They will just start throwing exceptions. So to play it safe, we start with the large model. Modern hardware can handle it quite well. So that is why, and you put the word LG in the end, English Core Web, and then LG in the end, right? So you bring in the large trained model, which has a lot of words and their relationships and word vector, everything is there. So that is the meaning of line three. You're building up an NLP. Now, what you have ended up with is an NLP model, a language model. Think of it this way. It's a spacey, loaded, and English language, large model, language model. That model here, we save it as a variable, NLP. Now to the NLP, suppose you give it an English text. I love the ML 400 workshop, which I hope is true. So then what it will do is now the language model, having worked upon this text, produces something called a document. A document. Think of a document as the result of applying a model to a text. Are we together? When you apply a model to a text, what you get is a document object. Now you ask this question, what does the document object comprise of? So you can do it in two ways. One is of course the usual way that you could say, hey, oh, sorry. Let me, sorry, dark dot type. It will tell you that it's an instance, it is an object of this class. This is a class. So it is easy now to go and suppose you want to know more about it. You can go and look up the documentation. All it means is a simple googling and it will take you straight to the API doc and you can read all about it. So it contains what all things it contains and the documentation for a space is very nicely done. You see all these examples that are here. Every single thing you can iterate. The important things to note about the doc is first, do you notice that it contains this powerful things get item. Right. I traitor etc length. So what does it mean? It will help you treat it almost like an array or a list. It will basically function or behave like a simple list in Python, which is a huge benefit that it gives you because see, look at this. Length means if you ask for the length it will tell you how many words or tokens there are in that particular sentence iterator gives you the ability to do all sorts of wonderful things for example you can do list comprehensions and whatnot and we are going to do that get item what does get item do if you are familiar with this in python it means that you can literally treat it as an array. Do you notice that here with me make it bigger or maybe to be If you look at this. Do you notice that you're treating this As A list you're indexing into the list and you you have all the powers of indexing into the list. you're indexing into the list and you you have all the powers of indexing into the list which is really a very well done api i this is the part that i like so much about spacey the api is really well thought out and so now going back to it what will we do we will use the fact that you can iterate over the document for each of the tokens. So let us convert each of the tokens to the corresponding text and see what tokens it found. So it says that for the word I love the ML 400 workshop, there is one, two, three, four, five, and the exclamation mark six. So you have six tokens. five and the exclamation mark six so you have six tokens now so the the word that people use in nlp is token the word token is there token word is pretty common uh in compilers parsers lexers and nlp world then comes the question of lemmatization. Lemmatization is the process of taking each word and asking what is the root word for it? What is the canonical form? So if you look at it from there, the I is a proper noun. It refers to you. Loved. Loved's standard form is love. The standard form is the. And all other words remain the same, but this love changes, change to love. To love. Let's maybe we need to try another example. Let's say we change it to excited excitedly and ran away with this. So I'm just creating a silly sentence here and see what happens now if we were to do this. So obviously we end up with a lot of tokens. Let's see what lemmatization does. So some of the words will change. Do you notice that jumped has become jump? Right? Ran has become run, the standard form. So you see some changes and these changes are called the lemmatization of the words, the lemma, root words you went to the root words guys is the meaning of lemmatization clear yes right so there is also something called stemming stemming means removing uh the words and keeping only that part that remains common those characters that remain common in all of its variants usually i tend not to use stemming very often. I mostly stay with the lemmatization. Stemming, there are some use cases for it, but in NLP you tend to use lemmatization more. Now you can do one more thing. And this is where the power is. Usually, see behind specie is a neural network so no doubt about it even when i call it traditional i don't mean that it is just using statistical methods it is using a neural network it is just not transformer based in its older form so you go to this and see how well it understands a sentence you go and ask tell me what is the part of speech of each of them. And so it comes out with this nice table. Slightly decrease the font. It looks like I it says is a proper noun, right? It is the lemmatized version. So i'm giving the text the lemma the part of you know the part of speech it is part of speech and the tag that you associate with it so let's go over it line by line i is a pronoun is a proper pronoun loved the standard form is love love is a verb and its tag is via vbr the is just a determiner this is a noun it has figured out that ml 400 is a noun workshop is a noun exclamation is a punctuation and is a conjunction and so we can go onlamation is a punctuation, and is a conjunction, and so we can go on. Jumped is a verb and so forth. Do you notice that it is able to do all of these things? Not only that, you can, so I hope guys, if you're not familiar with NLP and you come to it for the first time, you would realize that first of all, it is nearly impossible to get this level of clean results by just writing a lot of code by hand and without using machine learning. It's not so easy. And just writing even Alexa parser and so on and so forth soon you get into very complicated situations. So NLP is a very mature domain and these libraries are very mature and they use a lot of neural networks etc to get the job done. Then comes the interesting part how do you visualize it? So suppose we take a text go ahead. You are using underscore in that. What is the use? Did I use underscore? Where is that? Position. Oh, yes, yes. This is a very Pythonic syntax. What happens is that whenever you see underscore after as a member variable, it's a member variable. Whenever you see underscore after as a member variable, it's a member variable. Whenever you see an underscore after, like is in lemma underscore position underscore tag underscore, the library creator are telling you that this member variable is a computed value. In other words, it is the result of some sort of a machine learning exercise. If you look at scikit-learn, you will see the same behavior if you build a linear regression model or logistic. So if you do model.coef underscore, you will get the coefficients. Likewise model.intercept underscore, you will get the intercept. Okay, so that is how it's sort of a Python convention, especially for the very particular to data science. As if, yeah. Are the tokens case sensitive? Are the tokens a case sensitive? The answer to that is, see, yes, they are case sensitive, but you notice that what it has done, when you lemmatize it, it will change the case. And the part of speech tagging is, of course, smart enough not to be affected by the case of the text. So it doesn't matter whether you wrote it all in caps, the parts of speech tagging, lemmatization, etc., they are all pretty smart. Let's try that example. Let us say I will write one part of the sentence in capital letters and the cow jumped. Asif, why is there a U after NLP and then after? Just unicode it. That's it. It's a basic thing to write it in a standard format you don't have to if you don't do it it's just as good but i tend to do that so uh guys look at it this was i think your question do you notice that their capitals and the cow jumped so what happens and the cow jumped but it's lemmatized form it will make it into small letters very nicely so i think asif if you if you have a noun it won't it will stay as capital so if you say it's a cow named cindy jumped over the moon yeah keep the c is yeah you see look at this cow and became little cow yeah what i'm saying is if it's a if it's a proper noun so if there is a name it's a proper noun yes because the proper noun is the moment it comes to the conclusion that it is a proper noun uh so let us try that. I left at support vectors. Let's add that. I added a proper noun. Let's see how well it does. Did you notice that proper nouns are preserved? Right. And let's look at the part of speech for that and you now notice that a proper noun is identified as proper noun. Do you see that? As if. Yes. Somebody's asking a question. If you named the cow, let's say a verb like or an adverb like speedy, the cow named named speedy will it lemmatize the name speedy into speed and so let's try to so tell me what variation you want me to make to the sentence and the cow named speedy let's try a lot and see what happens how well it does and what happens, how well it does. And see, speedy is an adverb, but it was smart enough to know that it is a proper noun. It's a name. Oh, that's amazing. That's amazing. It's amazing, yes. So here's the thing, guys. The state of the art in NLP has advanced to an amazing extent, especially in the last two, three years. So here's the thing. It pretty much exceeds all your expectations. So someday you wake up and you just are amazed at the power of NLP. Then at other days you wake up and you're depressed at how fast it still has to go. And you'll if you are in this field, you'll just keep bouncing between the two extremes. Asif? Yes. What is this greater than 15? Oh, this is Python way of text formatting.'ll explain to you see this part you understood right so uh you're just looping through the tokens whenever you put a colon so this is the value this is the token dot thing whenever you put colon you give the formatting you are saying the greater than sign says right align it right align it and for this text give 15 spaces so you can imagine that 15 spaces have been given so let us play with it and see what happens suppose i make it less than 15. see what happens do you notice that the first column is now left aligned do you see that i don't know if you saw the difference this one is left aligned now okay so left bracket if i change this to 25 let us see and then see what happens do you notice that now it has a lot of space from here all the way to here 25 spaces right and if i do this i will end up see what happens if i do this what what just happened i center aligned it center so yeah python string formatting is just amazing it's's a really like it actually. Isn't ml 400 proper noun and it's taken it as a noun. Hang on. Let's go up and see. And l. It's lowercase. Yeah, it has treated. Because based on the context, right, it has figured out that it seems to be some kind of object. Cow, horse, duck, ML400. Isn't it the name? The name of the workshops wouldn't be proper noun? Yeah, in English English I suppose so. But see this is getting a sense of it, right? The natural language processing is trying to say the horse. If it was not the thing, it could have been the horse. Right? And horse would have been a noun noun so the context makes it look like a noun uh is it a noun because of the word the before i know 400 that is right exactly so the context makes it into the noun here see you don't see the harini right you say harini right so your child uh your uh for example your child may say i love harini your child won't say well okay i love mother or something but your child won't say i love the harini okay sir thank you that's So contextually, you have to look at what it is. And that raises the question, what happens if I remove the, let's go and do that. I love. Now, if this could be a name. I love Henry, I love John, something like that. Right. So now let's see what happens to the whole thing. Ah, now you see that it has, now the context of the statement makes it look like a proper noun, isn't it? I love, I don't know, Jane, James, whatever. And that's what it looks like. Does that answer your question, Harini? Yes, sir, thank you. That is it. So I will go back and change this, the 400-year chart so that we can go through this. And then, so this is impressive enough but then comes something even more impressive take a sentence i took a sentence which is let us learn nlp deeply at support vectors which i hope is our joint motivation and what it does and this is very interesting guys this is deep actually it is not just a pretty visualization so i'll spend some time some time talking about it see when you have a sentence the fulcrum you know just as if you have a seesaw or a liver the most important thing is the fulcrum, isn't it? In the same way, in a sentence, the linchpin, the fulcrum that holds the sentence together, is the verb. If you really think about it, if you remove the verb, you will do the most damage in removing the meaning of the sentence. So for example, if you have let us nlp deeply at support vectors you realize that what let us what isn't it but if i remove nlp from here let us learn deeply at support vectors it still makes sense if you remove deeply it still makes sense do you see that right So the verb in a sentence is the root of things. And things go from there. A verb needs a subject and an object. Are we together? So what is the subject of the learn? You see an arrow here going N sub, N-S-U-B-J that it is saying for this verb the subject noun is us. Noun or pronoun is us. And the object noun or pronoun is NLP. So us, learn, NLP. If you just put these three together you realize that you pretty much get the essence of the sentence. And the other things are after that, the modifiers in some sense, deeply learn deeply as an adverb and so forth, at is a connective and so on and so forth and proper nouns and so forth. So are we together guys, which is why you notice that most arrows are emanating from learn. But then there is another verb here, which is let. The let verb, there are two verbs in the sentence and if you observe carefully, things always start basically at the verb. The roots are always the verb. So this verb, let, is feeding into the verb learn. And learn is feeding into just about every other thing here. So in this graph, what is the root? The root of everything is the verb let. Let leads to learn and learn. So let learn. These two together, these two words together are the foundation of the sentence and everything emanates from there. So this is a very crucial thing in the linguistics. It is called a dependency diagram. And I would suggest that you play with many sentences and see how it one out I say text is equal to and let's see what happens when I do that now observe it carefully what happened the cow jump do you see that the root is jump everything starts from jump the noun the the subject noun is cow the object noun is where here right the object is here p object over the moon right the preposition and then the determiner and then here's the determiner. The cow jumped over the moon. So you see the structure. So in the beginning, the first time you see it, it looks a little bit alien, but if you play around with quite a few sentences, you'll begin to love these dependency diagrams. Now, these dependency diagrams are crucial. These days, people are trying to create all sorts of semantic web of knowledge. I myself, for example, am looking through specific subjects and from there trying to extract sort of a knowledge graph, a semantic graph. So when you look at the semantic graph, the crux of it often is to pick out the verb, pick the verb, and from there pick the subject and the object. And if you get these three triplets out you pretty much are beginning to get a sense of the right so cow jump moon they they capture to some extent the knowledge in the sentence right so that is that so when you make these dependency diagrams you can make it just the default one, or you can sort of prettify it a little bit. So suppose you have a sentence which is a bit long. This sentence, by the way, is from one of my favorite authors, George Eliot, who, despite the name, is actually a woman and one of the best authors in my mind. So this is from a book, Mill on the Floors. I don't know if anyone is familiar with the book. So in that she says that we would not have, we would never have loved the earth so well if we had had no childhood in it. And then she goes about talking how we developed a bondage how do we develop a bond with the earth right how deeply we begin we have developed affection for the earth because we a play as a child on it so anyway uh this is a pretty long sentence so what we will do is you can run it and when you run it you can get if you notice a pretty maybe I'll make it a little bit smaller yes I hope it is still visible it's a long sentence and here once again do you notice that all the arrows seem to emanate from the verb? So if you read this sentence, we could never have loved the earth so well if we had had, and this is also an important word, but this is the root word. And what is the subject? What is the subject here? We? What is the object? The earth. And let us say if it captures that love, subject, noun subject is we, and the object is, where is the object? Earth. Here it is. The object. And so you can make these diagrams. Notice that I did not use the default. I changed the background color to maroon. I changed the font. I changed the distance. Maybe I make the distance a little bit shorter and run it again. The whole thing looks more compact, right? Font is this, so on and so forth. You can play with all sorts of aesthetic elements of this. And so you can give those aesthetic elements as options, just a dictionary of options. But you could do that. So this is the dependency diagrams, guys. It's very, very important to make these dependency diagrams. Another activity that you do is create, extract the name entities. So let's read this sentence. Let us learn natural language processing deeply at support vectors. This Silicon Valley workshop is a great place to start. It's a sentence. So what are the entities here? You would agree that it is location is Silicon Valley. It's an entity or organization is. So this is it. When you run this. It will highlight it, you know, it will display it for you. So one of the things you have to do in this project in your web page, in your web application is whatever text you are given, you should output this. With all the entities highlighted and itemized. Am I making sense, Guy? Yes. And so it does a very good job of finding the entities in the text. So I'll leave it as an exercise for you. A very simple way that you can compare to text or decide if something is similar to something else and how similar it is, what you could often do is take the text, take the sentence, and limit it to just the nouns and throw away the named entities so that you're left with relevant keywords. Then you create a keyword vector. You take the word embeddings of the keywords, and now you can compare a sentence to other sentences and see how similar it is to other sentences. It is a very, actually for something so rough and ready, it is a surprisingly effective finder of nearest neighbors. And in fact, it can help you classify our text into various categories. Like for example, it can make your first order or simple classifier into subjects and topics and so forth. So all right, guys, I will just review what we learned in spaCy so far. We learned that spaCy is easy to install, go install spaCy. Then spaCy is the visualization framework in spaCy. It comes with visualization built in. Whenever you have this spaCy, it's a library, but you need to load a language model that has been learned by looking at all the text of the internet. So those models have been learned with great effort. So you want to download the model if you don't have it. Generally, all the literature, the tutorials, etc. They ask you to load the small version of the language model. The reason is they want to make sure that their tutorial you can follow through and it works on any hardware even a 10 year old hardware i suppose but don't do that you all have good laptops then load the large version the large language model the reason to load the large language model is that you get word embeddings it basically comes with all the pieces you don't have pieces missing it's not that you get word embeddings it basically comes with all the pieces you don't have pieces missing it's not that you went and bought a car and you find that you have a car with one seat and the other passenger seat and the back seats are missing now that wouldn't be terribly useful so uh it's like that so if one quick question on the downloading the model that you said, right? Are there any caveats to how to download it? Oh, yes. From within Jupyter? Yeah. When you do this, it will go and download the model to you. Oh, just asking for the load would trigger the download as well. Yes, it will trigger the download. And the first time you have to just twiddle your thumb because it will download a massive model okay so mine okay i was following through but it didn't work i'll troubleshoot it later okay then i might have missed something let's go and look for all these things this should be on the very first page of this thing maybe i missed something because i you don't do it often it isn't the first time you try to set up the environment so you do it maybe i missed a step let's go and see installation yeah here we go what do we do oh now that pip installs basically install using spaCy let's see maybe i missed that uh no this should have worked uh gpu compiled from source clone so uh by default it installs the small version a little bit louder no by default it installs the small version oh yeah by default yes that's right it installs the small version you have to download the bigger version yes, that's right. It installs a small version. You have to download the bigger version. Yes. Now that you remind me. That's true. That's true. By default. So the print it at this moment, let's put SM. It will, that will come built in with the thing, but the bigger models you have to download. Here's some instructions. Now I downloaded, but I think there's something else going on. I'll troubleshoot it later. Yeah. Let's move on I should work again sorry a little bit louder please yeah he can restart the notebook and it should do that I didn't do a restart okay yes yeah models and languages so here's the thing hopefully here it will tell us spacey load import yes this is a command guys so here right you don't have to do small but you have to do this part this command is essential give this command i must have given it at some point in forgotten en core web underscore lg so download the large model and only work with the large model, right? So there is you'll hit limitations. Are we together? Go with that. So usually I mean, the other side, the multilingual model. Go ahead. Shouldn't both of those be underscore lg for the large ones yes yes it should end with underscore lg and here also you should say underscore lg so this is the point i'm making that all over the spacey tutorials you will see them using the small model don't use that in practice you would want to use the large model yeah so i pasted the command in chat so you can use that to download nice so i'll give you a practical experience there's two models that you'll use a lot one is the n core web english core large model and the other is the multilingual model you tend to use these quite often. So download these two models. I just wanted to chime in. What Anil said worked. Restarting the kernel was the trick. Okay, thank you. Nice. All these are a pre-trained model, correct? Yeah, these are pre-trained models. And so now going back to that. So once again to review. After that, whenever, so what happens is that NLP here, whenever you load spatial model, NLP here, this thing you'll find the very common usage pattern, It's always called NLP. It is your natural language processing model. Think of it as the model. Through the model, when you give it a text, you're applying the model, you're asking the model to work on the text. So it will produce the result. The result is a document. It is in the specie of type document. And then this thing you take, I think here I've done that more extensive job. It is of this instance, basic dot tokens that dot the top. You can print out each of the words you'll see in the tokens, full stop and punctuations and everything is the token lemmatization is in some sense canonization or the standardization of each word you go to the root word of which you see many variations run ran running yes things like that so look love loved loving and so forth these are all different forms of the root word love so with lemmatization you get the root words where is the lemmatization is used i'm sorry where is the lemmatization so how it's used in look Oh, look at the sentence. The moment you create, when you apply an NLP model and you get the document, it contains everything. Each of the token contains the lemma, you know, token.lemma, do you see this thing? Let me blow it up a little bit more. Do you see token.lemma underscore? Do we? Yeah, yeah, I see. a little bit more do you see token.lemma underscore maybe yeah yeah i think that is it so no i mean uh how it's useful this one oh how it's useful you'll see see when you're feeding things into classifiers and so forth quite often you want so for example um very very often right if you're doing knowledge graph extraction and things like that you want to go with the standard form especially for verbs right and things like that you do you do need that you're extracting those triplets and things like that so uh i mean i off the top of my head what should i say uh standardizing a text to its lemmatized form is almost one of the most fundamental things you do and you do it quite often just before you do something else okay now that you have done that the other is part of speech. These things, these libraries are pretty good at understanding the parts of speech. You can see I love the MLP workshop at support vectors and the cow. You notice that we try to do all sorts of mischief we put capital letters and we put a word speedy which is an adverb or maybe objective what is it speedy the speedy car would be an objective okay so um um but it is able to detect that it is actually a proper noun, a cow named speak. So it is a pretty, you know, robust tokenizer, you know, part of speech tagger, right, tagging. And don't be I am impressed. You see this, it's not easy, right? It takes quite a bit of intelligence to get these things right. The next thing we learned is that you should visualize a sentence to see its core structure. And when you look at the core structure, you'll find that the core structure is often rooted at the fulcrum or the linchpin of a sentence often is rooted on. In fact, almost always is rooted on the work. You have to look for the words and from there you will know the subject noun and the object noun and so forth. And you begin and the rest of them then begin to come in the adverbs, the adjectives, the prepositions and so forth. a preposition sense of words sometimes it is thing similar to attention is given to that particular word right in many ways you can think of it like that right with the verb actually when you do this you will see that whenever you create an attention model over the sentences the verbs are important the sentences the verbs are important they'll get highlighted so the nouns will go and highlight the works and so forth so well this dependency graph it is this is the bare bones version but you can do a lot of aesthetics to it though i would say that the i quickly put in some um aesthetics there which if anything made it uglier compared to the simple form but your mileage may vary you can do whatever it is right so background is black, color is blue. I don't know if there is such a thing as a font size. Let's see what happens if you run this. Oh, there is no wait. Yeah. Well, I don't know if this looks any prettier, but you can play around with these things. Then there is such a thing as named entity recognition. I have made it a part of your project, guys. And so you all do these things. Now, do you already see pieces of your project falling in place, guys? Anybody realizes that we are already seeing some pieces definitely definitely so get started with your labs do your labs and incorporate this already and hopefully by saturday come as teams and get together see guys at this moment i'm a little bit depressed that you guys haven't formed teams yet go form your teams and get started look on the slack channel what is that look on the slack channel there's progress oh there are people are posting on the slack channel okay good let's do that guys so this is it and then i'll today, I suppose it's your first day with NLP. So we covered this one. So this is traditional. A lot of the standard things we do in NLP, translations. We learned about language translations. We can all learn about some completions and so on and so forth. So next week I will go into transformers. I will repeat it but I'll repeat transformer and the attention model more slowly. When we did it in the fundamentals, if you remember within a week we covered it all. We covered attention, we did transformer, we did BERT and by the end of the week I think most of you must be feeling rather breathless. So now is the time that we'll go more slowly over it. The other thing is I'll release the quiz over the material we have learned. So please go to your space book and review the material. I will give you a quiz on the fundamentals the linguistics aspects for the homework currently on the portal i see homework five and in that nlp libs there is not a spacey one notebook is that there is not there is uh yeah this project yeah i'm not able to download this project the one you showed right now but i think that's if you refresh it kate i just downloaded i've been following through on the lecture so it's there it's there okay week seven yeah that is true week sevens uh let's go let's go and make sure just to be uh just so that we don't miss out um hands-on labs and solutions let's go there oh yeah do you notice that this pdf is here and the nlp starter code is here oh it is not in the nl starter code is here. Oh, it is not in the NLP. Is it in the deep learning? No, no, it is in the handouts, you know, the same location. Oh, yeah, guys, let me just be clear because everybody here is coming from the 400 class and everybody who was in the 400 class is also here. So I didn't bother creating a new page because then you'll have to find information on two pages. I just of here so i didn't bother creating a new page because then you'll have to find information on two pages i just am adding to this page itself is that all right no no point in creating two separate pages i created the page but ignore it i'll just keep adding here. So our research that brings us to the research. So okay two things now for the week guys. We will have a quiz on NLP right. The basic ideas of NLP that we have covered so far. I will also quiz you on basically the structure of the project right. The practical aspects and the structure of the project so you should have read this document the eloquent transformers project but then you'll be able to so please take the quizzes seriously uh participation in quiz is the single biggest predictor of how successful you will be once you finish this workshop yeah it just shows that you are checking that you're learning. So do finish the quizzes and then get busy. Then the paper for this week, it's a, this paper, I will post it. Actually, let me post it on Slack right now. And I'm going to post it. Actually, why post it later? Let me paste it right now. Hang on. Let me add it on slack right now and i'm going to post it actually uh why post it later let me paste it right now uh hang on let me add it right now to the page turn editing on i can still do that it's a bit slow. I still don't see week seven on my portal. I just logged out and logged back in. Uh-oh. I tried to do it in an incognito mode. There must be some caching happening in your case. I'm fairly sure there's some caching happening. It is available, Asif. I'm able to see it now. You're able to see it now. No, but yeah, there's some caching. It is available as if I'm able to see it now. No, but some browsers. Even I don't see it. Oh no it is under a deep learning now it's not the deep learning page guys in fact i should go delete the other search for nlp hyphen one don't go into the nlp page guys nlp course pager Don't go into the NLP page guys, NLP course page. Go into the deep learning page. Welcome to the deep learning workshop. Remember, you should go here. So guys, I have added this paper, Rethinking Attention with Performance. This is the paper we are going to cover on Sunday. It is a very important paper. And we'll cover it. Anything else, guys? What was the paper again? I'm trying to figure out since the four different What was the paper again? I'm trying to figure out since the four different class portals listed on the page, remembering which one I'm supposed to log in now. Just the deep learning. Just ignore all of the... Well, there's deep learning fundamentals and in-depth introduction, and then there's introduction to neural networks and deep learning so it's really yeah it's so okay guys let me how should i do that deep learning fundamentals yeah deep learning this is the page you should be at it is okay i'll put a course page link on Slack. Once. Okay, fundamentals is the key. I see it there. Yes, very confusing. Yes. So I posted it now to and i'll pin it to the channel sorry i have to clean up and get rid of the other courses mentioned there i'll do that actually introduction the deep learning thing it's a category if you go into the category you'll again find only one course i think i don't think so so it's for everyone because I just see one category which is like deep learning. I can't hear you. Who's speaking? This is Prachi. Prachi, go ahead. I said I could just see deep learning fundamentals. That's it. Yes, I think Kate registered for an older version. So she's seen two of them. How many of you are seeing more than one deep learning course? Anybody else? Raise hands in chat and the participant. Yes, you put it in the Slack. Yeah, put the other one. So I put it on Slack, the one the course that really matters. Just one as a deep learning fundamentals. Deep learning fundamentals, right. Okay guys, so then in that case, there it is. You can look at the, so what are the things? You go to hands-on lab. There are two things to do, read the PDF, play with the starter code, right? So I'll be releasing the quiz. Are you guys ready for the quiz guys now on basic things what is lemmatization right what is a dependency graph what is pos tagging do we understand those concepts yes that's right the concept of a language model what in the world is spacey and things like that so and a couple of questions on our project what do we expect in the project how do we write a basic rest application which we just saw it is just a few lines of code so we do things like that in the quiz and then this paper is very important on Sunday if you have time do come and listen to this paper the most recent paper listed rethinking attention with performers rethinking attention with performance yes okay this is it so what they have done is those of you who did ml 200 and did the kernel methods you know the in support vector machines and so forth, you will actually have a slightly easier time, actually a much easier time understanding this paper. And it's a pleasure to read actually. So we'll do that. So with that, I will finish. Any suggestions, guys? Oh, by the way, I should stop the recording, but some of you still haven't paid. A couple of you have requested a direct bank payment, but there are a few of you who are pending payments. Please do go register and pay. We are at the end of the first week. So please do pay for the course. I'll be sending out notices to you if you haven't paid. It's all awkward and loose. Sorry, I didn't hear that. All right. Do you have that file, that flask file you were showing in the beginning? It's also there? It is all there. It is all there. It is called simple.py. Oh, simple.py. OK, thank you. With the hope that it really does appear simple. This file. So we need to download the week seven code correct week seven uh assets yes uh as far as the series concerned uh what would we go over in these couple weeks besides um attention see we will we will do sequence models in depth we'll understand what a recurrent neural network is in a fairly good depth we haven't done rnms lstns etc so much i won't focus that much on them but we should understand our encoder decoder uh thing slowly it will be quite a bit of It will be quite a bit of... the coming week will be a review of what we did in the part one. Okay. But we'll do it at a slower pace because this time we are laying deeper roots. And how relevant are our... in the modern world, I guess? How relevant are they in modern world? Yeah. Oh, NLP is the hottest area of activity at this moment in machine learning. Is that RNNs or just like transformers? No, no, no. Transformers are, but RNNs are not gone. They are very much the LSTMs, etc. Quite often you make a hybrid model with, you know, backward and forward LSTMs hybridized with transformers. So people experiment with all sorts of interesting architecture. See, while transformers have a lot of positive things to them, it isn't that they are LSTMs, etc. Those things also have some advantages which transformers don't have. So today, the way we look at it is that we use all of it, whatever architecture you can get. Having said that, one of the deeper realizations we are coming to is that transformers may be even more fundamental than a fully connected neural net. I'll talk about that. I mean, now that we are willing to transformers in detail, I will go into the conceptual or theoretical depths of it much more okay and this is important because unfortunately there is no good a good textbook on transformers most of them are just articles that you find in medium etc some of those articles are very good but none of them go into great textbook like depth and transformers so i would like to go in such depth because frankly the future is on transformer and its derivatives performer reformer and whatnot long former and whatnot right if you look at gpt3 everybody is everybody is either running around scared or running around impressed at the abilities of GPT-3. Another transformer that's been released. So we'll do transformers quite a bit, guys. From a practical perspective also, we will do impressive things with transformers in the lab part of it, so it doesn't mean that, because we are doing this project we won't have some labs I will still do guided labs and some of the more impressive things you can do with transformers. Vaidhyanathan Ramamurthy, Alright, so I will now stop sharing.