 Welcome to Lab 5. Today we'll be going over two notebooks. One is the MNIST digit recognizer and the other one is a classification task on the Wisconsin breast cancer dataset. So to begin with, let's download this folder lab5 from hands-on labs and solutions on our course portal. Unzip it next to lab3 and 4. It has all its dependencies so it will work fine without any additional notebooks from the previous labs. So if you downloaded it, we can start with lab. You can start with the Wisconsin breast cancer. Okay. So, the goal of this data set, the classification task is to find, so there are a lot of variables in this data set, right? There are many variables that kind of help you predict if the cancer is malignant or benign. So that is the goal of this, I mean, that is this data set. So based on a certain feature, based on certain features, we wish to predict if the cancer is malignant or benign. And we also want to know what are those features that strongly affect this classification. So those are the main tasks of this notebook, but for neural networks we are going to restrict to finding a good accurate model and not go much into the other methods that we use classification. So, again we start by importing all the necessary modules and libraries. So this dataset has 10 real value features, one for each nucleus. And the attributes are radius, texture, perimeter, area, smoothness, compact compactness concavity concave point symmetry and fractal dimensions and now there are three types of cells as two types of cell nucleus so we'll be that so there are total of around 30 variables okay for around 30 variables okay yeah so for to begin with we do an explicit data analysis so we just first we start by loading the data okay and we say data.info and then we see how our features look like so what are they so um so we did have these 10 features now they repeat for three different types uh three different types so one is the mean one is this standard error and then the worst so uh so sorry they're not the different cell types, but they are different statistics. So we have this information and there are 30 of them. These are the 30 features we have. Next is we see that the data type of every column except diagnosis is flow 24 since each column is a measurement, which is their numerical variables. Let us take a close, let us take a look at the column unnamed 32. So unnamed 32 is, there's nothing in unnamed 32, right? It's just, these are all NAND. So we can remove this feature and we can also remove ID because ID is not required for our classification. It doesn't add any meaning to as a feature. So we remove both these columns. And we just drop them in place. So these are all some pandas related syntax so so in this we just drop drop these columns okay and now we describe these columns so this is again these are all pandas syntax so, what we do here is we want to see some information, some descriptive statistics about each of these features. So, let me start with count. So, there are a total of 569 values in this feature. And then there are two unique values, which means is it malignant or benign so this is this is a target variable okay and then it tells you since it is it it is treated like a classification uh sorry categorical it um it just tells you what is the top uh value and then what is its frequency okay for the others these are all uh numerical variables and therefore unique top frequency is not shown but rather what is the mean value what is the standard deviation in max and then uh some of the quantiles okay so some of the quantiles. So we have this information. So we do see that everything shows 569, which means there are no missing values. So that's pretty good. And we have one and we don't have any other categorical variables in this data set. So these are things that you will sort of need to work with when you are working on real world data sets they they're not as clean as toy data sets right so we we do notice that all of them are numerical so we don't have to do any label encoding or you know one hot encoding so okay now let's just take a sample data dot sample dot transpose the sense i take five different samples and then sorry 10 different samples and then i do them again in the same manner just to see okay how does it look so we have m for malignant and b for benign this alone I think we will have to change into zero and ones because we want our target variables to still be numbers and as n categories instead of letters so let's okay let's do that. So the next part is we do missing value analysis. We see if there are any missing values in this data set. And then it turns out that there are none. So that's good. And we also see under data.diagnosis, we see what are the unique values that we have here. And they are malignant and benign and they are of type object so let's we'll have to change that okay so data imbalance check we see how many how many values of malignant and how many values of benign tumors do we have? So we have around 212 out of the 564 or so number values. 212 are malignant and the rest are benign. So we observed that the data set consists of more of benign tumor diagnosis than malignant tumor. Any classifier must do better than the baseline prediction of 357 by 579. Okay. So, what this says is that, okay, if I just, I see that benign is the maximum i mean it's the largest category among these two and any any class i mean any test data sample i get i automatically just classify them as benign because they seem to be the highest i mean they seem to be the maximum category here. So I can do that. And that is a baseline prediction. And that should, so that is 357 by 579. Kate, do you want to say something? Yeah, that's baseline. I think it's also called the 0R when just looking at the data and not in the predictors right yes another yes yes so um so that turns out to be 63.4 percent so we need to do better than that because that is just classifying very basically right so yeah that's the zero r classifier okay so then what we do is we uh first of all we map m's to zero and b's to one okay so this is a way of one hot encoding this is simpler way using map now uh to the next part okay have i been running this notebook? I don't think so. Let Okay. So the first thing we do is we separate our features into the features means feature standard errors and feature worst. And then we compare them amongst each other. the features means, feature standard errors and feature worst, okay? And then we compare them amongst each other. This is not necessarily any, we could compare all the features together, but then you see that we have 30 features and comparing all of them together, it is a lot. So we kind of separated into cat i mean groups and then we uh uh analyze them so here what we do is um okay this is some panda syntax we melt the data frame in order to easily visualize okay so these are not necessary for this notebook, but yeah. Okay, so now what we do is a violin plot sort of, it is like a histogram, but then it's like a smoothed out histogram. And this is one type of visualization where it's kind of easy. It's easy to the eye to see the distribution of the data. OK, this basically tells you how the data is distributed. And now the thing to notice is that zero is malignant and one. Wait, sorry. Zero is the nine and one is malignant i think that's that's how it is in the that has that how i coded it oh sorry no what i have done is i've done malignant as zero and benign as one okay so which means this orange color that is malignant tumor, and this is benign tumor. So this kind of shows you the distribution, okay? And it tells you for radius mean, if I kind of separate out my malignant tumors and benign tumors, okay, based on malignant and benign. And then I plot its distribution, okay, radius mean for malignant and radius mean for benign. This is how it would look. So it kind of like it's mirroring this distribution on top of each other, I mean, side by side, and then we're comparing how they distributed. So this sort of shows you that in case of radius mean, the mean of this distribution is well separated from the mean of this distribution. So we do see that several of these variables have that. For most variables variables we do see that there's quite a lot of separation between um you know malignant and benign so maybe except for this which is fractal dimension mean we do see that it is not this feature is not actually helping you much right it's not it's not actually telling you a lot about the a lot of difference between the you know malignant and benign tumors so this is one thing that we do notice here and this is that's why it's good to visualize your data and sort of understand it okay so yeah i am not understanding. So given that the means are are not centered, for example, the first one, what does that really mean, though? What it means is that when you plot this data, right, when you, for example, you do a scatterplot, which I have not done, what you would see is that there is the there are points like if you make a scatter plot the red uh the malignant tumor points would be well separated from the you know from the binocular okay when you have radius um in this case what actually the point is you cannot make a scatter plot against say zero and one so what this would look like is this would look like a line okay and then you you have some points on zero and some points in one. And then you plot the radius mean for all the points that, I mean, for all the data points that have zero as their, or say diagnosis, which is like zero means here in the case it's malignant so what what it looks like is like you have zero here and then you'll have one here and then you'll have like a line over line on which you'll have a lot of points jumbled up together and what happens is if you draw the distribution of those points uh that's what this looks like that that is what this shows uh that's what this looks like that that is what this shows does that make sense so what you're saying is that the orange line on the this radius um uh what you're saying is that um it's going to be appearing on us one straight line and that too not an arm like is this like a like is this like a curve equal to x equal to let's say one right uh let me just uh i i wish i can draw it somewhere wait let me yeah okay so what it would look like is you have um say zero okay and then we have one okay and now on this axis i'm going to plot my radius mean radius radius. And this is going to range from some number to something. And now if I plot these points, because I mean, I'm going to plot them like this, and then they're going to be on this line. And I'm going to plot some of the benign tumors here. I mean, these both come from the same variable, but I'm just splitting them according to their tumor. And now when I plot the distribution, this happens. They have a different distribution, which means malignant tumors have larger radius and benign tumors have smaller radius. That's what it means. Does that make sense? Yeah. It's a nice sketch. So, so Kyle, why, why do you have to build this complicated violin like thing? It's very straightforward, right? The larger the it's very straightforward right the larger the tumor i mean the larger the radius more yeah this is i mean you can compare them right like how else will you compare these things like what other plot can show you the separation uh this is one right yeah this this would also yeah that looks better yeah this is easy this is easier to tell uh okay i don't know it's maybe maybe that looks more jazzy but once you kind of went in deep then then i could get it i was what is going on right so you're okay okay but it's yeah now that when you explain it then it makes sense yeah i mean this does explain it but uh once you understand the plot you can quickly understand it from here because here okay i mean how do you i mean you do see a well separation between these two but you know this is like easier on the eye as well you know once you understand what a violent plot is it's easier to understand but yeah so this is the other way to represent it where say for radius mean this this is um it's a box plot okay so this line shows you the mean mean of uh the benign tumors and this is the mean of the radius i mean uh malignant tumor and we do see that for each of these variables it is again i mean it shows the same thing but uh it shows in a better way i guess like albert says so uh they are well separated and and then we do see that so the top and bottom line they show you i mean this this total uh total box it shows you 50 of the points the center central 50 of the points and these lines show you the remaining 50% okay so we see that what are these rhombus on top of them those are outliers points that are like further away okay so also the end of that line right is that like the last item or here like box is the 50 of the center population and then what is 25% and this shows you the first 25% that should complete the 100% that should complete but what this software does is sort of if it if it stretches it till here it doesn't make sense but this these are where most of the 25 percent of the point lie okay and then there are just four four additional points that are further away and there's no point in like stretching it till here so this is where majority of the 25 of the points lie and then there are a few more points additionally outside and then there are a few more points additionally outside so kyle to what abhishek was saying right so if you take the red red box the total line from top including the outliers so that is the whole whole uh total of all the points yes yes yes that is the that's 100 so and then you have uh the the mean dimension of benign once is in that box and the remaining ones that are not benign are in the blue box correct or am i kind of yeah yeah okay so so why this total is like like for continuity means like 579 points Okay, and then here we show some I think this is like to 200 points, and this is the 300 points remaining 300 which are. These are benign tumors, these are malignant. So Albert? Yeah. Is there anything else? Yeah, I think that's clear enough. Thank you. Okay. Okay. Okay. Okay, so we do the same for the standard errors and then we see how well separated they are. Okay, let's look at the box plots. And this one is not as separated as the one we saw before visually okay and similarly we look for the worst features okay here we do see a lot of separations right so again so yeah we plot the box plots and then we do see separations. So what we get from this analysis is that not just one, but almost all of the features tend to separate out the data set pretty well, okay? Right, and now what combinations of them should we choose is the question, and do we choose all of them? Do they, does each of them separate the classes to an equal extent or to what extent? So those are some of the questions that we should have in mind when we go forward. So next is we do a correlation study. And what correlation does is, so there are 30 variables or so um i want to see how correlated these are i mean they must be right we we look at it and we do see that they seem to separate for in this case in all of these cases we see that So, in this case, in all of these cases, we see that malignant seems to be having a higher radius, larger texture, more texture, more parameter, I mean, perimeter and a lot of, I mean, larger area. But these must be correlated because if I have a larger area then that automatically means that i also have you know a longer perimeter right so these must be correlated in way and similarly radius must also be larger so therefore we should see some correlation between these variables and that we should note down before we go before we move on because um because that that that's the case of multicollinearity where we have variables that don't give additional any more information than the others we can just remove those uh variables that you know if if for example radius mean kind of explains uh instead of perimeter and area i can just choose radius instead of having perimeter and i mean area right so they don't add any more value to the model so for that what we do is we do a correlation study and a correlation correlation is like if I have correlation one which means. What those videos are exactly the variables are exactly the same if they are somewhere between 0.75 and minus 0.75 they are correlate i mean as as we go from one to zero if i am at zero then i'm sure that okay there is no there's literally no correlation and you know and then again if i go to minus one which means that um those two variables are negatively correlated as in for example as I increase the radius the texture sort of goes down I mean that's not the case but you know that is that's one example so okay so we view this plot and then we see these colors. So this, this means that it's, you know, highly positively correlated. And we go down, this is highly negatively correlated. So both these colors, you know, tell us that there is high correlation, there is some sort of strong correlation. tell us that there is some sort of strong correlation. So we look for those values. We look for something that's really black and we look for something that's really white. I could have used some other color scheme here, which would be a divergent scale. That would have actually helped so i'll make that change so that we can better visualize this part okay so here um we do see that there are perimeter and perimeter mean perimeter worst area mean uh perimeter worst so these points these these two say these two features seem to have high correlation similarly it radius worst with perimeter main radius work with area mean so all these sort of are correlated so we do know that right like the the cancer is not like exactly spherical so radius versus like the worst radius so and that sort of influences the mean area so so kyle whatever whatever is red means it is a cancer cell in this whole i mean orange uh sorry can you repeat again so you see the color combination right so the the orange means is it cancerous is it positive no. It only means that there is high correlation between two variables. So this doesn't tell you anything about the cancer because all this just tells you the correlation between two variables. So area mean is one variable. Radius mean is another variable. Are they two correlated as in if i increase radius will my area increase by and does it like is it like completely influenced on the radius alone so these are these are this is what correlation tells you right so it does look at that area mean and radius mean they are highly correlated okay so the color means nothing here the color what it means is how how correlated are they that's what it means so if it's one it's like white tissue in color the correlation is like really high if it is black then it's like totally negatively correlated okay so okay thank you so we do see that there are a few variables that are correlated like that that are that have really strong correlations. So, let us so we have seen this visually let us go and like look at the number correlation numbers and then just highlight the ones where there is the absolute value of the correlation is more than 0.85 which means that the correlation is less than minus 0.85 and greater than 0.85. So we look at that. And so I've highlighted those. Obviously, along the diagonal, the correlation is 1. That is because diagnosis and diagnosis are definitely correlated. By increased diagnosis, diagnosis will increase. Sorry? A variable is always going to be very highly correlated to itself yes yes so so that that's the thing over here on the diagonal that's going to be the case but let's look at the other ones and we see perimeter is correlated with radius mean so there are other correlations as well okay so i've noted these things down and and i'm removing them i'm removing of not not both of the correlated variables i remove one of the correlations right and then so therefore I've removed a few of the variables that I think are not required for this model. So I've removed them. And as the next step, I am going to do some standard scale. I'm going to scale my data. So the next point is to visualize it along. So this is like what some i've removed some eight variables here so i did i still have 22 variables left um and which means my data has 22 dimensions and it's very hard to visualize in 22 dimensions i would want to bring them down to two dimensions and visualize it in order to to do so, we do principal component analysis, and principal component analysis just tells you, it just gives you, say the first feature tells you along which axis, if I slice my data, will I see the maximum variability? So where does my data vary the most? In which axis does it vary the most? So if I take the first two components and I do PCA, I see something like this. So now this axis is not just one of the features, it is a combination of the features right and in in some multiplied by some scalars okay so these are basically the i the first eigenvector and this is the second eigenvector okay and so this is how my data looks I have not added the labels. Sorry about that. So one of them is malignant and the other is benign. Okay. And this is like, I have not done any other processing. I'm just viewing the data along its principal components. And already I do see that there is almost a line that can separate them. I mean, not completely separable because there is overlap, but I still can draw a line. You can usually see the line that could be drawn. And this is what we will be doing okay and now we also see uh how many principal components kind of explain my very explain the separation okay uh i see that around one two three four five six six components six uh says six of my principal components explain about 88% of the variance in the data. So from then on, the contribution of each other component of the remaining variables, principal components is less. So these explain a lot. So we can probably use this for our model. You can try out things like that. So for my neural network model, which I'm going to train so that it can distinguish between benign and malignant which I'm going to train and so that it can distinguish between benign and malignant I am going to transform my data into principal components and then apply the model which means the first comp the it will still have 22 features okay you can probably reduce the number of features to six uh and try it out as well uh it will have 22 features but the 22 features are all principal components okay so uh yeah so here i'm importing things for my neural network and this is just uh you see why we do it okay i'm going to save if okay if i'm training my model today i want to save the model so that i don't have to retrain the model again and again so i'm going to save it with with the date and and the number okay this is for that now i do the same thing as before I load my data into simple data set, which is a PyTorch object. And then I divide my data into train and test. And so I train with 80% of my data and I test with 20%. So, and also I randomly select, so I randomly split. I just don't split like first 80 of my first 80% is my training set. And the next 20 is test. I randomize, I randomly select them. years test i randomize i randomly select them okay so 455 examples are for training and 114 are for testing now i print out just one value of the first value of my data set okay this is the complete one so i and then what i see is that i see um 22 features then what i see is that i see um 22 features okay this is the feature vector and this is the label so the label is zero which means in this case it is it is a it is a cancerous this is the malignant tumor okay so here i'm going to set some hyper parameters for the model. Right. So, and then I've highlighted these points that can modify these things right. What do you have to modify wherever you see these arrows you modify them, so when you see the in features, now it must be 22, right? There are a total of 22 in features and there are two out features. And why is that? Why do we have two of the out features and 22 in features okay so i start by just give the batch size is one which means each training example is trained uh one at a time okay and i am going to comment out this part and i'm going to train with train with just one with an input layer and an output layer alone so we know that this would this would just be a linear classifier and this would just draw a line between the two features. So let me train this. So this cell is just for you to set your hyperparameters. Now we train our network here okay so there's another option which i will explain uh after this cell uh is to is to like load your saved model and then train it from there instead of setting new hyper parameters every time you might have found a good model that that works really well you might want to save it save it and then reload it and you know works really well you might want to save it save it and then reload it and you know try to improve it further this is for that so here okay i'm training my model and i've just taken 10 epochs so it's gonna be fast and it's just two layers so okay so um and my score function is accuracy here. So accuracy sort of tells you how many points are classified correctly, right? So that is kind of like my metric here. And I'm just seeing how many points are classified correctly. I seem to have classified 90 98 accuracy in training but that's that is definitely not what we want we want to see the test accuracy uh the test accuracy seems to have converged so it's not improving any further maybe if i restart the algorithm it's going to start from a different point it might converge better so that's something you'll you'll have to test uh in this case the best i could do is 95 but is 95 good enough to classify between malignant and benign tumor is uh probably not we want to be really sure right and probably accuracy is not what we want to measure with um right there are other metrics which we will learn in ml 100 so in this case i've just used accuracy um okay so this part is is to save your network um so i'm creating a dictionary checkpoint okay and i'm going to pass in a few few of the hyper parameters that i've said uh which is a pop and this is the model itself i'm saving the model i'm saving the optimizer and if i'm using a scheduler i'm saving the scheduler okay all this to train in train again in the future but if you want to test again in the future you can just say you can just you you can just use this part but since i am assuming that i will train this model again in the future i'm saving all this information and also the results okay i'm saving it as a dictionary file. Now, you start short save and then save it as a pth file. Okay. So and then I, I've already set the path in the first cell, right, then is using that path. But and after I save it it i'm updating my index to i to i plus one and then i change the path again okay so that every time i train a network and i feel like saving i'm going to save a different file and it will show up it will show up here so i've trained all these models and they've been saved here okay so now i'm visualizing okay let me visualize this ah so this is actually not better the one before was much better so this has a lot of ambiguity okay so what what does this what what does this like background color mean uh so when i say when when it's like dark red it means it's like very sure that it is probably corresponds to a malignant tumor, cancerous tumor. But as I move towards this sort of place where there is a lot of intermixing, the model is less sure which side should i classify the point right so along this particular line it is like 50 50 it's not it's not really sure where to classify but as we move again to this side uh we see from white it slowly becomes to light blue and then dark blue so here it's like very short i'm so it's like it's sure that it is a benign tumor whereas we move to the center it's again not very short so that's what this sort of this contour shows you right and if you worked on lab four or lab three you would see these you would see this like probably even more here it is like kind of short it is it is it is pretty good but if i the last time i ran this same uh for the same setup i think it it was better right uh okay so this is just a linear model i tried with uh i tried with this model and this model had a better accuracy and this this sort of this ambiguity was lesser which means it was dark blue till till here and then only a little part of it was white which means it the model was pretty sure of what it was doing so uh after i mean after we go through ms digit recognizer you'll have time to play around with this and find better hyper parameters right and go over this code as well so this comes to the end of this notebook and yeah one more thing you should uh also note that um i visualized it in a lower dimension. This is in the first two principal components. And to facilitate that, I've chosen these points on the principal component axis. The way this has been visualized is I've chosen points on the print on on these two principal components and kept the rest of the components as zero and that's why I'm able to visualize it like this so that's also something to know so just wondering like how this principal component to uh like the formula you have uh like some uh 22 components are combined into one principal component right yes yes you can yeah i can print that out say pca dot uh wait a minute. Components on. Just components and I think there's one underscore after it right. She's here components. Yes, so this is the first component. And it has a 0.2 percent of the first variable, 0.1 percentage of the second 0.1. I mean, not exactly percent, but the values. This is the first eigenvector first principle common and this is the second principle common and then so on we have 22 of them abhishek, does that answer your question? I got like, so each component, the first component is have around like, how many? 22. Okay, these are 22 variables. So all the 22 variables are combined into one principle component so what does these 22 principal components represents uh they are in some combination of the 22 of the original components uh 22 of the original features okay so what i'm understanding is that uh this first uh part which you had highlighted is the first uh pca one of principle component one yes and we are drawing the graph between pca1 and pca2 yes so what are the remaining ones and uh what's their signal or what we are visualizing we're using them for uh visualization in in this case i'm using it for visualization uh because this is where i see the maximum variability i could also plot between say radius mean and area mean okay let's you know let's just you can try that out as an exercise right that part i understand like if you will choose two different variables the line might not be straight but is this the it will be straight there too it will be straight there too because the model is linear the only thing i'm doing is I'm just like rotating my axis. I'm rotating my axis to a different orientation where there is like maximum variability of my data. So the significance of the first component is that this is where my data varies the most. Okay, and then, and when I draw a line here, it makes more sense because now I can see a lot of variability in the data. Right. Okay, so yeah, so PPC one is so we have like tilted the plane in such a way. Yes, we're making a slice. Yeah. the plane in such a way yes they're making a slice yeah so then what what is pca2 it is the along this is another direction where it is so pca1 is the shows the maximum variability pca2 shows the next max like the second maximum okay so and then so on so if you see this plot over here so i see that first com first uh component explains 30 percent around 35 percent we have taken six uh taken six uh no we have taken all of the principal components i mean that is not how if you are doing principal component analysis you will probably take first two three four five and then so the till it takes up to point up to 90 percent and then you do the analysis, right? You just use the first six components and throw away the rest and work with this. But in this case, I've used the entire data set, I mean, all of the 22 of the principal components. What I'm suggesting is try with just the six. See, using all the principal components is same as not using principal components at all it just means i've just rotated the axis in a different way and the model is going to output the same results nothing different uh why i've done why i've used principal components in my model is like why I've trained my model on my principal components is that I can visualize it later easily yeah just to add it even reduces your feature space so here the each component will give you the direction and magnitude of each feature how important it is so each component is becomes a you know when we do pca each each components magnitude like how important it is that is you know that is how when you do pca1 and each feature is magnitude and direction is represented so suppose it's 1.2 then the first feature is 1.2 magnitude, that much importance, magnitude and that much importance in this, you know, when you take this slice of data or this eigenvector. So it depends on, in that feature space, this feature has this much importance in simple English, if I have to tell you. So each, there are different two combinations of it that's what pca you know that's and when you see the first visualization pca1 and pca2 gives you some clear clear what to say distinction to draw your linear separator so that's why i use it our main aim is to separate it right yeah yeah so that's how it works so like what harry is saying the first principal component it has a magnitude 69 which means um it's sort of how important it stretches yeah yeah you can consider it as uh importance so the first principal component is higher 69 and then it keeps reducing right the last component not not very significant okay so kyle you are multiplying by a tan h to get this number or what is important what is not or am i these are the singular values i mean eigen eigen values eigenvalues okay there's no transcendental functions that you multiply with to get that particular uniqueness with an activation function no no these are just the so yeah this is the value to compose the eigenvalues. Okay. So, yeah, you are basically like decompose. This is singular value decomposition where you decompose your matrix your covariance matrix and and then you sort of get this eigenvector and eigenvalues. The eigenvalues tell you how much in the first principal component you stretch your data. So it's sort of that linear algebra thing. So let's not go too much into it, but we can understand that this sort of tells you the importance. Actually there's a video on PCA by Asif sir, you can watch it, because you know I think then he explains it in more detail. Okay, thank you Harini. yeah it's an ML 100 but yeah if needed, I think you guys can access it and have a look can you put that on slack if you don't mind so yeah i think one of us will put it and you can go through that and previous students whoever have done ml 100 i think you can just recap i think then once you see the diagram if you guys remember it is that you know the elliptical and how the point is in that eclipse ellipse yeah so you can just see that i think once you see that you might remember it okay any more questions? Okay. We'll take a 10-minute break, and we'll do the digit recognizer. Okay? Okay, I'll pause. Sounds good. Yeah. Okay. So the next notebook we're going to go over is the digit recognizer. So this is a top application of neural networks and computer vision. So here we train a model to recognize handwritten number digits in the classic MNIST dataset. Okay. So we start by importing all the necessary libraries and modules. And similarly here too, we are going to save the model. So that's the thing. Okay. So one thing before I want to go forward is every time you run a notebook, please set this to false. Those who don't have latex i think money is that should solve your problem and and if you don't have uh cuda please change this to cpu okay when you run this notebook you might face an error if i mean both these notebooks so uh this has to be set to false right the text are used yes right i did that and i also did the cuda to cpu and still you're getting an error uh so i think um i did still get the error but the last change that you send me i haven't done that yet which is this which already did yeah i think okay okay try running the notebook from the beginning because i think that was a tech okay okay i think uh even uh let's even kate i think she got the same error let's even Kate I think she got the same error okay I will go over it after the end of this notebook okay so so let's start with the importing and exploring this data set so this data set is available with sklearn so we are going to just use sklearn's data set okay library and then to sklearn library and this is the fetch open ml and load digits so load digits is your homework uh fetch open ml is what we are going to do uh so this is a so in this data set we we have around 60,000 examples for training and 10,000 examples for testing. So in total, we have 70,000 data points. It is a subset of a larger set available from NIST. So we are going to use the mini NIST, I guess. So the digits have been size normalized and centered in a fixed fixed size image okay it is a good database for people who want to try learning techniques and pattern recognition methods on real world data while spending minimal effort on pre-processing and formatting okay so they've done all the pre-processing for us so all we do is we say fetch open ml and we're gonna get this m list 784 um version one and return x y true so this uh this argument is to make sure that what is returned is not in a single, say, data frame, we want it separately, X and Y. And also when you say return XY, this is automatically returning a data frame. Okay, so it's returning. Yes, Kate. It's fine. Okay. That's probably a cleaner way to do it than converting the moon-py array to a data frame, panel's data frame. Yeah. So, return x, y equals true. And then this is a series, of course, this is going to be a single, it's going to be a single column. So we look at the types. What is being returned? We check that it's a pandas data frame and then this is again a panda series. Now we also check what is the shape of our file. So it's 70,000 by 784. So what is this 784? It is the pixels, right, in the image. So this image is a 28 by 28 pixel image, okay? And there are totally 784 of them. And we see a lot of zeros here, so we see, okay what what exactly does mean and so i'm viewing the first uh five values and so we see from one to 784 okay pixels i have so these are this is the dimension of the data frame so it's it says that okay this data frame has 784 dimensions okay that's how we should see it and there are 70 000 data points okay so the first thing we do is okay let's visualize this image right so given an image data so we're going to use the plot plt.imshow to view the image. And also we note that this is in a single, like in one file, right? It's 1 by 784, but our image is 28 by 28. So we're going to reshape our data, our example to 28 by 28. Okay. And then we're going to use cmap gray because these are grayscale and so and i'm okay this two by two is probably too small but yeah you can see it here um you see this this this is the number five and so yes so x dot so example comma label this is my example and label is this corresponding label right which is in y so y has all my labels and x has all my features so i'm just naming i'm just saving them in these variables and then I'm using that as my title here. So title is five and this is its label. So this is the feature. Okay. So how does our, how does one example look like? So this is what it looks like, right? look like so this is what it looks like right so um it has zero and then uh up to 250 or 256 sorry okay so um it tells you how bright a pixel should you know uh light up so zero means means the pixel doesn't light up at all, it's 0, 0. And 3 lights up a little bit, 18, 126, 253 light up like almost full. Okay. So, that is what makes up this image. And we, so, in numbers, this becomes a vector, vector right of 784 dimensions so that's how a single data point looks like now okay let's visualize this data set again this data set is in the really high dimension it has 784 features we want to visualize it again in a lower dimension so to visualize in a lower dimension we use tca right so we visualize along the in this case i'm going to visualize along the first and second principal component also second and third principal component in so i'm going to take two slices uh on my data, I mean, on my data set and view them. So before I do that, I'm obviously going to scale my data. So also I'm not going to visualize all the 70,000 points, that would be insane. So I'm going to visualize, sample randomly pick 2000 points from my data frame and then visualize them. Similarly, I'm going to select 2000. I mean, with the same index, I'm going to select 2000 of those labels and the corresponding labels for these samples. Okay. So this is for my visualization alone. I'm going to train with 60,000 points. However, so I'm scaling the model. I'm applying PCA. Okay. And I'm visualizing along the first two principal components. Okay. So now we have 10 classes. earlier we had just two classes, we have from 0 to 9 digits, so we have from 0 to 9. So it is pretty mashed up, but you can infer a few things from this plot. Along the first principal component, definitely 0 and 1 are well separated. This is 0, blue points are 0 and this is 1. Right? So yeah, that makes sense. It's a circle versus a line. Very different. Yes. Yes. Circle versus a line. They're very well separated. The other digits, they seem to be matched up around here. Right? Also, what you could probably think of is like in in this image imagine that the principal component is like it it sort of um is along this direction okay it's some somewhat horizontal so obviously one is like sort of well separated from zero because you know, along this axis, one is here, zero is somewhere here, five is like pretty spread out, but one is like pretty narrowly spread out, right? Because it's usually generally the central part. So, this axis sort of tells you the distinct kind of helps you visualize the distinct i mean visualize the difference between these digits also we see that uh say gray or this is seven nine and uh four they seem to be sort of like meshed together. And what are these points? Eight, two, three. They seem to be again meshed together. Eight, two, three, and five. Does that make sense? And then six, six is sort of close to zero. Right. Roundish. Yeah, roundish. So, yeah, we get a we get some idea of like, OK, what the data looks like. They are they are sort of in clusters, but not not very well really distinguishable right if i if i want to draw a line it's impossible if i want to draw multiple decision boundaries um i would keep i would be misclassifying a lot of my digits so i need to do some non-linear transformations I need to add some activation. So again, so this is a lot, this is a different slice, right, along principal component two and three. In this component, I see that four is like, sorry, this is three, number three is like slightly away. So when you take different slices, will see your data in from different angles right so in this angle um one is like pretty much the center but it's like meshed with all the other points and in this in this dimension i see that uh say four and nine and seven they seem to be like pretty much mixed together but they are also separate from the other other values right so our model will try to learn from all of this it will do a better job than us okay so this is the this is trying to visualize a really high dimensional data in two or three dimensions for us to kind of understand what's going on okay we do see that one thing we do notice that drawing lines won't help in this case we we definitely need to add some activation unlike the breast cancer data set where a line was pretty was pretty good it already had 95 accuracy so i mean if accuracy was our goal then it was accurate okay so the next thing we do here is we're going to load the data and i'm going to split my train in data set into training and testing so I'm training with the first 60,000 samples and its corresponding 60,000 labels and the next is next 10,000 and the other thing corresponding labels so I see that 60 000 examples and 10 000 examples for testing now i come back to setting my hyper parameters for the model in this case i have i'm using a batch size of 16 um and yeah and again so now i'm going to set my in features as 784 and out features as 10. Right. So that is that. And then I'm going to use here I'm using. So you should try to modify the number of layers here. There's one input. There there's a second third and then fourth so there are two hidden layers right and and input and output layers so you can try modifying it try just having one one hidden layer okay or we could try how the model does without any let's just do a linear one model equals nn dot linear You can try this. I think it's not probably not best to do it now. It's okay. So have I been running my notebook? I don't think so. Okay so this gives a different representation. Strange. Very strange. Why is this happening? What do we see different? I mean, it still looks very similar, but taking a different slice of the data. That's because we're using samples, right? Oh, yes, yes. Yes, thank you. If we are using the different sample make sense okay so we're going to set the hyper parameters okay and this is to load your previously trained models and okay i'm going to okay And okay, I'm going to okay. So while this is working, okay. Let's wait for it to complete. It does take a lot of time because there are 60,000 samples. It's going to take some time. Testing is going to be pretty fast. So Kyle, this is a digit recognizer, right? Yes. Okay. Yeah, it's like OCR optical character recognition like used by the post office. So, after this I just go around some of these python files which you know the notebook depends on I made some changes to the like data set classes training loops in order to accommodate you know classification so early on we were doing regression but the same loops also work for classification and why they work is because this because i say classify equals true and if this lag is on then it does it does a different slightly different calculations right so the algorithm is slightly different we just go over what changes they are okay so now i've just trained for three epochs and i see my test accuracy is 97 percent that's pretty good right um 97.7 percent um i there's this scope for improvement right you can improve this greatly uh try different there are so many hyperparameters i've not even used scheduler yet so try scheduling try applying scheduler i have used a momentum oh no sorry no i add mw you don't use momentum uh this is this is the learning initial learning rate and if you want to add a scheduler then you can try some of the schedulers that we discussed in lab 3 so try try all of that and you can definitely improve this accuracy. If you go to their website, mNIST, there's like 0.1% accuracy, right? With neural networks you can go that far. You mean 0.1% loss. Oh yes, yes, I mean say 99.9%. Okay. Yeah, sorry. Okay, so here I'm saving this model. And so again, as a storing it as a dictionary and then saving a checkpoint. Okay, saving this file as okay I'm saving this best model, you can save it, I mean, shouldn't probably be that. Okay, and if I see it will be reload, is it showing up? I think this is the one, today's 21 here. So yes, this is the model, this is the file, pth file. This is a PyTorch related file, I think. Even I'm not very sure, this only this works when you want to reload your file so torch.save and save as path okay i'm gonna change this okay um and then if you want to load my model so this is for saving it so once you train your model you do all your training you save it and if you want to like test your model and you want to test your model with like the different different models that you created so i all these lines are probably not required these lines are not required but let it be uh what you do is you uncomment these lines you uncomment these lines and then just just give the path to the uh the file that you saved right if you saved it here you just copy this and paste it here and then you will be training on that model okay and also you will have to uncom you have to comment out this name comment this just keep alternating between these two so okay yeah so also make sure that you're in evaluation mode. This should not be commented out. So in this case, I'm loading a saved model. And then I'm going to test that model here. So yeah. So what I'm doing here is that this is what I did for visualizing this part two. How do I visualize I'm using this softmax function and argmax and things like that. So what did what our model basically outputs is, it's going to give me some some numbers some if I print out logic and okay I haven't run this part yet okay so now if I say logic so it out it outputs certain numbers for each class, right? But these are not probabilities. So, I can see that minus 12 is probably the largest number here, okay? And probably this number is actually zero but in order to convert all these numbers into probabilities we we use a function right you remember what it is what what do we use to convert these numbers into probabilities it's a type of activation function it was covered It's a type of activation function. It was covered a few classes ago. Does anyone remember? No. Okay. It's called a softmax function. Does that ring a bell? Yes, it was a few or two ago. Yeah. So we have recordings on it. So you see lecture three, we covered softmax activation function it's best to. Like the exponential one. Yes exactly we. Each of these values we take exponent we sort of like e to the power of this. Of these numbers and then we divide by the sum of e to the power of each one of them right so that way we get a probability right and once we get the probability we want to find the arg max so we remember we covered this arg max soft max both these things together and that's and that's because actually we put them together arg max and soft max together because we want to find the maximum probability the num the index that corresponds to the maximum probability so that's how we go about this thing. So in this case, I'm just taking 10 random examples from my test data set. And then what my model outputs is this logit, which is this number. And then I convert them into probabilities using softmax. And then I take its argmax. and that would be that is my y hat okay so i just print out what is the label for that uh image and what is its prediction so i print out both of them so okay we do see that there are some errors showing up here we will see it okay so seven has been predicted correctly two has been also predicted correctly one looks good zero looks good four looks good one again looks good okay this curly four also looks good okay this is nine uh yes this number i mean yeah i don't blame the model right it this is a pretty weird number yeah six yes it does look like a six i would agree with the model yeah so the label is five but the prediction is six um okay not bad and then nine has been predicted correctly so we do see out of the ten examples nine of them have been predicted have been predicted right except for one which is also it's a pretty good i mean it's a pretty good um prediction so the model works well and we we've been able to do that with the with a simple say feed forward network um and with a few layers right and so that's actually a really cool task that we've been able to do so far so with with all that we've learned nothing nothing out we've not added anything else other than what we've learned so far in the class so that's good. Any questions? Albert, Abhishek, is it okay? It's good, thank you. Okay. Yeah, Asif last went over the softmax function on June 1st. He was doing the makeup theory. He tried making the pre-lamps. So yeah, we do have the video on it. SoftNAS activation function lecture three is available. Also, so the next part is for you to try out the same classification, but on a smaller data set. This is the load digits. So this is a 28 by 28 pixel image, right? So here we're going to use an eight by eight pixel image. You will load, so this is your, I mean, you can do this now, lab work, and then leave the class, right right so you can start with that now I all you have to do is say load digits here import from sklearn data sets import fetch of openml comma load digits okay and you can make a copy of this notebook and start editing it out. So how load digits and then do everything similar to what you've done here. Load the digits, look at its shape. So you should make sure you know the shape so that you can give the right input features and the right output features and visualize the data set, make some inferences before you go ahead into the model and then train your model. So that would be your homework now. Does that sound good? Okay, I think it does. Yes. Yeah. So that's it for today.