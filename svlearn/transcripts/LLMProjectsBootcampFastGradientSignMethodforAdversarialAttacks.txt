 um you Yeah. All right folks, are we all back? I think we're getting a lot faster than we thought. All right, folks. So today we are going to read two papers that you have. Hopefully, and we are guys. Can somebody explain to me what is this paper saying? Anyone? Aditya? This one is, that was the first one. This one more of the interesting examples, the same example which is listed, they are listed only if the Fanger and Gibbon and the small perturbations can cause those problems they were listing over some adding some small amount of noise in your data right those perturbations and then all these problems yeah but why does it work how does it work why does it work how does it work okay the main point i thought in this paper was that they're saying it's a problem with the activation functions being too linear and that they recommend non-linear activation function just solve it yes that's right so uh that's it that's a great point So that's a great point. So guys, this is one of the earlier papers. The adversarial attacks showed up in 2013. Many researchers more or less at the same time observed that you could attack a neural network and with impunity make it mislead it. So you can have, for example, a panda get classified as a gibbon. So people try to explain why it happens and in this paper, like this particular method that is there, it's one of the simplest methods to confuse it. Now this paper goes into a lot of explanations of why it happens, why adversarial attacks happen. Since then, since 2014, when I believe the first version of this paper came, to now, many years have gone by, a decade almost has gone by. In this decade, we haven't yet, I believe, put the topic to rest. We don't know exactly why it is so easy to attack neural networks. Or there are ways to attack neural networks. People have different hypotheses. But we have learned a lot along the way. And I'm going to focus on what we learned after we walked through this paper. When you look back in retrospect at these ideas, this is Jan Goodfellow, one of the leaders in this field, and the author of the textbook, Deep Learning. Obviously he is as good as it gets now he he says that the problems have to do with the fact that uh with the activation function and with the fact that it's too linear the problem is neural networks are too simple and the decision boundaries they build they are piecewise linear in effect and that is the problem today then another school of thought came about that argued the other way around. They said, oh, neural networks are so easy to attack because they are too nonlinear. And there are many, many ideas that came out that try to explain why you can do adversarial attack on neural networks. I'm going to cover in the second paper that I read, perhaps one of the most plausible theories that I've emerged. It is called the dimpled manifold theory. And another related to it is, it's closely related, is the manifold curvature. Curvature theory, that at those places where the manifold has strong curvature, you tend to have more potential for adversarial examples. So both of these are beautiful geometric ideas, which is why I like them, and I tend to believe that they are close to the right answer. They are not the right answer. They are very close to the right answer albert yes so i went to a ai conference so they have security over your models they have this layer to protect and how do they do that i don't know we have all these companies that keep up come out to handle that that was interesting yes i don't know how they did that. I didn't focus on that. Now that you say it, I was thinking how could they attack a model? It's too easy. It's just too easy. But you can see their attacks and their defenses. And where we are is that if we think this is a vector of attack, we have a few approaches to defend, and we'll talk about those approaches, but it's a cat and mouse game at this moment. So this paper, take it, remember that this is an old paper, right? And some of the things that it says at that time, and this is why a brilliant thinking is very well thought out. Nonetheless, it is today I didn't think me quite agree or fully agree with the things that it says and that's the nature of research you you keep hoping that you have proven wrong. Because what is worse than being proven wrong. Not proven right is not to be noticed at all. Your idea. Most papers are not noticed at all. Most research they toil away in relative obscurity and their works are never, never, I mean they get thrilled if somebody even criticizes the work because it means somebody looked at it. But obviously Ian Goodfellow is one of the greatest and his ideas are obviously pretty deep and well thought out. And this way that he explains is genuinely very nice. So this paper is made up of brilliant ideas, some of which have survived the test of time, some of which we are not so sure about. So with those caveats let us begin so i'm saying that just so that you don't believe like when you read this paper later on again on your own review it you don't have the conception that everything that is here is what we think it is so if i look at this paper it says several machine learning models, including neural networks, consistently misclassify adversarial examples. So we know that this happens. By the way, this is not just true of neural networks. You can take anything. You can start with linear or logistic regression and you can do that right but those of you i mean just somewhat related if you remember even with very basic ml hundreds the linear regression that you learn we used to worry about points of high leverage do you remember those points that are hiding in there but their value has undue influence and if you remove those points then the model changes and they are not even visible outliers are visible outliers have an impact on a linear model but you can see them these points of high leverage are hiding in there and so you use all these techniques cook's distance and this and that to credit them out so then but you can do with the with logistic regression with all of these things you can easily cook up things support vector machines can be poisoned. So all of these models are susceptible to problems. several machine learning models, including neural networks, consistently misclassify adversarial examples. Inputs formed by applying small but intentionally worst case perturbations to examples from the dataset says that the perturbed input results in the model outputting an incorrect answer with high confidence. And if you see that the approach that these authors are saying this uh uh authors are saying it is not necessarily specific to neural networks it says given a loss function that and given a classifier that builds a decision boundary you can use this method to quickly create adversarial examples and basically to attack. Are we together? So how would you do that? The one thing that you need access to in this particular case is the loss function. You have to see how the loss function changes or doesn't change. But what you do is and this is the gist of this is this see you take an input this box is there this is your model right now and pay attention to the main idea is very simple suppose i have x going in what does the classifier produce Y hat, which is the probability of something. Let's say probability of a panda. Isn't it? Or more strictly, it is probability of a class, some class, some label, given x. Strictly speaking, this is what it produces. And then you apply a threshold, you say, if it is the highest probability, then this is the guy, or something like that, or more than 50, or whatever it is. All of those, and then you come up with the prediction. But this is the important thing, it produces a probability. Most of them. So far so good, guys? Are you seeing my screen? So now, and this is what it is. Out of the probability, you create a cost function, a loss function. I think they use L or J. I consistently use L. So forgive me if I, what do they use? work j j but the my l and the j is the same in this paper equivalent to j loss function and people sometimes use the word cost function these days loss function has become more or less the word most people use so So now I'll ask this. Would you agree that if I make a perturbation to x, x prime, I just make a small perturbation, shake up the value of x a little bit randomly. What will it do to y, y hat? It will produce hopefully a slightly different prediction isn't it the probability of that class that's a panda might change a little bit right so what will happen is it will lead to y hat prime is equal to y hat plus delta y hat. Now, intuitively, this is a vector. Obviously, these are vectors because this input is a vector. What do I need to do? Delta y hat is, of course, just a number. It's a probability change. Now, delta x, given x, it could be any one of the directions, right? And if you are in a hundred dimensional space or a thousand dimensional space any there are infinitely many things that you can do now the only question is if you happen to choose a direction in which i could decrease the probability of a panda or probability of the class i I can find which direction do I need to move. Pay attention to me. Which direction I need to move such that my move is just tiny, but the response change is high. Loss function change is high. Are we together? So this will lead to change in a delta. A loss function is equal to L plus delta L. R. V. Right. So you want to maximize the loss function. R. V. But in which direction given it's a loss function is a function, right? It is a function of at this particular moment, X and theta L, suppose the weights of the model you can't change because you're doing adversarial attack. You don't have access to the weights. So the only thing you can change is X, the weights and biases, which are represented in this paper as theta, right? You can't change those, but you can change the weight, change change x what is the direction of maximum change of x of l sorry what is the direction if i have a function of x in which direction do i need to go to get the maximum change in that function. Remember this thing, this inverted pyramid. What was that? Gradient. That's the whole idea of gradient descent, isn't it? So when you take the gradient of a function, gradient of L with respect to X, of course, implicitly, it will point in the direction that if you go that way my loss will increase the most did you see that and then what you do is what do you do in gradient ascent gradient descent you make a tiny step you don't just go hello just make a tiny little step in that direction right because this gradient may be very steep you don't want to take such a big step you take a tiny step what's the typical learning rate people use 0.001 0.0001 or something you make a tiny thing then again evaluate iterate back to it which direction should i go for each point in my input in this picture each so how do i change the picture how do i change this vector right such that changing this picture this 768 dimensional picture let's say right with this pixel value which direction should i go that direction would be you're basically saying that the x prime should be equal to x plus a tiny amount in the direction that is going to most confuse the loss function. Are we together? The model, because the loss function change would be the maximum. Now, think about it. You want the probability to increase or decrease? it, you want the probability to increase or decrease. Decrease, right? So you just want to make sure that if it is saying cat, it should suddenly start saying dog without making too much change. And that is the that is the gist of the idea. Now one more thing they do is that, see this can this will be a quantity, right? You sort of normalize it and make sure things happen in a discipling way. They don't take the whole of it. They just take the sign of it. Plus one minus one. They just want the direction, not the magnitude of the gradient. In gradient descent, you care for the magnitude also. x prime is equal to epsilon sine of gradient gradient of the loss. Does it make sense? Does it make total sense? So x is equal to or you can say delta x. The change that you make is equal to some some tiny amount of the direction. amount of the that direction so far so good guys great that's what it's called yes that is it and that is all there is to this algorithm okay so so as if you are reducing the loss function or Okay, so you are doing it in such a way. And so let's let's let's do that. See, let us read this back here. The linear view of adversarial examples suggests a fast way of generating them we hypothesize that neural networks are too linear to resist linear adversarial perturbation this by the way even if you don't make this hypothesis this this method still works right so uh so there we go too linear to resist linear with the lstms blah blah blah and max out are all intentionally designed to behave very linearly so that they're easier to optimize you can ignore that most morally such as sigmoid letters are carefully tuned to spend most of their time in non-saturated more linear regions of the same this linear behavior suggests that cheap analytical perturbation so he's saying that whether you use sigmoids you know the non-linear activations or explicitly linear activations small changes always linear effects right that's all he's saying this linear behavior is a cheap analytical perturbation of a linear model should damage you see the idea was literally that a basic calculus let theta be the parameters of the model x be the so what are the parameters of a neural network weights and biases x is the input in this particular case what is x x could be literally the image vector right and or it could be the tensor, whatever it is. But yeah, let's take it. So the target associated with X. R. R. The learning does and J with the cost function around the current value of theta obtaining a max norm constraint perturbation this is the perturbation this is the delta x kind of thing right epsilon the sign of the gradient we refer to this method is fast gradient sign. Just the weights and biases. You don't need to. That's the whole point. Because you're doing gradient with respect to input. The last function has built in in it just hasn't but you're not changing that yeah you're taking the gradient if you look at that below the gradient the subscript is x see what in reality what is the thing about it in reality if you want to i want to attack your model do i know your weights and biases i don't but what do i know i know my inputs right now if only I can see your output or somehow get a sense of the cost function. Now, suppose it's producing probability. Oh, good heavens. Now everything is easy. I do a little bit of calculus, I bring in my cross entropy loss function, do a bit of calculus and say, OK, I know the derivates. I can start doing my perturbations of the input that will create, I can quickly calculate the gradient of the loss and therefore I can attack this box. So your input then would be the merging of the two? Yeah, yeah. So what you do is your input, so this this thing. So see my delta x, what I call delta x. And so your new input will be x prime this is it plus let me write it see basically what we are saying is plus this is what we are saying isn't it so you take oops what plus x plus x the previous picture take the picture of the panda and add this but when you look at it on each of the axes which is actually each of the pixels there will be some tiny little bit of perturbation when you try to visualize that perturbation as a picture because this itself is a vector this is a vector when you do vector when you like time I mean square it or the tangle it and you see what does it look like. Vipul Khosla, Ph.D.: Right, it will look like noise right, it will look like this noise, this is literally the matrix that is it, so you see them doing that your X is the image, this is your epsilon tiny bit 0.007 right, and this is that this pixel map noise is literally the sign of this thing now why did he take 0.007 learning rate he was just i think a little bit of it is just experimentation but he goes on to say that it corresponds to like you want to just stay below the perception levels right equal to gradient of the cost functions here our to the magnitude of the smallest bit of an eight image encoded so it's taking implementation he's looking at that 8-bit image and he's saying okay when you convert it to real numbers then just this is about as tiny a step as you can take right so he's taking very tiny steps any any tinier than that and it won't show up as change at all yeah go ahead i'm thinking a lot here and maybe i'm wrong yeah yeah totally totally Yeah. Yeah. So that'd be a real life example of this. Yeah. One second. So, so the idea is that I'm a person. If I want to confuse the system is that they said that in front of the building is a system that does people identification right so it is looking at the camera if somewhere i could insert in the between the camera and the ai system somehow have a way of injecting noise right why are you seeing more of this because no because uh in this there is one thing that you need to have this is the easiest method but here the weakness of this is you need to have access to the probabilities that it's producing so it's a little bit more closed box but uh there are more attacks as we learn about it that are doing see today models are being attacked all the time you should just assume that see um is adversarial attacks happening they are beginning to attack they it's like a curve let's just say that not many people have heard of the topic of like Mercedes right it's still a small world, but that word is pretty active. yeah it comes in great security concerns. So i'm confused, how do you attack them on. What do you mean attack them off. You make it make wrong predictions. That I understand, but the model is hidden behind your code. See, your model. See, here's the thing. Suppose your model may be hidden behind the code, but suppose in the API or your UI or something, I also emit out the probability of this. You know, I emit out this number. Then what do I do? I just start to see the probability of this you know i emit out this number then what do i do i just start making tiny little changes and figuring out which way i need to go no we technically get this in a rank a rank is impossible without exactly this see guys machine learning right the whole of learning in i mean it's a bit of an exaggeration but a large part of learning comes from this particular symbol right the gradient is the guide to everything to optimizing to learning to you see this here the attacker is learning how to attack this is it and so this equation guys now does it make obvious sense it's a very obvious equation if i could see the probabilities and i could see the loss therefore i can do the loss gradient i am done i can create a pixel map of noise that may be sub perception when you add it this picture, there is no way you can tell it apart. Right, the first time I read this paper, I said, are you sure that they just didn't accidentally copy paste the same thing twice? Because the more you stare at it, the more you realize that actually there is no difference. There is difference, but you can't make out. So this is it. You have adversarial attack. Now the explanation that is given, I would not read the rest of the paper at this moment, because today we don't think like that. But I agree with Albert's point here. See, you have a model that's sitting somewhere on your GPU right? GPUs right either you're doing like reinforced learning on it or something where you're even giving me a way to give you bad input to modify it right so if I'm able to give you a second image like that yeah which has the disturbances in it only then you can and you're actually learning from it yeah only then this attack is possible exactly so. So the underlying is, this is the sympathy. This is an academic paper. It shows the possibilities. Once the possibilities are revealed, and now you have to think, so take on the person of the attacker. Now you have to think, okay, how do I go about it? Oh, here is an API. It's telling me the probability. And by the way, there is no restriction on how many inputs I can keep submitting to it. So then I can theoretically soon construct an input where a pandas looks like a given to the machine and then I can write a little blog. This algorithm is totally wrong. See, it is calling this a panda so if i suppose i join a company and i'm a rogues i can add that particular image you're talking about an insider attack yes but it's an attack right so it's vulnerable so there could be any type of attack my point is is that the words you could do you're on the inside the whole thing like what fun are you getting by part of the effect yeah see this is the one of the simplest attack forms but people have used this people have used this but it's not easy for i think sanjay is waiting for code symptoms go ahead easy for i think sanjay is waiting for quotes in the introduction where it says say the sentence which which page is in a paragraph number and line the second paragraph last sentence the second. The generic regularization strategies such as dropout, pre-training and model averaging do not confer a significant reduction in the model's vulnerability to error, but changing to nonlinear model families such as RBS can do so. So what he's pointing out, which is true, that I mean obviously you must have noticed too, that dropout won't help you. Multiple regularization techniques. Actually today we are not so sure of that statement actually. The next paper, which is much more recent, you'll realize that as discipline regularization a smoothing of the manifold regularization geometrically shows up as a smoothing of the manifold or straight straightening it out a little bit so there are certain forms of regularization which do help. Asif, is there a way to freeze the inference model parameters all the time? Is there a way to freeze? No, no, they are always frozen. Remember theta is not being changed in this attack, right? People are playing with the input and they are trying to see which input will fool the model okay that is it so here's an example suppose your boss has access your boss has access to something precious let's say that office room behind which all the good cookies are but you don don't. And outside that door, there is a facial recognizer. And you see, now all you need to do is your boss isn't there. You take a picture of yourself or somebody who looks who's somewhat close enough to the boss, and you start making tiny changes to the boss's picture and keep showing it to the facial recognition system, I mean to that picture that you started with, any picture, maybe it's a picture of a monkey. You start making tiny perturbations to the monkey and you see gradually can it fool the recognition system into the belief that it's the boss because what is your goal? To somehow make it info it's the boss because what is your goal to somehow make it info it's the boss and you go get the cookies so that's how you would look at it correct yeah you keep making changes until you prepare the last time until it looks like a boss and then you paste a picture of that monkey all over the wall and say boss boss. And then you paste a picture of that monkey all over the wall and say boss. But as you're gradually increasing the loss, it's just not being able to predict it as you, right? Come again? Because if you're adding more noise to your face, it's just making it more and more likely not to predict you, right? That is right. So what happens is right so what happens is generally what happens is and this is the funny thing with these complex networks generally like you can start with the let's say that more realistically you started with somebody who sort of has a facial structure similar to the boss and you start making tiny perturbations if If you can see the probability, like suppose that facial recognition very helpfully has given you probability, probability allowed, probability of a boss 99% or something like that, right? Or 10, not allowed, 10%. And then you make a tiny perturbation, 10 point probability of boss, still not allowed allowed probability of boss 10.01 first you say aha i'm on to something and you keep making tiny perturbations and surprising thing is you would imagine so i'll come to that i know somebody is waiting uh say uh you could imagine that by the time you hack that machine the thing doesn't look like either the first person machine the thing doesn't look like either the first person or the ups right but in reality what happens is it more or less looks like the original guy because the perturbations needed are so small and subtle that people would not even know that an adversarial attacker all they will know is somehow patrick was misinterpreted as the boss and now patrick has all the cookies right right that's all but contact is not to use exactly that exactly for example can you corrupt the data in a way that let's say it can it can influence somebody's credit score yeah so what happens is that that's the risk and this field has exploded the simple answer is yes potentially you just have to find specific avenues for making things happen and you have to hope that those people who maintain credit scores are a bunch of Equifax geniuses. Right? Is the word dimensions for text lesser than images? No, no, no. Easily. So, for example, I write a text, you classify it as a positive sentence. I can just turn it into a little word and then to suddenly say negative but i'm still saying the same right so the perturbations would be just making synonymous change synonyms or something and you're making very slight change perturbations in the semantics of it if you think in terms of vector embedding right think about it as vector embedding that vector is that point is just drifting a little bit it just is finding where is the decision boundary and it needs to just cross over a little bit the nearest distance to the decision boundary and just cross over that's all it's trying to do you see that right right that's it and so you try to start with an example so here is the trick part of it there is an interesting thing you would say that hey if it is a perfectly good picture sometimes you are far from the decision boundary right which is true and so i was hoping one of you would notice that actually the machine was not very confident that it's a panda you want to pick cases like that first like that first. Do you notice that? So in a way you can say, well, you know, it's plausible because this is already not a very confident answer that's upon them. And then you pushed it over the threshold. Isn't it? But what is interesting is what is it calling the noise it's calling the noise a worm so it sees this and actually i stared at it quite hard to see where the worm is but i couldn't make out so okay guys so that's that was it fun with this paper this is it Okay guys, so that's that. Was it fun? This is it. There's something about text attack. Yeah. Python libraries. Are you familiar with that? Oh, there are many libraries. I don't think I've heard it. I've not used it for sure. But at one time I was researching all these libraries and methods of attack. So it may have come up in the list, but I'm not actively. So my second question is similar attacks can be done against text or images? Text or images or anything, music, audio. I can... How do we protect against it? Oh, so that brings us to the defenses. Yes. Defenses. Okay. So for that, part of the answer I'll introduce now and the rest of the answer, unfortunately, I won't get because it's a long discussion and when I do fraud and anomaly detection, it is actually part of that course. So it's a big topic I get too much there. But in a way I'll start to answer that question in the next. And at the same time, I would strongly encourage you to read the reference see the references there you don't have to wait for me to give that course in the references I've given that to you. Let's go back to see after this, the next slide, you see that defenses, a survey of adversarial attacks and defenses. You see that, right? So just click on that paper. It's a I mean, there are many such papers that are survey papers. This is one of our survey articles articles this is one of them i believe this came out in ieee or something like that yeah so it's a big field guys that we see like action defenses is a big field and many people are completely dedicated to it now are completely dedicated to it. Now, it is important for you to know about it, which is why I'm introducing it to you. And some of it I also do as part of fraud and anomaly detection because these can be used for malicious intentions. But broadly speaking, if I were to go in depth into it, it would be a workshop of itself. Alright guys, so we meet again. Are you in a mood for the next paper or are you done? But have you please read it. This time I went around asking, what is the alcohol and I didn't quite get the responses so actually read the next paper it's a beautiful paper to me as a a person who loves manifolds geometry etc topology it's a very very beautifully argued paper read it i'll catch up with you in half an hour so so also for this So, so for this, well, this is being that, you know, you know, our LLM agents and all that are also prone to this kind of everything. Everything. So we can essentially disable a whole agent pipeline just by an adversary. Yes, yes, we can. It is like at this moment, right, this is the danger that people don't realize with AI. Of how susceptible, see, this, our training place is called support vectors. It's a pretty clever algorithm. And then one day I realized to my disappointment a paper or a discussion that showed that you can very easily poison support vector machines. So complex algorithms are easy to poison and even simple algorithms are easy to poison by the way nothing complex this is a point goodfellow makes is that even linear regression you can poison anything so just like all these classes patient and we're talking about how they identify a cow and a duck right because of the beak or because of the way. So these are kind of in that cases, the model is quite weak in the sense it has few features and if somebody modified that a little bit, then a cow can a duck can become a cow and a cow can and that's probably it when you add the noise there are commonality that no no the way i would say albert is not weak it's just that making the decision is complex and the because it builds up the decision boundary that it builds actually the real reason nobody really knows with the best hypothesis is the decision boundary that it makes is unstable, like it is not with small. Like with Like it's it is such and that's exactly the context of the next paper that which is the dimpled manifold hypothesis that inherently it ends up leaving patches where it is easy to create adversarial examples regions of the feature space where you can easily create and not only one quite a few examples adversarial examples you can do so initially the the hope was that you could localize all the adversarial examples to one region of the feature space and then you could just blacklist any input coming from that region but that hope didn't work out right so that simple blacklisting kind of argument now we know that adversarial examples are quite quite widespread and the next which is why see guys you're asking all the right questions it should make you study the next paper with great care and i can see that actually you guys haven't studied it so how about this i'll give instead of uh after i'll give you guys one hour take a whole hour but steady the paper right please get back into your rooms don't code don't do your implementations read this paper it will take you half an hour after that you can start coding. We'll take it from there. What are the ways to manage this? Oh, it's a pretty big topic, but broadly you have many, many ways of dealing with it, some of which we'll cover in the next paper, but broadly many things. So, for example, you deliberately give it adversarial examples, You create it, and then You create it, and then you make it part of the training make it part of the training data. And you say this is a under And you say this is a under not a. So then what happens is the hope So then what happens is the hope is that the decision boundary. will will shift and will shift and stabilize. does it. actually Does it actually really work perfectly? Partially works. That makes it better. And in fact, most of the companies that you're talking about Albert, my guess is that they are into some, see this topic is very fresh. Most people don't even know the easy solutions like this. Right. So, I was just wondering, the change of mind, what if we just resize it by like 10 percent would that affect the final percent because you know resize means like you get a thousand by thousand you convert it into 900 by 900 so wouldn't like you know because it's getting the size putting these pixels we can have you know more and you know like the end result in each wouldn't it have a different kind of you know you know the end result in each couldn't it have a different effect on the no you're asking that in a way if i change the picture and its dimensionality itself every model takes picture of one dimension pick a dimension whatever it is thousand thousand dimension or i mean let's say that if you take a thousand by thousand pixel you're looking at a million dimension, but you can't relate on it. So it's never really, it's still a very high dimensional picture. The moment you do any reduction, size reduction, yes, you, but you usually don't call it an adversarial attack. It is just has been a part of. Can we use that against attacks because you know the pixels get gone first. Now, whatever. No, you can't because it is a property of the decision boundary. It's a property of the way this neural net other all the AI machine learning models work. It's a property of the decision boundary. You can give me any data, you think about this method. Nowhere does it say what the dimensionality of the vector should be. Whereas the size of the picture, it only speaks to the dimensionality of the input. So invariant of the dimensionality of the input so invariant of the dimensionality of the input you can still apply this and a whole class of other methods this is one of the simplest ones but to uh trina's point is if i if i got that that uh panda with noise and then i i resized that that doesn't necessarily mean it's going to trigger a headlight. You should essentially have to resample the noise. You need to do adversarial injection. Just resizing is not going to make the thing look. And in more practically simple models, they take a one given dimensionality. So resizing anyway won't help because if it takes a million pixel input you can't give it a resized image because you'll have to again blow it up. So that's a very practical impediment. So it wasn't image that you resize. yeah. సిక్కారింది సందిక్కారింది సందిక్కారింది సందిక్కారింది సందిక్కారి the panda and the panda is no no no it's the opposite it is the fact that the human perception, see, you can theoretically assume that the middle picture in the paper, where is it? I have to go to the paper now. Give me one second. Where was I? Oh, did I close it? I might have accidentally closed it. Okay. So the, see the panda plus noise to our human eye, it is not sensitive enough to know the distinction so the attack was done by injecting a sub sub threshold noise there is a threshold of perception and delivery so you could say that if i was a green martian alien with much better perception, static perception, I may have noticed it. Yeah. So I'm just assuming like there is some big question. If I assume my eye as a camera, it looks like our eye has gone two or three bits. and that's going to your bits possibly our high is not i don't know how to answer that i'm not an expert but see especially you're looking at it from a picture and assign lesser bits for the results then your neural network solution also will be more robust no it doesn't solve it i'm telling you that the input dimensionality makes no difference this method can be applied remember it's the gradient on the input wherever you're getting into the shady zone or not so confident zone that's where you can buy a third decision right yes that's basically like a wireless absolutely a wireless yeah i actually have a very practical problem see i mean everybody