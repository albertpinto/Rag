 Hello. All right, folks. Today is Wednesday, December 16th. On Monday we covered anomaly detection. Anomaly detection is a vast topic that has a lot of applications. For example, you use it for fraud detection. We use it in the medical sciences for our discovery of diseases, tumors, and things like that. People are even using it to find anomalies in the heart condition. So you can use it for astrophysical data. You can use it. I mean, there's a wide range of places where anomaly detection comes in. It comes in for intrusion detection in networks, whether you're being hacked, banking systems, whether banks are being robbed. So the potential for using anomaly detection methods is vast. It's a huge field with wide applicability. With wide applicability, as I mentioned, this field is just being an expert on this topic essentially gives a specialization in its own right. And usually there are lots of startups and big companies that are working only in this domain. It's a very fertile domain. From what I understand, there are not enough people working in this domain there's always a scarcity of talent for this space so therefore this topic is important um we covered the basics last time it's a vast topic as i usually give sometimes in the past i've given twice a dedicated workshop on anomaly detection and those workshops were well attended though the audience was slightly different they came from specific companies or groups who wanted to pick up this topic sort of like a private class and so forth so today we will do one part of anomaly detection if you remember last time I mentioned that anomaly detection has two parts. They're classical methods starting with very basic k-nearest neighbor or clustering and so forth, isolation forest, one class SVM, and there's lots of them. A lot of the algorithms you can repurpose to be anomaly detection methods. Now, anomaly detection when you do, it could be supervised, semi-supervised, unsupervised, the whole gamut of it, you use it, and even reinforcement learning. And now we didn't cover reinforcement learning in this workshop, but they are quite actually effective in this space. And so today we'll do labs. Much of the lab is using pre-existing libraries. The emphasis will be on getting familiar with anomaly detection. Once we become familiar with anomaly detection, we will have another lab in which we will create an anomaly, deep anomaly detectors from scratch. It's not that much of a stretch. We'll take an autoencoder code and we will now repurpose it and we'll see how straightforward it is to repurpose it for anomaly detection. But that will keep after we have done a lab in which we have become familiar with anomaly detection techniques. So you folks are, most of you are coming here after doing ML100 and ML200. So this year's batch, the one that started in March. So at this moment, you don't have familiarity with with anomalies and outliers and so forth in a practical way. So we'll start with the basics and see how it works and we'll work through some examples. A lot of the examples that I'll show you are actually taken straight from the documentation or adapted from the documentation provided by the libraries that we'll use. We'll focus on two libraries. One is scikit-learn and the other is PIOD. OD stands for outlier detection. I would strongly encourage you that after this week as a homework, become familiar with it, apply to a few data sets, create your own examples, play with it. And it is important that you know how to use some of these libraries. In the next lab or in the subsequent lab, we will go, actually, this is a lab I need to do at some point. We'll do an extra session. We'll have to squeeze in a session, coming to the end of the workshop here. So we will add extra sessions somewhere, extra labs somewhere, in which we'll build an anomaly detector using autoencoders and possibly GANs absolutely from scratch. But that is a little bit more advanced than first becoming familiar with the anomaly detector. So that is the context for today. Now, just to give you guys a sense of the roadmap of how we are moving forward, we are in the subsequent weeks, this is the 13th week. We have five weeks left. The last two weeks will go to projects. So I won't talk about projects at this moment. I think the feedback from last week was that people are just getting started and they are waiting for the holiday season to do much of the work on the project. However, I will start, to do much of the work on the project. However, I will start, we'll cover the usual Saturday updates on projects if anybody has updates, some of you hopefully have a show and tells to do updates to show. So we will do that on Saturday. This Saturday we'll also have our quiz review. This Sunday we will have a paper, research paper. There's a review paper on the optimizers used in deep learning. A group of people have sat down and studied no less than 40 optimizers and how well they fare across a fairly representative real world situations. And the findings are quite interesting. We will cover that this Sunday. We will give two weeks or at least one and a half week or two weeks, depending upon how much time it takes to time series analysis. I also need to, but then that leaves us with two important topics. One is CNN, object detection. This is how, for example, very much used when you have automated self-driven cars and things like that. So that topic is important to cover in a deep learning workshop. We haven't done that so we'll keep some time for that and we'll keep some time for interpretability of AI models which is again a significant topic. So we will be covering a lot of topics pretty fast and sort of brace up for it gradually in the coming weeks. I will be throwing in a few sessions, extra sessions here and there just to make sure that we cover all the material that we have. Now in the last two weeks when we do the project, we'll give Monday, Wednesday, Saturday and pretty much all three days to workshop a project coverage and I'll be releasing solutions I'll be hoping that you folks present your solutions we will learn a lot from the projects now the learning from the projects I wanted to be different from learning from the labs in the labs these are guided labs I walk you through the solution. I will of course release the solution to the project and walk through it, which is a slightly bigger code base than what you have been doing so far. But at the same time, one week, when you folks, each of the teams will make presentation of their work, finished or unfinished, one way or the other, you'll be presenting your work and showing how far you made and you'll be presenting it to your colleagues. So do be mentally ready for that. Now, furthermore, there's one more announcement. It is possible that I may not be here in the last eight ten days of december uh two weeks so um dr chander who is here with me uh as part of support vectors and he does of course attend these workshops he has been following it is quite possible that if i'm not here he will take over some of the sessions it might be with either with object detection or much more likely with time series. So he'll be doing the time series sessions till I get back onto this. I have some really vital work that has come up in the workplace. I'll have to go and take care of that. Any questions, folks, before we start? Who would that instructor be, please? Oh, Chenda, you have listened to his you have attended his talk. Most of you have remember the Sunday talk on word embeddings? Yes. Yeah, he's good. Yes. So he'll be taking over two of the sessions. He's part of our little support vectors faculty list. So and I think he'll like it. He's just to give you a bit of a background. He's an extremely bright person. He's those of you who come from India, you know about the IIT JEE. He is one of the top rank holders from that, finished it, did his thing from IIT Chennai and after that came to US, did a marvellous PhD. Actually, I was together with him in the PhD programme. We happened to be roommates. And of course, we have worked together for many many years now um so and of course there's a few as you said who have attended his talks now that he can be he's very good all right so with that uh one last uh bookkeeping thing the quizzes the is the completion rate of quizzes is quite disappointing. Last week, Saturday, we couldn't cover the quiz review because almost there were just four or so completions. This week, we have about 10, 11 completions. Some of you are in the process of have started but not finished it. Many of you have not started. Can I please request you to finish the quizzes? There is a lot of learning in the quizzes. So if you're not doing, if you're not taking 10 minutes out for the quiz, it probably means you're not getting time to even review your books or read, do more reading on homeworks and so forth so it means that see it can mean only one of two things it means that we are moving much faster than you are able to keep pace with right so those are the opposing forces while we want to cover a lot of topic what i am seeing is a lot of you are being left behind. The completions on the quizzes is a bellwether. It's the clearest indication of whether I'm moving too fast. And clearly, it has been the indication for the last few weeks that I may perhaps have been moving too fast. You guys are all busy, and you are not able to catch up to it see at the end of this if you don't practice all that you have learned may gradually begin to fade out you have to go back and review learning is cyclical please do take out time and learn and finish your quizzes otherwise it's a very anomalous situation. Some of you are insisting that we cover a lot of material, which I am frankly willing to do. But when I look at how much amongst them there is a completion of quizzes or thoroughness in that, then the numbers are disappointing. And so I can imagine that even the labs you guys are behind, projects here. I know that in Saturday evaluations, when we do the reviews, projects, we are all far behind. So the reality is, guys, that we are moving pretty fast. And in fact, you guys are giving me feedback. I should go even faster and cover a lot of topics. I'm totally willing to. But if you want to get things out of it, do take the winter break. There's holidays coming up and do take time to finish off these things, finish off your projects, finish off your quizzes, all the quizzes and do the labs and so forth. Please do take care of that. So with that, I will get started now with the lab of anomaly detection. So as I said, this lab will do in two parts. One part will be just using the libraries that exist and the second part would be building our own anomaly detectors so let me know guys if you are seeing my screen you see it just fine is it too small too big nice and visible nice the lab is running fine on my machine okay you already ran the lab thank you so again guys all the lectures and labs are being continuously posted to the course webpage the for one thing here one more bookkeeping the sunday papers that we are doing, they are an integral part of this workshop. So the participation there is about two-third of you show up for the Sunday paper reading. Some of you are not able to make it. Understandable, we all have family responsibilities. But if you get a chance, do go and pick a few at least of the Sunday papers, read the papers and then watch our code review the videos of the not the code review the paper readings. part of developing maturity in a subject. I mean, when I go back and look at my PhD years, the one activity that was most fruitful that we used to do was paper reading. See, the course, the subject matter is, compared to the research level, is fairly trivial and it is things that have happened way back in the past, you know, 10, 20, 30, 40, 100 year old knowledge. It is in the paper that you catch up to the latest and greatest always. And taking a tough papers, forming a little group, reading through it, talking about it, and really being thorough at it. Nothing gives you more confidence and brings you up to the state of the art in the subject as paper readings, which is why I incorporated it as part of this workshop. So if you haven't attended the Sunday sessions, don't have to go and watch all the videos, but pick a few interesting ones and watch it. So last week, for example, we looked at how it was a more theoretical paper. It was about how a kernel, I mean, this deep neural networks can be reinterpreted or can be interpreted in terms of kernel machines or support vector machines. And that leads to interesting insights. This week, we are going to cover optimizers. And we are going to look at some researchers who have carefully paid attention to all the optimizers used in deep learning and seen how they compare against each other across a variety of real-world tasks as i mentioned so do please do that all the videos are there all right guys so with those preface let get started. So we are talking about anomaly detection. Anomalies, well, are outliers, right? Or you can also call them novelty. If you have seen certain kinds of data, you have established a norm, a sense of normality, a representation of normality. Then you need a data point. All you need to decide is, does it conform to your representation of normality or does it deviate from that? If it deviates, it's sort of a novelty detection. People these days don't distinguish too much between the two words. They treat it as more or less synonymous. So people talk about supervised, semi-supervised, unsupervised anomaly detection. And of course the one topic that I at this moment have avoided and won't be going into because we haven't done it is reinforcement learning. How does reinforcement learning contribute to anomaly detection? By the way, this is something I do cover in the anomalies workshop, but that's a separate topic. So today we will focus on using other people's libraries. And so there are classical methods of anomaly detection, there are deep learning methods of anomaly detection and so forth. So we'll focus on only two libraries, scikit-learn and PIOD. So remember that you can treat anomaly detection as a binary classification problem. Is a data point normal or is a data point represent i mean is does it conform to some definition of normality or is it an anomaly so you could write a classifier and one example is the comes from the credit card fraud data now credit card fraud data is something that i happened to cover last year in a in a boot camp that we took some participants through and we did it in the context of seeing how well can we use a classifier to work for essentially an anomaly detection situation. There, the problem was fraud, credit card fraud, and how much we could use to detect credit card fraud with that. Now, when you think of normal data and anomaly, by definition, to the extent that anomaly violates your normality, it's a rare event right so data comes terribly imbalanced when data is so imbalanced then the imbalance techniques such as under sampling over sampling they quite underperform and then techniques like SMODE which try to interpolate and generate more data between the outliers they can get quite lost or misleading sometimes for reasons that we talked about in monday session namely if your outliers span a bridge you know a bridge over the normal data, then SMOTE could quite likely create sort of a corridor of outliers that runs through normal data. So those are some of the concerns, not necessarily are they, but somehow crossing it. So those are some of the concerns with SMOTE and so forth. Today, Harini is going to kindly show her notebook to look at that sort of an approach and how far it takes us. And then while that is a valid approach for anomaly detection, and it's a question of judgment. When does class imbalance reach the level that you need to treat it as a problem more specifically as anomaly detection? So there are no clear-cut boundaries. So for example, let's say that 5% of the people or 10% of the people at any given winter season, as we are going through now, have cold and flu and so forth. So you take the temperatures or you look at some biometrics like the runny nose and whatnot. Now the 10% of the people, let's say that they are positive cases of fluid cold, are there anomalies or it is just a classification problem in which data happens to be imbalanced at the rate of 9 is to 1. That line is blurred and it's often a judgment call and seeing what works. In the same way, when you look at x-ray images of patients and you see some presence of some areas of interest, and from there you want to inquire if there are any anomalies in the lung x-ray. Is that a classification problem or should we apply direct anomaly detection techniques? So the lines are blurred. It's a judgment call. One slides from one to the other. Of course, if the data is 50-50 divided, then it wouldn't be an anomaly detection problem. But a significant class imbalance is a call for anomaly detection techniques. They may be worthwhile. So that is something to keep in mind. Now, when you learn about anomaly detection and things like that, one thing that I would strongly suggest, at this moment I've started with very simple data, toy data sets, because it's important to get a feel for these algorithms. But perhaps in the next session, we may not get time in this particular workshop, there is a very good place. This is this, which is uh sorry it is a site of uh lots of where am i this website and there are many such websites you can go to kegel dataset aws dataset and so on and so forth they all have a curated list of outlier detection data sets. The reason I gave this link in our lab is all these data sets that you see, many of them are very well known and well studied. In fact, I myself have worked on and practiced on many of these data sets. So it is certainly worth these data sets. So it is certainly worth playing with these data sets. When you look at it, you can see that the outliers or the anomalies, if you look at the percentages, it goes from five, six percent, sometimes even 10 percent, yeah 15% of Wisconsin breast cancer data, I suppose that's what it is. MNIST with the 10% anomalies and so forth. So there's a fairly big spectrum of how much, what proportion of data is anomalies, right? And what proportion is not. So you'll get a pretty good sort of exercise by playing around with this. We have standard data, the tabular data, you have time series data which you can play with, and then there's security related data. example this is a lot to do with as i said adversarial systems and so forth and quite a few right there's video data for anomaly detection and so forth so pretty vast field with a lot of data sets to play with so but today let's take baby steps play with them. So but today, let's take baby steps. Now, that's the publicly available datasets. Today, I will start with two libraries. One of them is PIOD. So if you haven't yet, if you don't have it in your system, and I assume you don't have it, you want to run this, uncomment this line, and run it pip install PIOD. Run that and it will install it on your machine and after that comment it out because you don't want to run it again and again so we will take a few things from the piod just to illustrate now and this is a straightforward method. What we have done is you decide how much of outliers do you want? Let's say 10% anomalies, then 0.1, 100 training example, 100 test example, or you can make it 1000 or whatever it is. It doesn't matter, play with these numbers. Number of features we took as two, simply because, well, the screen is two dimensional. We want to be able to see that, but play with it with multiple dimensions and see how well it does. One of the things you'll realize is that in higher dimensions, detecting anomalies, it becomes harder, outliers and all these things, it becomes much harder to do that because of the sparsity of the data. So there is that. Now you remember that we said that some of the simpler techniques are clustering and KNN. So we start with that. that and here is what it is if you go to this website here the PIOD website let me go to the PIOD website you will see that it's a pretty worthwhile website to visit you'll see a lot of implementations so do you see all of these names these people have implemented all of these algorithms and so there is code and examples for all of them I would strongly recommend that you like for example you take this lab and all you have to do is change one model to another. The rest of the code remains the same. Import a different model and just feed the same data onto that. And one thing I find very useful is if you go to the API reference, you will get an idea. For example, how do you use the variational autoencoder? You can go and use it. This is it, VAE. the rest of the code that we have written will remain exactly the same which is not going to change you can use and many many things you can use for example from very simple things like for example um a knn to obviously isolation forest to xg boost to whatever you know there's so many many algorithms that you can one class svm that you can repurpose for the purpose to apply it to your problems and changing it is as simple as changing the constructor instead of using the knn constructor you can use the VAE constructor, variational autoencoder constructor, right? And so forth. And that will serve your purpose. Just one simple line of change. So I haven't repeated those activities in the lab. I leave that as a homework because otherwise the whole notebook will begin to look repetitive right so but all you need to do is go so for example um if you want to pick uh let's say just basic pca how would you do pca you would just literally put this constructor there you would say pca instead of knm likewise if you were to go and do xg boost what would you do well you would just put this one line there. Literally that is it. If you look at the signatures with minor changes, they're all specificities. But if you just use a default constructor with nothing in it, that's good enough for most of these situations. So that is something to remember that you could use this library for that purpose. Now, in the beginning, once we generate this dataset, it's generating X-train, X-test. So why are we doing X-train, X-test? You realize that quite often outlier detection is semi-supervised or unsupervised. It is just to validate. If you know the answer, you can cross-check. So I'll play with two data sets here one are synthetic data sets generated by the libraries the other that we will use is the smiley data sets i don't know do you guys remember the smiley data set oh yes yes that's right for our clustering so if you remember in that data set a derivative embedded a lot of outliers and we'll see how well these tools do for those outliers. So we take this K-node detection. So quite literally the line four is the only line that would change in this library as you switch from one particular algorithm to another. So from that perspective, using this library makes it extremely convenient for you to use in practice for outlier, for anomaly detection. This is why I introduced you to it. So we generated some data here. Let's see how well it does. And this is this, I hope this looks like, and it follows a convention identical to scikit-learn. So x train, y train, x test, y test, literally the notations and everything is exactly like that of scikit-learn, which you are all familiar with. So this should look even the signatures are the same, dot fit and so forth. Okay, and then they give you some evaluation and visualization. When you visualize, you have two ground truths or rather the training dataset ground truth and the test dataset ground truth. The left column is the ground truth. Now let us see how well the predictions go. So the very basic KNN seems to have gotten the predictions like that, which is pretty good. Here, it made some mistakes, one, two, in the test dataset, this was obviously I don't know if you would interpret it as a mistake, because this seems like an inlier data point. So it works, KNN. Now the thing is, this looks rather flattering. It seems that KNN would work. This is not usually true. If you try with larger data sets or with more noise, KNN is perhaps not the best one to do, especially for higher dimensional data. The problem with KNN is, all neighborhood-based methods, they work very well in low dimensional spaces. We are looking at two dimensional spaces. In two dimensional spaces, the neighborhood-based methods are very good. The trouble with them, of course, is that as you go to higher dimensional data, the space is extremely, the feature space is extremely sparse and your neighbors are very far off. So the word neighbor, it's sort of meaning becomes questionable. Are they really neighbors at that moment? So in lower dimensions, of course, you can do, and for any of these, one fact to bring about, you can always build ensembles of your model. Whatever you do, you have hyper parameters, throw in that, run through that. And this example is literally again, it is not something that I created. The library people have this example in the directory. It's a very good example. And I thought about it, if I could add something more to it, and I realized it is such a simple, straightforward thing is hard to add. So the first part of it is very simple, you generate the data, so on and so forth. But the only important thing is here. The only change that has happened is, now you're looking at that has happened is now you're looking at many values of k, right? You take a whole set of values of k and go through that. So k list, 10 neighbors up to 200 neighbors. And then you just iterate through this. Now we have done that in different aspects of our lab. When we did our KNN lab, you remember, we also did, we sort of did a hyperparameter search over the values of K. So that is exactly what's happening. Nothing new. The rest of the code remains more or less the same. Right? And now they introduce a few notions which they have explained. Like when you look at it, take the average of all the base detectors, like are you saying, is it an outlier or not. Take the average of all the detectors. If the majority comes out that it is an outlier, mark it as an outlier. Or use that to create an anomaly score. Maximization simply means that if all of them take the biggest score, then so on and so forth. And so you can play around with these. Then you could also generate data in clusters. This is nothing new. Now, one of the basic algorithms is this LOF, local outlier factor. What it basically says is that a simple way to see if something is an outlier is by doing a local examination of it. If a point is sitting in a spot, you know, if one region of the feature space is less dense or has less neighbors than your neighbors, then you have a problem. So for example, your distance to the nearest neighbor is two miles. But those neighbors that you reach, the nearest neighbor is two miles. But those neighbors that you reach, your nearest neighbor has other neighbors within a hundred feet of them. Now, clearly you are an outlier and they are not. So that is a basic idea that a local outlier factor looks for. It's a very simple argument that sparse regions contain outliers and further and dense regions contain inliers. That is it. So again these are methods that work well in low dimensional spaces. The moment you go into higher dimensional spaces, are neighborhood based methods. It is a neighborhood based method. These arguments begin to get into trouble. Yes. Yes. Data set that we're using right now for this lab, what's the dimensionality of the data? Two dimensions. Remember. X and Y, that's it. X is only one. x is a two-dimensional okay x1 x2 y is the label why is the label it's typical you know this psychic learn convention that why you reserve for output or the label x capital x is the vectors for the vectors for that. Got it. Yeah. So clearly, if you see here, this data is a little bit more on the clustered line. This is the training data set, and this is the prediction. The predictions are fairly good, except in the training, there's actually flaws. For example, do you see this triangle sitting in the middle of inliners? There is no reason to consider them outliers. So one of the nice things is that the test data set is pretty smart i mean the predictions from this is good it doesn't consider that those as outliers right so see when you look at data in two dimension it can be pretty misleading you may come to the conclusion that oh my goodness the simple methods work so well but or try it on mnist or something like that and then you realize that when you are looking at data at 786 dimensions and so forth it begins to it begins to sort of degrade pretty uh pretty much like you need more powerful methods but that doesn't come through because at this moment all the data sets have taken in two dimensions so what is the purpose of doing this lab? The purpose is guys to become familiar with this libraries. You will use it in your real life quite a bit if you are in this space. These are easy to use. As you can imagine, this code looks like your typical scikit-learn code that you guys have taken with me before. It hardly even merits explanation. For example, do you think this sentence or this line of code merits explanation, guys? Straightforward. Very straightforward. It's just using the right thing. It's not like adding noise. That is right. And all it does is, the fact that it follows the scikit-learn convention means the learning curve is close to zero the only meaningful line here is lof do you notice that they use lof right and now pay attention to do these three lines because it's using lof here but whereas here we used where am i We used, where am I? We used KNN. Do you notice that we just changed one line between the two, the two algorithms, I mean, the two code bases are practically identical. The only change is this, that's all, right? There they use KNN, here we use LOF, and that is about it. Which is why, I mean, I was just thinking, is there any value add if I were to rewrite the code in some different way. The end of it I realized that it is the same thing you end up rewriting so I've left it as such. So this is the clustering part of it. Now what I would suggest is today in class, play with this, change the parameters, change the number of data points, change the number of clusters and so forth and see how well it works. So your homework literally is to go to this examples and change the values, change the number of data points and play with that. And I would rather, because now that I know you guys are very busy and you're hardly even taking the quizzes, I want to give the last hour to your just doing it here before we end the day, because otherwise you won't get time from typing it out. Now, what does it mean if you use this library? By the way, they give you a comparison on their website of how it is if they used, they have done a comparison of these creators of Pi, actually create, I think it's a one-man effort of this, has created a comparison between various anomaly detectors. And at the end of this comparison code, there's this beautiful picture. I want you folks to go and look at it and see how well different anomaly detectors have worked. They have taken a few methods and applied it to this. So you can see that each of these algorithms is approaching the same data in a slightly different way. Do you see how these pictures differ? So they all are taking a slightly different notion of what is anomaly and what is not and so forth, what is in liner. So orange areas are in liner and see how our different algorithms are they sort of resonate with each other to quite an extent the first one doesn't right but uh rest of them more or less resonate but they seem to approach if you look at the surface if you look at the way the anomalies score propagates across the feature space, from the color map, you can see that they are all thinking in different ways about the data. Same data, different approaches that these algorithms take. And it again speaks to the variety and richness of algorithms in this space. And the fact that nothing is perfect goes back to no freelance theorem. Namely for any is the data that decides which algorithm ultimately resonates best with it. The assumptions of which algorithm or approach of which algorithm resonates best with it. So that is something. And so do please go and pay attention to this particular diagram. Think over it for some time. You don't have to spend time looking at this code. It is very simple. It's the same code with now a for loop. They're going through many algorithms. That's right. Like here, you're going through many algorithms. Nothing special. i mean you can read to this code you'll realize that it's exactly the same thing wrapped up in a loop on loop of many algorithms that's what so uh then and i think to their credit uh this pi od the person who created it the developer to his credit he has made it very simple that at least the algorithms you can quickly go and use. How would you use a variational autoencoder? Now you would think that we will sit and build one, which we will do in part two of the lab. We will build this deep neural architectures from scratch, right from the base up, and then we'll train it to do anomaly detection. But today just become familiar with it. Now, again, the code. So he uses an autoencoder. The only change, of course, is using this library, you don't even need to know deep learning at all. All you're doing is for all you care, you may not even know what an autoencoder is. You just change it to the name of this algorithm, autoencoder. The rest of the code again exactly the same it's just repetition of what came above and so yeah but once you run it of course you realize what happens the model you see these are the for these are the things in your model 300 layers but you go from 300 300 300, 300, 300. It's a pretty significant. Then to 64. I mean, dense, dense. 300, 300, followed by 64, followed by 32. You see the narrowing down the bottleneck. And from 32 back to 64, back to 300. And so there it is. And then you run the number of epochs. He's running it for of epochs is running it for 30 epochs you can see that the loss begins to fall now what is the point of autoencoders just to remind you you train the autoencoder only with normal data really so in in the language of anomaly the outliers versus novelty it will you will consider it more of a novelty detector. It creates a sense of normality. And then you feed it data and you see how well it does in capturing the outliers, right? Because the outlier data will, anomaly data will cause it to have a huge loss in the output. The loss because it doesn't know, it hasn't seen that, anything like that. So it will try to match it to the closest thing it knows. And so you will end up with significant loss. So this is a recap of the theory we did on Monday. And so again, as you see, see the reason I'm not going into this code is this code is absolutely no different from the previous paragraph code, which is no different from the other previous paragraph code. It's the same code, but the only difference is this. And the niceness of this library is that you get the benefits sort of out of the box. The next thing we'll do is apply it to the Smiley dataset and see where we are. So just to recall, this was the smiley and this was the data set that I created, if you remember. And if you remember, this gave you guys no end of trouble when you tried to use a k-means or hierarchical clustering. Do you remember that, guys? That k-means and hierarchical were a disaster. Do you remember why it was a disaster? means and hierarchical were a disaster. Do you remember why it was a disaster? Because it wasn't as need of clusters, the different clusters. Yeah, for multiple reasons. First, this data has outliers. Secondly, the clusters are not convex. They are not globular. When clusters are not convex or non-globular, for example, the eyebrows and the lips, they're essentially non-globular. Then K-means, and you know the easy clustering methods, K-means and hierarchical, they get into trouble. So you have to use, and one thing if you recall, we applied density-based clustering and that was quite effective. Let us go and apply that again. This is just bringing back the code from the previous slide, but for those of you who joined deep learning directly, I think it would be a benefit. Density-based clustering looks for dense regions and then everything else that doesn't fall in the dense regions and then everything else that doesn't fall in the dense regions is outliers. We covered or recap the theory on Monday. So this is the data that I had produced. Well, this is, so you may wonder what mathematical function I used to create the data, I didn't. I literally sat down and drew this picture and then extracted the data. So now let's see what we do when I run this dbScan. It marks the clusters, deliberately marks the clusters with colors and the outliers as gray areas. So you can see that it is pretty effective in finding the outliers. Would you say that guys? Yes, and very nice visualization. Yes, thank you. So that is that. Then comes another novelty detection method. And by the way, novelty detection, I'm using it here to be aligned with the scikit language, but generally in the field is just as I said, people don't distinguish too much between outlier and novelty. Quite often you'll see the word, especially in the deep learning literature, people say supervised, semi-supervised, unsupervised reinforcement learning. So this would be like semi-supervised. Why semi-supervised? You have to carefully feed it only the normal data so that it internally creates a representation of what is normal. So there we go. This is the date. So here is the part and I will take one process here. We'll first go generate the data. So this is the data we generate. So this is a little bit of a trick what you have to do is go and pick two points and then around it sort of go generate a lot of data that's all uh nothing fancy here just some playing to create data so i've just now what is the training data do you notice that in the training data, there are no outliers? Isn't it guys? Yes, no outliers. So this is the data and I wanted to emphasize the point that this is the data that will feed into our one class SVM. So when we do that, the code is exactly two liners, even with scikit-learn, and this is scikit-learn, one class SVM and you go fit to it. That's why the nice thing with PyOD is it is aligned with scikit-learn convention. After that, you just go and check how much it worked and what the error rate was and so forth. And then you visualize the results. This visualization of the result, by the way, this code is a typical code that you find in psychic lens, very nice in the library, it helps you divide the feature space by the anomaly score. So what does it say? It is creating this, it has created this red line, I don't know if you can see that. Do you see this red line here? Yes. It is saying that these are my regions of normality. This is my definition of normality. Anything that is out of it is to various degrees an anomaly, right? A deep blue is soft anomaly. Now, and the lighter blues are big, more and more, the lighter it is, the more anomaly, the higher the anomaly score, right? The more anomalous it is, and the more it deviates from the definition of normality. So when you look at this one class SVM, would you say that it seems to be doing a fairly good job? Yeah. It seems to be doing a pretty good job here. So that is that. Then likewise, we have another algorithm we talked about in the class the last time, which is the isolation forest. What was the point of the isolation forest? Unlike random forest where you go about splitting at points of maximal information gain, in isolation forest you relax it. You split at random points, right? In rectangular regions of random points. And when you do that, then what happens is, and the same thing. Again, the two lines of code. Everything else remains the same. Just these two lines of code. By the way, this generator code here is exactly the same as the generator code we saw a little while ago. Nothing different. And this is the generator code we saw a little while ago. Nothing different. And this is the visualization code because it was all identical code except for two lines of change. I put it all together in one place. When you do that, see what the isolation forest has done. It has marked these red points here. I don't know if you can make out the colors these outliers are red and the in liners in layers are all with white and black right all these points have a black border but the inside of in liners is white which is hard to tell because it all just because big black and here it is red maybe the red ones I should make it stand out. Red and S is equal to, let me do this. Let me make it 50. Let us go and generate this. Yeah, now do you see that the red ones are standing out? Let's make it even bigger, 100. Good. So now you see the outliers standing out. I mean, the anomaly is standing out, isn't it? Very clear. Yeah. So that's that. This is isolation forest. Now, you can train in isolation forest in a purely unsupervised manner or as a semi-supervised you know in other words as a novelty detector so in the unsupervised you'll feed it all data if you feed it all data it seems to do a pretty good job right uh in picking it up on the other hand, if you want to feed it this, okay, sorry, I take that back. Isolation forest is only works with unsupervised, like you have to give it all data. You can't treat it as a novelty detector. I take that back. Now with isolation forest, there is one problem though. This is a problem with random forest, et cetera. Also in in isolation for do you notice that it it creates like a manhattan grid kind of manhattan kind of streets all you seem to get the sense of these streets when you look at the outlier you know these boundaries what it has created lanes which have the same outlier factor isn't it it? The problem with these kinds of a perpendicular distinctions is, do you see this red point where my mouse is pointing to guys? Yes. See, it has marked it as a very soft outlier, isn't it? Do you see the light blue sort of a corridor that is all around it and this point also it has marked it as soft outliers which is problematic but because if you really think about it this should be a hard outlier isn't it there shouldn't be a soft outlier it's pretty far from both of these and so that's a limitation because of the, what it does is that if data is along a diagonal and at the other diagonals, I mean the other street intersections, the points that are there, they tend to not be recognized as strong outliers, right? The outlier score is, or anomaly score wouldn't be that big. So this problem, obviously the easiest way to fix that is, don't go perpendicular to the axes, just make random directions. So it goes to the point that I was mentioning. One of the lessons people learn with decision trees and with random forests and this whole tree-based algorithm says, and it came rather late in the day that many and in fact many almost all cases where we were bifurcating along the perpendicular axis you know along a line along the axis if you change your algorithm to go in random directions instead right random slopes and intercepts basically random directions with a split. Then generally the performance of those algorithms, they increase. And that's a very good observation, right? Because it removes these artifacts that it is only making decisions along perpendicular regions. I mean, rectangular, grid-like regions. It doesn't happen anymore. So for example, for extended isolation forest, we can go to an example. The original paper is here. I've given a link to that. The summary of the paper is very readable. So it goes on to say that we extend the model of anomaly detection for isolation forest. This extension we called extended isolation forest. We motivate the problem. So basically somewhere it says that all they're doing is you take random directions. So here we go. Yeah, so you know, the street perpendicular kind of a street, this is not the best example that they take. Yeah, this is it. So what happens is, if you look at this example, which is the example that we are taking, what happens is if these and these are the inliners they're this ghosting effect this place where my mouse is in the off diagnosis they begin to look like soft outliers because they are sort of it this isolation force lays things in a grade if you want to avoid that what you do is you go in random directions and when you were to go in random directions you you have, and of course they take a deliberately crooked way like this. And try to find out pliers and so forth. And then they point out that if you were to use standard isolation forest, you would be in a pretty bad situation because regions will come out as soft outlines. So now instead of going like you notice that this is it perpendicular directions you go instead so they describe the whole problem at the end of it this is the gist of what they're saying if you just go in random directions right then what happens is this is how your regions divide and the effect of this would be, which is shown. There's a lot of arguments of why this works and so on and so forth. So let's go and look at the results. So you look at this compared to the standard. By the time you get to the extended, do you notice that it is doing a much better job of doing it. But more particularly, when you look at this dataset, the sine wave thing, the first one, the isolation forest is a bit bad. Extended forest does a pretty good job of creating or following the data and creating those soft score, anomaly scores in a more coherent manner. So it's a huge improvement specifically for these two diagonal data set you notice that in the standard if you have this ghosts of region that don't get well categorized but when you go to extended forest it seems to do a very good job let's compare the leftmost with the rightmost. Do you see the improvement guys? Is it easy to notice that? Yes, fewer lanes. Exactly, that's all it is. That's it is. And so I've given a link to a more readable article towards data science, you know, on the medium.com. You may want to follow through, like if you don't want to read the whole paper, a much more readable version is here, which literally takes the images from the paper, and explains it. This is it, we can read this paper, but at the end of it, the idea is exactly that there's nothing new to it. Then, for sake of completeness, I put then we we did the local outlier factor using the PIODE. Now this is using the local outlier factor using the scikit-learn, no difference. So even the sort of constructors look similar, very similar. So it's your choice which one you use. When you do this outliers, this code should be straightforward now with the scikit-learn background to all of you, which is why I'm not going over it line by line, given the simplicity of the code. We are all familiar with scikit-learn, numpy and so forth. So when you apply that, see what it has done. It has put circles around. The radius of the circle is the strength or the score, outlier score. A smaller circle means very soft outliers. Bigger circle means big outliers. So if you look at this diagram, you would agree that it seems to have done a fairly good job right and so and again i must warn you that when you are in low dimensional spaces the neighborhood based methods simple as they are they really shine right if you remember when we did ml 200 one of the things i told you is that if your data space is low dimensional and you have a notion of distance, use neighborhood based methods. They will beat all the sophisticated methods, right? Or at least match them very well. It's cheaper, it's very effective. The same thing is not true of course, the moment you go to higher dimensions. The problem with neighborhood, otherwise all of machine learning would just be neighborhood based methods. So in in higher dimensions we start getting into trouble. So this is it. Now you can treat this local outlier factor in two ways as go find the outliers, give it all data with normal data in inliers as well as outliers or the other thing is that in the and this is just a specific feature of scikit library you can give it with the explicitly say that you know when i'm training it i'm telling you that you're going to get only inliner data so what happens when you train the same thing with pure inlier data only well you train you train it, the rest of the code is exactly the same and you can again use it to find so-called points of novelty which would be anomalies. So when you do that, it creates a region of normalcy. Do you see that it is creating a red lined region of normalcy around the data points? The normal data points? Yes. And so after that expanded radiating out from that, it will start creating the regions of different anomaly as course. And as you can see, it seems to do a pretty good job. The new abnormal data is yellow in color, I don't know if it shows so well through this. Gold. Let me do this. Size is equal to not s but 100. Oh, why 100? 200. Let's make it stand out really big. Okay, now do you see the anomaly standing out? Okay. Now do you see the anomaly is standing out? Except that the yellow point, we are trying to do anomaly because we're creating random point. It accidentally got created inside the region of normality. So this system will of course consider it not abnormal, but normal data. was to consider it not abnormal, but normal data. And that is it. So one nice comparison. So today guys, I want you guys to play with this in most of the lab, actually do the lab. So instead of me just walking through more code examples, let me see. This is a nice comparison for scikit-learn of their algorithms. I showed you PIOD's comparison of their algorithms. Now this is a comparison between three, four of the simpler ones, four of the simple. I didn't cover the robust covariance. It basically says, what if we put a bell curve around normality data right i've rarely seen people use it i don't use it so i didn't cover it so you see how these algorithms are faring with respect to each other for different anomalies quite different sort of regions of anomalies and so forth. Each column, so these are one, two, three, four, five, five different kinds of data. And you see how each of the four algorithms work on these four kinds of data. So guys, today we have had one of the shortest guided labs. The reason for that is I want you folks to use the rest of the time to the last one hour to do the lab on your own. Go and play with this and change these values, change the data set that you have. And for example, in PIOD, try different algorithms. I've just given example with a KNN and with the LOC, LOF and with auto encoders. But there is a long list of models that they have. Go play with different models and change the data set and just do this lab here so that in case you get busy it's holiday season you're busy with kids then you would have at least some practice this lab is all uploaded to the website so please download it or put it in your notebooks directory you know in, where the rest of the notebooks are, put it in there. That is it. So in other words, it is here in my case, do you notice that if I go up, this is all the notebooks. So where you have all the other notebooks like auto encoders, et cetera, et cetera, unzip this anomalies directory, right in that directory and we'll be done. When you go in there, you'll find that you have. And here is the Smiley dataset that it uses. It's all there. And the rest of it is straightforward. So we'll get to it. So that is our anomaly detection, or at least this is using anomaly detectors, not building one from scratch. We will do the building one from scratch later. Let's consider this a slightly phase part two, or a more advanced part. We need to do that. At some point, we'll take our time and do that. But at this moment, it's very important that even if you don't get time to delve deep into creating anomaly detectors of your own, in practical life, you won't get that. But what this lab is, is something that is far more useful for practical life, knowing the libraries and using them. Having said that, there is still value in understanding how to create anomaly detectors from scratch. And we'll do a lab on that, especially, I mean, with a deep anomaly detector. The only ones that I'll do is with autoencoder. And we'll build it up from scratch. In fact, that was the reason I was leading up to, I paid so much attention to anomaly detectors. I will deliberately use the variational autoencoder. I did not walk you through the code of the variational autoencoder, and I will be doing that. So that will also be a instance in which some time is devoted to seeing a practical implementation of variational autoencoder. Remember that theory was a bit complex. The internal representation was not, just you didn't care for it. You specifically insisted that it should be a Gaussian, the hidden representation, and you sample from the Gaussian. So that theory, how does it get realized in terms of real code for variational autoencoder? We'll look into it. So guys, at this moment, the state of the art is in my experience, most of the time I don't, I, my own good thing is I always go and create a variational autoencoder for my purposes. Whenever I have to use it, I'll start with that. And then I may go on to more esoteric, like, for example, there has been further developments, there's a Wasserstein autoencoders, new family based on optimal transport. We never got time to go that deep. And of course it also gets more mathematical heavy and that's where the cutting edge is at this moment. But it's a stretch. I mean, first we have to learn to walk before we run kind of a situation. Maybe in one of these Sunday research paper reading, I may do the worstest time out in code is. But this is it. So it is 8.20 now. Let's take a break. And after that, please do the lab if you can. And the guided part is over. Any questions before we take a break? This lab works very smoothly yep actually i see if i wanted to say that um today's it was really smooth everything went through no hitches and was able to follow through every line thank you excellent thank you exactly i i completely conquer with Premjit and Kit. Totally delighted, the whole experience was very nice. Yeah. The lab went really smooth today. Yeah, so I would like to, before you guys break up, I would like Harini to present her work. She has done very good notebook on the fraud detection using what we would consider a supervised learning approach to detecting the anomalies and the fraud. So Harini, would you like to share your screen and take us all through that? Sure, sir. Asif, I have a question probably, you know, I will pose it after Harini's presentation. Sure. Yeah, do that. So while Harini, please share your screen whenever you're ready. So guys, anomaly detection, once you, the steps of learning is, or the maturity progression is, first you learn to use other people's library. It takes care of 90% of the situation in the workplace. Then you learn to create your own like for example I have created my own anomaly detectors which have variants of GAN and variational autoencoder sandwiched together then you go further you take somebody's research paper and sit down and implement it right and we will do that also I will take one research paper maybe what systems maybe we'll watch this times, and we'll just implement it from scratch and see, take you through that and use it for that. Just what will happen is when you have a large project, there is a budget to it. Like how expensive is it to miss some cases? So anyway, I'll give you one second before I let you speak. The point is that the way the game is played is that which is more expensive, letting those anomalies not be detected or the developer time that goes into creating and researching a better and better anomaly detector. So suppose you're a bank or you're one of those highly sensitive data places. The cost of anomaly, missed anomalies is extremely high, prohibitive cost. You may suddenly be out of 10, 20 million in the blink of an eye with one anomaly point may represent that that you missed. So there you put in a tremendous amount of effort continuously researching, looking into the data and creating custom models that only works well for that data. And ultimately it comes down to that. You make, you end up making a super optimized model that has no application anywhere else except for that data. And it's a progression, it takes a lot of effort. Most people's situation is such that 90% of the situation use the library of that. Another, like I would say 9% of the situation, create your own code, implement your own anomaly detectors from scratch, especially using deep learning techniques, and you're done. And the final one heart, one percent heart problems are the ones in which you just keep on researching and you keep on failing. All you can do is fail less, right right and the business will be fine with that i mean they'll keep hoping that you keep improving no amount of improvement is good enough in those situations go ahead somebody was saying something somebody was asking a question. I was asking the question. Go ahead. So you said you had to build something yourself. Can you give an example of what prompted you to build your own? That is India. I can't talk about it. Okay. All right. Right. But I can give you, hang on, let me give you a flavor of it. See what happens is when you have a data where it becomes important to catch anomalies and they're hard to catch, all of these libraries, you know, they sort of work, they take you close to the solution, but then you realize that it is not good enough. You have to sit, research, find research papers, see what they have. Then start from there, implement them from scratch. A lot of these papers, they don't reveal the code. So you have to sit through, deconstruct it, build it. Then improve upon that specifically for your data set so that's what it comes down to basically go ahead arnie please show us your nice lab i hope you can see the screen yes I hope you can see the screen. Yes. Oh there it is. Okay. So this is the credit card fraud detection data set taken from Kaggle and it has around 31 columns. So time is one of them and we won up till we 28 is PCA transform data. So highly one second by the way guys, this is real data taken from European transactions, I believe around 2014 or 2016. And the reason those features have been so opaque is because data anonymity is very important in this space. So they have done that to enforce it. But this is not toy data. This is real data. Heidi, please go ahead. Yeah. And classes are target variable, whether it's fraud or non-fraud. Okay. And so we have 284,000, 284, 284,807 rows of data, but even columns. There's no missing values. This data set is pretty clean. And this is the basic thing. and this is our class distribution for fraud and non-fraud cases the fraud cases are very like less than a percent so it is a thin green line here but yeah we only have 492 cases out of all these fraud cases so it's a very imbalanced data set. Okay. And here is just the, you know, the analysis of how much the mean of normal data and fraud data. Here I have just plotted according to the amount, whether fraud or non-fraud, just to see if, you know, I could get some distinguish something here but it fraud the amount was very high on fraud cases but not very low nothing i could find here that's the only thing i could come up with then i plotted the amount yeah one thing if you could go up see guys this is the this is the problem this i happen to know is the problem the people who do fraud right we think that they'll suddenly do a bank heist charge uh ten thousand dollars a thousand dollars onto your credit card no they're pretty smart they buy your credit card info probably in the black market and they make tiny transactions so that unless you are careful even you won't notice it on your statement. It's a fact. I mean, I personally forget. For example, I always feel guilty that I should sometimes look at my credit card statements. I don't get time. But by not looking, you make yourself vulnerable to fraud uh asaf one silly question on this so whenever uh uh like uh like these fraudsters if they buy from your credit card you usually get a notification on your call right like uh this particular purchase has been done so uh yes yes you get an email typically isn't it yeah even you get a messages stating that like if this is because they see the location which is like usually not in their regular locations where you usually buy from yeah so they are so is it not like a common thing over there that's why it's easy this is 2014 all of these practices have been gradually evolving we are learning from experience now it is the norm that you know we do two-factor authentication sometimes we do very you know the credit card companies are very aggressive if you yourself do a normal transaction you you try to fill gas in my case um if i'm driving down the california corridor to southern california and sometimes I like to take the scenic route and I'm going through the Joshua National Park and all of that. So is that the right national park? Yes, I think so. So anyway, you go through all of these territories and you go to a gas station to fill yourself up, fill your car up. You're in trouble because the first thing the credit card company machines notice is that you have never been in that area. So they mark you as fraud. And so now I've gotten into the habit of always calling them and telling them what is my travel path. But yes, now much better. Even if a fraud happens, if you're checking your emails or your messages, you'll know that something has happened. I can chime in with a little bit of context to what Prachi asked us a question right now. I'm aware of one of the credit card companies which is working with, yeah, a company that I worked with earlier, right? The models that they use for credit card fraud detection, there are runs into the 1000s. They actually, when the transaction is in flight, they use an early signature to figure out which model to pick through the secondary check. So they have defined tuned to the extent that depending on certain more transaction parameters, they pick an appropriate model to use as a secondary level of check. So it's gone that sophisticated right now in terms of how the credit card companies are deploying it. Nice, interesting. So it's not one model, they actually have several models. Yes, you have an ensemble nice thanks for that input so yeah this is a good plot nice and now i tried to see the time what effect it has on fraud and non-fraud uh if you see here so what is the x and the y-axis they're not labeled could you tell me what are they uh this is the amount yes and this is the time oh time with those amounts yes do you see it not just a little show the time on the x-axis, right? Yeah. Sorry, Kate, I didn't get you. Oh, no, I knew if you scroll down that the lower normal graph would show the amount in time in seconds, like you were saying. Yeah. Okay. So, but here, if you can see there is not a much of a difference between fraud and normal here i think they're overlapping both of them so i couldn't distinguish them based on time too yes uh no so it is about the dollar amount it's like this normal dollar amount and what time of the day those transaction happens. Right. And this is our heat map for correlation. Because it's already PCA data. Yeah, nothing much we could get even from correlations and in this data the time and amount was not scaled so i scaled it and just put it onto the data set and here i just you know try to do the scaled amount and scale the thing i just tried to see if there was an overlap between the classes for time count so that wouldn't happen here i did a pair graph to seeing each variable on against fraud like you know there's a clear separation between any variables yeah so except a few of them everything is overlapping right there is no clear separation Right. There is no clear separation, except a very few of them that we couldn't, I couldn't distinguish much there. Then did the... This is very typical guys, whenever you're looking at anomalies and fraud and things like that, the signatures are very subtle. It's not that easy. So a single variable analysis like you do on a feature wise like she has done, you must do it so that you get some sense of it. But generally the signal is very subtle and hidden in most of the situations. You go ahead. And so I'm dividing it into the target and features here and then doing the train and test split yeah directly on the data set and just checked if there is any missing values on the thing here applied decision trees directly onto my data I haven't handled my imbalance data set just to see what, you know, how the model would be. Precision was pretty good. I would rather say overfitting, not sure though, but this model seemed to be okay, but I think it is overfit data. And that is because it was classifying mostly the fraud cases. In a situation like this, the most sensitive measure, the one that I look for is the recall of the positive case. So one is your positive case, fraud case. If you look at the recall, it is 76%, which means that 24% of the fraud you miss, decision tree is missing. That would be the way to interpret it. And here also, yeah, here as Sir said, I'm more interested in precision and recall for this data set more than accuracy. We want to catch all our frauds. It's okay if we say non-fraud has fraud, but we want the frauds to be right. So yes, yeah, this is our confusion matrix and heat map. And this is the support. And here you go the ROC curve. Here, you can see, there's a few data points that are out. And this is the ROC curve for the class, the fraud class. So that seems to be close to about 90%. 90% yeah. 90 yeah. The curve of 90. Yeah, go ahead. And this is our precision. Yeah, precision required. Yeah, that's just giving us like, okay. So after this, we I have handled. Okay, I'll just take out this has become control minus a couple times yeah better now yeah much better okay okay oh too small yeah good okay this is good so now we i'm i have handled the data set like handle the imbalance nature of the data set yes so by doing uh some resampling techniques okay so first one i tried was under sampling so here what happens the minority class is the majority class records are reduced to match our minority class so only a few of non-fraud cases are taken matching the fraud number both became 492 492 so this resulted in a lot of information loss and the next one is over sampling i did the direct over sampling method here it copies our under sample data like the minority class and matches try to match our majority class here is what so as of sir was talking about smart technique here it finds the k nearest neighbors and the cluster points and that points are imputed into the data set to you know balance the data this is it creates synthetic data points. Yeah. So this interpolates between the outliers and fills it in. Fills it in, yeah. Okay. So now I applied logistic regression on all the three data sets to see what gives us a better model. So here using undersampling, a recall was 93 yes and precision here for fraud cases is really low accuracy was 97 but yeah right so the position was you know bad here and here you can see so it's really marking a lot of cases as from good cases as fraud cases even here it is yeah eight cases it got wrong so yeah admit two thousand two thousand five hundred mistakes approximately yes it's It's not good. ROC curve looks okay. Yes. But here you go. It says 98% precision, but I don't know. Yeah. No, precision is okay, but the recall is high. The recall is high. Back to that other end, yeah. Yeah. And here I used the oversampling. Here also, sampling here also it is very similar to under sampling yeah here it the mistakes are reduced though slightly yeah slightly yeah and the rc curve is okay the the again it's average precision and is 99 here then we use i use the small sm small technique here you can see the recall is improved 98 94 but again the precision is low but 98 this seems to be better in all the three data sets so i'm using smart data set to do the rest of the analysis. Here you can see it's significantly low, the mistakes compared to the other two data sets. Yeah. So here you go. The ROC curve is also better. This is also better. So I just did the threshold plot for the logistic regression using SMOTE here. Yeah. So you can explain this if you want to. Yeah, no, this is all right. So the basic message actually in these situations, some of these curves, they begin to lose some of the interpretability or validity actually the best way to look at it is directly staring at the confusion matrix so if you go to go and look at the confusion matrix you notice that you're still doing a lot of false positive and false negative type one type two errors are significant right so for example you are like in this particular example, you miss, I believe in this particular case, miss about seven cases out of approximately 120 cases. But you have a huge false positive rate. 1,700 cases are marked as false positive. So the whole question is, is that tolerable? What is the impact of that and so forth? So that has to be, that is the main problem with these methods. So continue on this, Harini. Now apply the anomaly detectors that we have and see how well it does. The best model I could get was decision trees on smart. Yeah. And even random forest performed really well. Yeah. So show us the confusion matrix. This is very good. Random forest. And even we got, I got the features that were important. Yes. So here as our graph showed V14, V10, and these are the five important features. Nice. That we could, you know, use to do our SVMs or further analysis. How did SVM do for this? Pretty bad, sir. Pretty bad. Yeah. Very bad. The reason is you're looking at pca data yeah so there's not much chance to go to a good type of space i mean to a good mapping space because it's already gone there and come back with pca so good okay so that's it and I'm trying to do the auto encoder bit, but I'm stuck somewhere here so that we can go about it. So hang on, you're implementing it from the scratch? Scratch, yeah. Very good, very, very good. This is what I'm, I did this, but. Yes, very good. And you are, okay, nice, good. So 30 input features, so I'm just using 30, 15, and then 15, 30. Yes. To matching the input and output size. Yeah, I would say that your internal dimensions, you don't need to keep it 15. Keeping it to 8 would be already good enough i think okay try that out see where it is okay i'll try that yeah and remember that when you train it you only train it on normal data you know with the label zero you don't trade it on all of it. That's the important thing to remember. Go down. Actually here, this is, okay. The model is coming up, but. The learning rate is a bit aggressive. Yeah, I was just trying, but here I got, it says it's not defined, but I have defined defined it you may not have run that section that's another something so what you have to restart and run the whole kernel sometimes you get weird messages very good very good notebook very good progress and very good notebook I'll just work on that and get back. Yes, do that. Excellent. Alright guys, any questions? Any other thoughts? Otherwise, the guided part of the lab is over. Please use the rest of the time to take your break and do these things on your own. The reason I am sort of keen is that, you know, you committed the time till 10 o'clock today to this workshop. So hopefully with your family. So why not use this time to finish off the lab, go play with it, change the parameters and see what you can do with it. Asif, Sudeer here. The question what I have is, you know, when we went through this 100 dataset workshop uh you know we uh you know spend time across this uh psychic learn uh lof one class spm uh localized outlier factor all this stuff but uh pyod is is that something specific to uh you know deep neural networks or even we can use the same for the tradition no no no they contain a whole mix of classical and deep learning methods they're positioning i mean see here's the thing i don't think this project after that has evolved very much we can see the last commits on it right but somebody made a very good effort to put all of the anomaly detectors in one place But somebody made a very good effort to put all of the anomaly detectors in one place. Okay. It is really an admirable effort. I don't know how much it is followed, but When I look around the landscape for open source libraries for this field, they're not that many available. Right. And this is the, I would say, pretty good library out there there so do please play with this it contains everything from basic knn and lof etc one class svm all of those are there when you look at scikit-learn it just contains three four of them right one class is from isolation but this one is a much richer it has a list of approximately 30 algorithms but it should be enough for you to get your work done okay okay right includes deep learning as well as normal thing but now it's base version for uh you know pyod so what what what is you know what is the foundation on which pyod is built on well they have written you can look at the code let's go and look at that code if you look at this pi od hang on they have given all this thing is here see this is a beautiful thing right the pi od oh sorry uh yeah this is the by the way this is nice let's let's go appreciate uh this particular creator he's a cmu carnegie mellon is a pretty top tier place in fact the top tier place at this moment in computer science and he has created this library and uh pretty respectable a very good library i must have been he has put in certainly a lot of effort if you look at the source code, let's go look at that source code. Oh my goodness, I am not indeed. Yes, so let me go back and let's pay tribute to this person who has put in so much effort to doing this. This is it, Yuzav, and he has created it. He's from Carnegie Mellon, PhD. By now, let's hope he has graduated with his PhD. Very well liked, he has a lot of followers, right, and CMU is of course the top tier place at this moment in computer science. course the top tier place at this moment in computer science and if you look at this particular priority you notice that it has received 3.5 i mean 4k likes and people are watching it if you look at when was the last time things were updated hang on we can find that out by going to the 10th of april 2009 that is right so you can pretty much imagine that he got his phd around that time and he has moved on and become busier oh no he has improvement see last month is the piod last improved categorical generation last month so he's still maintaining it it seems one, improved categorical generation, last one. So he's still maintaining it, it seems. Right. Here is the insights on active pull, not that many people, but this is it. So if you look at the code, I guess the question that you asked is, the code is very straightforward. Like for example, let's go to the autoencoder, because I just taught you guys the autoencoders, isn't it? So if you go and look at the autoencoder code, just taught you guys the auto encoders isn't it so if you go and look at the auto encoder code uh he's used tensorflow but even then we can you'll be able to figure it out hidden neurons activation so here build model do you notice that it's a very straightforward build like layer by layer no different from pytorTorch. And there is no magic, there's nothing extraordinary about the code. It is a very straightforward code that you would write. We have written that in PyTorch, he has written, but to his great credit, what he has done is, he has taken the time to write it for all sorts of algorithms. Even very basic ones, like for example, KNN. Putting it all together in one place in a uniform API so that you don't have to fiddle around with different things makes sense and very well documented if you notice. He has taken quite some pains to document it properly. And nearest neighbor is nearest neighbor right here this is it you just save some elements and then you do you just do a comparison with them then whatever else that you want to look at a local outlier factor or some of the start with some of the simpler ones so that they make a sense to you but the nice thing is that he has put in efforts to make it aligned with scikit-learn. You notice that he has put in effort so that things are aligned with that. I would suggest that you will learn a lot simply by reading through this code. So here it is, one class SVM. Once again, if you look at the detection, do you notice that no one file is too big? He has a lot of comments, but when you, and then you can read through that, but when you read through this, nothing very odd. And in fact, he has used the one class SVM from scikit-learn. So what is his value? If you're just using scikit-learn, then what's the value of his library? His library value is he has given you a common interface so that with one line change, change the name of the model, but the same code you can do anomaly detector across detection using so many different models. Like you see the value of that. That is it. Very neatly done and well done. So that is it. In terms of deep learning, he has not gone too far. Autoencoder is about the only one, XGBoost is here. I don't think he has used many, two, three deep learning libraries I think he has used. I mean, a model, here is he has used. We three deep learning libraries i think he has used i mean a model here is he has used we can go to the documentation and see which ones he has done uh installation somewhere here yeah implementation so if you go so do you see the long list of implementation and all of this you have to just change one line of code to be able to use any one of them. So that is the point. He has put it all together. Yeah. So all of his implementations are, these two are variational. I mean, these are autoencoders and this is GANs, Generative Adversarial Networks. That is it guys. Yeah. For training on the project, how much text do we generally need to have? Sorry, I don't know. To train a project what is the yeah for the project the training part yeah how much text do we generally need to have you see this particular the eloquent transformers you can get away mostly with pre-trained models and occasionally fine-tuning so fine-tuning you don't need too much data. Okay. Yeah, okay. Thank you. Yes. All right guys, so that was the topic of anomaly detection. Should time permit at some point, I would like to do an out of session sometime, an extra session whenever I get time to just show you how to build autoencoders on your own. In the meanwhile, you can just go and read this code. See, none of this is hard. It's all very straightforward. And if you become familiar with the technology gradually, then it all becomes easy. You know all the theory now. We have done generative adversarial networks, we have done just about everything. So read this. So let's go and see how the Sokal is done. Yeah, so this is it. Well, he has done it with TensorFlow, so you'll have a little bit tougher time becoming familiar with it. But if you know theory you won't see you see what do you have in a in a gan generator and discriminator right and you keep switching between training the two back and forth that's it the entire implementation is like including all the code and more than 50 60 percent of the lines are code so the total implementation is barely 100 lines or less 60 70 lines very easy to understand right to create generator the only thing that matters is your decision function create generator that's all from gal base so he has put it in the okay gal base here is where he creates the generator we can go and look at what generator it's great generator how does he do oh yeah so this is it this looks pretty similar to pie torch also isn't it except that the word wouldn't be dense, it would be the word linear. Linear layers. So no mystery to this. One good thing with deep learning is once you understand the theory well, the code, all the code looks simple and easy to understand how do we need to put you on mute okay so this is it guys so that's all i have for today uh i suppose it's already nine o'clock so you guys can have your break i would suggest that I'll close today from now. If anybody has questions, you ask. We are done with today's session, but do the labs, guys. Do play with it and create new examples. Go to three dimensions, do this, do that. Create more clusters, play with the data and see how it works. And play with all these other algorithms. The whole point of Piotr is you can with one line of code change from one algorithm to another and there are 30 of them here close to 20, 30 of them go play with all of them. Any questions? Asif, I do, but it's not related to today's topic. We needed your time tomorrow, so thank you. Sure. For your project, right? I'll give you. Yeah. Definitely I'll give you time. What time? Or you'll let us know later? It's very hard to tell. My days are not my own. Okay. I have to like for this, I have about a half an hour. I have one hour to have dinner then get into India meetings all right guys so have fun no no one is asking questions I will stop the recording and you guys please do take the quiz otherwise, we are in a peculiar situation. All the data suggests that you guys have fallen way behind, your projects are behind, your quizzes are behind, but at the same time, at least a subset of you want us to go faster. So you see the the divergence between intentions good intentions and reality so catch up catch up if you can that's it guys are we coming back when right now or you're standing there is no coming back back i mean right now or you're standing there is no coming back can we just we'll just submit questions on the lab channel in slack yeah you can do that i mean if you guys want me i can linger around for another half an hour while you do the lab like i mean for me i have actually run through the entire lab while you were walking through it so now it's more playing with it. So if you're gonna come back, yeah, do that too. Right. So all right, go have your dinner. I'll have my dinner and I'll be watching out the channel and if needed, I can keep the Slack going. I mean, this meeting going. Thank you, Asif. Okay. Asif. Okay. Asif sir, the last quiz, can you make it multi attempts please? Not yet actually. It screws up the statistics. See what happens is that sometimes I get the impression that more than half the class has taken and then when I go in there, it turns out that four people have taken it three times. Okay, because i lost some connection and i had to leave it halfway i couldn't complete it nobody's i'll do it after the review yeah i can do that sorry about that i'll see if i can just reset it for you so that you can do it yeah i'm just trying to keep track of how many people have done it. Eventually, I'll make it all multi-choice. I'll keep repeating it later on. Thank you, sir. You're welcome. Are you guys liking the quiz? Or why is it that attendance is so poor in that? Is it not proving useful, the quizzes? No, it's very useful. I think quizzes are definitely useful, Asif. I think they need to just jump in and go ahead and take the quiz and even if they do terribly, you know, you'll learn from it. That's right, that's right. The point is that this is not, you know, this there's no your annual reviews are not based on their reports on these quizzes, your annual performance reviews. So you learn, it's a reality check, that's all. Suppose you get minus four, what does it mean? You're like, your understanding is on shaky foundations at this moment, you need to go back and review and come back and take the quiz at a later time again. And in a few attempts, when you notice that your scores are becoming closer to max, you know that you know you have picked up the subject, the topic. All right guys. Thank you, Asif. Thank you, Asif. Thank you. Thank you. Thank you, Asif. Thank you. Thank you. Thank you, Asif. Thank you. Thank you, Asif. Asif? Mm-hmm. I think I can't, I tried the quiz. I don't think I can retake the last one, the word embeddings. Yeah, at this moment, as I said, I've disabled the retakes because otherwise I don't get the statistics of how many people have taken it. What happens is that some people take it three, four times and it looks as though the whole class has taken it. What happens is that some people take it three, four times, and it looks as the whole class has taken it. You have the situation right now where half the people who have taken it want to take it again. Yeah, yeah, I don't want to. Let everyone take it. And after a couple of weeks, when I know that no more attempts are likely to come, I'll open it up again to repeat. And also, you shouldn't take a quiz right immediately afterwards because you have seen the answer you should wait for a couple of weeks so that you have forgotten the answers from the quiz and then you have studied the subject and then you'll have a more realistic score otherwise you know if you have taken the quiz i give you the answers you'll remember that and you'll retake the quiz in your mind those some of them will still be remembered and then you will get a skewed understanding of your score i mean of your askew sense of your understanding of the topic so you have to wait to get the answers that's a question yeah uh forza teach these things that you were trying to do uh what would be sort of the daily structure