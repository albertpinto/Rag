 Welcome back everyone. This is Thursday evening in California. It is our fourth theory session. We'll start by reviewing in a very condensed form what we have learned so far in the previous week. Then we'll move on to the new topic. The topic for this week is classification. How do you create algorithms that can classify inputs into various categories of classes, for example cats and dogs. So with that background, let's start with a quick review of what we have. Now, last week we did regression. Regression is essentially one way to think of it is, it is a machine that can take inputs and produce a number. A vector goes in, a point in a vector space goes in. That's a more abstract definition. And then a feature vector. And then what it produces is a number as output. Examples are the entrepreneur, how much ice cream is he likely to sell on a given day, given the temperature? And if you bring in multiple variables, you can say temperature, wind speed, day of the week, whether it's weekend or not, and many other factors. So then you are creating a multi-dimension feature vector. We have been taking that approach. Now I'm going to give you a slightly different geometric intuition tool. And this will capture the gist of all that we have learned in regression. So try to visualize this. Imagine that you have a sheet of, you know, this aluminium foil that we use to wrap food in. So you have just taken it out. It's completely planar. Right? Now, imagine that suppose there are two variables there, the wind speed speed and temperature and you're trying to predict as a y as a vertical axis the horizontal two axis are wind speed and temperature and the vertical axis of the data is the amount of ice cream that you sell you have data now if the data truly has a linear relationship what you will find is that your solution will go through the data. It will be a plane. It will be like your aluminium foil, okay, that will be going straight through the data. Are we together? When it is a plane curved then you call it a surface it's no more a plane it's no more linear so in general the definition of a surface is that at any given point locally it looks we only deal with well the word that we often use here in this field is manifold. It's an embedded manifold, which is a fancy way of saying that if you look locally, it looks Euclidean, it looks flat. But when you sort of zoom out, you see that it's a curved surface. So the example of that was, for example, in two dimensions, we took this example. This line was going through this data. So this line is a curve. And, but if you look at any one region of the curve, let's say you look at this region of the curve, it, it can be approximated. So if you look at the data, suppose the data is like this, and you have fit this curve to the data. Now, the way, instead of thinking of it with all those algebraic equations, some squared error and so on and so forth, a much more intuitive way is simply to think of it that you have passed a smooth curve through the data. Are we together? And this smooth curve at any given region, if you look at it, small tiny region, if you are a little ant, it looks like a plane. That's sort of baked into the definition that it is smooth. So in other words, at any given point, if you look at the unit vector that's perpendicular to that point in that surface, this unit vector keeps changing direction. Here it's different, here it's different, here it's different, and it's a different geometry contusion we're developing. So now I will stop here. And for arbitrary reason I took it up, it could have been pointing down or something like that. So you can see that as you take this vector that remains forever perpendicular to the curve, so as you move along the curve, as you go from A to B to C to D to E to F to G to H, to F to G to H. The vector, orthonormal vector, is called, the word is a unit perpendicular vector, the word is orthonormal vector, or more briefly just the normal vector. It keeps changing direction smoothly, isn't it? Now, this generalizes to higher dimension. What is a curve here in two dimension? In three dimension, it would become a surface, right? And one special case of the curve is if the unit vector remains, if the unit vector at perpendicular vector remains exactly, remains the same, it points in the same direction, wherever you are on that curve, that can happen if and only if the curve is actually a straight line or a plane isn't it the definition of a linear a flat plane or a line or a hyperplane is that if you look at the unit vector or the unit vector everywhere would be the same you could have the same direction are we together so now the intuition will carry and so these kinds of surfaces are people often called manifold. There is a more technical definition of manifold that gets into a little bit into measure theory. We won't get into that. But for our purposes, a manifold is just something which doesn't have precess. So something like this is not a man because it has a pointy place where it is not smooth. It is a crease. It's a non-differentiable point and just a smooth surface will roughly speaking call a man. And for machine learning that is good. So now the intuition is that there are only two things possible. Either the null hypothesis is true. So the null hypothesis would say for this data example here. Is there any line or curve or any kind of curve or line, straight line, which is a special case of a curve, that approximates the distribution of this data. You would agree that there is no such thing, right? Because there is no underlying relationship. So for example, this could be the stock price, price of some company, let's say X, and this could be the number of penguins are jumping into the sea. The sea from some island, from some glacier. So you would, one can plausibly believe that one is not related to the other, isn't it? So here you would say null hypothesis is true. There is no relationship. And this is generally true. You have to ask yourself, is there genuinely a relationship? But when a relationship exists, it shows itself geometrically by the fact that your data points don't occupy the entire two-dimensional, like for example, it's a two-dimensional plane, right? Two-dimensional plane, they don't occupy the whole of it, they're not uniformly scattered everywhere, but they tend to be, they don't have to be uniformly scattered, but they're sort of isotropically distributed, in all directions they are more or less similar distribution. They won't be like that, but they will show, they will be clustered around or very close to some embedded manifold something in there like for example this curve is sitting into this red curve is sitting in two dimensions right the curve itself is of course one dimensional it's a it's the bending of a line but it is sitting there and all the data is proximate to the curve isn't it it's close to the curve and so one way of defining uh regression and all that we have learned from a pure geometric perspective is we are trying to find that manifold a generalization of a surface basically a surface smooth surface that is closest to all the points. So imagine that all the points are pulling the surface towards itself. Each point is pulling the surface towards itself. Now, the sum total of attraction from all the points, what will it do to the surface? It will make it at the equilibrium state or the place that it will stabilize would be the solution to your regression problem. Isn't it? There is a very interesting actually intuition that comes up. When you use the sum squared error, remember you use the residue square then the intuition almost is like this that if you imagine springs from each point there is a spring that is connected to the surface of the curve it is pulling the it is literally pulling it towards itself and as you know i mean if you this is a very basic fact of school physics, that the force by which the spring will pull this line towards itself, the surface towards itself, will be proportional to the distance. If this is the r, it will be proportional to r square. It's sort of our intuition that the further apart, the point is from the surface, the harder, quadratically harder it will pull towards itself. And in the case of a linear absolute error, some essay, some of absolute errors, it's a linear pull, like twice as far will just pull twice as much. But in any case, you can think of it as attraction, that all the points are calling the surface to come near it and pulling it towards itself. And the surface will therefore bend. And imagine that your surface is flexible. So it's like your aluminium foil. You can sort of bend it, right, and it will take the shape of your data. And this generalises to higher dimensions, to any number of features. So one way you can remember in a geometric way is that linear regression is the search for that surface in the data space that is closest to the data, such that data is just proximate to it, hovering around it. That's one intuition. So we'll carry that intuition forward. And now we will do a different class of algorithms. Any questions, folks, by the way, with this intuition before I move forward? scale folks by the way with this intuition before I move forward wait can you repeat what the vectors for each point are like how what is each like what is what quantifies the strength by like that each point pulls the surface toward itself distance see you remember that the sum squared error, if you remember, what is a sum squared error? It is summation over I of all the residues squared. Sum of all. Oh, okay. Now, if you think of a residue as the displacement of a string, connecting a point to the solution, then you realize that this is a measure. This is a measure of how strongly that point is pulling the line towards itself. But the curve- Okay. And it sort of maps to the intuition of a spring, very strongly, the further apart it is, the more strongly it will go. Right? Yeah. That's sort of a physical intuition, you can. To go with this geometry now we will now move to another class of algorithms that are called classification, so this is a week of our classifiers. patient is the process that we do what is Classification is actually quite interesting. The example that I gave initially when I spoke of classification is that the story was that you have taken children to Meadow. There are cows and ducks there and you are asking a child what animal is this is it a cow or duck and the child has only two choices start to say either it's a cow or it's a duck and this process of taking a input an animal and saying that it is a coward identifying it it is classifying it. You say cow is a class, duck is a class, and the category contains just two classes, cows and ducks. You have to choose between these two classes. Why do we call them classes? The reason we call them classes is because ducks is a class. There are many, many ducks. They all may differ from each other but there is a notion, there is a conceptual notion of a duck that the class represents, right? And likewise for the cow. There's a conceptual notion of a cow, right? So for example, your notion of a duck would be a small, feathery, beaked, webbed feet little creature. Your concept of a cow would include features or attributes like big with a sushi tail, horns on its head, and four feet. Right? Those would go along with your intuition or your conceptualization of what a cow is. Are we together? So this process of classifying or identifying the class from a whole finite list of classes or a category, it is called classification. It goes beyond machine learning. Some of the oldest theories of human knowledge have posited that to know something is to be is to be able to identify its attributes and identify understand the attributes and identify it as one of the known things right whether so if you look at the speculations for example going to the oldest school perhaps the oldest school of indian philosophy which is the sankhya philosophy in the writings of kapil and so forth a statement like this is made that all of human knowledge is you cannot gain knowledge unless you from memory you can say that you know when you see a cow you have a notion attribute of a size you have the attribute of a hoof and certain attributes like these things they help you identify it as a class of animal called cows identify it as a class of animal called cows. Right? If you, but if you were to encounter an animal whose attributes you have never encountered, one of the philosophical thoughts is, how would you even know it is there? Your sense organs are trained to pick certain features from the external reality. Your eyesight picks certain attributes your nose picks uh odor smell right your your skin perceives touch these all provide you with some feature of this unknown object in front of you because you can perceive it through your sense organs and your sense organs have evolved to to perceive this small subset of features that external reality it helps you identify what it is and that identification is to put it in a class that your mind somewhere in the memory you have the notion of a cow you have the notion of a duck and therefore you can can add to follow. So the speculation, one of the theories is that much of human knowledge presumes memory. And it is the interaction between your external reality or data that you data that your sense organs bring, and a mind is a classifier that immediately classifies it and puts it into the right bucket which is there in your memory, which is there in your mind. It's classification. So this is what classification is. We will formalize this speculation or this way of thinking by saying that suppose you have feature vector x1, x2, x3 and it could be for example size of the animal then or whether x2 could be the weight of the animal right x3 could be what should we say whether it has the size of the beak so obviously the size of the beak for a cow would be zero right perhaps i'm just taking perhaps not the best features but you can imagine that you can imagine a set of attributes that uh you the child will learn to identify maybe you'll educate the child and say do you see a beak or not if If you don't see a beak, then it is not a cow, it's a duck, things like that. So educating a child, the concept of a cow or duck is very similar to training a machine learning algorithm, a classifier. So you give these meaningful attributes, these features into so this we can represent by now you must be a familiar so you can represent it as a vector in n domain so let me say this is m dimensions so m dimensional input input feature space right and what do you do? You then feed it into a machine. This machine will say either one of these things. It will say cat or dog. Cat or well, not cat and dog. The one that we took is cat and dog has become cliche, so'll use duck and cow are we together right you it will it can give this and it will identify it as duck or cow but it can do as an intermediate stage it may also say probability of a duck it may say hey you hey, you know what, I'm never sure, it is not a binary answer that it is a duck, it is a cow. I may say that I believe it is a degree of belief in a Bayesian way of thinking, probability is the degree of belief. So you say that my degree of belief that it is a cow, let's take cow as the positive step, is something, 0.9. So then your degree of belief that it is a duck has to be the complement of that. It has to be 0.1 because it can either be a cow or a duck isn't it so your machine can produce either a probability and then from the probability you can set a cutoff so let me break it down into two steps you can say that i have a box and an input vector goes in, which stands for x1, xm features, the m-dimensional input vector, it goes in. This classifier first produces, what it really produces is a probability of a cow. It produces the probability of a cow. But then what you can do is you can apply a threshold. You can apply a way to break apart and say, well, if probably if p is greater than some threshold, the most intuitive way you may say in this particular case is let's say 50%. If your degree, if it is more than 50% or more, then you say it's a cow. Else, it's a duck. So you chose a threshold from a probability. Now I will pose a question. I choose the middle point of belief. If your belief is stronger that it's a cow than that it's a duck, you go for cow. Is this a good way of doing it? Can you conceive of a situation where you would want to put the threshold, identifying it as one thing or the other, and not like this, but as something different? I'll give you a minute to think about this. Anyone? Here you're assuming that x1, x2, all of them have equal weight. Those are inputs. Yeah. It doesn't matter. They're inputs. It doesn't matter. They're inputs. And internally, this machine knows what importance to give to each feature. So, assume that this classifier is insanely smart. So let us for the time being take this classifier to be insanely smart. For the time being, so don't focus on the inputs, think of the output probability and we are taking the threshold that if your belief is stronger than it is a cow, then it is a cow. If it is, then your prediction is a cow, if, on the other hand, it is less than your belief that it's a duck, then it's a duck. So that means at probability 50%, you make a decision. Is that a rule that we should always follow? Can you think of a situation where that would be a bad idea? So I said, does it depend on the input, right? You can, you're assuming that you're feeding only cows and ducks right yeah this attributes are about cows and ducks so this is example but think of a different example so for example you could say i am trying to predict whether i'll have a punctured tire when i go driving today let's take that example so you go and look at the tire the tread Let's take that example. So you go and look at the tire, the tread, you scratch your head. So one input is the tread on the tire. The second input is what kind of rough surfaces you'll be driving on on this trip. And then now you're trying to determine to what degree you believe that the tire will punch. So you have to say it will punch out not much right or it is like how what would you do change the tire or not change the time that's your decision. So where would you put the threshold? No, but that's not the same thing right? Because here it's not whether it's cow or not a cow. It's a cow or a duck. No, so as it take it as a binary thing if it is not a cow it is completely equivalent to it being a term assume that see this is not a trick fixate on that fixate on think about the threshold can you conceive a situation where the threshold should not be 0.5 like for the car example um wouldn't you want to be like beyond a reasonable doubt that your car tire won't explode because it would be far better to have a higher threshold so that um you're more sure that your car tire won't explode that is right that is right so what we do for example with car says if we have even a 20% probability that you're likely to have a puncture, your decision would be change the tire. Isn't it? So you would put a threshold, depending on your risk tolerance, pretty low. Even a 20 percent chance of making treatment isn't it on the other hand if you want to give um if you want to determine whether somebody should okay let's let me give a few examples and you guys tell me where the pressure is these days we all go through cancer screening right So these days we all go through cancer screening. If you reach my geriatric age, then there's a special set of screenings that you go through to look for this or that cancer in the body, age dependent scans. Then if you are a women, then one of the big breads is the breast cancer and so on and so forth. So suppose you're going through a screening test for breast cancer and let's say that there is a machine learning algorithm that is automatically evaluating the the x-ray the diagram image of that extra the digital image of the digital image. And it comes up with the probability that you may have cancer. At what level would you send the person and say there is an anomaly, there may be an anomaly worth investigating? Where would you set the threshold, high or low? Low. You would set it very low. Even you would say that even if there's a 5% chance or 3% chance, depending upon how good your, accurate your machine is, with very low probability, you would want to do that because uncut cancer evolves rapidly. And then the later you catch it, the worse it gets. The worse, the harder it is. The cost in terms of both human suffering and in terms of the hospital care needed financially rises astronomically as the cancer progresses. So you want to catch it early with the least doubt that it may be cancer. You want to mark that case as positive for cancer because what will happen is that is the screening test. That person will be scared for a few days. The doctor will put the person through some other tests which are much more precise, much more expensive and accurate tests, and then it will turn out the person may not have cancer, doesn't have cancer. So it was a temporary scare, which is an acceptable amount of, acceptable thing, rather than letting a truly cancerous patient slip by, isn't it? Telling a cancerous patient that they don't have cancer. So you want to put a threshold very low. Let's take another example. Suppose you have saved money for your kid's college education. Right then, your very good friend comes and says, oh, I am bankrupt, right? I lost my job. I lost my house. I lost everything. Can you please loan me? Take an amount, $100,000, whatever you save for your college education. I will definitely return it as soon as I get onto my feet. And I'm pretty confident I will be on it as soon as I get onto my feet and I'm pretty confident I will be on my feet within a year. So you want to assess the probability. Suppose you have a machine that can tell you the probability that your money will come back in a year. At what threshold of probability would you typically be willing to give the money that you have kept for your kid's education? Because if that money doesn't come, your kid will not have a college education. So at what threshold would you of confidence or belief, would you be willing to lend them money? Hi, right, like 50% or above yeah much higher much higher no very higher minimum 95 that is right how close the friend is um so you would put the threshold high isn't it and so where you put the threshold depends on a case-by-case basis. That is a lesson that we were trying to bring about. So now that we have a basic idea of a classifier, we will go into the sort of mistakes a classifier can make. Let's take the example. Let's take, well, we took cow as the positive case. Maybe cow may not sound to you like a positive case. Let's take the example of cancer, right? It's a morbid thing to consider, but we'll consider that. Suppose your machine is saying, I have this probability that the person has cancer. has an answer and you have let us say that you have actuality actual and actual positive right positive and and negative so let us say that you know you are in God mode. You happen to know that there are let's say that there are about 10 positive cases and 90 negative cases. Let's take a number for the sake of argument. And let us say that you make a prediction, positive. I'll put a prediction as a hat. Is that okay for the sake of brevity? Right. So how many positive cases were identified as positive? Let us say that I mark it as for whatever reason, your tool is not too good. You got three of them right how many positive how many negative cases you detected as positive let's say that that was you got it you went and scared two people right how many negative how many positive cases you predicted as negative? Well, if they are 10, it has to be seven, isn't it? They have to add up to 10. And likewise, 98. How many negative cases you got as, well, 88, sorry. 88 you got as negative. So this kind of a matrix where you put, you know the actual answers when you are training an algorithm, and this is your classifier's predictions there is a there is a name for this it is for very intuitive reasons called the confusion matrix are we together? It is called the confusion matrix. Now which part of this confusion matrix is a good news part? When would you say the classifier is doing its job well? For this part and for this part, isn't it? Those are the positive cases it did identify as positives, right? And these are the negative cases it did identify as positives. What would you call this? A negative being marked as a positive, it is a false positive, because this is not really positive. Your prediction is a false positive because this is not really positive your prediction is a false positive would you agree with the language it's a false positive two false positives and then which what is much more serious is so you go and scare two does this is this language making sense guys any questions here so this confusion matrix is the root of lot of measures that people have i will write a few of those down one of them is actually accuracy is the proportion of correct proportion of light correct predictions so looking at this what do you think is the proportion here anybody would like to venture a guess what is that 91 divided by 100 91 91 right so it is a 91 over and therefore the errors what is the error rate which is the opposite which is the proportion of errors 9 by 100 it is 9 over 100 okay and so now comes quite a few other measures at this moment apart because i'll give some context later and then talk about it those are called precision recall and there so you will see a lot of jargon associated with it there would be things like sensitivity sensitivity and um what is the other one specificity and i'll explain mathematically what each of these means specificity and people also talk about the type one error, type two error. And the reason there is so much of vocabulary is because these things have been used in statistics for a very, very long time in very different branches of knowledge. And so they have all developed their own vocabulary. But at the end of it, they are all derived from the conclusion. This is the ground truth of what happened. This is what really happened with the classifier. Right? And its predictions. And the confusion matrix is the thing that tells you how well it works. And from it you derive all of these other measures. Are we together? So these are model diagnostics. Let me now introduce you to another interesting model. It is called a very interesting name. It is called ROC receiver. Receiver. It is an operator. Curve I think I got that right, the C bar operator curve. Correct me if today I'm a little bit tired. So when I'm tired, I start making mixing up these little things. So anyway, ROC curve. But ROC curve intuition is quite beautiful. But IOC curve intuition is quite beautiful. Where it comes from is this. So imagine that this thing actually comes from signal processing. Imagine that you are sending a signal through a channel. So you're sending zeros and ones. You have the signal. And here is a receiver. you have the same one and here is the receiver and just for a moment think how signals are sent through a wire if it is a copper wire right you would send a typically a high voltage and a low isn't it a high voltage may signify a one a low voltage will signify a zero and a low voltage will signify a 0. And so you send the 0s and 1s through the pipeline. Let's say that the high voltage is 1, like let me just say 1 volt, and the low voltage is 0 volts. Now what will happen is, as the signal goes through the wire, what will come out is not necessarily a perfect one volt and zero volt. You will notice that because of the noise in the channel, at the receiving end, things start getting garbled up. Somewhere along the line, there are all sorts of transformers amplifiers what not right and you're you're never able to transmit to copper exactly this sort of a way which will start smoothing over and things will start happening so you will start getting voltages different voltages like 0.6 0.3 what do you do with these right you need to determine what you would do with this should you interpret 0.6 as one should you interpret 0.3 as zero it seems likely if your threshold is half right but what will happen is based on and let's say that it's pretty noisy after a little while your system gets pretty bad so now imagine this situation the worst case situation is that all the you know the one this channel has pretty much destroyed it is at capacity or exceeded the capacity you may send one what you will get back get out at the other end is can with equal probability go to zero as a prediction or one as a prediction equal chance right and likewise the opposite of one is zero it may go equally to zero or to one right so let us say that we create one as the positive case Let us say that we create one as the positive case, high voltage case, when should we mark the signal as one? At this given time when you sample the voltage here, you see a certain value, should you mark it as one? So what will happen is, let's make this very interesting plot. It is called the false positive route. And here, the true positive route. Now, if this channel is absolutely useless, there is no real information being transmitted. Anytime you want to increase the true positive, you want to identify all the 1s as 1s one easy way not to miss the 1s is keep your voltage threshold at which you say it is a 1 high or low if you set your voltage threshold close to 0 anything above that 0.001 volt you mark it as one right what will happen will all the ones get identified the true ones get identified as ones right yes but what will happen to your false positive rate zeros will also get identified as most of the zeros will convert over because they will shift a little bit to higher voltage in the and they will start showing up as well so you will have a very high false positive rate i mean very high true positive rate capturing all the true positives will also cause a very high false positive rate. So now you say, well, that is not something we can do. So you increase the voltage a little bit. You say my threshold is now, let's say, a little bit higher. So now, again, some zeros will get interpreted one way, some other way. But let's go to the other extreme. If you go and set your threshold voltage threshold at 0.99 volts now what will happen your true positive rate will be close to zero because all the ones are getting pushed down a little bit in voltage so none of the ones are being interpreted as months isn't it so you have true positive rate will go to 0, more or less, very small. And the false positive rate will also be 0, because if something was 0, very unlikely that it will be interpreted as 1, because very unlikely that the voltage fluctuation would be so much, isn't it? Or something like that. Maybe it is. so this is the other extreme right and and you can think through and realize that if you increase the threshold like if you bring the voltage down a little bit and set the threshold there then the true positive rate will increase but false positive rate also increases because some of the zeros are now getting interpreted as ones right and so they will there's a linear relationship so if this is one and true positive rate is one like we are talking fractions this diagonal line is the line of a channel is the line of a channel that loses all signal, isn't it? What comes out as a voltage has nothing, has no relationship whatsoever to the input. Would you agree with that? Right. Now, on the other hand, let us say that the signal is fairly clean let's take the other situation you have another nice channel one zero one and zero goes in let us say that one gets diminished to 0.9 volts 0.75 volts etc and zero at most gets pushed to 0.1 volts 0.2 volts. R. Vijay Mohanaraman, Ph.D.: The boys right there what will happen and very, very rarely a one will get usually because you know some glitch in the system, it may suddenly become 0.1 so you may still have some error rate for one. may still have some error rate for one right as it passes through and for zero occasionally uh just by happen chance you might enter this 0.7 uh 0.8 or something like that right so the channel is pretty clean with just a little bit of noise so there what happens you would have if you set a threshold let's say you set the threshold to something very say you set the threshold to something very modest, you set the threshold to 0.6 volts or 0.7 volts, right? At 0.7 volts, what will happen? Your true positive rate will be near perfect, isn't it? Because the ones did not get pushed down to any further below that. The lowest value you see here is 0.75 so your true positive rate is very high but what about your false positive rate very small proportion very tiny proportion of the values will actually become you know ones so what will happen is at a very small value of this this point would be you would achieve a very high true positive rate now this point by the way is constant which is that if you set the threshold as absolute zero then even the zeros are becoming point ones so then your true positive rate and your basically you will have a pretty bad situation so what you do is oh no sorry if you set it at exactly one exactly one goal because this represents one vote then all the one volts are being slightly depressed what is your true positive rate zero right then false positive rate is also zero even in a good channel isn't it so what happens is as you go and bring the voltage a little bit down to 0.7 your performance of your channel goes like this it saturates very very quickly and because there is still some noise in the channel it goes it goes something like this are we together that you know that this is pretty much at this much is the sustained error rate if you can tolerate that error rate you get a pretty high true positive rate right so this is the signature of a good channel is the physical intuition clear guys any questions if it is not clear i'll repeat it Any questions? If it does not care, I'll repeat it. Right. So in other words, a good channel, you can set threshold reasonably and very quickly you reach pretty high true positive rates without the false positive rates going up, right? As you can see. So this curve has been used in this industry for quite some time, I suppose. Now, coming back to our situation, what is the equivalence to it? If you think of a classifier, the input to the classifier, you know the ground crew, you know the ground crew. You know whether it's a person has cancer or not because it's training data. Isn't it? So you know whether it is one or zero. But you are feeding it through the classifier. And the classifier is producing a degree of belief that it is cancer or not. A probability. That probability is again between 0 and 1. Do you see how the two situations are equivalent? Isn't it? So a thoroughly useless classifier will randomly put numbers. So it will be like the noisy, hopelessly noisy channel. It will follow this diagonal line. whereas a very good classifier will be able to distinguish the patients with cancer from the patients without cancer. And by having a very small false positive rate, you still get a very high true positive rate. Most of the cases you're catching. Are we getting the point, guys? So this thing, the way you distinguish it is you say that, how do you mathematically now represent this intuition of a curve? The way you do that is you look at the area under the curve. The area under this curve is, well, if this is one and this is one, what is the area under the curve, under this useless channel curve or the useless classified curve? What's the area? If a triangle has a base, exactly. So what happens is that the area of. It is called, let me call it a dummy classifier. Which is just making random guesses. Is one so area under the under the. The curve for a useless classifier is the straight line, isn't it? But for the red person also the area under the curve of the red classifier will be closer to 1. Sorry, this is half. Sorry, I should have corrected half. 0.5. This will be closer to 1. So, for example, this could be 0.85 or something like that. The better the classifier, the higher the area under the curve. The higher the red area under the curve in the good classifier isn't it so there is a term for this particular metric, I want to say, the term is area under ROC curve. Oh, by the way, I got this. Now I remember it is not curve. Receiver operator characteristic curve. Characteristic. So this is ROC stands for this thing. ROC stands for receiver operator characteristic. It speaks to the channel. So area under the ROC curve, therefore, is a good diagnostic measure of your classifier. Another good diagnostic measure, just like accuracy error rate precision recall sensitivity specificity etc all right somebody had a question yeah so like in this curve we are showing i mean the area under curve we are showing a false positive rate not the actual i mean the true positive right or the true positive is along the y-axis the false positive is along the x-axis okay so in the case of like cancer as if like because the threshold will be very less it should be i mean this curve should go bad right i mean it should go very down right so in the case of cancer what you do is you want to catch all the true positives right so where will you put your threshold you don't mind a high false positive you look for the saturation point if it is not moving you see at one what point you reach your asymptote right or saturation once you know that you're reaching saturation let let's say here, this point, right, you would put your, you would consider this the acceptable, acceptable nuisance, I suppose, error, because you want to make sure that you have as high a true positive rate as possible. Isn't it? You catch as many people with cancer as possible. Yes. You look for a saturation point. So this measure is actually one of the very reliable measures of a classifier and it needs a lot more explanation than just the ones derived from the conclusion matrices. And it has all to do with where you set the section. So in a classifier, like in the simplest form we say that a classifier is a box where x vector goes in and out comes an identification a cow or a duck but it's a two-step process take one of them as a positive case probability of a cow then on that impose a threshold Then on that impose a threshold. And based on where you impose the threshold, you will have a certain false positive rate and a true positive rate, and you have to decide based on real world situations, what is acceptable. Isn't it? You would make a determination based on that. This is how it works. So probability, and what is probability? There are two interpretations. So let me take a moment to talk an aside on probability. You notice that I keep saying probability is the degree of belief that it is something. There are two actually. It turns out that the word probability is differently interpreted in two schools of thought within statistics. One is the Frequentialist and one is the Bayesian Themes. And both of them are right in their own different ways or more practical well there's no right the def the each definition works well in certain situations the frequency definition says that probability is nothing but the proportion in a long number lots of trials what does that mean proportion of lots of trials lots means really lots so for example if a coin let us say is biased it you keep on throwing it you throw it once it comes out here you throw it twice it comes out here would you say that the probability of a head is one you wouldn't say that. It may be just fluke. Now you keep on tossing it. When will you know that the probability of head is, let's say 40%, heads come up only 40% of the time. And it was just a fluke that twice the head came up. One way is that you keep on passing the coin gazillion times and then you see what proportion of the time it came out. That is the probability of the head. Are we together? the frequency list probability is proportion in lots of trials so i'm deliberately using colloquial language to get the intuition across lots of trials are we together right so for example quite often you have taken the weight or the height of let's say 1000 people and you have built a histogram. One easy way to do probability is to just divide the histogram, the frequency plot, scale it down by the number of people, scale it down by 1000. So you will know for each weight bracket, what proportion of people fall in that height bracket. And so you say that is the probability that a random person would have that particular height. Does it make sense guys, the intuition? So that is the Frequentialist way of doing probabilities. There is actually an alternate way which is belief and degree of belief that something is true. So for example, we have global warming, human-caused global warming. Now, what is the probability that we are screwing up the Earth? Right? It's a belief based on some scientific evidence, right? So at this moment there is no very direct causal thing, the chain of causality is not that strong that you can say, look, this equation tells you that we are causing global warming. It's a very complex phenomenon, right? So all we can do is look at all the evidence and have a belief, degree of belief. That degree of belief, if you think about it, it sort of is also something that will go as a proportion from no belief at all right so if you talk to one group of people they will assure you that maybe the pr department of fossil fuels will assure you that there is no human human causation of global warming right human activities don't cause global then you can have the other end of the spectrum where somebody may be absolutely convinced that we are causing global warming, right? So the second person's belief is with the point of certainty and you go between back and forth. So if you say no belief at all is zero and certainty as one and all shades of belief in between a belief that human beings are causing global warming in between you realize that it is it fall it you will realize after a little bit that it is um it follows the rule same rules as the rules of probability. Are we together? Right? So, and we won't go into that, but I'll lay it out as the same probability of A plus B if they're completely disjoint that the probability of A times probability of B happening and so on and so forth. But leaving the mathematics aside, we'll leave that. So why is that belief, when you consider that belief as probability, there was very uncomfortable situation historically. Remember, I told you about that wonderful statistician, data scientist, Fisher, who flew in from London to argue that it is not smoking that causes cancer, but cancer that causes people to smoke. Remember, we talk about that. So he was actually a great statistician who contributed many, many ideas. One of the ideas we'll talk today is called maximum likelihood estimation he was uh died in the world frequentianist he believed that that's the only way to interpret problems and he looked at this whole belief based on this what you would call biasian thinking as utter nonsense so because he called it utter nonsense, literally in the 20th century, for many, many decades, Bayesian reasoning was outright dismissed. Until a lot of scientists from physical sciences, physicists in particular, mathematical physicists, they started coming into this field and they were inherently biased because everything is a hypothesis and a degree of belief there right so then the whole thing was revisited and today biational probability is definitely very legitimate and we take it to be true and in fact the tables have turned when we bring in the whole Bayesian approach, it turns out that in the pure frequentious world, some things have become dogmatic to the point of being wrong. So you will often find jokes saying, one meme that I found, it said, move all the statistics books to the, what was it? To the mythology section, because that's where it belongs. It's not facts. Implying that much of the statistics that was done in the 20th century was actually wrong. And so there's lots of thinking around it it wasn't that the theory is wrong but its application the way people were using even the frequency statistics had deep flaws for example there is something called a p-bar right it is a measure it's called the measure of statistical significance now Now you may have noticed that I went through chapter one to three, I asked you to review your textbook, but when I was teaching you I did not emphasize p-value very much. P-value is very much baked into the frequentialist way of thinking. It is a valuable measure, but biaisons don't tend to get well they use it cautiously let's put it this way but it was abused or not so what would happen is you would create some theory you would pull up some data and if the p-value was was more than five percent you would say my theory is current there's a relationship a lot of, lot of theories with nutrition and medical research of the 20th century had to be thrown out. And we have thrown them out. People have revised all those theories. When you look back at the paper and see what went wrong, it turned out that it wasn't that the frequency theory is wrong, the statistics is wrong, but the way people in a very cookie cutter way used it and made rules like this and make claims using that was deeply flawed so there was a huge backlash and today many papers many journals they prohibit you from using p-value itself as evidence of a theory being right. So now we are in a different world. So there's a biation we're looking at. Let me give you an argument for the biation. Again, this example, we have global warming caused by humans. It is or is not caused by humans. or it's not cost-effective. If you look at the people who are arguing against it, so quite often the arguments are silly. But if you look for the legitimate ones, they all come back to one statement. Where is the evidence? What are they asking for? They're asking for, in effect, if I have to interpret that, do this abstract experiment, let humans do whatever they are doing, no change of behavior, wait a hundred years and see, restore the earth if you could, and let this trial run again and this time see what happens and keep on going it. If most of the time the earth gets screwed, you can say, well, you know, we are heading towards disaster. There is evidence. On the other hand, if most of the time, the earth just self heals itself, and then whatever we are doing doesn't matter. Right, the evidence, you have to run the trial quite a few times. At the end of it, if you look around at the legitimate things, it points down to that, where is the evidence that humans have caused, where is past historic data, essentially. is past historic data essentially we have indirect data causal data physical laws and other things are there but not that but the bianchian way of thinking is sometimes you get only one shot like for earth we don't to the best of our knowledge have the ability to find in time if what is done is done it's an irreversible process the arrow of time is irreversible, isn't it, to the best of our knowledge. So you don't get to do that trials. What you have to do is based on everything you know, all the data, you have to weigh it and come up with a belief. When you do that, you may have a prior belief, but seeing the evidence, your belief may change. For example, suppose I land in a city at night and I have no idea which city I landed in. And I now believe that I've landed in Arizona during the summer. I've landed in Arizona, let's say Phoenix or something like that. But next morning, I look out of the window and it's raining. Right? That should displace my belief. I should feel a bit less sure that I'm in Arizona because it rarely rains there in summer. But it may be one of those rare days. Now I stay there the next day and again it rains that should diminish further my belief that i'm in arizona do you see how it goes data takes you from a prior belief to a posterior belief and in fact machine learning or much of science is this journey from a prior belief to a posterior and the statement, if you have enough data, it doesn't matter what is your prior belief, enough evidence will always take you to the same posterior belief. Isn't it? But if there isn't enough data, then of course it is better that you took a sensible prior belief, right? So instead of believing that you took a sensible prior belief. So instead of believing that you landed in Arizona, pick a place that's halfway, where it rains sometimes and doesn't, and then start from there or something like that. Then revise your belief. And then if it keeps on raining every day, you might finally accept that perhaps i've landed in seattle right things like that so that is the thing so i wanted to bring this once and for all that probability at its heart itself has two interpretations but now i'm going to use one of them to make the argument to introduce two more concepts. One is surprise. So bear with me guys. So we will say, how do you quantify surprise? Or let's put it this way. How many exclamation marks would you like to put in your, in your, what is that? I am emoji or whatever it is that people call it when they work on their mobile devices, right? Their cell phones. So how many exclamations? So let's try to quantify sometimes. Let us say that you know that it rarely ever rains in the desert, right? You're in a desert. Let's say you are in some place, big Sahara desert, right? Then evidence comes. The evidence is today it did not rain. Data is it did not rain. How much is the surprise? Because the probability of rain, actually let me call it probability of no rain, if I may use a negative here, probability of a dry day, rain-free day, a dry day, is in the desert it is close to let Let us say 0.999, right? Or even one, why not? And then it did not rain today. How surprised are you? Would you be surprised? You wouldn't be surprised, isn't it? On the other hand, so in this example, you would not be surprised. But suppose the probability of if the probability of a dry day is this high, what is the probability of rain? 0.001. It is 0.0001. So it is the Q is for the rain. P is for dry. Well, suppose, take the contra faction. You wake up in the morning and lo and behold, you notice that monsoon clouds are over there and it's raining. Now, how surprised are you? You are very surprised, isn't it? So now look at this. The probability, when the probability of something happening is low and it happens, you have a high surprise. When the probability of something happening is high and it happens, you have pretty low surprise. right that's it isn't it does it agree with our intuition guys so now take our common sense intuition and try to figure out how would we write it as nothing else right so you can say that see nothing else right so you can say that see it cannot can it be proportional surprise can it be proportional to by the way can it be this is the proportionality can surprise be proportional to the probability of a given event happening can it be then will it be so that when the probability is high the surprise is low that doesn't actually it should be inversely proportional right yes so one way one by p be inversely proportional right yes so one way one by p yeah you could do well you can start by saying maybe so so let's take let's take this journey so this seems like a bad right so the next thing you could do is well let's see let's let's try better maybe you can say uh let's try and there are many ways you can do it but uh let me give one of the simpler ways you can say what about one over P, just like I said so. Do that so. market like that now what happens when probability of something is low one over probability will be high so one over 0.0001 will be high. So 1 over 0.0001 will be a big number. So this we seem to like. Right? Better. Better. Right? But then it has one problem. If the probability of something is 1, what is the surprise? Well, it turns out you are still surprised because what is 1 over 1 is 1 whereas really surprise should have been 0 it would have been better if the value was 0 are we together is this making sense guys you want the expression that would give zero surprise when the probability is one and high surprise when the probability is very low we are getting close except that we seem to be lower bounded at one so whenever you have information that is lower bounded from zero one and going to infinity so it is like this interval one to infinity there is actually there are many ways to convert this convert this to a scale in which it truly goes from this range it goes to the entire zero to one one simple trick is you say surprise one by p minus 1 by 1 by p so let's try 1 by p minus 1 minus 1 by p no no no 1 by p minus 1 whole divided by 1 by p 1 by p whole divided by p let's try that. If the probability of something is sure, you could you could say it is zero. But when the probability of something is very low, then this would start bothering you. So this is one way that you could do that. There is actually a name for this. So Hari, it seems that you have taken a statistical course. So I would advise you to at this moment, help other people make guesses. So there's a name for it. It's called the odds. In statistics, we call it the odds. Odds. What are the odds that something will happen and if you ever watch the news you may have noticed that these newscasters never never speak in terms of probabilities they always say the odds the odds of somebody and the odds go from zero to infinity isn't it so that seems to be one way of doing it right but there is another way that you could do that that is and that i would like to bring here you could do it like this take the log of 1 over p and there's there's a deeper mathematical reason i'll come to it why not this but this when you take log of 1 over p you you will see that it has the same effect when p is one log of one is what's the log of one? Zero. Zero. Right. So this will now go from zero to infinity. So this gets the job done. And there are other mathematical expressions that you can do, transformations you can do, phi of probability, that will get the job done. There are many, many things. Well, if you have log of p, that will also give you the same thing, right? If p is 1, log of p is 0. Let's work it out out for p is equal to 1 log 1 over p is equal to log 1 over 1 is equal to log 1 is equal to 0 right on the other hand for p is equal to 0.001 let's say you're doing log of 1 over 0.001 that is equal to log of 10 to the power 3 isn't it and that is equal to a positive number right let's say based upon if you're doing base 10 or base e let's say actually you always do log E traditionally, right? But if you were to cheat and say log 10, well, it will be 3.27. Okay, let's say that it is a thousand would be, I don't know, seven. I'm randomly putting some number here. It is probably equal to that. So you can check it out if you have a calcite. So it is now a positive. You're seeing're seeing this right and the smaller you make it the more it tends to zero the more the surprise increases it tends to zero surprise tends to infinity Raj you're seeing this right yeah it makes sense right so now why would i pick so you realize that this is a legitimate way for me to capture surprise isn't it well odds why not this expression or why not any other mathematical function that will also do this well there is a deeper reason for it and a lot of i think this surprise that you just saw, right, a lot, it is related to the concept of entropy. And we will come to that. The average, so suppose you do a long-running experiment, right? And sometimes you see this, sometimes you see that. What will happen is the average surprise that you will get, averaged over all the outcomes that you see, and we can do a little bit of a derivation test. It is actually a p times. So I'll give you this. Let's say that you sat there in that desert for a thousand days. The probability of rain is 0.1. And let us say that, so how many days did it actually rain? 10 days. So 10 days, you had huge surprise. And a lot of other days, you had very little surprise. So what is the total surprise that you that you encountered? The total surprise that you would encounter is for the days that it rained, the proportion, and you would agree that the proportion of days it rains would be literally p, right? A p times this log of, because the surprise, how often were you surprised? Effectively effectively if i were to multiply it by thousand the probability i will come to know the number of times that it rains isn't it thousand times p is the number of days it would have been basically give or take and each day that you trained i got this much right and then for a lot of other days, 1000 times Q, which is 1000 times 1 minus B, this would be the number of days, dry days. Isn't it? Approximately. Let's say 990 days it didn't rain. When it didn't rain, you were very mildly surprised, isn't it? The surprise was very mild. So you would say, so you would say the, when it didn't train, the didn't train probability was Q. So you would say 1000 Q log 1 over 1000 Q, right? And so the total surprise is this. Total surprise is this. And when you take the total and divide it by the, you take the average surprise divided by the total. So, so in average, what is the surprise? It would be this total divided by thousand isn't it total over thousand and that that would cut out the thousands and it will leave you p log 1 over p plus q log 1 over q because these many times you'll see this are we together proportion of times that you will see this and literally what you're looking at this is called the entropy actually this is the this expression so if you were to like each of these pieces is actually entropy entropy so entropy is defined so one more measure entropy which is a measure of average surprise for something happening. So average surprise for it raining which is the positive case is a p log 1 over p. Now in the literature you don't really see entropy expressed in this intuitive way typically in your books you will see entropy mentioned as minus p log p right actually well it's p log p but in in the machine learning literature you uh you look at it like this you look at it like this. You look at it like this. p log p is the entropy and this is the negative part. Negative part of it actually. So this is not exactly entropy. Entropy is p log. So this part is the entropy part. And so the total surprise is the negative of that. Average surprise. You see it like that. But remember that intuitively it's much better to think of it as this right this is the surprise this is the proportion of surprise right gives you the average surprise and that gives you the intuition so we created another measure that we have now when you do it like this now ask yourself suppose you did a machine learning. So, so far, are we together guys? I know it's been a long session before I take a break. I'm almost near the end of this long intellectual journey. So imagine that you have a box, a classifier. You know the ground truth, whether it's a cow or a duck so and you want to create a error function right one way of writing the error is literally the error the trouble is with the error rate you can't do gradient descent it's a number how many times you got it you need an expression that is mathematically enable so that you reshape the probability the output says that it does the best classification isn't it so you want to be able to do gradient distancing you need something that has the probability baked in and therefore it turns out that using this negative p log p log 1 over p the surprise is a good way of thinking about the error think about it it was a cow cow is the positive but your machine predicts probability of cow is 0.9. What's the surprise? Oh, yes, close. But you know this is a cow. But your machine predicts like one tenth the probability that it's cow. It has a strong belief that it's a duck. Now how much is your surprise? If you believe that your machine is right, you should be terribly surprised, isn't it? You have a high surprise. And so now we are getting something. This surprise which is written in terms of probability, and therefore it is somehow derived from the input data, the way it learns from the input data you can fudge your probability predictions based on your data set you can do gradient descent and find ways to make better quality now you're coming to a measure uh error measurement that makes sense so this sort of error measurement in machine learning even the sum squared error and all of this, there's a general term for it. These are all called loss functions. This is a measure, in the simplest form it's a measure of how wrong you are. Are we together? And the amount of surprise you are, if you know the ground truth and you keep seeing the predictions of the machine, you would be surprised if it makes wildly wrong predictions, isn't it? So the more the loss. If you add up all the surprises, that is a good candidate for the loss. Are we together? That is why. So let's say that this is your, for each of the outcome that you have, let's say that the this is your for each of the outcome that you have let's say that the positive outcomes are whatever the positive outcomes are p i you will do log of 1 over p i sum over all the positive cases i belongs to positive case right and you would also add up all the surprise belonging to the negative case isn't it the negative cases log one over well what is the probability of it not being a cow it would be would you agree that this is the probability of the negative case q and so the loss function this is all and this long journey has taken us to a very interesting notion that a good loss function for classifiers is loss function for classifiers. i log 1 over pi plus summation over j log of 1 over 1 minus pj right and so in this thing people try to be clever what they do is they say notice the fact that posit if we mark so here is a little bit of now so this is the way you should remember it and always think of surprises classifier losses are measure of surprises if you carry this intuition you'll always remember this expression otherwise you'll tend to forget it now here comes the thing mathematicians can sometimes be clever just just to write things in a more succinct way. So if you take the case, positive case as one, like you say cow is identified as one and duck is identified as zero. So you can write this as one times this. We can write this as 1 times this. So now let me just rewrite this. Summation i log 1 over pi. And see the trick that we do. I'm perfectly OK to put a 1 here, isn't it? When it is genuinely a cow and positive for belonging to positive cases. And when it is a duck, this is again one but i can write it as one minus zero right now why in the world would i do something so far you'll see in a moment yes you say well where am i going with all this well remember in machine learning we use the literature that we call why the label these are the labels or ground truth isn't it this is the ground truth isn't it guys so this expression is I influenced a positive case yi because you know all of this positive case yi's will be one it's like literally multiplying by one log one minus now one minus what is pi this is the prediction right y hat. y hat your classifier's prediction is literally the probability by definition because what is your machine emitting of probabilities degrees of belief that it's a cow are we getting it and that is the y hat y hat being always in machine learning whatever it emits or whatever it predicts we call y hat so your y hat is there 1 minus y i hat right this plus summation over negative cases j belongs to negative case and oh boy this this part was again y j hat so for negative cases one minus yj y so not not yj hat it's the ground truth yj and of course yj is zero but for ducks the label is zero so one minus yj will make it one right and this is just mathematical cleverness the reason i'm mentioning it is we'll do the opposite journey in your mind and this is why j hat right this this expression now is the end of the intellectual journey we said that the loss cross entropy loss is relatively cross entropy loss implicit word is function loss function is equal to this thing are we together it is this now log of one minus y i is what is that if you do log of 1 minus plus it is y i i'll convince you that it is this see log of log of surprise the surprise 1 minus y i the prediction i can write as log 1 minus log y i hat log of 1 is 0 and so this is equal to minus log y i hat does this make sense guys right so in literature you'll always find it mentioned like this in your textbook you'll find log y1 minus yj log 1 minus yj hat. So typically, now it's one of the things that typically books will often write this expression in this form. And then you feel some squared error was so easy to understand. This expression looks, if you were to encounter it, you would wonder where is it coming from? So now I've given you hopefully an intuition. So whenever you see this in your mind, never remember it like this. How do I remember it? I actually remember it like this. You know the version that I remember in my mind, I tend to remember it like this because I like to carry the intuition with me otherwise I feel uncomfortable. So this is to me more intuitive. It is basically surprises. Right? The model has made prediction, you compare it to the ground truth and find the total surprise you went through. That is your loss. The same thing after a lot of you know mumbo jumbo, it is the same thing as thing as this right but let me just say more more intuitive true okay as if i think the second one also should have negative sign right in the cross entropy loss it's not right yes that is true plus what i forgot to do is that in both the cases this take the minus sign out in both of these cases there is a minus sign good this is right and if you break it up yes you have negative sign in agreement right so you can melt it as minus yi log yi hat minus 1 minus yi hat yj log 1 minus yj hat. Now you say, well, this is believable, but of all the loss functions, why did we take this loss? I'll let you ponder over the mystery. Could we have taken something else? Right? You notice that we went through a very interesting derivation. If you know the concept of a surprise or entropy, then maybe it is obvious. But I don't know if you have, those of you who have taken other machine learning courses from elsewhere, if you have encountered this sort of explanation. this is this is more intuitive way of looking at it but you could think of other ways there is actually a justification a more rigorous justification that comes from mle maximum likelihood estimation and we'll talk about it after the break. Let's take a 10 minute break. It is 844 by my clock, so we'll come back. I'll pause the recording stock so just let's summarize what we learned from here surprise is we chose to define surprise as log one over then entropy is essentially the measure of average surprise. P, the proportion, the multiple surprise, each of the events in that would give you this. And it's traditionally written as this is a more traditional form of ultimate and cross entropy loss function for a classifier, which is making predictors, looking at the ground truth and comparing it to the predictions, how surprised would be. And that is given by this expression. And I also said that while this expression, to me at least, looks a little bit intimidating i like to think of it more simply as like this i belong into the positive case log y log 1 over y hat plus summation no negative plus j belonging to negative case log 1 minus 1j. This is the same, it's just in my mind, I prefer this simpler simpler way to remember positive case is one negative cases so any questions here before I continue? As if it will be YJ hat, right? Oh, sorry, I forgot the hat. Thank you. So guys, thanks. Please keep correcting me. I can be quite sloppy and careless. Is it all making sense guys so now the question comes why did we use entropy why why not odds why not something else for the loss so so let's get the easy ones out of there why not use error rate well you can't do learning with error right because error rate you have imposed a threshold and make a number prediction you can't do gradient descent in that right it gets hard for many reasons now why not so there's another subtle reason why not this thing at any given moment why did we not do y minus hat because whatever the probability is of it being a cow and it's a cow i can look at the difference between the two isn't it and i can square the gap why not some squared error why not do the same thing here not do the same thing here. And the answer is a bit subtle. The answer comes out that in the case of a classifier, it doesn't, first of all, the your loss surface is not convex. You have difficulty doing it. It's not stable. It doesn't quite work well. But there is another reason that comes from which is a more rigorous reason it comes from a maximum likelihood estimation so let us say so let us define a concept problem the next possibility is likely but likelihood what is likelihood we know what a probability is. Likelihood is given by the symbol. Likelihood is a short form of likelihood. That. Given hypothesis likelihood of a hypothesis. Given hypothesis. Is producing. likelihood of a hypothesis, given hypothesis is producing the evidence, namely the data, the data, in simple terms, the data. So I'll give you an example. Let's take two hypotheses. I will take hypothesis H1 that the probability of rain probability of rain is equal to 0.1 right I will take hypothesis 2 it says the probability of rain is 0.9. So these are two hypotheses. Now I ask you this question, suppose the data is this. Let me put the data, part the data here. Data. Let's take five days worth of value. So what should we do? Rain, rain, dry, rain, rain. Let us say that the evidence came out to this. Now what is the joint probability of this happening by hypothesis one probability of rain is 0.1 0.1 probability of a dry according to hypothesis one okay so let's go through this according to hypothesis one the first rain 0.1 second day it rains the probability was 0.1 the third day it is dry the probability according to this hypothesis is 0.9 the third day it again rains the fourth day again it rains isn't it so according to hypothesis h1 the joint probability of seeing this evidence, this data, this what actually happened is pretty low relative compared to this situation here. The joint probability that it actually range was 0.9 times 0.9 times 0.1. This was the dry day, the 0.9 times 0.1 this was the dry day the 0.9 times 0.1 now which of the two hypothesis looks more probable which has higher probability the second isn't it the second one second one beats the first one hands down by a factor of almost 9900 well nine times 981 square whatever that comes to right by a huge amount by a huge factor it beats the probability like this so which is the the but suppose you have many more hypotheses h3 h4 and each of these these things these joint probabilities are called likelihoods of the hypothesis right given the hypothesis the joint probability of seeing this event because because these are independent events, so the basic probability theory, if you remember, if two events are independent and both happen, their probability, joint probability, is the product of their probabilities. So I multiplied those numbers to do. This is sort of high school probability. probability right so and even if you have forgotten the probability in a very real way suppose the probability of your tire being punctured is one in thousand right and the probability for getting a speeding ticket also is one in 1000 you get a speeding ticket every three years right thousand days so what is the probability that on the same day you'll have a punctured tire and you'll get a speeding ticket you realize that it is very unlikely that both of these will happen it It's the product of these two. So it will be one in a million. You multiply these two probabilities, unlikely probabilities, together. So that is joint probability. So this is it. Now, the concept of, if I ask you of all these hypotheses, which of the hypotheses would you use to pick the hypothesis as the best one as a model to predict the data the maximum one the maximum one what you would do is you would say what you have is for each of these hypotheses h1 h2 hn you have a likelihood of that hypothesis h2 l n right and what you are doing is find the max of this max of h1 l h2 l so the hypothesis, actually the right word is ARTMAP, that hypothesis, find that hypothesis, which let's say l, h, n. So find which of the hypothesis leads to the maximum likelihood, maximum value of the likelihood, isn't it? So that is the concept of maximum likelihood. So the traditional word for hypothesis is because if you use that hypothesis, you can use to predict, you can estimate, given input, you can estimate the output. So I would use the word maximum likelihood hypothesis in my very peculiar way of thinking, but the traditional word used is maximum likelihood estimator. That hypothesis which leads to the maximum likelihood is the right hypothesis. So you're most likely in Seattle and not in Phoenix, Arizona, if this is the data that is the most likely to produce. So make predictions. If you make predictions using the model, and if this is truly representative of the population rather than just a sample of data is representative of the population, population, rather than just a sample of data is representative of the population, you would rather pick the hypothesis that you have landed in Seattle, rather than landing in Phoenix, isn't it. So now let dry, I call it q. q by definition is 1 minus p. So now let's use instead of 0.1 and 0.9, let's take this. We don't know what p is and therefore we don't know what q is because q is just one minus p suppose you see this evidence a particular evidence which is that rain rain dry rain rain so what would you say this is the first data point one two three four five you would say probability one right given this data whatever the conditions of that day were right pressure temperature etc suppose input variables are temperature wind speed pressure right humidity Right humidity. These make your X is your X1 X2 X3 X4 and so you can go on this goes in to your box what comes out is a certain P probability function. So you can use that probability function to the first data to the second data but according to this this has to be q q3 isn't it q3 right and p4 p5 this is the likelihood this is the likelihood of this right likelihood of r r d r r would you agree this, right? So now let's generalize it to all cases. Now let's go beyond rain and so forth. Suppose you get data, it's cows and ducks or whatever it is, and you get thousands of data points. So you need to create a likelihood function function which would literally be all the positive cases right multiply so what are you doing you're multiplying all the positive cases and you're multiplying all the negative cases isn't it so a pi the product of all the pis i'll use this symbol to imply b1, p2, p4, p5. This stands for product in the language. Just like sigma stands for summation, this stands for product. So this . Likewise, a product of qj. Well, in this particular case, it is just q3. And so it is basically, you would write it like this isn't it and therefore your likelihood function is likelihood of this hypothesis p is product of all the pj's for positive or pi's i for the positive case and product of all the well in this case there is one but suppose there was another dry day all the negative case qj so i hope this agrees this is all i've done is point rewritten common sense as an expression as a more succinct expression isn't it and you would agree that you want to pick that value of p that maximizes l each of the probability is a hypothesis probability value is a hypothesis so you say that maximize find P that maximizes L P. L P and probability is of course because it's the output, it is a function of the input. Somehow it is built, the classifier has built it out of the input. Some function of the quality has to be some function of the input for it to be meaningful. So you need to maximize this. And how would you to maximize this? You do, you need to maximize this product. Now a couple of things come about. This way is good in theory, but it is problematic problem is properties are small. Numbers between zero and one so whether P is point one or point nine doesn't matter if you get 1000 data points the product of 1000 numbers between zero and one what will you get. You will get a number so small that your computer will think of it as zero right because computer has a limit of how small the number it can understand and so you will become smaller than that so that doesn't seem very useful multiply its side so there is a trick you can use what people do is they say all right let's take the log of it log likelihood log of the likelihood right because if log will make it additive so now it becomes i belonging to belongs to the positive case log of pi isn't it plus j belonging to negative case log of qj is it beginning to look suspiciously like something we have done before now the thing is you need to maximize this Now the thing is you need to maximize this. Max this. But it turns out that most of the numerical computation libraries, they are designed to minimize things, not maximize. So how do you convert this to a minimization problem? Very easy. Suppose if you have to maximize this equivalent to minimize the negative of this minimize minus log likelihood right likelihood of p x if i do this maximizing this is the same as minimizing this do you see the guys maximizing the log likelihood is i can either do that or i can minimize the negative of log the bigger the log likelihood is the smaller the negative log are we true and therefore that is nothing but minus of this I you know into positive cases log P I. minus log. Well, what is Q j j to negative Q is of course one minus P j. And this and what was this, this was the original log function with loss function which it was a measure of supplies. You see that guys. So this is actually a more formal definition or derivation of a good loss. Which is exactly the same as what I gave to you with a more intuitive notion of surprise. This is your loss function. Loss function like this. And of course, you can reformulate it in my language by saying, forget the minus, you can absorb the minus and say, I log one over p i right plus summation j log over one minus pj isn't it in my language you would do it like that thinking in terms of surprise and of course pis and pjs are predictions so now you can translate it back to the equation. So we went through a long derivation there. This is a very important derivation. It's a foundation. And it hopefully gives you a justification of why we took the measure of surprise to be the log. And we started with that rather than some other mathematical equation. Isn't it? Now you may ask this question, something is suspicious. Maximizing the log is the same as minimizing negative log but are we sure that maximizing the likelihood function is the same as maximizing the log likelihood function is it really true well it turns out that that works because if you look at the law of numbers it is i hope i'm getting it right it is it's a monotonically increasing function function so as x increases log x also increases isn't it you see that the log function if you remember from your basically high school or middle school it looks like this so it is just a part maximizing a function and maximizing its log are equivalent right and so maximizing the function is through all this derivation, the same as minimizing the negative log. And that is the loss function that is used for all classifiers. Now, what. So let's summarize. Once again, we get back to the same summary. This is what you need to remember. Surprise entropy, cross entropy. That is a loss function. And I've given you two derivations of this. One of the reasons I gave these derivations, usually this derivation is missing from books. Sometimes the book will have the maximum likelihood argument. The maximum likelihood argument goes back to the same Fisher, the statistician who argued that cancer causes smoking. I think we'll never forgive him for that, isn't it? So now you say, all right, this is good, but we haven't actually learned of a classifier. How would we classify? Let's do a real-world example. So one of the first classifiers that you will encounter is the easiest classifier. It is called the dummy. It is also called the zero-R classifier or the it is also zero is also called the zero r classifier or the dummy classifier so let me illustrate it with a story so imagine that there is a valid doctor who actually has degrees and did graduate in the better half of the class, near the top of the class and hasn't forgotten his job, learned from experience. And there is a quack. You can have your pick. All of these chiropractors what peculiar healings and so forth. So there's some quack. You go to the doctor, you go to the quack and you say, you know, oh, my goodness, I am in trouble. I'm sick. And the quack gives you a pill. And lo and behold behold you are fine. And you notice that people who come to the quack, he tends to give them this pill and they tend to get better. They go to the doctor and that also gets better. In the majority of the cases, both of them manage to cure people. How would you know that one is quack and the other is not? Looking at the evidence, all of them got fixed, or majority of them got fixed. How would you know which guy is quack? Just from the evidence, you're not allowed to look at the decrease. What would happen is, let's say how would the quack function? Most diseases are self-limited. It's a fact. The human body has an incredible healing capacity. And it is amazing actually how much it deals with the environmental factors, adverse environmental factors and just recovers from it. You have a fever, you walk in the snow, you have a fever, then three days later you're fine. You eat bad food, you have bacteria in your guts, and then you have upset tummy, you vomit and all sorts of things. And again, a few days later, you're fine. Most diseases are self healing. In fact, one of the great discoveries of cancer research was, I think it was made 10-15 years ago. And from what I understand, the discovery was that actually cancer keeps you know the cancerous events keep happening in our body all the time right but our body has a pretty strong immune system and it goes and squashes it once again you heal from that you don't even know that you had cancer for very episodic amounts of time. But when there is what is called an immuno escape, the cancer cells manage to evade or do something to beat around the defenses of your immune system, your fighter cells, that is when they win because now if the fighter cells cannot kill them they will proliferate and so they proliferate and it becomes then you need much stronger measures radiation whatever it is and it is recognized that you have cancer right so the human body is very suffering this is a little bit of us so as a quack what is your best bet who saver comes you give a pill and says my my dear boy you'll be absolutely fine i assure you just take this and most of the time he'll be fine let's say that one in hundred actually has csd that may lead to more any kind of morbidity or mortality right but? But most of them, they become right. So this person, how often is this person wrong? One percent of the time, right? So for the quack who has no information, has no clue of medicine, what is the best approach? What is the best thing he should say? Give people ash or something or chalk in a capsule and say, you'll be fine. Prediction is you'll be fine. Isn't it? Because that prediction will be true 99% of the time. And how did the quack come to that? Because the data shows, data has 99% recovery rate, even without medicines. Isn't it? So what is the purpose of the doctor, the genuine doctor who has gone through 10, 15 years of medical training. It is to catch that one person, to be able to identify the one person who is genuinely sick and won't recover, isn't it? So a good doctor is a valid classic, can distinguish between a truly sick person and somebody with mild illnesses. a truly sick person and somebody with mild illnesses. But a quack cannot. So the quack is a, this is a quack. Right? So first thing you do, or the simplest prediction, which is not random prediction, it is still a sensible strategy. Right? You just predict all will be fine. Now we know that today it's very interesting. As modern medicines, frankly, started around the time penicillin was discovered, right? It was the first thing that could hit bacteria. Before that, if you have a wound, it gets infected, you die. Very high chance that you would die. Battle wounds would lead to death. People develop infections, they would die. But doctors have been around. Most of the modern medicines that worked didn't exist before the 20th century. Very few of them existed. But doctors have been around for tens of, like at least a few thousand years, if not tens of thousands of years so what were they doing it is a worthwhile question to ask how were they curing people or were they curing people because all of them with great confidence were telling the patients eat my this medicine and that medicine and you'll be fine today when we look back at the history of medicine we realized that actually all this it was a confirmation bias what would happen is somebody would find a leaf or herb or something develop a lot of build would see that if you give it to the patient the patient gets cured and so now you have a you thought it would cure the patient and now you have evidence that it cured the patient. It's called confirmation bias. So the most generous interpretation is you get the confirmation bias builds up and you begin to think that this really works. The only thing that was missing was a double blind study. Right? Given this medicine and not given this medicine, what would have been the chances? Which wasn't done. But double-blind studies were a rigorous method that is of recent discovery, recent establishment as a scientific method. So today we know actually that most of those medicines, if you go back and look, they were silly, right? So those doctors were well-meaning, but they were not too far from being quacks. Today we have modern medicines and of course a good doctor can cure it. Think of a good doctor as a good diagnostician and that's what your classifier is. So anything better than that dummy classifier, what should it do? Pick the majority label. Look at the y values, the actual labels, actual outputs, and pick the majority. And always predict that. So if you see a preponderance of cows and only a few ducks, the child can easily fool the parent by always saying cow, cow, cow, cow. that reminds me of a lot of these people who forecast the stock market and keep saying the market is going to crash every single day they wake up and say market is going to crash and the moment eventually the market crashes and then they have become you know they write books on how they were the only people who could predict that the market was going to crash I don't know if you have encountered such people in the financial news any website you know go to fools I don't know whatever they were yes you're fine why because if you keep making the same prediction every day of course it will be true someday right but uh the opposite of that is that you pick the majority out and every day say that because majority but inherently means 50 more than 50 percent of the time you'll be that what if it's 50 50. then just pick one you have 50 percent right randomly pick one randomly okay which 50% right? Randomly pick one. Randomly, okay. Which is why you should always pick odd amounts of, just pick odd number of samples. You can pick a majority. So it's a good thing. What you do is when you get 50-50 sample, you draw another sample and see what happens. Or you just pick one and you know that it is. So why are dummy classifiers, why am I even considering? Because they are the benchmark. I need to compare every good classifier that has actually learned something to the dummy classifier. Isn't it? It must outperform the baseline dummy classifier. If it doesn't outperform the dummy classifier that classifier is no good would you agree with that uh yeah so basically dummy classifier is like null hypothesis it is the equivalent sort of like that. It is the baseline. How would you know? I'll give you an example. Suppose your error rate is only 1% or 2% or something like that. 1%. People go and say accuracy 99%. So I hope one lesson you learn from this is the accuracy is a meaningless number unless you know the class asymmetry. Right? What is the class imbalance? Sick versus healthy. If it is one is to 99, then a dummy classifier will give you 99% accuracy. So if you see a lot of marketing and a lot of things claim, we have 99% accuracy or whatever, some number, right? Those numbers, now that you know about these things, you should always take a grain of salt because accuracy without knowing class imbalance is meaningless essentially because in this highly asymmetrical data you need to do far better than one person to be called a valid classifier right the error rate has to be far lower than one percent am i making sense guys that's how you do you compare to the dummy classifier now leaving aside, we will now come to a more legitimate classifier and we'll give the next one hour to it. Do you guys want to take a little break? Five minutes break? What we have done so far is talked about classifiers in general the theory of classifiers like how two things we did how do you do the diagnostics of a classifier you look at accuracy error rate precision recall the the roc curve by the way i remember receiver operator characteristic curve and the area under the roc curve auROC did I put that thing there yeah I should mention that this there's a name for it it is AUROC it is usually written like this capital A little u ROC capital, ROC curve, right, and the last c is omitted, area under ROC curve. So then we created a sense of a loss function, we wanted a notion of loss function, and we started with the notion of a surprise. We did two derivations. One is using an intuitive notion of surprise. And we said that the loss function is simply the total surprise when you compare the predictions to the ground to the levels. Along the way, we encountered entropy, which is average surprise for instance of data. And then we did. So we did. So this is the summary. then we did a different approach, we looked at the likelihood and likelihood is something remember likelihood is not in the traditional sense of probability. It is likelihood that a given hypothesis is producing the evidence, the data that you actually see the data that you actually see, the trading data that you actually see, what is a, right? The full form of a likelihood is there. If you remember it like this, that likelihoods are associated with hypotheses, whereas probabilities are simple probabilities, right? So you look at it like that. Then you're looking for that hypothesis that is maximizing the likelihood of the evidence and that approach is the maximum likelihood estimate approach and then we took the example of rain and so forth and we realized that if you maximize the likelihood you end up with products which is great this expression product of well positive cases product of negative probabilities and in theory great mathematics can stop there but you can't do that in computing we have practical limitations we have under things will become too small and everything will be considered so to solve the problem you take the log log is legitimate because log is a relatively increasing function if x is increasing log x is also increasing so you maximize the log but because numerical libraries they are designed most of the machine learning libraries they like to minimize it's not maximizing very easy if you instead of maximizing the log likelihood minimize the negative log likelihood and therefore minimization of a negative log likelihood is the cross entropy loss and it turns out when we do that ah surprise we ended up with the same equation that we came to a very different route by collecting the total surprises. Then we talked about dummy classifier, which must be a baseline. So guys, many times I've seen people do this. They will write a classifier. They'll bring you a Jupyter notebook or they'll bring you Python code and say, see, here is my model and here is the data and look at the model diagnostics how good it is right this is how most people do the one thing they miss is the lift the only thing that matters in machine learning is what is the lift over the dummy classifier right and in regression what is the lift over the null hypothesis how much better are you doing than the null hypothesis right because that is the that is the measure of learning that is evidence that some learning has taken place from the data isn't it all right so we will learn about an interesting classifier called the logistic regression. This is very poor use of terms because it uses the word regression, which is most unfortunate because it's a classifier so you can always sit in interviews and ask okay what are the regressors you know regression methods you know people will say linear regression logistic regression you can end the interview right there right because logistic regression is for classification despite the name it's a logistic regression is for classification despite the name. It's a logistic regression classifier. It's a classifier first. So we'll do that. We will do. There's a different way of looking at it. We won't talk about the exponential family and so forth. So we'll do this. Five minutes break, guys, and we'll start. As if in the loss function right can't can we make it a maximization problem of area under roc area under the roc comes is a derivative effect after you have worked all the probabilities right and and it doesn't admit gradient descent okay you're too far away it is too a much more direct thing is to work with the loss the loss function I mean the log likelihood itself amongst other things it's a convex function right and as you know convex functions are much easier to do. I agree. For simple situations, for logistic regression, for simple situations, these are convex things. And I forgot at the start of the, you told that recently the p values were not being considered as what you have to say that where we say that generally if p-value is less than 0.5 then we accept it like the two-tail test the stuff why they are not accepting it i did not get that point no no see people have abused the p-value okay let me say this in science you can things that confirm or agree with the theory, right? The more data you find that agrees with the theory, the more you believe in that theory, but it doesn't prove that the theory is correct. One single data point that contradicts the theory is enough to demolish that. It is called the Popper's falsifiability criteria in the philosophy of science. It's a landmark position. Karl Popper, by the way, was a professor here at Berkeley. So science inherently is a body of falsifiable knowledge, falsifiable theories, falsifiable theories. Right? In a quantitative way, you can falsify, you can produce a data that will dismiss it. The only point is, like for example, the theories that we know that have stood the test of time, we haven't found data to contradict. Therefore, the fact that we find more and more data that agrees with the theory, but never find the data that contradicts the theory, us believe that that's how science works now in this world now in bringing it to statistics and machine learning what happens is that when you find that your let's take an example your r square is perfect does it prove that your theory is correct? No. We realize that you also need to do residual analysis, right? If residual analysis works well, or reasonably well, again, it is just adding to that to support it is increasing your belief that the theory may be right, or effective, never use the word effective. Right? Nothing says, you should never trust one metric and say, my theory is right. For example, if you get very high R squared, you get very low, like your residuals look fine. You may still be completely wrong because your model may have hugely overfit the data. Yeah. There are many subtleties in this world. So the basic thing is never, no one metric is proof that you are right, ever. The problem that happened is because mathematics is used in social sciences and many fields, psychology, medicine, etc. It was used with a half-baked understanding. And p-value in particular became a tool of appeals. So 5% p-value, smaller than 5% p-value was often considered evidence of something happening, some relationship. And so there is actually a lovely book written, actually it's written with a, it's a bit sarcastic, which is a negative side, but it does make strong points. It's called The Cult of Statistical Significance. P-value, the formal name is statistical significance. Read a book called The on why a p-value should be used with caution. It's not that the p-value is entirely wrong, it's just that you should use it with caution and it is very easy to abuse it, very easy to abuse it, to such an extent that you must always have other, many, many other confirming, supporting evidence and p-value should be just one of them. confirming, supporting evidence and p-value should be just one of them. See, high p-value will contradict it. Low p-value doesn't confirm it. It just happens not to contradict it. So not contradicting doesn't mean confirming it. That's the subject. Guys, the rest of you are extraordinarily quiet. Is it all making sense? Is it proving interesting so far? Yes. Good. All right, our five minutes are over. I hope those of you who took a break are back. Now guys, today will be a little bit longer session, I hope. Is there anyone who would like me to have a hard stop at 10? Most of you are at home, so it shouldn't be a problem. I'll take the liberty to go about half an hour more. So let's take a situation. So let's create a story. We will take a very simplified world or data that may or may not be realistic, but it will help us learn a pedagogical way. So we are trying to distinguish whether a fruit is a grape or is it a cherry. Now grapes and cherries are about the same size, well or let's say yeah same size but one is a bit heavier than the other. If I remember right, cherries are significantly heavier. And if cherries is not plausible, let's take what is bigger than a grape? Which fruit? I was going to say blueberries, but they are smaller. Mango. Kab smaller. Mango. Okay, how about you? Mango. Mango is huge. Okay, we'll take blueberries and grapes. How about that? Blueberries and red grapes, like volume or size and the weight. These are the only two features you have, X1, X2. And you have to tell, is it which fruit? Grape versus blueberry. Now blueberries as you know are light and small usually but we will imagine a world in which there are some giant blueberries too all right that can compete with grapes tiny the tinier grapes. So you have data now because it's a two dimensional, let us plot it out. We will plot it out on the what are the two features? Weight and size. And let us make the blueberries. Blueberries are tiny, they will exist as hopefully somewhere like data points like this does this look plausible guys something like this and your nice champagne colored grapes let's take a nice champion by color which one should i take maybe Maybe I'll take this. This here. Because even some grapes can be small sometimes. And here are huge grapes. And here are huge grapes. You're supposed to write a classifier that tells, this is your data. You need to write a classifier that distinguishes between a grape and a blueberry so one thing you could do just visually inspecting it it's very tempting for us to do this let me take a green color here it is very tempting for us to draw a line like this does this look reasonable guys or maybe a little bit bent down. Yeah. This was not the best line. Let me choose. I had data points to agree with it. Now you may have a few big blueberries. Now you notice that if you draw a line, so we will take a line in this case because in this course we are making linear models let's take i will take four of these data planes i will call them a let's say something like b and something like B and something like C and something like D. What can you say about A? How strongly do you believe this tiny little thing light little thing is a blueberry very strong you would have a very strong belief isn't it but what about d a strong belief that it is a it's a great and if you ask yourself geometrically why is it you notice that it is quite far from this line isn't it but the closer you come to this line at b right so suppose this is b what about b you can't be sure it is probably blueberry but you know it could be also with certain degree of probability a grape isn't it? Likewise, C may be more, a slightly stronger belief that it's a grape, but it could be a blueberry too. You see this, you know, it's not a perfect demarcation between the two. There is a bit of overlap between the blueberry weights and sizes and the grape weight and sizes. Right? So let us concoctord a notion we realize that the further away from the decision from this line we have so this line has a lovely name it is called for obvious reasons decision boundary boundary decision boundary are we together decision boundary so what do you do if I have to find out first thing is I can compute the distance of a point let's say point x the distance of x distance obviously perpendicular distance from the decision boundary. Why am I using the fancy word decision boundary rather than a line? Perhaps you have anticipated. As we move along this course, there may be decision boundaries which are surfaces, curves and surfaces, not just a line of play. Are we together? So the general term we use is decision boundary. One side of the decision boundary are things belonging to one class. What is this class here is? Class here is all blueberries. is all blueberries. And the class, the other side of the thing are grapes. This is the grape territory. In the feature space, the grape territory is on one side of the decision boundary. The blueberry territory is on the other side. But you do acknowledge that there are some mistakes, isn't it? There is a certain amount of interweaving of data there, overlap of data there. But the decision boundary is a pretty good intuition for deciding. Let's take a notion. If you go in there, for no reason, if you go in this direction, northeast here, you call this the positive direction. And if you go south south southwest is a negative direction because we need an axis right so let us say that when dx is very much much smaller than very very smaller than negative like very negatively you can be sure what is it this is a blueberry blueberry it's a blueberry on the other hand when the d is positive dx is greatly greater than zero surely surely surely great isn't it? But if d is smaller than 0 but close to 0, but small, is only a small distance away from it, distance away from it but small then maybe blueberry isn't it for point look at point b this is point a this is point d and let's look at point c if dx is greater than zero but again small in size, it implies maybe grape. So is the intuition developing? What does the intuition say? That somehow the intuition, somehow probability of a grape, let's take this as a positive probability of great increases. With. D of X, the more positive D x is the more likely is it to be a grape isn't it and the converse of this is probability of a blueberry decreases with d the more the d increases the less likely that it is a green isn't it do you see this guys is this a clear intuition and now comes a lovely piece of art Do you see this, guys? Is this a clear intuition? And now comes a lovely piece of art. D, what is the range of values of D? Values of D. It can go from, well, in this particular case, it can go from minus infinity to plus infinity. Isn't it? What is the range of values? Well, obviously, in my world, imagine this peculiar world, there are even negative weight blueberries around. Blueberries are not only small, they can even have negative sizes. And so then your distance can go to minus infinity. Obviously, that's silly. But hey, we are learning what is the range of values of probability zero to one but we know that there is a monotonic relationship when d increases probability of When D increases, probability of it being a great also monotonically increases. So I will write a statement because it shouldn't be that the distance increases anywhere and the probability decreases, isn't it? It's a very linear movement as you go in this direction. Probability of it being a great keeps on increasing from zero to one isn't it so let me write this thing increases with monotonically now we are trying to look for a relationship between p and dx that relates them to a monotonic function, isn't it? So let's go search for it. We realize that if you take probability, we already remember that if you take, now look at odds. We already encountered odds. Odds was p over 1 minus p, probability of something happening versus probability of it not happening. People in common language talk about the thousand to one odds that it will happen, that India will win the cricket match against hopefully Pakistan, right, or something, right? So this is called odds. So this takes on value. What is the range of values? When p is approximately 0, p1 minus p will be 0, right, odds, i.e. odds is 0. i.e. odds is 0. When p approximately equal to 1, then odds, these are odds of course, both sides, p over 1 minus p is equal to what? Tends to, for very high p, this odds tends to infinity. That is what common people say common people say right oh there is a million to one odds of this happening i'm sure it will happen that's how we talk in common language isn't it odds so now this is good p was varying his range was from zero to one Odds ranges, odds, which is p over 1 minus p, its ranges from 0 to infinity, right? We're getting close, but there's a problem. This is only the positive x-axis, positive real line, right? But distance can be negative. The whole point is that distance is also negative. Do you notice you can have like in fact all the blueberries are sitting in the most of the blueberries are sitting in negative distance if you look at this picture right and all the all the grapes are sitting in the positive this thing so we are not done yet we need some other trick to make it go from minus infinity to plus infinity and once again the shining the knight in shining armor that comes to the rescue is the same old log function so here comes the log function on its horse and and it says take the log of this take the log of odds and then what happens what is the log of zero or vanishingly small number epsilon very small number what is the log of zero or vanishingly small number? Epsilon, very small number. What is the log of a very, very small number? It is minus infinity. Right? One way to see that is log of a number which is 0.0000001 is the same as log of 1 over what is 1 2 3 4 5 6 10 to the power 6 isn't it and that is equal to log 1 minus log 10 to the power 6 and that is equal to 10 to the power 6, and that is equal to minus 6. You see that it's negative. So the smaller you make the number, the more negative the log becomes, isn't it? So the range of log is minus infinity to plus. Ah, we're getting somewhere. So could we hypothesize that the log odds and the distance, now log odds also increases monotonically. They both have the same nature. P odds and log odds, they both increase monotonically. As P increases, they increase. The distance also, of course, by definition increases linearly. But could we hypothesize that log odds, would it not be a nice theory to say log odds is equal to distance i mean why why even bother putting a proportionality constant let's just say equal to distance log odds then it would agree with our intuition here it would agree with our picture, isn't it? If we were to write it like this would it? Guys, have you followed the chain of reasoning? Log odds goes from minus infinity to plus infinity and it just agrees with our notion of the distance in this particular picture. As the log odds increases the more likely it gets that you are talking about grapes the more you are in this particular picture. As the log odds increases, the more likely it gets that you are talking about grapes, the more you are in the grape territory. The distance increases, you are in the grape territory. So to say that is actually to have discovered one of the most powerful classifiers out there. This is the logistic regression classifier. So I will omit the word regression because to me, it is confusing. So I will just call it a classifier. And as you can see, I tend to use the vocabulary a little bit different. I tend to use it to be more aligned with the intuitive way I can think through it. So the theory that says that logistic regression says that indeed this will work right now what was log odds let's remember let's fit back all of this it says that do you guys by the way guys do you see the sheer simplicity of this expression when you put it next to that picture let me draw the picture again my blueberries over here right and my grapes for here and we realize that what we are searching for is a distance from the yes distance this is distance to know whether x is a blueberry or a, all we need to know is its distance from the decision boundary. Positive distances must be great or gets more and more likely that it's great. Negative distance, the more negative, the more you go in this direction, the more it becomes likely. Actually, let me use blue. Okay, let it be. So the more negative you make it, the more you go in this direction, the more likely it becomes that it's a blueberry. Blueberry territory. So imagine that you are a car starting here. If you start driving along the negative direction, southwest, you will see, imagine that you're traveling in this winter space. What will you see? You will see a world full of blueberries, isn't it? All around you are blueberries. And occasional grapes, maybe. But the further you go, the more you drive, the more into blueberry territory you are. If on the other hand you drive your car in the opposite direction, the further you go, the more solidly you are in grape territory, isn't it? And so we capture it with a very simple equation. Log r is proportional to distance. So let's make it an equality. And this is the logistic classifier. It works. Let's now put it in a more traditional form that people see it. It is written as log. What was log r? P, 1 minus P is equal to the distance or dx. I will just write dx. This is also often written as z, especially in neural network literature. People write it as z. So I will write z is the same as dx. people write it as so i will write z is the same as dx right and now we'll evaluate what the x is and so if you write it like this then what happens is if you take the entire log of both sides it becomes one what does it become exponentiation of z would you agree now if you add the numerator to the denominator which is perfectly allowed what is the denominator here one right e to the z i can put the denominator as one now why in the world am i doing this by the way this is an algebraic you can add the numerator to the denominator so long as you do it on both sides right so this will become p 1 minus p plus p Right. So this will become p, 1 minus p plus p, right, is equal to e to the z, 1 plus e to the z. Do you see this? And so we came up with an expression as p is equal to probability. And obviously, we are talking about x probability of this x point being a grade. Let's bring back our notion. And suppose we divide both sides with e to the z. So e to the z divided by e to the z would be 1. And 1 over e to the z plus e to the z by e to the z and that becomes the more traditional form one over one plus e to the minus z and so the the this is the classic or celebrated logistic regression equation an example of this it is people often write it so this is the logistic equation in its more general form just beyond the world of probabilities why any function that has this structure it is called the logistic function, logit function logistic function, logit function. When you apply this to the world of classifiers, this is the same way as writing this. Now most of the time the books will write it like that. Now I haven't explained what z is. I will explain in a moment. Or z is equal to dx. I'm going to make a claim. equal to dx I am going to make a claim remember distance from the decision boundary so tell me all points which are which are at zero distance from the decision boundary what do they what are they where are they they are on the line they are on the line so would it be right to say that decision we can turn the arguments around and say decision boundary is literally defined, is defined as dx is equal to 0, literally, right? And it turns out it's a line the line the line in this particular case the line is defined by dx is equal to zero now and i'll give you a derivation of it in a moment but take it as a part thing the distance from the decision boundary can actually be written as beta naught plus beta 1 x1 plus beta 2 x2. Now this begins to look interesting. Have you encountered an equation like this? When did we encounter an equation like this? This is polynomial regression. This is regression. And that explains why people sneak in the word regression in what is actually a classifier the distance from the decision boundary is given by this right and so the full expression your probability of x being a grade is equal to one plus e to the minus beta naught because when i plug it in here minus beta 1 x1 minus beta 2 x2 you're getting the point now right and suppose they are n variables of some this is the equation now interestingly most books they sort of drop this equation onto your head including your current equation you come up and they say this is logistic regression equation classifier equation and the first time you encounter it i don't know about you but first time i encountered it i don't know about you but first time i encountered it a transcendental function my first reaction was where did it come from what's the derivation so i hope that i've taken you through a very intuitive set of steps to reason through this of where it actually comes from how one can essentially rediscover the largest equation, the largest classifier equation. Isn't it? Now, I haven't proved that distance is equal to beta naught plus beta one plus beta two, that the distance function, this is called the distance function. And this reminds me that on Saturday, i start the lab by first giving you a proof of this today we have passed the time so this is it probability this so what does this look like what it looks like is see what happens is that if you look at the distance function d of x and you look at p of x the p of x being there you see an interesting behavior probability goes obviously from zero to what is the upper bound of probability one right so this is one this is what happens when distance is hugely negative probability is what is the probability that it is a great when when you are deeply negative territory what's the probability that it is a great when when you are deeply negative territory what's the probability that it is a great look at this picture when you are in hugely negative 0.5 0.5 why 0.5 when you are deeply in negative territory any other answers guys when let us say that you are here no probability will be zero sorry nearing to zero the further away you are from the decision boundary in the blueberry land the more sure you are that it is a bluebell isn't the more negative the distance the more sure you are that it's a blueberry so the more sure you are that it is not a great isn't it sure you are that it is not a grape isn't it right and so for much of this territory here you will be in the blueberry land but as you come closer to the uh to uh the this point what is the value at zero when you are literally sitting on the line the best line exactly so this would be at 0.5 so this will gradually start rising and when it rises the second part of this you are in the positive distance so this is negative territory this is the positive territory and then it will go and keep on increasing and then it will saturate because after you have gone through a certain distance, you're practically sure that it's a grape, isn't it? Grape territory. Grape territory, blueberry territory. Now look at this curve. In this particular case, you notice that it looks like a, it's a very interesting curve. Let's look at its qualities. The no-brainer is it's a continuous function, probability, continuous curve, and therefore a function. I hope you wouldn't contest that, would you? Looking at this curve it is smooth not contestable right in other words differentiable again no brainer why in the world would i want because actually these are good properties it's monotonically increasing increasing isn't it it increases it never makes a dip back right so for example it doesn't behave like this this is non-monotonic non-monotonic. Non-monotonic. And this is monotonic. Yeah, a monotone is, uh, I don't know, something like that. And if you're into music, you would say that. You would go like that. So, well, here we go. So far so good, guys. And it has also an interesting property. It seems to have lower and upper bounds, lower and upper saturation points. Right? And you can summarize all of this by saying you take an s take an s s letter oh sometimes this reaches the limit of how much text i can and i seem to be reaching that limit. Okay, I have reached the limit. So I'm going to write here because I'm almost done with it. Let's see if it will let me write anymore. No. It turns out that OneNote has a limit of how much it will let you write. We can try to sneak past it for a little bit. May or may not work. Does it work? No, okay. So let me write it in another page. No, what's wrong with that? It's not even looking into this. Okay, I'm not able to write for some reason as a bug in the screen oh yes new pages have come up so just to recap if a function is like this this is the zero line and this is the one line it just it is like a take an s and stretch it by its tails by its tail tails horizontally when you do it you will end up with the shape. You will end up with the shape that looks like this. Would you agree? It looks like a stretched out s. This function, does it look like a stretched out s? Guys, I wish somebody were speaking up. Yes. Yeah. Yes. Yeah, it does, right? All such functions, which are all such functions, all such functions that have the same property, that are continuous, differentiable, monotonically increasing, have upper and lower asymptotes. The word that you use, the word for the limit says in the language of mathematics, you say that these saturation points, there is a formal word for it that you may remember from calculus, these are called asymptotes. S-Y-M these are called asymptotes s-y-n-t-a-s-e-n-t-a-s that have asymptotes and in in other words that look like stretched out s they are all called sigmoid functions and it turns out that it turns out out that the logistic function is an example of a sigmoid function. So now let me just mention this. People often say sigmoid and logistic function. They think it is synonymous. Right? Often in textbooks it is written like that. And you have to parse through and see what is the person meaning in neural network literature often something like this is done, but actually it's not true. So I'll give you a homework. find other sigmoid functions and discuss them them in slack why did we can we use them and discuss them in slack for example would we have Could we have used any of them in place of logistic function? This is a thought experiment do that and discuss this is your homework are we together, so now i'll summarize very quickly, we learned about one classifier logistic regression classifier or just the logistic classifier it very well handles. situations like this. Like this, blueberries and grapes. And remember, a decision boundary will never do perfect separation, because data is interlinked. There is overlap in the data. It's the nature of the data. So when you're sitting on the decision boundary, you're never sure whether it's a grape or a blueberry. On one side, you're a little bit more leaning towards blueberry, on the other side more grape. But the further you go from the decision boundary, the more you have. And you can capture it through the notion of log odds. And log odds relates directly to the distance function. The distance function, I have given I've not given proof of it, in terms of the distance function look is a linear equation. Why it is so will be a derivation will do next time we're out of time, I would like to give the last 15 minutes to take any questions from. So this is our sigmoid function. Let me see. Upper and lower and upper asymptotes. Right, our saturation points. Saturation, saturation values. That's better. It's a sigmoid. So we covered quite some territory today. We learned the general idea of classifiers and their loss rate. Right? Number one, which is cross entropy loss we learned about a various classification metrics i talked about accuracy and error rate right now i give you a homework another homework is i said this precision and recall right precision and recall i did not explain to you because we ran out of time let's make it a homework please go and understand what precision and recall is we'll cover it in the lab but it would be nice if you are try it on your own first go read about precision and recall likewise and people often talk in terms of precision recall or they talk in terms of sensitivity and specificity, depending on which literature you're reading. Specificity. So go learn about how these are defined and think of it as simple geometric equation. What is it trying to tell you? What is it measuring in real life? So that's our homework guys and if possible, please review this. We are doing chapter four of your book. Review the logistic classifying in chapter four of your text. Now chapter four of the textbook will essentially start from here. It will say this happens to be the largest logistic classifier is this, right, and you have to move forward and it feels a little, at least I like to see where it came from and this is a derivation of that by the way this is my way of thinking if you think of an even more simple way to derive it please please share it with all of us so with that questions uh as if i have a question when you made that log odds equal to d of x so my question was yeah range it was i agree with that range minus infinity to infinity but my problem was when log is zero to one only it goes to basically minus infinity to zero don't you think it's not an uniform distribution kind of stuff like let's say dx is there dx has equal chance of being negative and equal chance of being boss too but in the case of log it's it has to be between zero to one uh no no you're looking at the odds when you're between zero to one it goes from minus infinity to zero right that is true that is not a problem it's a if it's a non-linear deformation taking the log is a non-linear deformation right but ultimately you have bent it to be the range that you wanted and it raises the question can you do other functions this is a curve right for x log of x is a bend is a deformation it's not a straight linear relationship but that works now this is only one of the ways and in fact your homework is precisely that you could you could think of other sigmoid functions and think which of them will work uh so my question was actually since we used log right so we have odd that zero to one only we can get to minus infinity so my question was whether it was probably equal like yes it is the point is that um when you look in terms of the distance rightance is distance. From probability you are trying to manufacture a function which goes from minus infinity to infinity, right? And one easy way of doing it is taking log odds. Yeah, it is true that when you compare it to odds, it seems that we are taking a tiny range, 0 one and stretching out stretching it out much further whereas one to infinity seems as though we are not stretching it as it is true but at the end of it so what uh it does get the work done any other questions guys all of you are extraordinarily quiet. I am beginning to fear you may not have gotten. Are you getting my feedback? Is it easy to understand? Yeah, go ahead. Yeah, so like this is true for like two class, I mean, if you like yes and no or like true and false. Is it also true for multi classification? Oh yes, that's a generalization. We'll come to it. Okay. Multi-class classification is a very obvious generalization from this. It has to do with soft maths and so forth. We'll come to that. Okay. That's a good question. Anything else guys? So I did a lot of mathematics, but please review this. It's not hard. And if you find the mathematics worry, then you can much of today's lecture, the derivation of logistic regression, you can entirely forget. The only thing you should remember is there is a decision boundary. The further you go from the decision boundary, the more sure you are of your answer. Right? And there is an equation that helps you, that captures that. And its derivation is fun to know, but at the end of the day, if you don't get it, it doesn't matter. In practical terms, it doesn't matter. In fact, I doubt most people know that elimination. Okay, that is it. And the other part is the loss function. Remember, which I hope was just basic algebra. It basically says that if you look at two ways of looking at it, maximum likelihood or leads to minimizing negative log-likelihood, you always take that hypothesis which the data supports in simple common sense terms. Rain rain, dry day rain rain. That supports you in Seattle rather than Phoenix, doesn't it? Literally common sense at large. That is one way of looking at it and the other derivation comes from looking at the aggregated or collective surprise right we came to the conclusion that log of probability and your output y hat is probability log of inverse probability is a measure of surprise isn't it and so if you add up all the surprises you get the total surprise that is a good loss function because if your model is really good you should it there should not be any surprises they should make predictions very close to the true value isn't it that's that so that is another way to come to the same same expression same loss function so all right folks at this moment i will end today how do you the likelihood in real scenario like because this is like multiple i mean multiple it's like voting right somehow uh no no please say that again so this likelihood likelihood is there, it's like some, somehow it's like voting, right? Multiple people votes and like whichever is like higher, like probability multiplication, it is winning. So how are we using in the real scenario? Yeah, log likelihood is like voting. If you take log of each term, because that becomes pretty much the surprise element i mean like multiple data but how did i mean how will you join the right word you use a joint the joint probability of independent events is the product of probability of each of the events in isolation right so for example if, if you go with the hypothesis that today it rains and it has no impact on whether it rains tomorrow, these two are independent differences. So if two days it rains and there's 90% chance of rain, the likelihood of seeing that data is about 80%. So the right word you use is joint probability. Let me write that down. Joint probability of independent events is the product of probability of independent events is the product of each event's probability. In more mathy terms, x1, x1, x2 probability in a more massive terms x1 x1 x2 xn is equal to the product i is equal to one to n of probability of x i right or in simple terms it is independent probability of x1, probability of x2, probability of x3, probability of xn. This is just saying it in a succinct mathematical way. It's the same. Any other questions, guys? Alright then, in that case, I'll close the session.