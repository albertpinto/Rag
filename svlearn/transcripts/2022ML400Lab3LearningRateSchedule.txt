 So this is the notebook that we will be running in this lab. And these are the auxiliary files these Python scripts as well. But for now, let's just start by running a few cells in the notebook. Okay, so if you see we are importing, as usual whatever we import we import mat.lib because we want to plot a few um you know plots here then these are the imports from um from the python scripts uh which are in the same folder so from dataset classes we're importing a simple 1d regression data set so this is a data set object that we will require and these are the training loops so there are three different training groups that we're using in this notebook okay one is a simple ne just it's a simple one the next one a train basic network it it trains the network it trains the model as well as tests the model at the same time so for each epoch it will train and test and train network with scheduler it again chains with uh trains the model and test the model for every epoch uh plus we can specify a schedule like we can provide a learning rate schedule in the third uh in the in this third uh training so also then one more thing we would want to do in this notebook is we want to measure how good our model is. So usually we measure with loss, right? So to improve our model, we minimize the loss. So that's how we improve the model. But in reality, to check how good our predictions are, we generally use metrics. And for regression problems, we use regression metrics, which are mean squared error and R2 score, if you remember from ML100. So we use these. And for classification, we use different types of metrics, if you remember accuracy prediction precision. So we're going to use that here, I use mean squared and r2 squared. Okay, so firstly, these are some, before going here, let me just open this up. This is what we're going to do in say that simple train, simple network. Okay, so first we're gonna get a dataset, right? So there's the raw data. Then we're gonna put it in a dataset object, which is a PyTorch object we are going to specify a learning rate so these are all things that are set previously and we won't alter it alter them in our training similarly we will specify a model which will also not change in our training loop right so what happens is then we pass this data set to the data loader and data loader object will give say give this training loop a batch of data okay so it will keep giving in batches of data so that data can be you know in a in a tuple right so it will keep giving in batches of data so that data can be you know in you know in a in a tuple right so it will be um the input and uh its corresponding label it's gonna pass in that for in batches so we specify um how many um how many items we want in a batch. So that's gonna happen here. And then we're also gonna specify an optimizer, right? If you remember from lab two or lab one, we specify an optimizer, which will keep updating our parameters, okay? So we are also gonna link our optimizer to the model's parameter. So that is going to happen. So now everything is in place. In a training loop, as you know, we get the data and parameters. They're going to give us a prediction. And from the prediction, we compute the loss. From the loss, we compute the gradients and with these gradients we update the parameters right so this is the training loop, and this is what we will be doing in this train simple network. on till data loader exhausts all its data points. So, and that would mean one epoch is over. Then we'll start again for the second epoch and the data loader will again randomly send in data in batches and then that is exhausted and start the third epoch. So an epoch is also a hyper parameter which sort of tells you how many times this loop has to run so a quick question uh for the epoch to run over and over or in the subsequent epoch does the data go through some sort of randomization so that the next people okay that makes sense yes data loader will shuffle shuffle data, so that is a that is a thing that you specify with the data loader you say do you want to shuffle the data set every time you run through any book and and you say shuffle equals true. Then your data will be shuffled. Got it. Okay. So, all right. So that is my, uh, chain simple network. So this code, all this code is, is like heavily borrowed from, uh, say the, um, inside deep learning book, right. book right it explains things much better uh i mean it has uh explanations and code and even better uh you know visualizations like this so you might want to refer this after the class okay so uh for the first for the first attempt we are going to let me just show you how the data looks okay so this is our data and forget this prediction this uh we have x x is this is x axis and this is the y axis and our data looks something like this and we can see that it's it's not a very simple model I mean it's the reality is not very simple it's pretty complex it has all these undulations and and even the randomness kind of keeps changing as we move along x. So that's how our data looks. So now we want to fit a model to predict the outcomes, say predict y from x. So our inputs would be x and our output would be y and this is a simple one-dimensional input right x x is just one-dimensional and our prediction is also one-dimensional so you may want to note these things before okay so our input features is one and output features is one as we just saw. And to our model we specify a model with just a single layer, we say input features and output features and all it does it just does a matrix transformation and out gives us the output so it's a very simple this is this is what you basically do in linear regression and linear regression can be done like this as well um so we see a training loader in this case we are going to load the data and say simple 1d regression data set so we will go over this simple 1d regression data set in just a bit so what exactly it does and why it is important so and here i pass my inputs and my out and my true y values okay these are the observed y values so if you see um i load my data set from source um i read them into a pandas data frame and then i take out my input and and my true y values as so and then I pass them into my simple 1D regression data set and that would be my training loader so if you see that is the data set loader and it's going to continuously give it give me my data in batches and for for now, if you notice, I haven't specified any batch size, which means by default, this will do stochastic gradient descent, as in it will every for every iteration, it's going to throw out just one pair, right? It's not going to do batch yet it's it's going to only give me one say one tuple with input and y values so Kyle I think I might have asked this question before x is capital and y is is that's that's just the convention okay um if you see in in many books and notebook uh even i think um this inside deep learning you will find that x is capital because x is a matrix. Okay, that is a vector X is a matrix and Y is just a value. Y is a vector, Y is... Here it is... Capital X and little y, it is a convention in machine learning to represent the feature data matrix with capital X and Y, which is just a column vector of responses. So suppose you have 10 rows of data. So Y would be a simple column vector of 10 values, single column. Whereas X, let us say that x in this case is just one feature, but suppose it was three features. So then you would have a 10 by 3 matrix, 10 rows, three columns, one column for each of the features. That is why, and conventionally, matrices are given capital letters and just column vectors are given small letters. Capital x and little y are pretty much established convention for uh feature vector and responses thank you yeah okay um so now uh we have the training loaded and now we're going to train with our simple network so it takes in a model it takes in a loss function chain takes in the training loader and a device this is what you might know this from previous labs and why we use the device and we set it to gpu so let's train this network So let's train this network. So if you see here, I'm outputting, say, what is actually happening in the model. So these are the batches, right? So there are 1000 data points and our training is happening for all those data points. And I've set my epoch as 20 by default. So I haven't specified it here, but by default, it is 20. So it's running for 20. And this is the loss, it's okay so with the with with a very simple network as this we are we will get a simple uh say an output and even if you add uh multiple linears together without any activation without any activation functions you will still get an output like this so yeah so a question guys why do you think the linear layer even if i put why is the linear producing a straight line See, linear is just w dot x plus b, isn't it? It is by definition the equation of a plane and in this case in single dimension of a line. Now, even if you add, Kyle said that if you add more linears, it would still be that because suppose you add two three linear layers linear times a linear does what it does an affine transform some rotation some scaling and then you add one more linear that will do some more rotation some more scaling and you would agree that whatever it does to the input the composite effect of multiple linears is nothing but one overall rotation, one overall scaling and translation. The biased term will translate it. So which is why people say that linears, you can't build a neural network of only linear layers because it will always produce straight lines. For you to get anything meaningful, you need to bring in activation function. If you just, like for example, in this case, Kayal has put just two linear layers together. It seems as though we should get some result, isn't it? There's a hidden layer with 10 neurons. And you will see that when you run this, what happens? It will go down. And what do you notice? So just having hidden layers is not enough. The crucial thing is a neural network becomes a universal approximator. For it to become a universal approximator, what is that secret source? What is that magic that you have to throw into it? Could one of you say that? Activation function. Non-linear. Yes. You need to deform the input. At each layer, each neuron not only produces a fine transformation wx plus b but taking that wx plus b it needs to do a distortion of it in some way right and it has to learn the distortion the right distortion through back propagation which is right so as if when you said distortion it is all about that uh last class you explained uh it is momentum plus gradient descent both no no no that is completely different that is about how to converge in a non-convex loss loss surface okay don't mix that with that you think very simply why is this line coming out why is the prediction not shaping itself to the data because there is nothing that is bending it if you look at this there is nothing in your network that is bending this line isn't it yeah makes sense you need a bending element and that bending element that deformation element is that activation function. Remember we did this whole paper, this collab, this network topology. Do you remember this reading, Sunday paper reading that we did? This was the main message. If you haven't seen this, I would strongly suggest you watch the video course video go back and see it just shows why you need a distortion function in your networks why you need activation functions without activation functions neural networks are no better than just a simple linear equation right yeah this was one of the most useful paper or one of the most beneficial of all the things, how it explains the like how it distorts everything then only able to kind of separate or make a decision with respect to the different planes. So, yeah. Yes, certainly. Alright guys, so let's move and add a few distortions, activations. So here I'm using tanh edge activation functions. So we see that we have an input, we have one hidden layer and then we have the output right so let's run this the hidden layer is denoted by the another n dot linear 2020 isn't it? Yes, that's the hidden layer. Okay. And do you know, see the first hidden layer input goes, it becomes to 20. There is a activation of that input, right? And then there is one more hidden layer, which also gets activated. So you have a net total of two hidden layers. Is there any way to run the code if we don't have CUDA like a GPU? Oh yes. At this moment, it will run just fine. Uh, the way to do that is actually device a touch that there is one line code. It says, make it either cpu or gpu yeah based on what your thing is but if you comment out this line it will automatically run on cpu yes okay so i'm talking about the simple regression model uh that one doesn't have this line uh and it's and it's giving me an assertion error saying uh torch not compiled with cuda enabled so should i just put uh should i just put cpu yeah okay actually for simple problems like this gpu doesn't, CUDA doesn't give you too much. It runs just fine on the CPU. So here we go. Now, do you notice guys that if we go down, what do you see? What did the distortion functions do? It aligned with the true value of the yeah so your job is guys today play around with the and obviously uh kyle is going to take you through the exercise she'll play she'll try out some linear of this sort of learning rate scheduler but as she does that it will work partly your homework is to find the best hyper parameters to make it work for each of these three learning rate schedules exponential step drop and cosine your job is to find the best combination as a group and post your hyper parameters to the stack later rather than many yeah And obviously don't cheat. If you know about automated hyperparameter tuning, don't use that. The point is to by trial and error, figure it out and think why it worked and what did you learn from it. Please continue. Okay. please continue. OK. So the next type of network we are going to look into is a training loop with testing as well included in it. So for this case, what we are going to do is we are going to train, say if the model is is in training mode or we are going to test uh if it is in the evaluation mode so there's a bit of code behind it um it's should we go over it acid before moving on just just take them through examples and then take them through examples and then okay okay so you just know that the next training loop is going to train and test as well okay so for that case i'm going to split my data set into two um say i'm going to strain with 500 samples and test with 500 samples okay so i have total of thousand and equally splitting it but generally training would be more uh make it 0.7 okay so we have 700 uh train for training and 300 for testing so you can you can play around this with this as well right so that's that now i'm going to uh create two data loaders one data loader for my training and one data loader for my testing okay for training i'm going to set the shuffle to true because it makes sense for testing. I don't really need to, right? And training, I'm going to use a batch size of 64. Okay. And you can, this is also hyperparameter. Change, you can change your batch size and or remove it and just use stochastic gradient descent. We'll see how that works. So this is the next loop and for this loop i am going to pass in so this is my train basic network and then passing in the model loss function a train loader a test loader and number of epochs so this is core function so let's just go back where have i talked about score functions oh yes it's here um so these are my metrics i want my model to evaluate the predictions um and compute the predictions and as well as just see how good it is right so these are the two metrics i'm using one is mean squared error and r to score as we talked in the beginning of the class so let's run this Sorry. Oh sorry. Okay, well it's not it's not exactly good. So this train basic network it outputs a pandas data frame. So what it does it gives me the epochs yeah see right you know that your model is terrible if your hard loop values are squared values remember the coefficient of determination if it is negative means you have lost it completely right so it should be uh like and your mean squared error the test and the train uh basically should be going down it is going down but not by much so there needs to be a lot of yeah a lot of hyper parameters that needs to be true and you can see in the picture you have to show in the picture. Yes. And now you would all agree that the model has nothing to do with reality, isn't it? Yeah. Yeah, it's diverging greatly. Yes. It's a good illustration. Yes. And so we let's also plot train MSC versus test MSC. Okay. All right, but still, it's not useful. So now we come to learning date schedule. And in this third type of training uh training loop we're going to add uh different learning rate schedules right so this loop will have will keep updating our learning rates so for the first one let's start with an exponential and um okay so this is my learning rate versus epoch and i haven't run the loop yet but this is what i want i want it to start from 0.1 and then exponentially decay to 0.001 yeah 0.001. Rupali Kusumastu Rao, Okay, so in the beginning of my training my learning rates are large and slowly go down. Rupali Kusumastu Rao, So to do that I specify an optimizer here i'm using Adam. Rupali Kusumastu Rao, Adam instead of SGD. I'm using Adam, Adam, instead of SGD. And this is my schedule. So this is how I specify a learning rate schedule. And here I'm using exponential. I'm going so the scheduler will need access to my optimizer. Right? So optimizer is what actually uh stores the learning rate right and optimizer gets the parameter and learning rate and then keeps updating the parameters right so scheduler needs access to the to the optimizer so that it can update the learning rate and here uh this is my gamma and if you remember from class what gamma is is that um it is the constant by which you keep multiplying your uh initial learning rate so to calculate that we use this right you can i was supposed to put a screenshot so guys this is from the notes remember i had given you an expression of how to compute the best gamma naught the initial gamma right you know the gamma the decay rate if you notice this graph at the bottom, it's decaying. But how do you determine the decay rate? So in the theory part on Monday, I've taken you through the approach that if you wanted to exponentially decay, you have to ask how many epochs there are, what is the final learning rate I have, I want, what is the initial learning rate I want, and from that you can compute you can compute a good effective learning which is what she's doing okay um i think i uh why i understand why it's not working so i already compute gamma um i've already computed gamma earlier but i keep changing the epochs here and but that doesn't keep updating my gamma so yeah i need to change that so you can run it okay You can run it. Okay. So this should this should work. So I'm running it for 50 epochs now. And if you notice, I'm also adding batch normalization to it. So after, so each layer, we add a batch normalization layer sort of in between here. So, okay. Not much improvement. So there's definitely some hyperparameter tuning that needs to be done. Okay. So if you see, this is step drop scheduler. So this is similar to exponential except that you don't you don't take constantly modify your learning rate but you modify it once in every say a certain step a total three steps then you did epoch yes three steps steps. So guys, this is an excellent opportunity to take this notebook and try it out right now. I think we have been making too many changes. Okay. All right, why don't you show the results? Don't show this one. Yeah, sure. From the previous runs, you show the results? Don't show it this one. Yeah, sure. From the previous runs, you have the results. Yes, but it got, the data frame got lost, but you can see. So you can see that it is a reasonable approximation to the actual data, but it misses out all the undulations. It misses out all the,ulations, it misses out all the ups and downs. So therefore, you need to experiment and see. And this also speaks to a fact. See in neural networks, soon you realize that there are many, many hyperparameters, which learning rate scheduler to use, what the value should be, how should the neural network look, and so on and so forth. So one of the topics we'll cover in this course is how not to have to do so many experiments, how to quickly find the best neural network architecture, that is called neural architecture search, NAS. It's a very interesting topic. And then how to have the best hyperparameters for you. And when you use that, you don't get your answer in one run, but you get your answer in the minimum number of run. And that process is Bayesian inference, Bayesian optimization. Continue, please. So at this moment, we'll show the results of the runs, and then all of you as I exercise today play around with it. Let's all do it in the class after you have opened the code. Yes, so this is for cosine annealing. See it captures more undulations and this is the training versus test error so so this is the end the r squares are pretty good if you finally look at the bottom they all achieve near perfection do you see that the r square the coefficient of determination the higher the better and they they are in the high 90s given the fact that the data has noise of course it will never achieve one but it seems to have done a pretty good job isn't it the cosine and for around 10 to 20 epochs it has already reached it without many boxes yes but play around try to make each of the learning rate schedules perform just as good by playing around a lot with the hyper parameters in the architecture that's the main homework today or lab work try to do it in class itself sometimes it's a little hazardous running notebooks if they require a lot of CPU or GPU because it can conflict with having your zoom meeting not freeze up sometimes that's an issue. So move to is a function which helps you move tensors into CUDA. Now if your tensor is say if it's wrapped inside a tuple then how do you move it to CUDA? So you do it recursively so you can move tensors in a list, tuple, set, dictionary, you move them to CUDA. So that's what this code does. Now, let's look at simple 1D regression data set. So remember that for each data set, you will want to create a separate class okay um in in this case so what does this depend on it depends on what my data raw data what's what's the format in which it is right um mine was in mine was a pandas data frame uh if you remember and then i converted it back to say numpy so it depends uh what exactly your input is so you can do all sort of manipulation within this class so you can just uh you have x and y and then do whatever here so in in my case i want my x to be, it should have one column, but it can have any number of rows. That's how I want to reshape my data. So I use that. So I convert mine to numpy so I convert my inputs to numpy and then I reshape them so that my I can have any number of rows I don't specify the number of rows so if I don't specify the number of rows I can just pass minus one um and it will be inferred and but i want my number of columns to be one so i specify that so i have x and y that way and here in simple 1d regression in in this data set class uh you need to define two uh two functions okay and you basically are overriding these functions from your super class right so uh the get item and length so get item is is what um so the data loader will ask your data set for a certain index okay so you know that data loader is going to randomly pick, say, 16. If it's a batch gradient descent we're doing, say, mini batch, then it's going to ask for 16 items, 16 data points. So it's going to ask iteratively. It's going to pass in an index, and your set object is going to return an input and a label right so we are going to override that function get item where we pass in we it requires an index so your data loader object will will do this work for you it will pass the index and now how we want to this is where this is the important part how we want our data set object to return an input and a label so for each data set you might want to change it so if it's a if it is like an image object you might want to say dcd like it might be in three channels and you might want to change it all these manipulations happen here okay so you make sure that you correctly specify one input and output that that corresponds to what you want to do in your model right so i want just one i i want my x um my x so my inputs to have like one column right so i make sure that it's that way and then index so this is just uh numpy syntax just numpy syntax. And then finally, I also wanted to return the length of my dataset. So this is what the dataset classes does. And if you look at this notebook. okay you see i say length of train data set it will return the length of my data set okay that's where um that's that length function and the get function is used by the data loader okay is that clear shall i move on it seemed good okay good so the next part is the training loop. So I have three different training loops here. One is the train simple network, the train basic network, and the train network with scheduler. go over this one so it takes in a model it takes a loss function it takes in the training loader epox device okay so by default it to cpu i i think uh amrit or russian i don't know uh did this work or did you just have to say cpu yeah no i just changed like how how you had it before okay uh yeah like this it worked for me okay good right so in this network i do not specify an optimizer uh you know outside this net this particular function, it's automatically set for you. So this is what it does. It uses stochastic gradient descent, right? Torch, Optum, SGD. And so you know, right? The optimizer needs access to the parameters. So it's going to take in the parameters and the learning rate. parameters so it's going to take in the parameters and the learning rate so if you remember we need to pass the optimizer to the scheduler so that it can constantly alter this learning rate for you but in this case it's just going to be a constant learning rate you're not changing anything and here i'm going to move my model to device. So neural network models can also be moved to the device, and that's taken care of by PyTorch. So that's that. Now we go to our loop as usual. So we say for epoch. And then I'm going to set my model in a training mode. So this is just a flag. It doesn't actually do anything specific. It's just so that you can always test if your model is in training or if it's in the evaluation mode, right? So it's just a flag. Then I'm going to set my running losses to zero. I'm going to compute my running losses to zero i'm going to compute my running losses for each epoch right so at the beginning of the epoch i'm going to set it to zero and uh and then finally i'm going to print out the loss okay and just make a space here because i remember when it was printing out it was not there wasn't a space so you can add a space here um right so then for input and labels in so this tqdm is just the progress bar in the notebook if you remember right so uh in the notebook you saw those progress bars just to show those progress bars and using cm you can remove them and your code will still work fine and if you want to run this uh run this like run this python file you may want you you'll want to remove this notebook okay only then you'll be able to see it see those this is specific this progress bar is specific to notebooks if you want to run your file in pycharm just remove this notebook part and you'll see your progress bars in your uh in your terminal right and so uh that's that then uh that's that then um yes right so for inputs and labels in training loader so um like i said the training loader is going to give me a batch of data points and and i'm going to move them move my inputs and labels to uh cuda in right and then I'm going to set my optimizer to zero grad so I set my optimizer to dot zero grad it's just going to go and set all the parameters gradients to zero grad okay that's what optimizer does because it has access to your parameters then i'm going to compute my compute my predictions model.inputs and then compute the loss then loss.backward then optimize the step so this must be pretty comfortable with this then we go over comfortable with this then we go over running loss plus equal to loss of index this is just going to keep adding my losses uh for each of these uh loops and then finally when i'm outside this loop i'm going to print my running loss okay so this is what happens in your train train model so that's all it's just going to train your model uh i you test the model outside here no basically not test you see how it performs um remember so we're just visualizing it so when you want to test your model, or you want to get predictions from it, make sure that you use the torch.nograd. This is important because we don't want to do any gradient tracking or add items to our already existing model graph. So, we don't want to touch anything. So, make sure you use torch.nograd. Okay. don't want to touch anything so make sure you use torch.moprad okay otherwise the parameters like you know uh you might be adding say more items to the graph so you don't want that so then you say y dot red and then you how do you evaluate. So then you say y.pred and then you, how do you evaluate your model? You just say model and then you pass in your tensors. I mean, your inputs, it's going to give you the prediction. And if you want to plot it out, this is how you do it. If you remember, this will be, it will be in your GPUpu if you're using gpu so then you say dot cpu dot numpy to convert it back to a numpy array in your cpu and then and then you plot them on okay so here i'm plotting x versus Y. And then I'm also plotting X versus Y thread as a line. So this is what I get. Okay. Is it clear so far? Hello? Yes. Anyone there? Okay. Not here. Okay. Is my pace okay? Am I going too slow? Too fast? Any suggestions? I think the pace is good. Other students? Yes, I think it's good okay all right next is train basic network so here if you remember what we do with basic network is we train as well as test so it's going to need a train loader and a test loader but test loader i'm going to generally like by default set it to none so you can run this code even without specifying a test okay and then these are my score functions and then epochs and then torch and this is my device so um right again in this network uh so in this training loop i'm not making any change to my optimizer so i'm using the default stochastic gradient descent but you may want to change it you can alter this code to specify your own optimizer which could add them and that's the only other thing i know as well but yeah you can change that to that so here here in the end, I wanted to, if you remember from the notebook, the output was a pandas data frame. So I'm collecting all the information I want in the data frame in a dictionary called research. So there's one column for epoch, one for train time train loss test loss okay and then uh i'm going to add now add more columns depending on my score functions right actually i should specify one more line here don't miss my course which is it might not work um score funcs is not none yeah guys you might want to add this this line uh okay so this will this will add more columns only when your support functions is not oh is that not currently in the code yeah it's not that on the So this will add more columns only when your support functions is not. Oh, is that not currently in the code? Yeah, it's not that on the just the score functions. We can copy and paste that into the slack. Sure. Like then you'll have to you'll have to then it could. OK, yeah, Python syntax requirements. Okay, sure. Okay. So this is just to create those columns in my data. And that's line 98 in learningloops.py. I'll mention that. Yeah. Training loops. Yeah. Training loops.py. So this is what I do for each epoch. So for each epoch, I'm going to set my model to, first, I'm going to set it to training mode. Then I am going to run a function called runEvoke. Right. So then I'm going to append the train time here and then also, right. So I'm just going to keep adding things to this, to my data frame. Then if my test loader is not none i'm going to set my model to evaluation mode and remember when i'm testing i'm going to make sure that uh torch on no grad then run run this run this function again. And finally I return my data. So what this run epoch does is, so this is what happens inside each epoch. Again, I'm going, these are some values that I want to note. So for input labels in QtDM, right? Similarly similar to my previous loop, inputs label compute y hat compute loss. So this is the code that's different. If my model is in training, only then I'm going to update my parameters. If my model is not in training, I will only compute the loss and then do this part. I'm going to evaluate how well it does using my score functions. So if my model is in training i will go over this part update my parameters and then evaluate my score functions right if not i will not do this so that's the only difference between training and testing so why we said to model or training is uh when we said model equals model or train model or training is set to true okay when model or model equals model or eval model or training is false okay so this helps us distinguish between training and testing so once that's done um here i'm just like uh working on my score functions so you can read the score you'll understand it and then finally oh yes i'm also calculating calculating how much time it takes for each epoch. Okay. And in the end, I'm going to say n minus star, that's going to be the time. Oh yes, this is. And also one more slight change. Guys, can you add a plus sign over here? And oh yes. Okay, let me take notes for those who might not be attending. I don't know. I changed it but it's not showing up here. Am I in the right place? Yeah. Okay. So total train time equals, first you want to set it to 0.0 and then update it, add a plus here. Skate, just here, you can make note of this. Yes, doing that now. You mean change it from total train time equals zero to total train time plus or minus? You're changing it? Oh, okay. to total train time plus or minus? You're changing it? Oh, okay. Wait, let's line 108 is the, no. Where'd it go? Where's the plus or, oh, okay. Line 108, total train time plus or minus the run at buck. That's the new one, right? You added the plus there? Yes? the run at buck that's the new one right you added the plus there yes am i taking note of the right thing cal hello we cannot hear you. Can you hear me? Yeah, I can hear you. We can't hear Kyle. Oh. Kyle? I think we are having an audience. She's going to rejoin. She just said in chat. Okay. Right, so where were we? It's okay to recap a little. Yeah. Okay. So yes, so in my train basic network for each epoch, I'm going to run to run I mean train the model and test the model simultaneously and then return a panda state of frame once all my epochs are run okay so that is uh my train net train test uh loop now train network with scheduler it just has some minor changes so i'm just going to add so here i'm not going to explicitly specify an optimizer i allow us to i mean you you can specify it outside you can just change whatever you want with optimizer and then uh also specify a scheduler okay so learning rate schedule and by default learning rate is set to one if you specify your optimizer you don't need to specify a learning okay so these are just options for you to play around with so if you don't specify an optimizer outside, no problem, your this this function will specify for you. You just need to specify a learning date, even if you don't specify a learning date, even that works. Now we go over the same thing. There are not many changes. The only change that we do here is that you specified your scheduler already outside this function now the only one change that I am going to do is where is that change ah yes here it is so after I train my I first train and then test right so after I train i first train and then test right so after i train um i'm going to update update my learning rate after every training okay so it will depend so this lr or schedule it will it knows when to exactly change so if it's exponential it's going to change it after every epoch. If it is step drop, then it will change once in the number of steps you specify. So it will take care of that for you. So it's not a problem. So the only code that I'm adding here is if learning schedule is not none, take a step. Otherwise don't so i take a step in my op with my optimizer takes a step after every iteration and my learning rate schedule takes a step after every epoch okay so i don't keep changing my learning rate for every you know iteration i only change it for every epoch so that's that's actually an important thing to note I only change it for every epoch. So that's actually an important thing to note. Yeah, so otherwise the code remains the same. So that's all for the code walkthrough. Is there any questions? Are there any questions? Anyone there? I'm here. We are all good. Thank you. Guys, this code, you have to walk through this code. It will take you a while. You're seeing it for the first time, so you wouldn't have understood everything. You have to slowly absorb it. But at the same time, it should not look strange to you. All these things must be familiar. You train a model, you use an optimizer, data loader feeds data, many batches to the training process or to the evaluation process. Then devices, of course, where the Q down loss function. We have talked so much about loss function in this course. The score functions are just the measures. How do you see if a model is doing well? For regression it is mean squared error. We talked about it. Again, it has to go to the quality of it. Then the R accuracy and so forth. So those are the things that you see in the code. Once you know the concept, these things should feel familiar piece. Now the only thing new is PyTorch itself. These are all very PyTorch syntax and as I mentioned, PyTorch syntax is unfortunately this course we won't have time to go over the PyTorch itself. To a large extent we are counting on you to spend time practicing pytorch and writing code playing with it taking the tutorials we can give you some help take our help like if you first notebook or the autograd notebook it actually helps you walk through pipeorch a bit so kind of makes you understand pytorch and like you get a little bit of practice with that notebook that's right so guys it's very important that you practice the pytorch it is the one thing that will block you from making progress. But if you understand the PyTorch syntax, then what this code is doing is very straightforward and repetitive. You must be seeing that the training loop is more or less the same. What is it? For many epochs now, within each epoch, for many steps, you do the forward step, you compute the loss, you do the back propagation. I mean, you compute the gradients and fourth step you do back propagation. Just four steps to learning that you see everywhere. Now, the only difference is that when you do the step, the step used to be gradient descent, pure simple gradient descent. In the last two weeks, what we learned is that gradient descent itself can be more complicated. You can make it more powerful, first by playing around with the learning rate, and secondly by playing around with the gradient itself, adding a momentum term of some sorts to the gradient. So that is all that has changed, but other than that it is all exactly the same. So please play with that PyTorch and all of these things, especially because next week we'll get into new territory, we'll get into autoencoders. Very interesting space, A tremendous way for doing dimensionality reduction, data compression and so on and so forth. And it will be a first introduction to generative models. How do you, looking at the data, how does the machine figure it out and say, I can create more data like this? That sort of question will be answered as we move forward with this. So that is that. Play with this, guys. Take this seriously. Play with the notebooks. Make it run as you understand the code. Figure out which are the best hyperparameters of the model. Post it. Compare with each other on Slack. See which of you can find the best hyperparameters. At this moment, engagement is low guys. You know, pandemic has a way of muting the engagement level, but please do get engaged. Otherwise it will become a lot of theory that you'll listen to, but you won't get fluency. Fluency comes by doing so please do things take the quizzes quiz attendance is low at this moment and do the labs right and today now kyle will after some time we'll now start walking you through the solutions to the homework right you can take this time to try out the homework notebooks Right, you can take this time to try out the homework notebooks. If you have any questions, you can ask us, we can help you. So, what, in like 25 minutes or? Yeah, let's give at least a half an hour. Okay. Half an hour to 40 minutes. Guys, please do the homework in the lab right now itself. See how far you make progress and then we'll review it right uh if or ask if we could review their notebooks on saturday if they want like they can spend this time just to work on it um well let's let them get started get into it and then then another help session it's always good to get a little um initiative rolling okay so yeah let's do both guys play with the schedulers today and also get started with the homework because we need to do the doing part is a bit lucky let's do now