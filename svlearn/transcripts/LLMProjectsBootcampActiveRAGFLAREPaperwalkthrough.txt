 I'll give a moment for you to to get ready. All right, folks. We are. This is one 3112, the one 30 sorry a little bit later than I had anticipated. We are going to. This is reading of an interesting paper that came out a couple of months ago which is called Active Retrieval Augmented Generation. This is a good milestone in our project. We did last week, or at least we intended to do, RAC, Retrieval Augmented generation. So in a way, this is the next step to it. Please go ahead. You can't see me or the board. Okay. Is it better? My screen is shared. Yeah, but the screen is shared. Maybe zoom on this. No, I will zoom on when the time comes. It's not yet there. Okay. All right. So all is well. All right. So guys, I'll repeat what I said. Now, we'll do the first paper reading. Just to give you a format, we are going to read through three papers. Between any two paper reading, there will be no more than half an hour break. Have your coffee, come back. We are going to have marathon session. We are going to read three important papers. The one that I'm starting out with and all of these papers are directly relevant to the project you're doing. So if you ask this question, why are we covering these papers? Because you need it in this project. It's literally the next step to your project. Before I continue, just to recap, last week we did RAC. If you remember, we covered the RAC paper. You guys remember retrieval augmented generation? What was the main idea in RAC? You could take the best of searching, AI based search, knowledge base and also take which people in this field called non-parametric search and or generation you can take that content that comes from a document corpus use ai to search to it use your sentence sentence transformer cross encoder and then sparse searching like bm25 and so forth and we did all of that we have we have all of you have implemented by now all the teams have in one form or the other completed your actual implementation from the ground up of that hybrid search would that be a fair statement we have done that so we are moving on to you guys are at different stages of moving on to multimodal search and to rag retrieval augmented generation the point of rags are multiple benefits accrued from rag one of them is you you now just like in hybrid search we merge the benefits of sparse and dense approaches, dense vectors and using VM25 or EF-IDF and these approaches, more traditional approaches, you bring both together. Why? Because for short phrases, sometimes it turns out that AI semantic search doesn't do well. There isn't enough semantics in the phrase. and the phrase and keyword search does better. By putting both of them together, retrieving results and then sieving it through across encoder, which tends to be a little bit more expensive, right, far more expensive actually, then and you're sorting through a smaller list. The net benefit is sparse search is lightning fast. Vector search isosla, Ph.D.: Sparse searches lightning fast. Vipul Khosla, Ph.D.: Vector search is lightning fast. Right. But both of them, you're treating them as mechanisms to give you a shortlist of candidates searches us Vipul Khosla, Ph.D.: Isn't it Vipul Khosla, Ph.D.: From the candidate search results. I don't know if you guys are seeing my expressions. So you get a candidate search results. Now comes the question. You're not saying that these are the search results. You're saying, okay, I got two candidate lists. When you do such fast operations, what you gain is obviously speed, because if you compare a query against all the billions of documents you have, it will be extremely slow. But you have to take shortcuts, you have to use approximate nearest neighbor approach and this and that. And so you shoot for getting high recall, not high precision. Then precision is low, you don't know which of these documents are more relevant than others then you see the two across and cross encoder to re-rank it and hopefully in the process gain back some better precision the more go ahead Patrick I said is there a general rule on knowing uh once expensive in terms of evaluating yeah yeah it's very simple to see uh Patrick's question is is there a general rule for knowing and say, in terms of evaluating these adiversaries. Yeah, yeah. It's very simple to see. Patrick's question is, is there a general rule for knowing what is expensive in this? So here is one general rule. Anything that scales as linear at inference time is very expensive. So excuse me one second. inference time is very expensive. So excuse me one second. So, I apologize. So see, if I have a million documents, and if I have to compare a query against millions of documents to know how similar it is to each of them, then when it becomes 2 million, my latency will double. When it becomes 10x the documents, so at inference time, you need things that scale sub-linearly, right, like that. And that is what approximate nearest neighbor algorithms give you. That is one. The other thing they give you is cheaper comparisons. See when you, for example, in a typical bird, the two sequences that you feed in, two sentences, you do that, it typically will take you let's say 100 milliseconds or 200 milliseconds per inference, right, or 50 milliseconds per inference, whatever, based on the GPU you have, or 50 milliseconds per instance, whatever, based on the GPU you have. But if you compare two vectors, a dot product, a cosine distance, it is lightning fast. It is like you wouldn't even take a millisecond. So that brings about its own performance gains. And that was the whole point of doing sentence code, like using a cross intro, like a contrastive loss, you found a mechanism to just land a vector in the neighborhood of other vectors that mean similar things, semantically similar things. And then vector search is extremely fast. So that's the thing. But then along the way, what you lost is the more derivative things you do, you're not comparing a sentence to other sentence. But you're comparing a derivative object, namely a vector of one sentence of the query to the vector produced from the documents. So those are derivative objects. You lose some precision there, some things there and some information there. And the second thing that you lose is you do approximate nearest to the center because still it is order and search because if there are one million vectors sitting there you don't want to do one million comparison the point of ann is you can do sub-linearly you can do logarithmic time or even less time right so that is speed So that is speed. Now, when you come back and then you say, okay, I have 20 results, I have to produce 10. The question is of re-ranking. You have solved the recall problem in one way of thinking about it. The other problem is you need to make it more precise. You want the most relevant results to go into the top. Now is the time to bring out your big guns, things that will take a lot of time. So now you bring in your full, you know, matcher, the BERT matcher or any form builder, encoder matcher, cross encoder, because that even if it takes, let's say it takes, for the sake of argument, 50 milliseconds per comparison, you're comparing the query to only 20 sentences, isn't it, which means that you'll still have a response in 50 times 20. That is in one second, which is pretty fast. That is one way of looking at it. Though ideally with good GPUs etc, it doesn doesn't even take a second it takes much yes and a gpu bank so that is getting the benefit so what so this hybridizing or taking the best of both the worlds is exactly the idea that rag essentially brings right now says that see when you answer the question there are two ways of answering it you go to an llm and you say all right tell me the answer to this now think about an llm lm is a giant transformer right what is it made up of weights and biases it's made up of parameters right and somewhere in its vast And somewhere in its vast number, vast ocean of weights and biases, these parameters is a statistical model that sort of mimics a form of memorization, right? Or effectively can remember and regurgitate, well, not quite regurgitate, but come up with an answer to your question isn't it but remember it just produced the multiple aspects to a llm and this goes to the heart of it guys see at the element um suppose you have this imagine that you have this vast ocean of weights and biases all interconnected so this is my weird way of fighting an LLM. Right. And at the end of it, you have a dictionary, a dictionary of English. Let's say 30k words. What is happening at any given moment? It is conditioning on. So the next word that gets produced probability of a token the next token t next is conditioned on the tokens that have been generated so far isn't it and the input right this is in a way one way of looking at it is, it is a giant condition, a Bayesian condition in some sense. Does that make sense? And in some sense, obviously using sort of hand-waving arguments. And it produces a next token. But when it produces the next token, what it means is that this guy, to different extents, these things get lit up. Let me just say different. This gets lit up much more. This gets up lit less. This gets up lit really high. What is it saying? Different tokens are getting lit up. If you think of them as bulbs, the formal word of course is logic, but I always think of them as bulbs. All those bulbs, the 30,000 bulbs, they get differentially lit up. Right? Now what happens? Even when you softmax it, namely make it into probability, now there is a thing. And just to recap, what do we do? When we softmax this, the next stage is softmax this. Logits and a temperature parameter p. What does temperature do? By the way, temperature usually in this LLM, they give you 0 to 1. Or 0 to actually open ended you can put whatever temperature you want when they mean temperature 0 actually in a more mathematical sense they mean 10 to the power 0 means they are not scaling the value what is temperature you scale each of the values. So suppose this logics here is output one. Let me just call it theta one. Theta 30k. What you're doing is the temperature just says scale everything by this before you exponentiate it. Isn't it? So the temperature zero is actually temperature one because implicit is that you're doing 10 to the power that check whether it's 10 to the power or each of the power i don't know um it's escaping my mind in case it is correctly so but in other words you're exponentiated at that moment what happens is there is a dampening effect so uh this if you if you raise the temperature too much, then what happens is this thing becomes shorter, right? This gets even shorter. So the relative differences are not so pronounced, right? And then comes sampling. The next stage is you sample of the softmax. When you sample of the softmax, what are you saying? I'm going to flip a coin in such a way. So here is one way to think of samples that I think of. Whatever these values are, imagine that you have divided a land into patches of that size, rectangular patches, whatever patches of that size so if some logit is big or even after temperature scaling after soft max the and soft max is if something has a high probability let's say 75 percent probability just give it 75 percent of the land area right make it texas sizes alaska size or something like that but if it if it is small one, then give it a tiny amount of a size, maybe what would it be? Main size, right? A little territory. So imagine the whole patch, the whole probability map is the United States map. And each word gets a territory the size of its probability. Then you do one thing close your eyes throw a dot magical dot from the from a spaceship right and throw it towards united states and see which state it lands yeah it may land on the biggest one it may not isn't it there is still a probability it might land somewhere entirely different it might even land on the least probable word you see that so that is sampling so that is why the crucial thing that you have to remember in all of these is that you always sample of the And think of it like that. See guys, I tend to use sort of, I don't know, it looks weird or corny examples, but to me it just makes the intuition clear. If you think of probabilities as area on a page, on a land, on a map, and you're throwing darts at it. Remember, sampling is throwing darts and asking where did it land? And wherever it lands, you pick that word. That word is your next word prediction. And remember, LLMs are next word predictors. That's what an LLM is. In effect, that's one way of looking at it. That's how they're designed that is both its strength and its limitation now why would you want to keep the temperature high when you keep the temperature very high what happens things get dampened down flattened down right so uh reminds me of a saying in uh hindi i don't know if you remember it, I'll just say it. It goes something like this, Andher nagri chopat raja, takese bhaja takese khada. Anybody has heard of that? What does that mean? Anything you know. Yeah, it basically means there was a foolish king in a kingdom, dark kingdom, filled with darkness, where the king had set the price of everything to one, one rupee. So sweets would go for one rupee, very precious things would go for one rupee or one dollar, and trivial things would go for one dollar, uniform prices. Right? So obviously it's supposed to be not a very wise thing to do isn't it so that is exactly what you do when you raise the temperature too much you flatten everything down high probability low probability each of the words get an equal real estate on the map isn't it so what what is the consequence of that when you sample off that does the language model have any sense at that moment? Is it even distinguishable from randomly picking whatever? So asymptotically, that is one limit. Now look at the other limit of it. What if you set the temperature to, well, they say zero in these APIsis but actually it's uh exponentiation of zero what is the exponentiation of zero one it means that i'm dividing each of these bulbs brightnesses logics by one means i'm leaving it alone so the tallest guy if it is really tall it's quite likely that your dart will fall there isn't it and so it's less it's more likely that your dart will fall there. Isn't it? And so it's more likely to be generated. Now what is the implication of that? Now tell me, why is this a desirable? Because you get a result that you probably expect, right? LLM in action, because the whole biation conditioning is coming through at that moment, well and fine. Why would you not want this? Why is it that people sample? So let me ask another question that sort of prefaces this. Why do we sample of the softmax? Why don't we just take the next, the highest probability? why don't we just take the next word the highest probability patrick says we need some variation in the output anybody else yeah shanava says even though it's a probability it's still a probability there's still a probability there's a chance that it may be wrong. Anybody else? See, there are many answers to it, and one can say it more mathematically, but here's the way I would say it. There are two answers to that. First of all, remember, somewhat like what Shonwa said, that in machine learning, there's always a risk of overfitting. And when you just always deterministically pick the highest probability world, right, you realize that you are, it will produce the same answer over and over again. The same query, isn't it? Assuming that you have not retrained the model, it's a deterministic model. One good thing with it is it's a deterministic model. If you want it, if you want repeatable answers, well throw away something, but you don't do that because these things tend to, you don't know how much overfitting it is and whether what it's producing is the correct thing or not, right? So sometimes sometimes and also it doesn't leave you possibility for reinforcement learning right all of these llms need to go through reinforcement learning what do you do if you sample off it you generate one word and the next token and the next token because there is conditioning on these tokens what will happen is even slight deviations will lead to Raja Ayyanar?n this trajectory your next part is a random one from there on it's like this is not exactly random random walk assumes that there's no meaning there but it it becomes so suppose the next so if you think about it a p probability of t n plus one now depends on the what you committed to the token that you accepted isn't it so it determines what the next token will be are we making sense right and so what happens is that if you think of it as a space of possible trajectories let's say you you can go like this you could go like this you could go like this you see what i mean by that right you could have entirely different trajectories what are those trajectories those are sequences of tokens that are coming out isn't it and therefore you're generating entirely different outputs but why would we want to do that because we just paid a heavy price we lost determinism't it? We just made it into a stochastic machine. Yeah, go ahead. So, so I said at this point, we don't know if the LLM is overfitting the last part of the . We don't know. We don't know. But we, but you should always go with the assumption that with this gazillions of parameters, there is a lot of overfitting. When we do know that it's overfitting this, why is it able to regurgitate sometimes entire passages? The reason it's able to regurgitate, I mean, see memorization is another word for overfitting, isn't it? Yes. I'll ask a very basic question. The problem is where we did the hybrid search. The hybrid search isn't lag, right? No, no, no, it's not. So we'll come to that. So now my question is, why isn't hybrid search a research paper itself? Because, you know, semantic search. No, it is. If you look at the sentence word paper, right, hybrid search is deeply rooted in the very first paper itself, which is a section in there. That or the follow up paper, I forget which one, but it is there. So, all right, coming back to this thing, guys. So follow this lead of ideas, because if you want the intuition on this paper correctly, please follow through and pay attention. So you realize that you have a diffusion, you different trajectories different sequences coming out that opens up the possibility of what many things yeah unpredictable so there's stochastics there now you have introduced stochasticity you would say well is that isn't that idiotic? Why would I want that? Why would I want that? Because what it has learned from may not be right. You know, web has plenty of disinformation, misinformation, errors, this, that. So what do you want to do? You want to feed it through two loops. So first of all, fine tuning, but that is separate. But more specifically, reinforcement learning with human feedback so to do reinforcement learning with human feedback what you need to do is you need to produce let's say 20 answers then ship it out to human beings and ask them rate these answers which are which is the best which is not good etc etc then have a reinforcement learning cycle there. Do you see it? Right? And that is, that is the point. You cannot bring in to open the door to reinforcement learning when EasyBay is to sample off the softmax and produce a variety of answers. But you pay a price, couple of prices. One thing you lose is determinism, isn't it? And the second thing that you lose in this process is, see, it is called the diffusion of that. John LeCun, very nicely he said it, and I still remember going for a walk at about 5, 5.30 in the morning in the dark. And I was listening to one of his talks. I believe it was with Lex Friedman. And he used the term diffusion of errors. I liked it very much. So physics-like. What it means is, if the probability of the right answer, what should have been, even if your LLM is really well-trained, it is 99.9% 0.999 the probability that 100 words later is producing a wrong word is how much let's work it out guys you all have Python I want somebody to answer go open IPython and do this 0.999 to the power 100 see what it is it's asymptotically zero right it tends towards zero what does it mean the more long form generation that you do the more likely it is to start erroring out. You sort of control it through reinforcement learning. You sort of show it special passageways that go along these well-defined paths. There are less landmines here in a way of speaking, in a metaphorical way of speaking. But inherently,, reinforcement learning won't eliminate the problem. And the residual problem that you have is your hallucination, isn't it? The models hallucinate. So it is in a way the top scent problem with large language models, isn't it and one reason when i when we did this llm bootcamp i wanted to bring you guys quickly to the core problem that has to be handled if you build production grade systems because hallucination may be funny or hilarious when you're just sitting and talking with friends right uh to find that uh let's say joe b Biden was married on the moon or something like that, right? If LLM says that, which is patently untrue or something like that. But it is not so funny when lives are at stake or legal matters are at stake, isn't it? So how do we deal with that? We started with search for a reason. Search, of course, we can do hybrid search, very productive, very useful, a lot of utility. But to me, one of the core values that it brought was retrieval augmented generation. Namely, now we could hybridize the hybrid search itself with large language models. We ask this question. If a question is asked, how many ways do we know to answer it? We ask this question. If a question is asked, how many ways do we know to answer it? We can just do keyword search. We can do AI search. But those two we already hybridized. But then search we can hybridize with large language. We can tell we can do prompt engineering. We can say, here are the facts, you prepend the facts, right? And then you prepend the life story of Joe Biden. And you say, he went to this college, he did this, he did this, whatever you can scrape off. And then you ask this question, right? Which university did Joe Biden go to? did Joe Biden go to? But do you realize that now the likelihood that the LLM will hallucinate is much, much less because remember it is also obviously the most obvious thing is it is conditioning on the input and in the input you have given it a fact dynamically at inference time isn't it? So and this fact that it so the the fact that the LLM can answer this is not because of its weights and biases, those weights and biases those parametric response would have been maybe something else. Right. But let me finish the argument guys, but you have used sort of a side channel of information search to bring it bring in facts and give it to it and therefore it is in the literature called non-parametric approach right and so rad hybridizes two approaches a non-parametric approach and a parametric approach and says now and it is a beautiful beautiful example of good prompt engineering, isn't it? Properly prompt engineering or instructing is good instructing, right? And you're saying now LLM. So what it does is it materially reduces hallucination, at least for short answers or for very pointed answers. Are we together? But it doesn't quite solve another problem. If you ask it to do a domain summarization, like let's say, explain to me, well, I was just going to say, explain to me quantum chromodynamics, but that would be, I don't know if it can even answer that, but let's take something a bit more suppose you say explain the solar system describe the solar system and it's a long form answer it might i don't know whether it will or not, but let us say, let us take some liberties with what will actually happen, but hypothetically say that it might make some things up along the way. It might suddenly posit that there is an 11th planet, or 10th planet, or something like that. Or it might entirely get rid of Pluto. Poor Pluto seems to be having a tough time these days. It is voted out of the planetary system and then voted back in. So anyway, it might do that, from the rates and biases. But when will it not produce the right answer, in spite of you giving it facts facts about astronomy. Suppose I pose a question like this. In view of the given astronomy, describe the properties of the 10th and 11th planet or something like that. Now what happens in the body of knowledge, it doesn't seem to find it. And somewhere in there may be a fact that says there are only nine planets known in existence. But for whatever reason, your AI search will not bring up that sentence. I mean, it gets harder and harder. So what you can do, so you see the problem with it, the longer the text you bring, the more you're fighting that problem that you can call the diffusion of errors. Isn't it? So the more likely you are to hallucinate for long passages. You are fighting it with drag, yes. But not. And there are other things, for example, as you know, when you feed it too much text, what happens? The context, it gets, attention gets lost in the context. There's a whole paper called lost in the middle did we did we do a paper reading on that or not yet or we have to do that okay we'll do that lost in the middle so so so poetically said right so uh anyway so they are shortcomings so all of this so now there are two ways of answering it one is to wait for a theoretical breakthrough. What can we do besides just having a next prediction engine, next word predictor? That will take some time, but a more practical approach that this paper comes up with is this. It says that it latches onto a key observation. What does it do? it do whenever remember and this is a fact that people tend not to notice that actually it is not just producing a token tn it is producing tn and its probability isn't it our llm is ultimately sampling off a logit and whatever word it produces it says okay I sampled that but by the way this word was less problem I just happened to land upon that the dart happened to land on that right. Logit is the soft max value. Soft max value right no logit was the word, actually it's the softmax value, sorry. The softmax value after the temperature scaling, even though that probability, so that he called just the probability, even though the probability is low, when we threw the dart, oops, it landed in Maine, right? Tiny bit of territory from the spaceship. And so I picked up the wood and then that word, consequent to that word and all the words that I've produced, here is the next word and the next word and the next word. So what this thing latches onto is that, hey, why can we not look at those probabilities and somehow leverage that? Leverage that to go to our other method, which is search. That which is search. So what it does is I know some of you are waiting to ask questions. Give me one minute. So what it does is it observes. So first thing is some things are very easy to do. It is producing. So now, should I tell you what it does or should we walk through the paper first? Let's walk through the paper and you'll see it. But by now you must have guessed what will it do? How will we use that probability? The token's probability, how can we possibly use it? Come again? I've created trigger words for it. Yes, like Patrick said, we can use that low probability and ask, is any word in the sentence that I've generated so far? What you do is you park your sentences in a buffer. So what you have is, so, OK, let me explain this paper. Let me give you the big picture so that reading the paper becomes easy. This is the llm right it is producing tokens steady stream of tokens so what you do is you have a you this is not what the output is output is so all of this is behind a curtain right what you do is you take this token you can use spacey nltk these guys use, these guys use NLTK, I would have used spaCy. Figure out the sentence boundary. Keep track. At any given moment, what they do is they just make a reasonable guess that most sentences are less than 64 words, 64 tokens. Then they look for sentence boundary and they look at the last sentence. Then they look for sentence boundary and they look at the last sentence. Now what they do is that let's say that the sentence is made up of the last sentence T is made up of the words W1 to WM. M words, M tokens, sorry. I keep using words because token always evokes words, but it could be punctuation, it could be question mark, it works. And I asked myself, what was the probability of each of these tokens? The probability of W10. And I find the min of this. the min of this. Right? I find which was the weakest word, which was the word on which the LLM was least confident. Now, why would I do that? It's interesting, it's sort of plausible and empirically seen, though I don't know how you would mathematically prove that it is always so. What they have found is, people have found, is that if you observe carefully, before it begins to go wrong or hallucinate, it tends to produce tokens of low probability. Right? It's an empirical observation. you do is you set a threshold and you ask this question is this greater than equal to a threshold or not is that if the minimum probability of a word is below a threshold if it is above a threshold now you let this sentence last sentence escape into the outside world right push it beyond the curtain but if it is not then what you do is you take two approaches though those are the implementation details but basically that is the time to one way or the other dip into your search right and ask this question what do we know and whatever we know what we will do now is we will make that part of the knowledge base are we together and then then have the next continue the continued generation happen and then the whatever tokens it has produced because it needs to still condition on the tokens right so that what happens is that instead of T T and being conditioned on T and minus one all the way to so what happens is then you erase this in a way you go and you erase this you this is your think of it sort of like a checkpoint you use that sentence which had low probability as a way to search but when you got it you fed in this much this is your tn minus one to t this and the x the input that you give which is both the query plus search results search on what a query term and we'll talk about what do you query with those are the details we'll talk about in a moment right patrick so when they say context window asset in terms of this like autoregressive, does this mean that once it's generating more tokens, the context is also adjusting so that it starts to forget the original? It does actually, unfortunately it does actually unfortunately it does beyond a certain context length right see no matter what you do the more recent always tends to have effect stronger effect so it's really advantage just minimize either the context or the length of your problem. Yes, you should. Absolutely. You get the most out of it. Yes, yes. So Patrick's question is that when the whole context window becomes huge, either coming from what we have produced so far or what the instruction that we give, the information that we get from Sir, it is in our interest to keep the whole thing short. Shakespeare said it best, simplicity and brevity is the soul of it. Very true here, I mean, I'm being jocular, but the bottom line is the more brief you can be and simply you can say things to the llm the more likely it is that those attentions cross attention center okay that's that so so that is the gist are we are we getting it guys that's the gist of this paper now i'll read through this paper so here one of the interesting thing is those of you who are used to parsers and compilers. Do you remember look ahead parsers. So this is a pretty much along the same idea. What you do is you just stop you just wait, you don't get the next sentence be the final sentence you wait and you check inspect it if it is below a threshold you throw it away you don't throw it away you generate a query from it that you will use in your search engine your hybrid search engine for example right or your vector search engine or whatever in this paper actually they didn't even use a vector search engine if i'm right they just use a bm25 which probably means that they're going to do. Right, but if you were doing it when easy enhancement could be use your newly grown hybrid search thing. To do that. So that's the so let's read through this paper a bit carefully, I would like to finish in the next 1520 minutes. You. You just stop the generation. Yeah, yeah. At that point you stop the generation. Keep everything that has been written. Yeah. How do you do that in the terms of programming? You keep it in the buffer. You keep inspecting the buffer. And when you have high confidence and you say, yeah, so the apis give you a way to stop oh and you can see what happens is if you look at open api or anything and darwin c and so forth they use davinci you can ask it to generate x number of tokens that's why they say 64 tokens no more than 64. so obviously it will stop at 64. then you look at the last sentence and you say will i accept it or do i throw it away right that's that yeah as simple as that i'm going to resume from there yeah then you make the next api call to get the next set of things another api yeah so you're not you're in a perpetual loop of making the policy right so let's read this paper and I should make a lot of sense. I hope that this will look straightforward now. Despite the remarkable ability of large language models to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. We all agree, right? Augmenting LLMs by retrieving information from external knowledge sources is one promising solution, namely RAC. Most existing retrieval augmented LMs employ a retrieve and generate setup, which means first comes the retrieve and then generate first what do you do you take the query go get some search results then make that search results part of your instruction in fact you prepend it even before the query you put the the body of knowledge that you got from your document chunks. In your case project, you call it those chunks. You put those chunks there, one, two, three, four, and then you ask the question, Who is the current king of Timbuktu? Or I don't know, something. Whatever it is that you want to ask me uh am i making sense so conduct is not a country it's a place i'm being silly okay and obviously they don't have a thing so that's what you do isn't it guys right we studied this week that's what we do right clarity there okay so this is it that's all they are saying uh employ a retrieve and generate setup that only retrieves information once based on the input and the crucial operative word is once this word only only retrieves information once based on the input this is limiting however in more general scenarios involving generation of long texts, right? Where continuously gathering information throughout the generation process is essential. You have to, as it produces. So it gives an example. See, think about human beings. How do you find out, how do you write an essay or a paper? You yourself write something, then you research some more and write something and research some more and write something. Isn't it? So why not make the LLMs do the same thing? Researching here means dipping into the body of corpus of facts. So. So sorry, but I think this makes sense in the coding of coding part of the programming. No, no, no. If I ask you to write. The history in the search rag actually makes sense because it looks at the keyword search and the indexes, but then the coding all of the, you know, what do you mean by coding basically, you know, this. GitHub has this. Let's not bring that up. Let's not let's keep that separate. We'll come to that later. So just keep this. I do have one point i think he's along the same lines on what i was asking that you know when we do rag we know it's for short answers but given that you're doing it for long form shouldn't it you just do it in a loop that's common sense in programming no but how would you know when to stop and ask and go search for more and what will you search for it is a good idea yes do it in a loop but it leaves open two fundamental questions basically you can go infinite like i mean that's where the threshold comes in yeah so no doubt there is a nice thing but i believe that he is referring to programming what he is also saying is if you're generating a long form and we know drag is for short form text no you don't know that people have been using that for pretty much all because it's the best we have basically we're using rag and then we say well it's hallucinating and now this is saying well if you're doing long form yeah think of an alternative yeah see so let me ask this question what is so what is so insightful about this paper this paper got it will get published i'm pretty sure i'll do it in some time what is inside i mean these are pretty good guys these are meta and carnegie cmu and so forth c labs these are big guys what is insightful is see the idea first is the key observation that you tend to have less hallucination when your question is very pointed but how do you ask a pointed question when you're asking a llm to describe the entire let us say the entire, let us say, solar system or that inherently needs long form, essay form answer, because there is, it's not pointed, number one. The second thing is that, how do you, what is your clue, dynamically, programmatically, how do you know that you may be hallucinate, right? And what this paper really says is that, see, let's use the interplay of two things. We will use something that may be a clue, which is never to show, but we notice that empirically, when things hallucinate, precursor to that are a few low probability targets. Now, see, that observation could have been made by anyone right anyone could have done that actually you and i sitting with our little llms could have done that these people observed that and i think some other people may have observed similar things but that is one crucial but what do you do about it then. So people before then the prior work has actually tried to do something with it. What they did is that they fed in everything that has been generated before as the search query. Right. All that is already also. Yeah. So that search query also wasn't good. And what turned out to be good is the specific approach these guys. For which there is clear evidence that it beats the stage, beats all of those approaches. That is what is rational about this paper. And see, always look at these things in a broader picture, guys. What are things coming from? This is a paper. What are things coming from? Why do LLMs hallucinate? What really is going on? If you have a handle on the theoretical stuff, the math behind it, it is plain as day. But could I have thought of this myself? I may make excuses, I'm too busy, but fact of the fact is I probably couldn't have. These guys did. The interesting thing about this field is once things happen, it looks- It looks like common sense then hindsight yeah in 2020 hindsight is like what of course all right but but the thing is that rag has been there now for more than two years and now and now this has come right in between there have been approaches that have sort of made small small improvement, but this may be a significant improvement. So this is what it is. So, let's look at that. Yeah, this limiting however is more generally in more general scenarios involving generation of long text, we're continuously gathering information throughout the generation processes essential. If I asked you to write a thesis, now you better start searching the web, right, kind of thing. By the way, that thing is called open domain summarization. There's a term I want to introduce because this paper uses it is open domain summarization. uses it is open domain summarization open it was used in this paper what does it mean anyone what is open domain summarization where there's not a lot of information or it's an open-ended question where it's an open-ended question and not a lot of information. What else? Like an open book exam. Like an open book exam. Chanda says. Close. What is open book on the web? Because so far these LLLs don't have hands. So what's an open book exam? You have information all you want and I'm asking you to summarize. That is, yeah. that is yeah yeah yeah yeah so chanda hit pretty close to the mark basically open here just means that open web search you're allowed to do open web search on that topic gather all the information that you get and then apply the llm to summarize right so obviously you can do it with langchain and so forth. So some of you who are using the agent in like change you remember it will be we did that. We did the open domain summarization as one of the exercises. So that's that. So this term does come up in this paper. So I thought I'll remind you what it means. So, So they have been some past efforts to retrieve information multiple times while generating output. So you go on retrieving information. So one extreme case is every sentence that this LLM produces, you search for that. Right. You get the search results, feed it in. And now that becomes part of your prompt instruction. What it has generated becomes part of your instruction and your question becomes part of your prompt instruction what it has generated becomes part of your instruction and your question becomes part of your instruction right what they have found is that by the way that for ai might make sense for any engineer systems engineer that would be a blessing or a nightmare nightmare think of the performance impact of that right every sentence you make Think of the performance impact of that. Every sentence you make, you're using a whole system to generate the next sentence at that moment. You're searching, you're doing this, you're doing that. A lot of things are happening. It's a very tedious and expensive process. It turns out that you don't need to do that and if there is a point of diminishing returns, you don't get much. So the other limit is of course just plain rad you retrieve information once and then you produce things from that there have been some past efforts to retrieve okay blah blah blah in this work we provide a generalized view of active retrieval augmented generation means the llm is itself actively participating in that, helping you do that. And by giving you the probability that you use and you do that, we propose forward-looking active retrieval, augmented generation, FLAIR, right? So by the way, the word FLAIR, there are other algorithms in AI space with the word FLAIR. So if you Google FLAIR, something else comes up up but remember that they seem to have also used the same word what part is forward looking means whatever the next sentence that is generated don't just let it go out hide it behind the curtain make a decision do you want it to go out or not right or you want to throw it away and just use it for something else right that's the part a generic retrieval and they say it's pretty generic means you can apply to a whole class of problems right uh uses a prediction of the upcoming sentence to anticipate future content which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it were if it contains low confidence tokens by now i hope after all the explanation this should look obvious we test flair along with baseline baselines comprehensively over four long form knowledge base intensive generation tasks data sets. FLAIR achieves superior or competitive performance on all tasks demonstrating the effectiveness of our approach. So obviously this paper wouldn't be worth it. These days it's a sine qua norm that you have to beat the state of the art. So we move forward from that. I will leave this introduction because it's about the background and what others have done. For basically what it says is for simple things it works. I will get to this diagram a little bit later. Let's go straight to there. So you know what retrieval augments. So this is a bit of terminology, setting it up. Can I skip the intro part guys, which is our background and what others have done and so on and so forth. So what we will do is we'll start with the notation. D is the corpus. D is the corpus. What is the corpus? What does that scripted D calligraphic D stand for? It's the set of all documents. And remember guys, here the meaning of documents is different from your meaning of documents. What have you indexed in your project your index chunks so what are your documents chunks the the little di are the chunks right here wikipedia the goal of retrieval augmented is to generate the answer why why is made up of these sentences s1 s2 m sentences of which is the same as n tokens how can it be because n tokens are partitioned into m sentences right containing m sentences and leveraging information from the corpus in retrieval augmented lm the lm typically pairs with a retriever that can retrieve a list of documents this is the rq is the search result right for a query queue so if given a query queue you get the retrieved results namely the search results and you use that right since we focus on examining various methods for determining when and what to retrieve, we follow existing methods, blah, blah, blah. So I hope this is obvious that the large language model takes as input those search results and the input. Isn't it? It's written like that. It works on the couple. It works on this. More common sense common sense away prosaic way of saying saying is how is it just write it down one document two document three document literally write the text down right and then follow it with x namely the question q the in the question right now let the language model act on it so it's a one shot it's just forward pass it is here it is, and now I sit back relax and wait for the text link generate right, I will say generate 1000 tokens. Right, or 4000 at least 4000 or something like that or generate, and it will go on generating till LLM comes to the conclusion that it's done so that now when i put it like this obviously it looks not so good right so that was a single time augmented model so what are they saying they are saying that keep let us say that up to this sentence, y, t, all the prior sentences using the same mechanism, they have been accepted. Right now, that is your query. What you do is you use, you have to formulate a query. What you do is you take x. Let's let's form that let me read it out because this is worth to you to aid long form generation with retrieval we provide active retrieval augmented generation it is a generic framework that actively decides when and what to retrieve through the generation process resulting in an and this word is sentence is important resulting in an interleaving of retrieval and generation right remember we had two ways of getting answers from the surge and from the lm language model and you're interleaving right sometimes you're throwing away what the LM produced and say, no, no, let's go and get some more facts and do in context learning. You know, literally do like sort of educate the LLM a little bit more and then see, does it produce an answer? It has more confidence. Right. And you keep doing this until leaving this to approaches, formerly at step T greater than one is assuming that the first time around first time do you have a choice you just ask it to produce. Right. The retrieval query cutie is formulated both on the user input x and the previously generated output so far. So what is it that you query for? What you have produced so far and the X. It makes sense that the entire thing you can get. Now you say that, well, isn't it a lot of text to give to a search engine? What happens when in Google you start writing a Ramayan? What happens when in Google you start writing a Ramayan? Yeah, Google produces all sorts of results. I mean, Google actually does a pretty good job because it's very AI driven. With Elasticsearch, OpenSearch, what happens? It will produce lots of irrelevant results, isn't it? Keyword search engines. But why would we? So why is this a good idea so explain to me guys why did these authors believe that this is this looks like a good idea why would you say that this is not a your brain thought you don't want to search with huge text if you go into that well multiple reasons first is that see especially if you're doing ai search the more context you give the more richer the semantic of your query isn't it and actually you'll get better answers the whole power of semantic searches the bigger the input the better usually better the output so long as the input is all about the same topic right but you just said that there's a paper which is lost in the middle no no that is on the context that's not a difference that is for the llm not for the search okay remember there are two different words uh ai search loves long text so long as it's about the same topic because the context the the sort of semantics become clearer and clearer. OK, the search is different is happy when you have more tags. But that's what I said. LLM is a great informer. Yeah, that's it. OK, I don't want to put in too much context in LLM because then it's like lost in the middle. Yeah, yeah. When you put too much context is lost. Okay, so here it's a good idea because, again, as I said, search doesn't mind the better the more the better. Which query is the query formulation function at the start of the generation, the previous generation is empty. There's nothing. So what do you search for you search for the users query. Whatever comes, you put it in the thing user's query and you you ask the llm to go at it right given the retrieve document dq so it will return the documents dq llm continually generates the answer until the next retrieval is triggered or it reaches the end so suppose somewhere along the next sentence, the LLM comes out with this saying that, okay, I'm done. You're done. But barring that exact terminal condition, all you need to know is, do I take it or not? So LLM will base it on its answer on these things. Where does DQ come from? From the from search. Where does DQ come from? From the from search. From search. Right? Where Yt. Yt represents the generated tokens and blah blah blah. The user input text and the previously generated. At each step we discard previously retrieved documents. Of course. You don't want to keep on the documents that came long ago. And only use the retrieved documents from the current stuff so by the way this is a clarifying point because somebody may ask oh by the way are you using only the documents from the current search or are you also using all the previously retrieved information that could have been relevant so they are saying no we only use the documents on the current search right now why is that a good idea the previous documents from the previous part also are relevant i think biryani is having its effect yeah so he just came he didn't have okay so um the pre why would you not do that? The reason is because you're already feeding it when you query, you're proving it all the output, which was conditioned on the previous documents anyway. And in some sense, the coloration from the previous documents is already well there. And so you do that. It would be redundant to pass it again. Right. And so you do that. It would be redundant to pass it again. So I still have an accessory to ask you a question. See, let's say the previous search has resulted in lots of documents. And that document... By the way, you don't when you think in this rag, please try not to think in lots. You don't think hundreds. Yeah, maybe let's say start with 20 results now how who will determine that these uh document one and query one is going to yield another 10 result and are they out of this player no no that is exactly the point no this is the method uh what it says is that, so let's start from the beginning. All you have is the user's query. Describe to me the ancient, let's say the Harappa Valley, the Indus Valley civilization. So that is your query. It generated, it will say Ind this value civilization is known to have existed between blah blah bc to blah blah. But somewhere along that you just check this one sentence we check the proxy let's say that this is already 64 tokens, it is not. You look at this and you ask Okay, is the probability is there a token any one of their token softmax probability below a threshold if it is not okay one sentence good sentence right then you say okay continue right it starts producing this and it says right it was located in ancient it was located somewhere between in one of the greek islands in the mediterranean sea right and then you notice that the probability looks rather low so what do you do so i'm not going to print this sentence i'm not going to emit this sentence so now what happens happens? What is your query? We will talk about the query in a moment, but because that's where the whole technology comes in. Right. But your query describe the civilization is still there. Right. What we will do is we'll do something interesting about this wrong sentence or potentially wrong sentence they use some clever magic to it which we'll talk about to go and search okay okay okay follow up question sorry so let's say it is still behind the curtain it is not produced and dumped into the yes no the first sentence which which it was confident on it okay yeah but it's only when it is not sure so it is like a layer it's a decorator pattern right if you think about it this way in programming right it intersects what the llm produces and decides should i let it be the output or should i go back and re-instruct the lm i thought like it is still like uh it has collected all the high probability output and then try to figure it out more no no it's a that is why it's an active process that's why they use the word active okay it's like a forward-looking you like the probability you let it go you don't like it you stop it and you do so exactly so that's why the word forward-looking forward looking because the output statement can have a lower probability than the middle of some statement the minute you have lower probability you go into the big set now even though that's the output which you have generated there has to be some cutoff probability let's let's take a cutoff probability 0.2 i'll just take that right 0.2 is actually pretty harsh but okay let's take that so what happens is that one second i'll just take one right point two is actually pretty harsh but okay let's take one so what happens is that one second i'll come to it let's say that the first sentence went through it's forgotten forgotten you just remember the tokens why one you remember it's part of your history that is committed right think of your database language it's a commit point checkpoint it's committed right now the question is i'm looking at the next sentence either everything is fine i let that also go through or i say no no so now is the question i need to what do i need to do i looked at the output of the llm but i'm the interceptor i did not let it go out i am going to now dip into the search engine get some date documents use that use the original query right and use the tokens that i've generated so far namely the first sentence and give all of them right Because those are the things I'm sure of. I am taking it as a gospel truth, the documents that are there in my office. I'm taking your question as valid and whatever the LLM has produced with high confidence, I'm taking also as truth. And I'm saying, these are facts, what now? Right? So that is what is going on. Now, it still leaves the question, one second Amit, it still leaves the question, how did I search? I said, I will use the last sentence, the defective sentence, or the suspicious sentence, right? And potentially the things that tokens that have been produced so far and some things as material and it says to go to the search engine. But how do we do that? There are multiple approaches that we talk about and we go over and you said that i'll let the first sentence go and start the second content and deep into the this again what are you looking for what information you are sending no no we'll come to those i mean we are starting on the paper that's see guys here's one thing about research papers it is called the imc imcr right introduction method the IMC, IMCR, right? Introduction, Method, Results, right? Conclusion. And then of course the references, right? Oh sorry, not IMCR, IMRC. Introduction, Method, Results, Conclusion, right? So at this moment they're describing, they're just beginning to describe the method. Introduction is over, right? Introduction was section number one. That is why in most papers you notice that it literally spells it out. You see one introduction, right? Good papers are all written like this. Two, if introduction is over, what will two be? Method. So if you want to save time, jump to first lead abstract jump to method. Right. One second guys, I'll come to you in a moment and then we'll come what after method comes what results empirical results. Did it work prove that it works So it would be section four, perhaps implementation details. And then four is still implementation details. And five is the again the methods and then will come results. So from that you conclude that two, three, four, five. They have a lot to say. They have done a lot. Right. Experimental results and experimental results. Right. Well, obviously, experimental results will always be one thing for sure. Right. In the majority of the cases, they would have done some really good work. Right. Do you notice that all the green bars are doing better? Green bars are the forward looking the FLAIR method. Right. And then there would be one good thing that good papers have is something called the ablation studies. So you took this approach. This approach had multiple parts. You used trick one or technique one, technique two, technique three, technique four. So now you need to give in some sense of weightage. How much did technique one contribute? How much technique two contributed? And so forth. Think of it in some sense as shape values, contributory factors. How much did each of them contribute? Doing that is the the the the the the the the the the the the the the the the the the the the the the the the the the But when you do ablation study, it turns out that results stay still strong. What does that mean? You may love it as much as you want. It's doing nothing. So ablation studies are important. And then finally, conclusion. There we go. Conclusion. And another thing good papers have. See, good papers always not only show the positive results, they have limitations. Right? And one of the criteria of research papers to be accepted is, which reviewers consider, is does it show the way forward? Usually in the limitation section, you not only say, oh, this thing it cannot do, but you also open the door for other researchers to continue your work. Are we together? It's very important. Research papers that, suppose you say that I wrote such a, I did such a fundamental work on this topic that nothing remains to be done. Right? Actually, you said the last word on the topic. That may be lovely in history or some other thing, sociology or God knows what else, but in research, in STEM fields, it's never the same because knowledge inherently is incomplete. Right? In fact, there's a, I don't know if one can directly relate, but there is such a thing as a good good else in completeness there that says that scientific knowledge by its very definition will forever remain incomplete. Right. So that's important. So anyway, going back to it. I wanted to finish by now, but I'll just take another 10 minutes and i'll just show you the gist of the technique guys so now forward looking active retrieval what it does is it basically looks for i mean you can now read this paper it will make a lot of sense to you so there the techniques they use is direct flare what they found is What they found is, since we cannot fine tune black box LLMs, right? Okay, so let me actually backtrack a little bit. We found this part is interesting. We found that LLMs can effectively combine two skills and generate meaningful search queries. What are the two skills? An instruction to guide LLMs to generate search queries. What are the two skills? An instruction to guide LMs to generate search queries. See, one of the questions is, suppose the LM comes to the conclusion that I'm producing a token and I don't have confidence in it. The sentence is not. You still have a task. You need two tasks. You need to generate a search query. Full search query that you will feed into the search engine. But you can use the LLM itself to create a smart search query. So that is one skill that it must have. The second is an instruction to guide LLMs to perform a specific downstream task. You have the question answer, whatever it is that you were trying to do. specific downstream task you have any question answer or whatever it is that you were trying to do like to create the instruction right an instruction to guide lms to combine skill one and skill two and that is this so basically you know it's somewhat self-referential you can use one second guys i know there are a lot of questions you can use another llm to generate all of this or better still you can use the same llm to guide itself right it gives meaning to coloration to the word active right okay guys so um for positive time just give me a little bit of tech so there's some technical details um here okay i'll let you get to that one of the things they found is get to that, one of the things they found is what are the two ways that you can use, what queries that you produce, there are two ways of doing the query. Suppose you have a last sentence, right, and you, some tokens send it to a low probability. What you do is you make it a masked language model of sorts. You mask it. And you give it to that LM, the language model, and you say, hey, here is what we know. What would be a good question? What would be the question that would help fill in these blanks? Are we together? So let us say that it came to the conclusion that the capital of Bihar. Is Paris. Right, and so you must Paris. Right. And then you say these are the facts, whatever you have produced so far, what would be the question to ask in order to complete this sentence, this, this, put the mask there, fill the mask. And that think in the because there are so many questions and we have nine minutes left, I will let you guys read the paper. But guys, did you get the main idea of the paper? Read through this. Now you can figure the head out. OK, questions. First, there was a question in the back. Who was it? Way back there, last time. Oh, Albert, you had a question? And then I think Amrit, you had a question? I had the same question. Okay, Albert. So this confidence level, what is the confidence? Is that the probability of the answer? Just the probability. And go ahead, Patrick. Is that the probability of the answer? Just the problem. And. Go ahead. As for the implementation of the last language model, is this is this going to be a separate API call that says you are to you are to generate a question? You can do this. So we have to extract first the uncertain sentence right yes so patrick what happens is that see this is like in programming you do exception handling try catch blocks. Try to try except blocks. It's literally like that. What happens is, if you remember in programming, the moment you go into exception handling, performance is always lower. Isn't it? Because the compiler will always optimize your main code at the expense of your exception handling code. Exception handling code will be thrown all over the place. Seems similar thing. As it is generating, if it is generating good confidence sentences, all is well with the world. The moment it generates something that it's not confident on, it's almost like an exception being raised. Then you bring in a technique to handle that exception. So another way of looking at this entire paper from a programmatic way of doing it is you detect an exception and then you handle it, right? You have essentially put your generation in a nice try catch block from a programmer's perspective, isn't it? To get better yield. That's what this paper is. Okay, guys. So one last question. Can you find tokens with low probability? better yield that's what this paper is okay guys so it's a two-step process right first you generate the perfect question right and. From the element. There is one API. From the element. And then the second one is to do the search. The search with that. You go and search with that. You get the search results. You now create yet another instruction. Forget your previous instruction. You put that knowledge, your generated text, and finally your question. Usually it's defending here. So one of the questions you ask is what order. Def everything before your question so and before that there is a ragster yeah so in other words first first narrate the entire ramayan and then ask sita kiska so it's sort of like that patrick it is like there's a whole story in which there is a king and a queen they're gods right and there's war and many things happen. It's very much like the Greek mythology. But at the end of it, the queen, the joke in India is if you don't get that story right, then you are still asking, who was she the father of? So sort of like that. Yeah. So anyway, that is it guys. Another 3-4 minutes left. Anybody else has a question? Any comments guys for those of you who are remote? Is this paper clear? Obviously we didn't get time to go away. Right now the paper is clear, but we have not seen the implementation. So the flow goes as a blank to get contents. Context comes into LLM. LLM starts to stop when it gets an uncertain sentence. Sentence gets pulled aside and then we do a second call that says given this uncertain sentence mask yeah mask and then what's a good question and then go to search and then yeah go to search and then third column given continue now continue now with this and that's all on the stack so you have to go back into the biggest column yes yeah so given my well-being this is your next one try to drag on steward yeah this is I mean unless you don't have rag how do you do this yeah yes so guys it's it's it's worth it if you can do this. And look at the GitHub. They have a GitHub and all of that. You can do that. Sometimes, you know, you learn a lot by reading the code. And there are websites like a code with, you know, annotated papers, right? Paper with code. Code explain. I mean, paper explain with code. What's that? Mark. Yeah. Right. So there are many such thing, resources. Actually, you have a lot of these good resources. Send them to me. I'll add it to the course code. So use that guys. Read this paper carefully, please. And read, go to the GitHub, look at the models and better still if you have this stamina see guys create a implementation a basic implementation like this on your own try to do it don't have to do a full-blown one like a complete state of the art but try to it's not a lot of i i hope it's not see getting these ideas is hard right in hindsight they look obvious but once you get it you realize that you can do it. isn't better to use just a local model or like a. No, not PC we are bound by realities all of us are sitting on 4090s. So, whatever fits into 4090. But like gp2 versus mr oh i would any day prefer mr so it's still a smart smart model smarter model oh so we are searching for mr okay guys so remember half an hour break and the next paper is totally up to you remember this paper readings are sort of optional. Here onwards and your projects which we will start evaluating at 530 is also optional, so if you guys want to dedicate your focus to coding please continue to make progress on your project, whether or how many of you found it useful. Alright, so we'll do video llama. Half an hour, Shah. Video llama.