 Welcome to the second week of the NLP workshop, part two of the machine learning series. This week we covered in the theory session, introduction to topic modeling and semantic analysis. Today we are going to do a lab to get some practice in that topic. Now, topic modeling is about taking a corpus of documents. A document could be a sentence, it could be a paragraph, it could be a sentence, it could be a paragraph, it could be many paragraphs, it could be in some people's mind any logical unit of text. It could be, for example, one email and so forth. So you take one review from one customer about a product or something like that. So any logical unit you can consider a document. A collection of such documents you call corpus. Now, given the corpus of documents, there would be a finite set of words used in that corpus. That would form the vocabulary of the corpus. of the corpus. That vocabulary would be a subset of the language in which the corpus exists. For example, if the language is English, then there would be a much larger English vocabulary. The vocabulary that is found in your corpus will be a much smaller subset of it. So we will use that to build the vector space representation of words. Now, the only vector space representation that we have started out with them in this NLP so far is just the count vector or the dfidf vector for a document. Both of these represent a document. Now, we will stick to that. Now, it is true that in part one of this ML400 series, we did the word embeddings, the word embedding vectors, which are more powerful in many ways. I've deliberately eschewed it from today's lab just to have continuity for those folks who have not taken the previous workshop, who have just joined into the NLP directly. So I will bring in the word embeddings in these exercises, which would be a straightforward extension when we do word embeddings in detail, which would hopefully be next week. So with that discussion in place, let's launch forth into the lab. The code is here, guys, right at the top of your course webpage. It is here. For those of you who have been quite busy and may not have noticed, the video recording of Monday did come up in time and it has been on the website for a while. So you can review the topics, review topic modeling. Well, that's the topic of this week or the theme of this week, topic modeling or semantic analysis. You can review that as preparation for your quiz and the lab we do today also should give you enough practice. Now what I will do is as I go through the lab I'll mention a few things as extra work to be pursued as part of the part of topic modeling exercise. Consider that as homework. I'm keeping it very lightweight because I do realize that in the United States at this moment from where predominantly most of the audiences are participants are today, we are going through a sort of nail biting experience where everybody has its breath, has his breath or her breath held in anticipation of the presidential results, election results. So in view of that I've kept it fairly lightweight but still it is worth doing some practice as we go along. So with that in place let's go into topic modeling. Topic modeling in simple terms, it is the pursuit of finding the hidden topics behind a corpus of text. Now, the word that I'm using, which is of importance is to call it hidden or latent topics. The topics are latent variables. In simple terms, what it means is if you look at the interaction matrix between documents and words or tokens, a document is made up of many tokens or many terms and a term contributes to or occurs in many documents. For example, the word, let us say, hungry, might be in many, it may be in documents that pertain to health, it may be in documents that pertain to social sciences, and humanitarian documents and so on and so forth. So a word can occur in many, many documents. Now, and a document of course is made up of many words and many terms. So the topic modeling is a supervised learning, predominantly a supervised learning exercise that asks this question. Behind these documents and the words that they contain, there must be some latent explanation. In other words, some topics. So documents are about certain topics and topics that contain certain words have a higher frequency of certain words. For example, if you're talking about artificial intelligence, this is where you'll find words like regression classification, you know, precision recall and all of that clustering and so forth, and you look at politics, you would not find these words present on a typical article or documents from a politics website. Actually, to be sure that that is so today, just before the class, I went to one of the political websites that I found, whose name apparently is Politico itself, and I checked if I could find a page that would have a lot of techie terms like neural networks or so forth. I couldn't find it for obvious reasons. So it illustrates the point that topics sort of own words to different degrees, and documents, they're primarily, they are mixtures of topics. And that understanding is at the foundation of topic modeling. I'll repeat that. We think of documents as a mixture of different ratio mixtures of topics, talking about different topics to different degrees. And a topic has a certain distribution of words. For example, the topic of artificial intelligence will have a preponderance of words like neural networks and support vectors and precision and recall, classification, regression, clustering, and so on and so forth. So you get the idea. So that is it. That is the way. More mathematically speaking, for those of you who did ML400, you'd say that the topic represents a certain specific distribution, probability distribution of the words. It is uniquely defined by a certain probability distribution over the words. And a document represents a vector, a very simple vector, low dimensional vector, where in the topic space, where the axes are the topics, and basically in simple terms what it means is it contains different topics to different degrees. Right? So that is what a document is in effect, right? If you, another way to do that, a slightly alternate way is the way I think of it. If you think of a unit and dimensional sphere in the topic space, a document is any one document represents a point, some point on that sphere. And so two documents are similar based on how close they are in the sphere, how a smaller distance you have to travel geometrically in the sphere to go from one document to other. So that's a basic idea. We covered this in the previous session on Monday, so I'm not going to go over that in too much detail. One of the requests I got from a few people was that we should try to finish by 10 or by 9 30 and the other request I got is to see if we could cover the core of the material within the first half so I'm going to slightly change the format to address this feedback I will give you an overview of what I'm going to do in the first hour and then I'm going to go ahead and do it. So what we are going to do is this lab, which is about topic modeling, the specific libraries that we will use, which are very prevalent, first of all, spaCy will use, because we have been using for most of our NLP tasks. So we'll continue to use spaCy. It is very simple. It's a very high performance implementation, no reason to deviate. I will, of course, we have used scikit-learn for most of machine learning. I think occasionally I may have, just for a small exercise, I will use scikit-learn. Now when it comes to topic modeling, pretty much the common libraries used are, comes to topic modeling, pretty much the common libraries used are one very good library which is written in Java is called mallet. Mallet tends to outperform other libraries. Now, then there is within the Python world, there's a library called Gensim. Gensim is also a high performance library. It is implemented in C, has a Python binding. So, Gensim also contains Python bindings to, not bind, but Python wrapper for mallet. So, you don't have to really go down to Java code unless you are familiar with Java to do topic modeling using mallet. Some basic to do topic modeling using mallet. Some basic activities of mallet you can do right from within Gensim itself. And if you're familiar with the syntax of Gensim, you could just continue to use that with small modification. Why is that relevant? Partly because Gensim comes with a visualization tool for topic modeling. And if you want to benefit from the visualization tool and the metrics and so on and so forth, then it is good to see other libraries conform or have some bridge over to the Gensim syntax. And when you do, it is a good thing. Mallet, in my experience, is generally in terms of topic modeling, very, very often it produces superior results to the built-in Gensim topic modeling. But that's sort of my experience and your experience may vary. Remember the no free lunch theorem, no one thing ever always outperforms other things. So for this particular thing, I was going to use three particular methods. One is latent Dirichlet allocation. This is the type one. Please correct it in your, obviously I've already released the code. This is latent Dirichlet analysis would be less frequently used word allocation. location. Asif, where is this file? Which file you opened? This file exists. Give me a second and I will show you where it exists. It exists here. Here, if you go to the course webpage, do you see content of this week right in the top? Yes. You click on that. And where does this file exist? In this directory. See, when you download it, by now you must be familiar that your project is in the deep learning workshop, and your Python notebooks are in notebook everything to do with nlp like today which is in the nlp libs directory i see okay right so everything we're going to be noticing that the prior labs they are all here so just go to the nlp live and you have your site and so all right coming back to it, latent Dirichlet allocation, linear discriminant analysis. Actually, I wanted to give a simple explanation for it. I sort of didn't get the time. So this is off the table. So don't be worried if you get this notebook and you scan it up and down and you don't find the linear discrimin determinant analysis. I will release that code or example perhaps in the next lab. Those of you who have done ML100, which would include almost all of you, you know that LDA is very simple and I challenge you to, on your own, just try it out. It is very simple and you'll get there, right? But I'll certainly release the code for that. So this is it. Now we are going to do the lab on a very popular data set, which is the 20 News Group data set. Now this 20 News Group is this. People sat down and they extracted a lot of emails, about 11,000 emails, more or less uniformly distributed, not quite uniformly, but more or less uniformly distributed from 20 news groups. So we start, so let me give you a general idea of the code. In the first part, I'll just walk through the basic structure. This should be the familiar set of imports. The only thing you have to change is wherever you install the code base, you know, download the starter code, change this one line so that it refers to that. Then. Rest of these are imports, we are importing Gensim, we are importing spaCy, Pandas, Escaler, actually UMAP. I didn't come around to doing it, but I'll show you beautiful visualizations next time with this Matplotlib and so on and so forth warnings. So now let's look at this we load the data it turns the reason i used sk learn scikit-learn is because scikit-learn contains this 20 news groups data built in right in its data sets listing so it's very easy to just go get it now what are the 20 news groups as i I said, it is the emails, about 11,000 odd emails spanning about 20 different news groups with 20 different topics. So which topics are as follows? You see those topics here. Right now, here is a sort of a homework I'll give you guys. I am spanning 20 topics. What I would suggest is pick three topics that to you look fairly orthogonal. For example, you can take sports, one of the sports or two of the sports topics, and then take science, let's just say space or something like that, and take one from, I don't know, politics or something like that, and just take three of the topics. You can do that by putting filter, remove, and then there's a filter clause also that it takes, if you look at the documentation of this, and then you can then do the topic modeling. Here, what happens is we already know the topics, right? So you might wonder, if you already know the topics, we know the 20 things it is coming from, why in the world are we doing topic modeling? The idea is in real life, you won't know the topics. You will try to figure out some good topics in the data. In this particular case, it happens to know that. And so what we'll do as we do the topic modeling, we'll try to see how well the topic modeling algorithms are able to sort of surface or figure out what the real topics were. That's our exercise. So take that. As you can see, it's more or less uniform distribution. Some topics are less. The talk religion mask seems to be rather less and atheism seems to be less, but not by much. We will do the first approximation, consider it uniformly distributed. So what I did is you take this data and you just put it in and again I'll go a little bit fast because then we'll come back and do it slowly. I just put it in a pandas data frame to see what it looks like. When you look at this data here let me sort of increase I'll increase the font a little bit so you guys can read this. If you look at this, do you notice that it contains a lot of strange characters, I mean a lot of things which are not really English but sort of are other things. article from a certain article by a certain person, and so on and so forth. So there is a lot of end line, there's that characters here. So one of the first things you do is, and then also you notice that if I look at the topics, there's a lot of overlap, like politics, maybe you don't want to distinguish between all the different categories of politics and religion all the types of religious discussions talk and so forth so what I'm doing is to simplify I'm breaking it up into just six groups and I would suggest you can do it with smaller number of groups but even before we do that are the here's the thing. What you do is you have 20 news groups. Here I'm reducing it to politics, religion, sports, computer, science, and foresight. And here's a hint I'll give you. Actually, one simple exercise you can do is you can merge computer and science. They are both STEM field. They are both STEM field. So you could merge the two and perhaps you could merge religion and politics. They, despite all the effort to keep them separate, they all tend to overlap with each other. So you could do that. So you will get fewer number of topics. And for sale, you can add into I think in this case, it's equipment, cars, or so forth. You can merge it with sports so that you have three good topics. And you can try to do topic modeling. So it's just a basic idea. So what I'm doing is for each of the row, I'm adding a new column called topic. And when I do that, you notice that if you look at the columns here, the rightmost columns, the target is what came with the data. And I'm taking another topic, simplified topic, which is now sports, computers and whatnot. So I'm just augmenting the data with info, so some topics that. So now when you have a data and you try to do any linguistic analysis or so forth, or any kind of a semantic understanding, you know that some things don't contribute to a semantic, understanding the semantic structure or the semantics of the data or the meaning of the data. For example, the basic stop words are the has this, that the very common words you don't want to keep them. So every single library that you can think of in NLTK, sorry, in natural language processing, whether it spaCy or NLTK or whatever it is they all give you a list of stop words and quite often the stop words are either derived from or directly the famous Stanford WordNet's list of stop words so uh we will remove the stop words likewise we'll remove punctuations like uh now whether we should remove punctuation or not, it is a little bit of a thing. I decided that in this exercise we could remove the punctuations. Does email contribute to semantic meaning? Like, for example, when you look at this, is it going to contribute in any way to that? It is not going to, so we are going to remove emails. Remove proper, actually I removed that. I kept the proper nouns because then I thought it's cozy and things like that. And quite often they are proper nouns that are worth keeping. So I should change this, remove this from your mind. Lemmatization of the word, would one of you remember what lemmatization is? Getting to the root word. word, would one of you remember what lemmatization is? Getting to the root word. Getting to the root word. Now, somewhat conspicuous by its absence is stemming. I am not using a stemming. I tend to use it less often. I prefer lemmatization for many more situations. But you know, actually, it's more laziness, you should try both and see which helps you more. Now, so the other thing is, let us go and clean the text. Before we clean the text, we will use the language model. NLP's English language model and you remember which model are we talking about? We are talking about the English core web large. If you remember, I'd suggested always use the large one. I'll continue on. So once you have done that, I'll explain to you what a clean text looks like. I was wondering if anyone out there could enlighten me on this car blah blah blah do you notice this this thing right now what does that do if i try to clean this text now by the way i've given a clean little utility very simple function to clean the text let's go and see what the clean text does it is extremely simple actually. It is again where would you find the NLP libraries in the NLP SV NLP directory. So go into utils and you'll notice that I am doing I'm basically and this is something you can play with based on your context. Like spaces don't mean anything. Pronouns don't mean anything. Punctuations don't help much. Symbols don't help much. You know, left and right brackets. So you can decide which you want to keep and which you want to remove by commenting or uncommenting. And consider it an exercise that you want to do that. Now, let's look at what we do. What we do is you're giving a token and you're trying to decide whether to keep it or not. And the code is just one line. If the token is in the ignore part of speech list, punctuation and symbol and so forth, it is not worth keeping. It has not much semantic value. Likewise, if the token is like a URL or it is like an email or the lemma, when telematized, the length is less than two. Now be careful with this. Sometimes on rare cases, it might have meaning, but here I just said that okay if it is just one or two letter word drop it and if it is a stop word again drop it right that is all so you drop you see how it is I drop the not so useful parts of speech drop common words drop urls and emails and so forth and you can have more conditions but to keep it simple I stayed with these so far. When you do that, in that one line of code, actually, and this is the power of NLP libraries or NLP itself, it's pretty mature for all the pre-processing tasks. You notice that this thing has become this, right? We could make it even worse. Let us say that I make it this I say are you separate like this Let's see. Now what happens? I run this. Do you notice that it has managed to remove all of those things? You could play games, sometimes the email addresses come wrapped up with angular brackets and you could do that and it would still be able to figure it out. It's pretty good at that. And if you read the words that gets left behind, I hope you would agree that these are likely to contribute to the meaning of a text. So now I just illustrated this function, clean text. Now what do we do? In this data frame, we are going to clean up all this text and create a new field called cleaned text and in the data frame the newsgroups data frame i'm just applying clean text to each row and yeah go ahead the topics that you assigned earlier those are random if you grow up go up there you assign those topics yes yes yes they're just random assignment yeah it is my choice i wouldn't call it random but it is subjective you could do your own assignment am i clear um so these i'm not talking about the the here that you picked, you know, the seven topics. I'm talking about the output seven. You assign those. So that assignment I said, was it random or? Output seven. You mean the last one? The output of the seventh. Output of the seventh. Yeah, topics here. No, no, no, it's not right you see the topics are the ones that we assign but the target comes from this list in the data itself yeah correct the target is part of the data the topic is something that you assign that i am assigning yes very much true and remember this is not topic modeling because here, what we are doing is we already are setting up an answer. And then we will see how good the topic modeling algorithms are in coming close to our answer. So you know what that line, the text is then. You know what you're assigning. Yes, yes. This is my choice. Very subjective assignment. So so far, we haven't gotten to Simon Borgerson, Dr. A topic modeling at all. What I'm doing a standard exercises of NLP which hope by now you are all very familiar with. Right, so Simon Borgerson, Dr. Do guys again to recapitulate please do make sure you're reading the green book and the green book. The one green book being the spaCy and the other green book being the Manning Natural Language Processing in Python. So make sure that you're reading it. And then there's the O'Reilly book. Pick your choice. It doesn't matter where you read it from or whether you're reading it straight from the spaCy website and so forth. The important thing is to become fluent with at least one natural language processing library for all the classical stuff, right, and do it. And these are small libraries. I chose spaCy partly because it's high performance and very small length, very small library. It's not like vast like NLTK and so forth where there's so many many things you get the learning curve for specie you'll realize is very short so what I'm going to do is we have one quick question before we move on line 10 you're importing something called en line 10 no no the line is different sorry line 8 in yours yeah yes yes you remember what what is this this is the english language okay but about that it'll be equal to spacey dot load n is that how we do it or was it a longer string no yeah okay for some reason i get an error with that okay i'll figure out yeah you can give the full name if you prefer you can give it the exact full name okay so let's try that i think that's what is needed perhaps yeah you can do that and that will also work right yeah it will also work so yeah it's sort of these are variations. Now, let's go in here and see what I'm going to do. I'm again not reached topic modeling. I'm just setting up the system so that you can compare the topic modeling the output, this becomes a case of supervised learning, classification. I can train a basic classifier and see that. Now to have a basic classifier, all is well. The trouble is the input must be a vector. And so the only vector space representation of document that we have learned so far are the count vector or the TF-IDF vector. So we'll use the TF-IDF vectorizer. We'll convert each document, each document being one row of text, into a vector. Again, simple. Once we do that, this is just an illustration of the kind of vectors each of this becomes, the words become, and so forth. So I wouldn't go into that. This is just illustrative. I added it here. Now what do I do? This is basic by now, you must be familiar with. If you want to write a basic classifier, what do you do? You take the data set, split it into test train, then you take a classifier, you fit it to the data, you make predictions, and then you come up with some prediction metric. So I wanted to do a very quick and dirty result to see what it comes up with. And if you notice, I use a very basic algorithm so that it runs fast. It literally executes in six milliseconds, I mean here, executes in 37 milliseconds, very fast. To get a very rough and ready idea of how good a classifier, how good the classification is, how well are the topics separated. 82% F1 score, that is a balance of precision and recall, is I would consider it pretty good, like 83%. It a pretty good separation uh between the topics so now one of the things i taught you is uh mentioned the zips law for distribution of word frequencies what the law says to recap is that the rank the frequency of a word the frequency of a word is inversely proportional to the rank of the word in the corpus, right? With respect to other words, the frequency rank, right? And so we can try and see whether it is true. Now, it was more than that. F was one over the rank to the power n, to the power s, where s could be something. For simplicity we assume one, but all right, that is f right. Let us see if that holds in some form. What I do is you take all the words that are there in the corpus. You know this is your clean text. you can extract all of the words and it turns out that they are unique 114 000 words 289 but let's go with 114 000 words that's a that's a pretty large vocabulary of words there. So let us see if the Zipf's law holds. What I'm doing is I'm just creating a frequency table, you know, the word frequency, very basic. Then you see what happens is I happen to know that you'll end up with a very long tail. You will get gazillions of words with, you know, that have occurred only once or twice in the corpus so just to keep your just for the purposes of visualization and no other reason i deleted any words i removed any words at least for the visualization purpose whose frequency is two or less right so then i end up with a smaller number, 28,000 unique words. And do you notice something very interesting? We started with 114 and 114 has already reduced itself by a factor of almost one fourth or one fifth it has become. By the time we go to words of length three or more, I mean, words of frequency three or more. That already is suggestive of the sort of a Zipfian distribution to some extent, but let's continue that. And then I make the frequency plot. Do you see this one over rank kind of distribution? The X axis is the rank, Y axis is the frequency. I hope you can see the classic signature of a long tail distribution. I've taken only the first thousand ranks because otherwise the graph looks, it doesn't have, it doesn't look nice. So I just made it like this. And then if you take the log of it, the log of it should have been more or less a straight line. It isn't exactly a straight line but let's just say that okay i will call it a rough approximation to a straight line would you agree guys on the right hand side right sort of like a straight line but not quite a straight line so it is a very rough approximation to the ziffian law distribution so that is that approximation to the Zipfian law of distribution. So that is that. I'm illustrating the fact that Zipf's law of frequency distribution does hold and that is the reason why remember in the TF-IDF definition for the idea if we take the log right just to linearize things a little bit. Okay now we come to the topic modeling with Jim Sim. Now, guys, we know enough about the data. So we will start the main thing that we want to do, which is topic modeling. We use a library called gen sim. It's a fairly popular library in the Python world for topic modeling. And there's a pretty good job. It's written in C underneath it. So it's a fairly high performance implementation. What does it need? It basically needs an array, a list of documents, each document being itself a list of tokens, list of words. So it is a list of list of words, right? In some sense, a two-dimensional array, a two-dimensional matrix of words. So it is a list of list of words, right? In some sense of two dimensional array, two dimensional matrix of words. So what I'm doing is I'm just creating it like that for every text. First thing, do you notice that I'm using NLP? Where is this NLP coming from? Spacey, right? This thing, you must be familiar with the previous week's lab. We get to the words and one of the things I'm doing is token for token in dark if token.text in keywords. I'm creating a list of words which are frequent enough at least twice they have shown up in the corpus and keeping those words and then when I have that words I'm looking at the very first sentence and what it first email and it decomposes into this wonder wonder and lightning car see two doors sports blah blah blah uh now now what you do you realize that the words that we found in the corpus are a limited subset of the words in the entire English dictionary. We have about 28,000 words. So we will create a limited dictionary of only those 28,000 words, so that our TF idea of the vectorization is not irrationally big. So we do that. Now, when you do that, you do it using a dictionary. You give your text, and you create a dictionary. This is a Jensen thing. And now you have a dictionary, and it says that there are 28,000 unique words. It starts with these kinds of words. Now, this dictionary, by the way, has a way. Remember that I removed for the process of visualization, I removed the infrequent words, once or twice words. Now, it's because I did visualization before, so I removed it. But usually, if you haven't done that, then Gensim gives you a way to remove these uncommon words, filter the extremes, the very common and the very uncommon is there. It contains a feature called filter extremes. We are not going to use it because we know that we have already sort of removed the stop words, which are the common ones, and removed the rare words. So I skipped this step. But as a homework, you can consider implementing it and seeing how it affects. So now comes this very simple. We have a list of words. Now we are going to create a Gensim model. So when we create a Gensim model, it's a two step process. We create a bag of words. Remember bag of words was what? That long representation in which one hot encoded sort of a representation. Given a word, what is, whether it's present TF-IDF model. Now remember one thing, guys, that you can do the TF-IDF vectorizer. Every natural language processing toolkit will give you that. This is the syntax for Genseng. If you just look at scikit-learn, there the class is literally called tf idf vectorizer right there once again you would give it the bag of words and it will do that job for you so these are just minor differences in syntax and i won't go over it it's something that you become familiar as you do it then i talked about the singular value decomposition that you could do that linear decomposition into latent topics. Here I've taken the number of topics to be six because we happen to know the answer what is a good number of topics but in reality you'll have to play around and see what gives you good topics. So we take these topics and then we look at the topic. I let the LSI or LSA do the modeling. The modeling is very simple. Do you notice that this is just one line? You give it the corpus, TF-IDF corpus. Now when you give it the corpus, TF-IDF is based on a word's index in the dictionary, which is not very readable. It is much better to contain the actual words. So the ID back to dictionary, you can look for that in the dictionary, and the number of topics is six. Then you go and run it, and then what happens? You ask it to print the topics and the important words found there. so let's see if it makes sense let's look at this one topic zero no people think like good use problem thanks work well i don't know can you guys infer what it is probably hard the second topic is file card drive, drive, tags, there's God, and then there is windows, people, driver, window, desk. Can you guess what this could be amongst the topics here? Topic one could be what? Anyone's guess? Religion? religion religion would probably not have windows in it no windows it looks like something though with operating systems or yeah it probably could be computer yeah computer yeah but it's not very clean because it has the word god in there and so forth so topic three topic two uh it's negative right on god yeah negative what's negative doesn't it's the size that matters the magnitude it's still pretty okay now the topic is key game team that also seems it has game and play maybe sports sports perhaps, or it may not be here. It says chips, so could be gambling too. Chips could be gambling, yes, but it has encryption which, ooh, maybe signs. So the topics don't seem very easy to see. Topic three has game, God, Jesus, chips, encryption. So you can see and government also thrown into it. Never a good idea to mix God with government, isn't it? So well. Did you have a constraint in terms of how many topics it will use? Yes, literally that. If you look at it, you can change the number of topics. And this is one thing you should do, guys. I would strongly it, you can change the number of topics. And this is one thing you should do, guys. I would strongly encourage you to play with the number of topics. Try three topics and see what happens. Hint, that's a hint actually. Change this code, make it three topics, then see what you get. All right. And you'll see something worth it. So it's hard. This one seems to be religion, quite definitely religion, right? Chastity, skepticism, shameful intellect, may or may not be. Sur god but there seems to be god everywhere it's a bit hard to tell so that was the latent semantic analysis based on linear you know this singular value decomposition which is sort of a linear factorization process now perhaps that didn't give us as good a result. Let's try the latent Dirichlet allocation, the one that we are talking about. So here, not much difference. The code is more or less the same. When you do this, this is what you get. All right. The topics have first topic is drive game card problem. Thanks. Disc Port controller work. The second has card video driver surrender skepticism. Just to shameful. So I'll let you decide. The third is Israel, food, delight, prize, Jewish, so this clearly seems to be moon. It's clearly seems to be about geopolitical situation. So that's that. And this seems to be about sports, bike, ride, motorcycle, rear car, but there's a bit of morality thrown in. Auto, rider, captain. Would you all agree that this looks like sports? Isn't a car in sports? We don't have a category for sports. We have put it in the car. Bike, rider. So topic three seems pretty clean. Yeah. And we will see in a visualization if that is true. Topic four is no people think like God want and we'll see. Topic five is core-ish, armin-ish, plain, antenna, liar, and so forth. So let's see. There is a measure. There are two measures actually one is perplexity measure you can measure how well the topics are separated out and that measure is called the perplexity measure you can see a value of minus 10 source of value but i want you to read up on this the other thing is there's another measure which i leave as homework for you if you guys can take it down. It's called the coherence score. So just like I did the perplexity measure, I want you to as a homework add right in this analysis in this notebook, add coherence score, right? Coherence score model to this. So now let's visualize this topics, how well it has separated. Remember six topics. So this is a visualization. So I'll just make the thing a little bit smaller. Okay. Yes. So when you look at this, you see, what do you see? Topic one seems to be really separated out. You know, the more the separation between the topics, the better. So clearly topic one is dominant and it seems to be well separated out. Can you explain the axis here? Yes. So this is the principal components pc principal component first two principal components of the topics itself right you further take the topics and reduce it to two dimensions so that you can represent it in a plane in those two dimensions are clearly the first topic and these are called principal components um so let's take it as a frame. PCA takes a bit of time to explain. So I won't go into full detail, but here's a simplified view. If you were to take data and you would project it down in the most meaningful way possible to two dimensions so that you can visualize it. In a linear, and it's some sense, a linear projection downwards, PCA is the classic technique to do that. But in terms of those, when we saw those coefficients right now, they were totally about what, six or seven variables, right? Those were the word variables yes about the words so in this visualization it's actually condensing that from i think there were about eight variables there no no no you had six you had six topics right so you have a six yeah you you're taking that yeah so all of those variables what you're doing is you're projecting all those topics down to a two-dimensional plane right so all the documents that are in the out there and basically the topics here putting it down in a two-dimensional plane here you're saying how do i put if i were to project all the topics down to two dimensions, yes, okay, take your word space, but projected down to two dimensions, what would it look like? Okay. But so the more the separation, the better. So we clearly see that this is far, this is far, but these guys are all pretty close to each other. So let's look at it a little bit in detail. Topic one, know people like God. See God is an expression that people use in computers and science everywhere. People will keep saying, oh God, right? Or oh my God and so on and so forth. So it's a, I wouldn't pay too much attention to that, but look at the rest of it. Believe, come, system, work, year, point, car. Do you see a clear topic here? I'll leave that for you to sit and judge. The next is this one, drive. This one, this one I would say is very clear. It's about computers. Look at the words drive, game, card, problem, driver, controller, modem, motherboard. Would you say that it has separated out this one well? And then let's look at this guy. This also seems to be, interestingly, it seems to have mixed computers. The lower half of the words are computer words and somehow it has thrown in skepticism, chastity, shameful, which are religion-related topics inflict into this so not good. Dr. G R Narsimha Rao, Ph.D.: This is Dr. G R Narsimha Rao, Ph.D.: Clearly about sports topic three seems pretty clear. It's about sports bike ride motorcycle car auto rider captain wheel. Dr. G R Narsimha Rao, Ph.D.: What about number six. rider, captain, wheel. What about number six? It seems to have some degree of overlap, but to me, it looked like a fairly separated out topic, though I couldn't make out what topic it is. It seems to be about geopolitics, location-based or something like that. But it has science in it, Mars, Venus, space basically in it. And then the fifth one is much more a mixture of space and politics. You know, the countries Israel, Palestine, Turkey, and so forth, and so forth. It seems to have mixed up politics with some amount of science to it. So this is how Gensim worked. There is another library, Mallet, which is a Java-based library, which is a very effective, actually. In my view, quite often it gives you superior results. By the way, when I'm doing all this here, guys, I have not done any kind of tuning or optimization. In all exercises of NLP, you gain a lot by properly tuning your models, playing around, doing things, and you spend a lot of time doing that. And so what I would encourage you to do is take, as you work with the lab, try to improve it. So here is a hint. I've deliberately left a lot of basic optimizations I have not done because it would have complicated the code and made it hard to read. So I've not done those things. Play with those. For the very basic, you can play with just three topics and see how well separated they are. Then you can go around and play with whether lemmatization was a good idea or not. The other most important thing that I forgot, or not forgot, I deliberately didn't do it, is something called bigrams and trigrams. I hadn't taught you that. It is basically putting two adjacent words together into a pair as a single unit. It's called a bigram. So happy cow, right, the two words. I saw a happy cow. So I saw would become a bigram. Saw a would become a bigram. And happy, a happy would become a bigram. And happy cow would become a bigram and happy, a happy would become a bigram and happy cow would become a bigram. If you remove the stop words, then it would become saw, right? Happy cow. So the bigrams would be saw happy, right? Happy cow and so forth. So and as you can imagine that those add meaning so please take this as a homework i know i happen to know that for this particular analysis when you add bigrams and trigrams those are expensive to add by the way to explore your vocabulary so start with just bigrams see how much your results improve if you add bigrams to this analysis so what is the intuition for n-grams helping in topic modeling yeah so think of it when you use words some sometimes the words don't convey the meaning. For example, the word machine learning together. Machine will convey a sense, it will contribute a sort of topics or contribute a lot to mechanical engineering. Learning will contribute a lot to education. Machine learning together, you realize that as a pair pair they contribute a lot to artificial intelligence okay the word artificial intelligence itself do you see why biograms are useful and worth adding to the vocabulary okay please do add biograms to the vocabulary the hint is it's just a small trick a few lines of code, enhance this, and see how well your topic modeling works when you add bigrams to this exercise. All right, guys. So now I'm using mallet. Using mallet, now good thing is that, do you notice that you didn't have to write Java code? You just literally stayed within Jensen, use mallet as a wrapper. It gives you a wrapper, Gensim wrapper for mallet. So you can use mallet for again, a latent Dirichlet allocation. It's a twist on that. And then let's see how well it performs. Do you feel that the topics are separated out guys? This time? The bubbles are separated out? Yes. It is better separated out. It is better separated out. It is just a general experience, by the way. In general, I find Mallet superior. But then, whether or not it has done a good job actually I'll give you a hint guys add the bigrams then your results will significantly improve but for what it is work as a quick indeed here I brought it here you can decide what it is about. These topics are what they seem to be fairly clean, right? This seems to be about mechanical, aboutim, Brent, Satan, problem system, I can't make out. But anyway, that is that. But at this moment, without adding bigrams, this is how it looks. So add bigrams, guys. Now, there is one thing I wanted to bring in at some point. It has not anything to do with topic modeling actually. It is independent of that. See, Python code when you run, it runs in a single thread in the sense that it is a sequential run, all that code. When you work with large data frames things can get pretty slow because suppose your machine these days has many cores for example uh my workstation has 64 cores you know if you have a threadripper the it will have 64 32 physical course means 30 to 64 high sort of a hyper trading or sort of like that virtual course or something let's say that it has 32 or 64. what will happen is normally when you write run your code only one core is an effect so your performance is 164, or if you have more cores, 100th or something. So how do you speed things up? So Python has this very nice library called job lib. If you can make your problem in some sense, a parallel, parallelizable. So for example, if you're doing an operation to each element in a list, and that it doesn't have to deal with, there is no interdependence between them. Every word, let's say that you're capitalizing every word or breaking every word into NLP tokens, right? Tokenizing the words. You realize that you can deal with every text separately, every email separately, no reason to do them sequentially. So the point that and so to illustrate that point, I have put this code here and hopefully this is your starting point for doing parallel computing in Python. So you notice the code is straightforward. You create a parallel object and you then give it a function, you give it an argument. What will be the argument to the function and then from each text from a list of texts, which is news groups remember the text column had 11,000 such records so it will execute faster and you'll get the same result so this is about job parallelization not necessarily about topic modeling but you can and should use it for this purpose or like you should use it everywhere that you can so alright guys so one of you requested that in one hour i should give an overview of what i'm going to teach in the remaining time and so we are done with the overview any questions before i go through this a bit more slowly Asif? So what are some examples that this could be valuable? So one thing is, let's say, I'm trying to figure out a topic. Look at, for example, I look at the app called Flip, which gives me the news. I could choose data science politics anything right and then those articles from newspaper and then it gets summarized for me okay that will show so i'm trying to figure out like when when i do do this modeling in the practical corporate world what are the use case you have in mind that could be benefit that we could use it let me see that uh see first of all before i answer your questions if you put one card here see guys do you notice that if you just run through the analysis in a simple way direct way your topics are not terribly good right they're good enough that's all you can say but they're not terribly good there is a huge scope of improvement of tuning this and so people who do topic modeling they often spend a lot of time tuning with this playing with the number of topics seeing how it matches adding bigrams and trigrams checking playing with different parameters the hyper parameterameter tuning. Machine learning, the difference is quite high in performance between straightforward code and optimized code. Obviously, optimized code means now the code looks more complex, right? Or there is more going on in the code. So when you're just learning, you don't do that. Learn to do the straightforward way as here, and then do a better job of tuning it and making it better.'t do that learn to do the straightforward way as here and then do a better job of tuning it and making it better having said that to your question so there are many examples like for example you know you're getting a lot of jira tickets from your customers a lot of issues are complaints and suggestions and feedback are coming to you you don't know what those things are about. They're just plain running texts. Like suppose you sit on a division, let's say that you sit on the WebEx division. Besides the fact that they're all about WebEx, it's the feedback that you're getting or the reports that you're getting could be free-flowing text and you don't really know what the topics are, isn't it? So what can you do? You can take all the text and do an answer. Topic modeling is unsupervised. You can see what topics they seem to be about by doing topic modeling, by looking at the coherence score or the perplexity metric, and trying to come out with clean set of topics. First you have to find how many topics it sort of decomposes into, you visualize, you do all sorts of things, and then finally you gravitate to a stable point you converge once you have converged now you look at the topics and you try to give it an interpretation for example one topic i hope is that webex does a terrible job in allowing you to add a lot of virtual backgrounds now for example they have given that feature. But if I were to guess, with Zoom giving that feature, a lot of people must have given that feedback. We want it here too. Yeah, right. Or basic direct integration to YouTube, it doesn't have it. So I'm sure there would be, I mean, I'm just giving the points that help me, made me move after so many years of using WebEx, made me move to Zoom. So if you go and look at the feedback, you'll find something. But how do you find those? How do you find the core themes in the feedback? That is one. Look at the tickets, ticketing system. When you do a topic modeling on the ticketing system, you may realize that a lot of people are talking about certain themes or topics which don't fall into your traditional buckets of this product, this component. You see that, right? And so on and so forth. You can use it. Suppose you see a lot of news articles emerging. You can quickly do a topic analysis and find the most representative documented sentence around that topic and see what is it. It could give you an idea of the emerging news. So it has a wide application. Topic modeling is a big deal. Now these are the these are the examples that i just thought at the top of my head but if you spend a bit more time then more and more examples will come to you got it i have a design question here so right now when we did this exercise what it's essentially done is for each of those topics, they were essentially a combination of words, right? That's how we basically define topics. We have six topics, each of them were a combination of eight or nine words. Oh no, no, no, no, you don't think of it like that. See, looking at the lab, you may think it is like that. What the topic is showing you is that how much weightage it is giving to some of the more important words for that topic yeah i was building on that so so yeah so right now it's done it on words but the i was wondering is there a way to define these in such a way that it becomes a two-level process what i'm looking for is let's say these words some of these words were essentially things that let's these words, some of these words were essentially things that let's say describe technology. Some of these words are things that talk about emotions. So I want a two level. Yes, do that. You could do that. See your problem, for example, you could do something like that. See this is where the, you know, okay. See, there's supervised learning and unsupervised learning. If you have some label data for this system to learn that this is about emotions and this is about technology, then in the first stage you could just run a classifier. And in the second phase, then having classified the data, within technology you can find topics, and within emotional stuff you can find topics. Or you could try your luck by just doing topic modeling into two topics and hope that a good topic modeling result would be that everything to do with technology falls in one topic and everything to do with emotions and experiences falls into other topics. So you have to play with it and see. A lot of this is experimentation. Another way that I would see is, do you notice that when I did a classifier, the classifier had a very high degree of accuracy, 83% accuracy. Supervised learning. But this topic modeling is unsupervised learning and then the results are more mixed. You have to work a lot harder for it to give you good clarity. This is a general experience you have. See, the reason supervised learning, you tend to get higher accuracy or quicker results, is because the hard work has already been done in giving you the data. Creating label data is extremely hard and extremely expensive, right? Except in such situations like this, where you already know, you know, you're going to news groups and harvesting the text, in which case you know what the news group is right or let us say that you're looking suppose you want four subjects and you want to gather you know from the web some content around the four subjects if you specifically know this subject you're searching for the web and putting it in a directory, all those content locally in a directory marked science. You already know you have labeled data. That is a gold standard. But now, remember that used human effort. Increasingly, the big push in machine learning is how can we do things either in a semi-supervised or unsupervised manner because supervised doesn't scale very well right in many situations in some situations it's cheap to get labeled data in most situations it's very expensive to get label data most of the data comes unlabeled in real life so that is why the importance of techniques like topic modeling, because they help you in that or clustering and dimensionality reduction and so on and so forth, right? They have tremendous importance because if you can just sit here and keep on refining your mathematics and you can therefore discover very good themes. What you have just saved yourself is a massive spending on people sitting and labelling data. Okay. So that's the practical aspect of it. Any other questions, guys? Asif? Yes. It's just some feedback. I like this format better than the old one. I'm not sure how other people feel. Okay. A quick walk through the whole thing in an hour. Excellent. So I'll follow this process, guys. Any other feedback guys? Otherwise we'll declare it a break. Let's meet after dinner and then we'll go over this code a little bit more in detail. This code is actually quite simple. There isn't much more detail to go after but I'll still go over it line by line in the next and so those of you who understand this code you all have downloaded it if you understand this code then you don't need to come for the second half but if you do need some help with that then do please come back and we'll go over the code in detail Thank you.