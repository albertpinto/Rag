 So let me recapitulate what we did the last time. We learned about support vector machines. The support vector machines are actually very simple machine learning algorithm. It's an algorithm, it's quite simple actually. And based on the notion of a kernel. Now there are many kernel based methods. If you look at the literature, this whole field of kernels, using kernel methods is pretty vast and pretty deep. They use it for a lot of things. So today we'll use it perhaps a little bit more but broadly it's used a lot support vector machines just happens to be the simplest and the first one of them the basic idea in support vector machines is that if we have the notion of support vectors these are lighthouses essentially the idea is that throughout the data you find some inline of points some specific points which you can light up and you can think of it as sort of lighting up those points declaring them or these are your support vectors these are your lighthouses once you have discovered these lighthouses what they their purpose is they help you draw a decision boundary, a nonlinear decision boundary. So how do you discover these lighthouses, the support vectors? The way you discover these support vectors is the implicit idea is that you take data where the decision boundary is nonlinear or potentially nonlinear and you project it into a higher dimensional space the underlying belief is that in a higher dimensional space every decision boundary no matter how complex that how complex it is as a hyper surface there will be there will be a higher, even higher dimension where, or some of the, sometimes you don't even have to go to a higher dimension. Sometimes you just need to go to another dimension, another space of the same dimension sometimes. And when you do go there, the problem simplifies itself. It becomes a linear problem. Once it becomes a linear problem, then everything that we learned about maximal margin hyperplane, you know the soft and the hard maximal margin hyperplane, comes in. It's a straight river. So the basic idea is you want to flow the widest river possible through that data set in the higher dimension, a straight river. When you project it down to that, in the higher dimension, a straight river. When you project it down to that, in the original space, that straight river is obviously a very, very curved river, potentially. It doesn't, it need not be straight. Now, those points that are there in the river, or on the banks of the river, those lighthouses, you find those in the higher dimension space, banks of the river, those lighthouses, you find those in the higher dimension space and then you come back. When you come back of course those points, those lighthouses will become, will be setting along the very nonlinear decision boundary in the original space. Going from one space to another to solve the problem is a classic mathematical concept. It is often used. And today I Realized that there was a simple motivating example that I could have given which I didn't give to illustrate this point. And I'm going to start with that. So you see why it is true that in higher dimension, it gets all things can be linearized. Now that's a good news. The bad news obviously sometimes can be it's not really a bad news but it is a fact that you might not be able to visualize the dimensions very easily. Sometimes these are infinite dimensional spaces. So and that is a little bit hard to get your head around. How do you deal with how do you imagine infinite dimensional spaces? For that matter, how do you even imagine a four dimensional space or five dimensional space or something? And the way, well, the joke is, the way you do that is, you think in terms of the basic two and three dimension that you can, but loudly you say, as you can see in n dimensions or something like that because quite often your intuition carries forward so long as you know the traps places where the the higher dimensions are radically different from the three dimension and you understand those differences in all other situations quite often your basic intuition of three dimension, it does carry forward to higher dimension, even to infinite dimensional spaces. And this is what we do. Like I remember when I was learning general theory of relativity and even the special, the basic idea is that space time, space time already is four dimensional. It gets a little bit hard to understand that. And then comes general relativity which goes and bends the four dimensions into a fabric which has all sorts of bumps now. Now it is even harder to imagine but what you do is you imagine in two and three dimensions and you from there you carry the intuition forward to a higher dimension. That's how it is and the same is true for string theory string theory takes it even further to 10 plus 1 dimensions we all carry our intuition in three dimensions and then we go there and we sort of manage with that because we haven't evolved an ability as a to get intuition in high dimension. But usually that's not an abstraction. Now, I gave the example, if you remember, of a curve which could be represented as a polynomial. And the moment you could represent it as a polynomial, those polynomial terms become the axes of the higher dimension space. But sometimes you don't even have to go there I want to give you an example which is quite simple to illustrate the point that going to higher dimension can be need not necessarily is not necessary the higher dimension could be the actually of the same dimension rather so I'll take this example imagine are you guys seeing my screen imagine that somebody gives you a unit disk inside the disk let us say guys could you one of you please confirm you can see the yes okay thank you so suppose you have points like this. And this thing usually in the classroom, it's so much more easy to illustrate. These are, let me call it the interior points on the desk. And then associated with it, and are some exterior points to whom let us give another color oh this is rather multicolored all right we'll go with it anyway then you have points which are outside it the other points are outside it and we can sort of generalize and extrapolate. This is, if you remember, this was actually one of your labs in ML, ML 100. In fact, classifier three was this. And you might want to, it would be instructive if you revisit that and apply SVM on it, support vector machine on it to solve it. Because that problem is essentially tailor-made for a support vector machine. I'll post the solution with the support vector machine. So let me introduce some noise points here. Make it look more realistic. So assume the data is something like this are we together and it's something like this and some points are outside too so there we go we have this data what can you say about the decision boundary you would agree that the decision boundary is quite manifest. It is, it happens to be this thing. This is your decision boundary, isn't it? This is your decision boundary. Let me make it very, very, decision boundary. Now this is evidently nonlinear decision boundary, isn't it? So I'll just say nonlinear in the original space this original would you agree this is a r2 space let us say that. Now could you guys see an intuition of something that will make it linear? When you look at a problem like this, you ask, okay, can I use logistic regression straight away? And the answer that comes to mind is perhaps not because it's rather a curve decision boundary, it's a circular decision boundary. So actually this is one of the interview questions I give to data scientists. Should they have the misfortune of being interviewed by me? So, now I'll tell you something very interesting. Suppose you go to a new space, let me call it X prime is equal to X1 square, X1 prime. X2 prime is equal to X2 square. And see what happens. Let's see what would happen there in that particular space. So you realize what is the equation of a circle guys the equation of a circle is x1 square plus x2 square equal to yes radius square excellent of the decision boundary if you write the decision boundary as an equation it would follow this constraint it would follow the constraint that X 1 square plus X 2 square is equal to some value R squared like some sum this let me just take this as R it would be R squared the radius squared but when you go to coordinates like this what is this equation become if x1 square is x prime it becomes x1 prime plus x2 prime is equal to some values so suppose the axis is x1 prime and the axis here is x two prime. Do you see that guys. So what will happen line. It was a line. The line. In fact, this is If you go here. If this is our and here also are It will actually be This Yeah. This would be a this is the equation for this is this yeah this would be okay this is the equation for this is x1 prime plus x2 prime is equal to r squared which essentially is the same equation and so what you will end up with in the transformed space are it is things like this all the blue points would be inside the decision boundary here would be inside the decision boundary here in the inner triangle from the origin to this this little triangle and all the outer points will become like this are we together guys so it should be R square yes in fact this is R square you mean the coordinate you're right you are absolutely right right that correction thank you for pointing out the correction is R squared so does this make sense guys so do you notice that you actually went from here to here, one space to another space, not necessarily in a higher dimension, you were lucky enough to get it in the same dimension. Are we together? Right? You could, so that's the intuition that you can use to do that now there is actually another way of looking at it what you do is suppose I created one more measure of it the measure is I looked for every point this is two-dimensional guys this this thing is two-dimensional if I look at it in three dimensions let me look at it here three dimensions and see what happens if the Z I create a x3 dimension think of it as Z rather actually typically in school we think of a not x1 x2 x3 with xyz we think of it as Z this thing suppose I create a z3 by definition it is x1 square plus x2 square let us see what it will look like what it will look like is it will look like a surface that goes to through this. And so let me make it. Convince yourself that this is how the surface looks. Parabolic. Parabolic surface, isn't it? This equation is very easy to see if you just forget x2 and you just think in terms of intuition if you want fusion just think what what does this bar or Z is equal to what is this equation Y is equal to X square look it looks like this Y is equal to X square. That's the intuition. I just generalized it to three dimensions. So this is it. Are we together? And if you look at this, you will realize that your blue points were actually on the surface here. on the surface here it is on the outer surface here actually this bit this begins to look like solid but isn't solid let me emphasize the fact that this is this and all the yellow points if you think about it, they will be again on this surface, this parabolic surface, but they will be up here. So guys, just you could do this in our in the physical classroom, we do that you can take a page and you can just put a points like this just draw it color it in a piece of paper the inside the circle and all you have to do is if it is a think of it as a rubber band page or sort of a sheet of rubber and then imagine that you're deforming it in the into the shape you would agree that you could do that and this will become this becomes this and when it becomes this where is the hyperplane that is the decision boundary this is the hyper plane hang on it can use a different color this is the hyper plane let me use well I have used white what can I use for the decision boundary blue is green let's go with our green yes let's go with green you would agree that the decision boundary would be this plane that of that sort of cuts here you see that this plane that is cutting it right here. Imagine a horizontal plane. The green is a horizontal plane. Plane cutting at some place at Z is equal to R squared here. Sorry, X3 is equal to R squared, if you think. All the blue and yellow data points are on the surface of that three-dimensional parabolic object. They're not inside it. No, yeah. So yeah, they are on the surface of the parabolic object. Yes, that is it yeah so yeah they are on the surface of the parabolic object yes that is true and so so in other words that parabola is not solid it's just a surface the yellow points are above a certain threshold above the plane and the blue points are below the plane is that it's almost like a telescope as if like a telescope it sort of looks like a telescope at the surface of a telescope or radio telescope parabolic or something like that yes it is like that so guys I hope you can see how you can take data do you notice that in this space the the decision boundary was circular by going to a higher dimensional space what have you done you have lifted you have created another dimension in which actually on x3 there is just a cutoff point above that it is so when you write an equation x3 is equal to R squared what is it x3 is equal to some constant so means that's your decision boundary all points above the plane in on the parabolic surface will be yellow all points below it will be blue I don't know is this intuition clear, guys? Yes. So this is your main intuition of going to a higher dimension and solving it, and sometimes going to not necessarily higher dimension, but going to some other space and solving it. If you want to put it a little bit more poetically, you can say that for every data, there is a native space in which it's much easier to understand it. It's very simple. It simplifies itself. So that is that. So that's the intuition of the support vector machines. The job of the machines is to discover these high dimensional space in a systematic way. But what is more interesting is you can say all right now it is complicated because i need to go from x1 x2 to this find this complicated mapping mapping that will go to some other higher dimensional space let me call this x1 prime x2 prime and x let's say d prime and you don't know what D is first of all and you don't know what they would put the complicated rules of psi are actually people use not psi they use phi sorry I apologize phi are right that takes a point to a higher dimension yes go ahead please uh i said i said i'm just uh it was a little difficult to grasp the three-dimensional thing so uh for me intuitively it was a paper and i didn't it's like a rubber platform i folded it into a nice parabola yes so uh x square plus so x one square plus y one square equal to r square was the initial two-dimensional circle. So at a distance of r from the center is where the separation boundary is, the decision boundary. But when you fold it up, you are still going up to r square or are so that that part I Slightly couldn't understand. I know you're projecting it in the third dimension But intuitively I was holding the paper up into a parabola and then I have to go our square distance up in the x3 direction To hit the decision boundary so that part So this parabola very good question actually so this a little so see this decision boundary that is here this thick decision its height everywhere everywhere would be exactly R square R square plane this line is sticking up exactly at the height of R squared hey I'll say yes a will that will the height be R square or you know from the origin to the point where the plane is cutting that is r squared now think about this you know on the surface of the surface of the parabola yeah see every single point here right this point corresponds to the x3 value right we are x3 okay is r squared which means that x three this plane. What does this plane represent this plane represents x three is equal to our square the constant. Right. So think about just think about this. It's a horizontal plane. Isn't it Yeah. On this horizontal every single point is at the same height from the ground. point is at the same height from the ground of the ground made up of X 1 and X 2 isn't it and so all the points on the parabola the decision boundary points but there wherever this plane cuts the parabola the whole decision boundary is at height R squared because every point on this plane is at the height of r squared right r squared are we making sense i i was just thinking definitely they are of constant height but will that be r squared that's what i was thinking it is no because uh this is literally the definition of this x3 the way we have defined x3 Or we have defined x3 like that. Okay, pretty like that if you choose you could define x3 square is equal to x1 and x2 square because then it looks more uh sort of symmetric, but you don't need to I mean it doesn't matter This is it. So so the point is that your real boundary in higher so now the intuition is look at this in higher dimension I'll put it in higher Higher R3 space. The decision boundaries. Is the linear Is the linear If you look at x is x three is equal to R squared plane it is this plane isn't it guys and you can literally see that happen this plane is touching the hyperbola or sort of parabola I think this but parabola parabolic so it will not parable of a parabolic surface, well, not parabola, but parabolic surface at this circular place. Isn't it, guys? I have a question, sir. Yes. Let me finish this out. So this circle, when you project back into, you look at the projection back onto the two-dimensional surface, X1, X2, you can literally see a circle on the ground the ground right this thing projecting down to a circle in the ground right which is the original circle that we have go ahead so far so sir circle in three dimension isn't this sphere how but we did not we did not choose to lift it into a sphere we did a very specific lift you remember x3 is x1 square plus x2 square i didn't say that x3 squared is equal to that right that would have made it into a sphere yeah but in three dimension our data uh uh distribution looks like this so here so i'm a little confused like how it doesn't think about it see here is the mission the height of a point in three dimension is its best is proportional to the square of its distance from the center from the origin think this way focus for every point the x3 value the lift right the going up in x3 is equal to the square of its distance from the origin from the center so because it's a square it will look like a hyper it will look like a quadratic surface so the distance every point is x square plus y square plus G square no it is not it is not if we are not talking about that every point is given as see this is the square of the distance the square of distance and we are saying that this the Z axis value that x3 axis value is simply the square of the distance what we are not doing is x1 square plus x2 square plus x3 square is equal to some constant because that would be all points on the sphere yeah that's surface area yeah that's right and we are not doing that yes we are declaring that the X 3 value the lift or the height of a point from the ground is proportional to the square of its distance from the origin right so when it's proportional to square it looks like a curved surface right it looks like yeah yeah then it's a curve so so so z uh is g height is x square plus y square yes exactly the z height and here i call z by x the Z height and here I call Z by X yes and the basic intuition is you look at this plane this boundary is very obvious it is actually the hyper play is just a plane the horizontal plane where X 3 is equal to R square that certain height distance from the decision boundary from the origin at a certain distance from the decision boundary from the origin to certain distance from the origin is that plane x3 is a plane is that obvious support x3 is equal to R square is a constant decision boundary it is a flat surface in three domain and therefore the decision boundary is a linear decision boundary that green surface that you see uh the green that you see is the decision boundary come back and look what does it look in two dimensions in your two dimensions it is basically the projection of the intersection of this plane with this a parabolic surface and that is this circle and when you project it down to the ground it is still the circle which is exactly the circle that you are looking at so in other words in what in three dimension is a green plane in two dimension becomes this wide solid line you see that right are you anyone else, are you guys getting the intuition? It's a simple geometric intuition. So yeah, I think so. Yes, I'm getting it. You just lift it in the height. Exactly. All you did is you lift it all by the equation, x equal to x squared, you lift all the points up and then slice it by looking for some products. Yes. You just literally put a horizontal slice imagine just taking a knife and going horizontally and cutting through this thing isn't it yeah now i got it thank you yes it's a very simple and beautiful intuition and this is the foundation of this idea is the foundation of support vector machines. Now this idea intuition is great, but there is a further fact which makes it computationally possible, which is that there's a kernel trick. You will learn about the kernel trick in the math class. When we get to that, because quite literally it's about dot products and vector spaces and all of these optimizations, constraint optimizations. So this is literally what it is. But we'll leave that for the next class. The basic intuition is you can do that with the kernel trick. What happens is you don't have to truly find this phi, this phi that you have, you don't have to find Because there is a trick. I won't write the equation, the, the dark product between x and let me just say x and x. Well, I used explain for the different space. So let me just say that the dark product between two vectors, a and b. Right. okay it has a certain relationship if these things get transformed into the other space we could say that there is a Phi a Phi a B and the like let's say that this function is there this can be computed this kernel can be computed without necessarily very explicitly understanding what in the world is this phi in closed form. You don't have to do that. It's sort of the thing that makes this magic computationally possible. So that's a computational detail. Let's not get into that too much for the time being till we get to the math class. So I hope I summarized the concept of the kernels. so i hope i summarize the concept of the kernels what is the phi and the k here for this going from two to three dimensional you read it manually but yes so here so in this particular example let us see that see i give you two possible journeys from here to here and then from here to here. For this one, the phi of x actually, why don't I leave this as exercise because this is actually one of your very, very basic exercise in the math. I don't want to give out the solution right away. I'll let you find out. How do you do this? It's taking you in the face, but I'll let you do that. It is, well, okay. It is literally x1, the vector spaces are x1. So suppose you have phi x1, x2 is literally x1 squared, x2 squared. A new point in a higher dimension. This is it, right? X2 squared. The square of the points. And then likewise, this one is very simple. You have the new points are five new points of X1, X2 are this set. X1, X2, X3, where X3 has this relationship, the constraint constraint equation x3 is equal to r squared. Thanks, I'm getting the hang of it. And k is just the dot, I mean in the simplest form think of k as just a dot product a dot b is the simplest kernel you can you know the dot product or in basic notation a dot b. Think of it like that in a simple example of a K could be simple simple example could just be a dot product but actually it could be any function of the dot product I will together so for example in this the polynomial kernel it could be a dot B to the power some polynomial power in the case of a radial basis function minus gamma a dot b so you notice that case are these kernels are just functions of the dot product they're just functions of the dot product that's all it is there's nothing very magical about this we use the word kernel it's a literary it's sort of the word used in the literature we do that so now i'll tell you about something very interesting in the last ml 100 we talked about dimensionality reduction and we learned about a technique called PCA does anybody remember what PCA stands for principal component analysis principal component analysis without recapitulating the whole theory of it let's just say the basic idea is if you are looking in two dimensions if your data has a linear Has a direct linear correlation is a correlation or there's a linear relationship like this in some form it could be two dimensions, three dimensions, four dimensions. There is a ellipsoidal envelope, you can wrap data basic intuition is you can you can shrink wraps the data in an ellipsoidal ellipsoidal envelope. Isn't it? So here, the ellipsoidal is not actually necessarily three dimensions or four dimensions. It's very simply, this is the ellipse, isn't it guys? It's a simple two dimensional ellipse. Yeah. When you can do that, an ellipse has the major and minor axes you know the principal axes this is one axis this is another axis those axes so then this major axis let me just call this axis as u1 u2 let me put unit vectors along that so you can you will have directions u1 u2 etc and then you will have a certain amount of stretch along those axes and by the way these axes are also if when we did the eigenvectors of the covariance matrix a covariance covariance matrix x1 x2 there's a matrix so we wouldn't do the whole thing but just say that there's a bit of mathematics that says that if you take the matrix and you do something called a eigenvalue decomposition then these are your eigenvectors but forget the math just think about the intuition that you can shrink wrap it into an ellipsoid it unit vectors along the principal axes those are your principal components and the degree that the you stretch along each of these the lambda 1 lambda 2 lambda three so forth these are called the eigenvalues values and the the beautiful thing is that they have an order lambda one is greater than equal to lambda two is greater than equal to lambda three it is like that and not only is it like that but it is it follows an interesting behavior a sort of a power law decay behavior quite often you will notice that lambda 1 lambda 2 lambda 3 lambda 4 there will be lambda 1 will be pretty high lambda 2 will be let us say if you're lucky like this lambda 3 will be let's say like this lambda 4 will be like this so what will happen is if you connect these lines right you notice that there is a elbow here right so this is the value of lambda one two three four lambda one lambda two rather a value the magnitude of it so elbow point and you basically say that if a situation is like that i can keep only these three and throw away the rest of the components right rest of the dimensions i can throw away and i'll get a pretty good approximation of the data just like in this case you could argue that you have I can keep only one axis of the data and for any point I just forget about this perpendicular axis I just tell where a point is along the main diagonal axis and that is the coordinates in one dimension and I don't care about its location along the other direction. You can sort of ignore it because it doesn't matter or you don't lose much by that. To do so is to do the dimensionality reduction. So that was the intuition that we have. Do you remember that all your videos are still there? If you go to the page that I created for ML100, I hope you all have the hyperlink to that page. So all those videos are there for you. You all have access to it. If you want to review it, please do go review that dimensionality reduction session. I think it was the last session that we did in the previous workshop. So now what does that have to do with kernels? So for example, what does that have to do with dimensionality? What do kernels have to do with dimensionality reduction? See, let us say that you're talking about sort of data which is on a surface something like this now how do I draw a three-dimensional surface or two-dimensional surface let us say that most of your data is along this surface, this curve. This is your dataset two. It is like this. If you go back and look at your, well, obviously I did not make it symmetric. So let me make it a little bit more symmetric. Not that it has to be symmetric, but data set to look like this so I'll put it like this here you remember this was your data set to from your ml100 so in the previous class introductory class so if you look at this curve, if you try to create a covariance matrix between X1 and X and Y, a covariance of XY, you will get something approximately zero actually, correlation, the covariance would be zero or the correlation would be zero. Why? Because the relationship is not linear. At some point, they seem to be positively correlated. At some point, they seem to be negatively correlated and again positively correlated and the net effect is probably a zero correlation right are we together is the intuition obvious hey so now the question is is there a way to linearize this can be linearized this this this so that this begins to look like our old problem of data along let us say something like this can be converted to it doesn't matter what I'm sorry what did I just do I apologize I was up to something give me a second guys I might have closed accidentally the one note the desk yeah okay so is there a way and it doesn't matter at what angle or tilt it is, but can we somehow convert it into a linear problem is the fundamental question. How can we go from here to here? You can say, well, that there is a way to go to a higher dimension where all curves and surfaces get linearized isn't it but once it gets linearized then the old technique of PCA comes in isn't it guys you see the two-step process we can still do dimensionality reduction on a nonlinear curve like this by first linearizing it and then applying a PCA to it let me write down the two steps first a first linearize the problem, linearize the problem, but going to a higher dimension. dimension right so x goes to some phi of x right some this and b then do the pca then do the regular pca what say you guys is this uh is this more straight is this obvious this two-step process I do some feedback why we are doing PCA once we then the whole point was that we need to do dimensionality reduction we need to let's say to just here it is two dimensional data we want to use it to one dimensional data right so you realize that we are doing something very counterintuitive we want to reduce the dimension not increase it but this way we do it is we first increase the dimensionality we go to a higher dimensional space where the data is linearized and from there we come down back to a much lower dimensional space so you're effectively sort of going from two dimensions to three let's say three I'm just taking a hypothetical example in this case of a sine wave which is let's say a five dimensional space or five dimensions and something and then coming down to one dimension which you at the end of the day you still have come to a lower dimension the only dimension that will matter is that the straight line dimension that and that is called kernel PCA that is the kernel PCA right so you can generalize a lot of these linear methods using the kernel approach applying this people call it the kernel method often that going to a higher dimension solving it and coming back like the kernel track is more widely applicable beyond just support vector machine so there's a large body of literature on kernels kernels are still going very strong they're in many many areas of machine learning so guys before i move on from here i would like to know are we clear did we understand what we just did we use the kernel to go to a higher dimension which is linear and from there we actually found a shortcut to a much lower dimension right so it is almost opposite kernel takes you up PCA brings you down but the net result is down lower dimension. So the kernel we'd want to use in this example be like a sine wave or? Yes, it could be a sine wave kernel, you could use a you can just custom make it or you could use a actually polynomial kernel would serve you better. Because remember, we did the fifth degree polynomial and that served us quite well in ML 100. polynomial and that served us quite well in ml100. Okay so if the data distribution is like a three dimension in kind of blob or like let's say amoeba shaped in three dimension is it possible to reduce the dimension in that thing? The problem is mathematics is insanely powerful so the idea is that yes you can do that the question is mathematics is insanely powerful so the idea is that yes you can do that the question is so there does exist some space where the problem is linearized the question with the kernels that you have implemented in code see the code doesn't the libraries don't go and try every possible function mapping they have just a few tricks up their sleeve. They will use a linear kernel, they will use a polynomial kernel, or they will use the exponential kernel, you know the radial basis function kernel, the gossip kind of kernel, these three mappings that I mentioned. Where did I mention? It's these three mappings, right? These typically are the ones that comes built in with most libraries now libraries are extensible so to Kate your problem could only have just used a sine wave kernel by all means but you can you have to plug it into the library it gives you a way to plug in your custom kernel so as if can we use like a DB scan because it is like different uh type of a non-globular structure no that is for clustering oh that is for clustering it doesn't have relevance here here we are doing dimensionality reduction so uh sir i'm sorry pat. So I have actually a real data set and we, it's a clustering problem and we are trying to reduce the dimension by PCA. Exactly yesterday we were trying to do that. So as Anil said, you know, in clustering problem where we don't have the level for the data, can we still do dimension reduction, right? See dimensionality, so it's a very good question. Can I rephrase it in simpler terms? Sure. Irrespective of whether data is meant for classification, regression, or clustering, does it make sense to do dimensionality reduction does that summarize what you're asking so yeah so that is the question because there is at there is one point where where we have overlap of something and then we have to pick up you know through pca uh like what are the most important features and based on those features we can further take it to like uh into consideration to develop some you know something like that so uh yes so let me answer that question for you the simple answer for dimensionality reduction is, as a data scientist, it's your responsibility to always try dimensionality reduction with complex situations. If the problem is very simple, it's all right, but in all situations, you should try to reduce the dimensionality of the problem because a it leads to better understanding b it makes the problem uh computationally attractable means manageable yes but you should always do invariant of what problem you're trying to solve you must always try out dimensionality reduction now comes this question of what for technique of dimensionality reduction will actually work. If you're lucky and the problem is linear, then direct PCA will work. Direct PCA has been there since the 1950s. In fact, it was independently discovered by a lot of people, including Korsambi from my university. So it has been there for now what 70 years. Then over the years, you know, the data has nonlinearities. You can then try other more powerful techniques. Today I talked to one of them, kernel PCA. Now when you try kernel PCA, there are two ways of doing it. Either you let the, for example, the scikit library do it and you tell it you know the three kernels are linear polynomial and the rbf this gossip so try all three and see how does your data look see suppose one of the tricks you do is you you you deliberately force it to look at two dimensions like x1 and x2 project down to two dimensions then two dimensions you can visualize three dimensions also visualize you can see how well the separations are you've got the scree plot you see where is the elbow in the scree plot how many dimensions do i really need and then there are mathematical measures to see are they enough there words like proportion of variance explained we did that in ml-100 so I won't repeat that so it just says that how good is your model is your lower dimensional model in capturing the nuances of the full data it gives you that so you can try could if a linear model works fine if a kernel PCA works, kernel PCA quite often works. The trouble with kernel PCA is that the three kernels that are built in may not be sufficient, right? You may have to plug in your own kernel. And when you try to plug in your own kernel, now we are getting into custom business. You need your own intuition into the data. That is where a good, it is essentially a part of feature engineering you're thinking about data you're getting an intuition of what sort of kernel I should apply and you can get away with it a lot of people don't but there are more techniques of dimensionality reduction example in my bookshelf I'll give you a whole book this book it's a it's a very good book I don't know if you can see this book. So, Asif, can we use like T-SNE and UMAP? Yes, please hold on to them. Guys, do you see this book? Partially. It's a class-parried book, sir. No, we can't see it. It is a book that I'm holding on to look into my face. Yes. It is a book that I'm holding on and to look into my face not this. Yes Right, so this book actually is one of the many books and one thing that I like in this book is it has a pretty Good section. Let me go here on Nonlinear dimensionality reduction and manifold learning. So what is the book name? Asif? Can you post that in the slack, please? Oh, yes Can you post that in the slack please? Oh yes. I will do that. It's a modern multivariate statistical techniques. In this, there is actually a chapter called Nonlinear Dimensionality Reduction and Manifold Learning. This book is very dense in the sense that you have to know our math and data science before you can even begin to study it. When you do that, you will see that it has a lot of techniques for dimensionality reduction. So if you can see that. And most of these techniques have to do something called manifold learning. You try to learn the underlying surface to which data hugs. If you can discover that surface, you can linearize it. There are all of these techniques. So like Anil was using the word T-S-N-E and UMAP, those are all techniques to do exactly that, to figure out the deep inside, what is that surface to which data hugs and then to find a way to linearize it bring it down and separate it in in the spaces that you look at for example two dimensional or three dimensional spaces so the field of dimensionality reduction is very rich especially this whole thing about manifold learning and so forth and about manifold learning and so forth and LLEs locally linear embedding I won't use those words now we will do those perhaps sometime in the deep learning workshops yeah it will be there TSAE and all this will come there there's a whole vast literature on dimensionality reduction so what we did today is we went beyond just PCA and use I use the kernel to go to our first step in handling nonlinear data so do that guys why should you do dimensionality reduction because simplifies the problem not only that you gain understanding of it you gain understanding of the data so to answer somebody's question should we do that I said well your question the initial t reduction is something you should always try you have to know see the point question is will you succeed in the abstract from a pure mathematical perspective there does exist a space right really okay yeah where the data is linear and now the question is in that space can you come from that space to a lower dimensional space like in that is the data the sort of can i shrink wrap in that space in an ellipsoid. And when I do that, can I throw away most of the axes of the ellipsoid so that the remaining axes that I'm left with The sum total of those axes is less than the original space from which I started. If it is true. Sometimes it may be true. Sometimes it's not true. If it is true. Well, you can achieve dimensionality reduction if not you cannot right so for example if you imagine data in a unit desk like this one you realize that we created an axis x3 is equal to x1 square plus x2 square yes sir and so as far as classification is concerned just one dimension x3 is enough because at a certain value of x3 we could draw a hyperplane and classify right so from the classification perspective we actually reduce the problem this one dimension you see that guys the x3 dimension that is sufficient to solve the problem yes that's it and so and that is just the height from the ground plane from the ground of the point any point just ask how high is it from the ground and that will tell you whether it's a blue point or a yellow point this is a classic example of how you went to a higher dimension right only to come down and discover a much lower dimension that will solve your problem right just one dimension would became your classifier for the classification purposes you have reduced the problem to one dimension that's the beauty of it go ahead so sir a small question maybe silly question so feature engineering do we do before pca and after pc does it make sense both see these things you know i wish i could answer it one way it goes around in circles see machine learning is not a linear process people think of it as a pipeline they say in production it's a pipeline but the discovery process is not linear it is you know you cleanse the data you scale the data you fill in values you do outlier treatments anomalies and so forth and then finally you do you start doing let us say some feature engineering some dimensionality reduction when you do modeling it looks as though it is like that but actually it's not what happens really is once you have done the first few data preparation parts the data rambling parts you bring the data first of all data doesn't exist as vector spaces, right, the X, the X we always represent the X vector. Space column vector and dimensional vector. It doesn't you look at the data it exists as what it exists as text. Right. It exists as an image. It exists as a sound it exists as columns in a database. image it exists as a sound it exists as columns in a database a table and you know the data is captured across many many tables so one of the first things in your journey is to do those data transformations or what people used to call ETL jobs or things to just bring in the pieces together and build the X vector once you have built the X vector then you do all of these you do the cleaning of it the anomaly detecting the missing value treatment and all of that and then you prepare that into it you create the clean out X vector but you still don't know if these are the right feature vectors to go into your algorithm. You don't know. What do you do? Then many things can happen. Maybe there's a lower dimensional space that represents it. Maybe there is another vector that we can build out of it, the feature engineering like we did for River. Or like you see in this problem, it is not X1 or X2 that matters it's xp that matters yeah so how do you discover x3 then you go start going around in circles you partly use intuition partly try svm rightly try different techniques and you see that when they begin to work you ask why did it work can i for example use it go back and do something so in after X, there is a journey to let us say, explain the feature engineered vector, the feature vector. And this is the thing, usually the input vector and feature vectors are different. Right? So this is sort of an iterative process, you go back and forth, you go all the way to the classification and model building and you keep coming back. It is the circle of learning. I suppose in the US, we teach children a very simple phrase that science is captured, the scientific spirit is captured in three words that form a cycle. You ask a question, is this true? You start with a hypothesis. Then you measure, you gather data to test the hypothesis, get the data in, learn from that. What did you learn? Did it support your hypothesis or not? And then you come up with a different hypothesis. You go back to asking the next question. So you are forever in the circle of ask, measure, learn. And that's how you build better and better and better models. And this is the classic journey of science. So you keep going in circles there. So any non-trivial science project you do that is the reason you it takes a long time see I'll give you a real-life intuition so in your career for most data scientists to be successful in their workplace they don't work with too many data sets you know here we are doing different data sets at a time we're getting practice is good for learning in workplace you will just get one kind of data set. Maybe you're working for, I'll take a hypothetical example, Zillow. All the data set will be about real estate, doesn't it? And one team will be trying one sort of algorithm or variations of it for a long long time before they have a breakthrough idea and start trying different algorithms. Quite often, there'll be going around in circles, trying to optimize, make it better, the predictions for just one problem. In the case of Zillow, it's like better predicting the house value, let's just say. And then all sorts of other sub-problems associated with it. You can keep building models. So your entire career there in a team can be associated with solving. You can keep building models. So your entire career there in a team can be associated with solving one problem. So obviously it can't be a linear problem that you went through the journey correctly and you have a model. No, you keep on moving around in circles, getting better and better and better models, which also sometimes means finding the right features or right inputs and then asking people to go gather those inputs so that is how are we together guys so alright today I covered the second topic which is kernel PCA I would like to before I go into the break cover one more topic, which is using the distance kernel for KNM. So I should I close I'm closing this topic guys. Should I do you want to take a break before we start on the next topic? Or can I start in the next topic and then take a break. Go ahead. As if we use a kernel to express our data on a higher dimension, does this simply translate the understanding of the data into something that's easier to wrangle or will this transformation introduce a likelihood for variance errors? No, not really. Actually, when you go, when you do, when you go in a disciplined way up to higher dimensions, you don't necessarily need to go and introduce a lot of variance into the problem. Look at this thing from here, the circle, if you, when you lift it up to the parabolic surface, it was a very controlled function isn't it it was just not a haphazard expansion of the dimensionality of the problem you don't you know you're not introducing more variables more flexible parameters it's a very disciplined approach to go there so this won't affect your directly your bias way in straight up like it's not in the obvious way okay yeah alright guys so would you we did something I can see that we did something conceptually a little bit heavy at this point would you guys prefer taking a short break to absorb all this think about it and then we'll start in 10-15 minutes yes yes yes that works let's do that so guys to recap what did we do in the first part of the session the first part of the session was that we were trying to get an intuition why is it that when we go to a different space from the original input space, quite often and usually a higher dimensional space or just a transformed space, you can see that the problem has become much simpler. It has become a linear problem. And therefore you can build linear decision boundaries for classification and for regression of course you can do a linear regression in effect and for dimensionality reduction your old technique of PCA for example can be applied you just need to take a kernel you need to go to a higher dimension where the problem has straightened out so you can wrap an ellipsoidal shrink wrap around your data and then go about doing your PCA thereafter. Does it always work? Not necessarily, but it does work a surprising number of times. You should certainly try it. One of the questions that was raised is if I do kernel PCA will I always succeed? The answer is different. First is that the built-in kernels in the libraries are limited, linear, polynomial and the RBA. If you are capable of growing your own kernels and thinking through the data, this is again feature engineering, you might have much more success but if you use a vanilla one you may not succeed. Yeah, you may not have as much success. The second aspect that comes to it is that the problem truly cannot be reduced in dimensions it is it takes the whole space for example you would agree that this problem is something that looks as though it's true actually this is a simple problem we could reduce it to one dimension the z-axis the x-3 axis but there are problems which cannot be reduced to lower dimensions truly information is scattered in the entire volumetric space of the feature feature vector space like in the entire volume of the feature vector space then you can't do dimensional reduction and lastly not every technique has its limitations so kernel kernel PCA, while it is more powerful than PCA perhaps, well, powerful is a word, not always more powerful, sometimes more powerful, but it's not so powerful that it can handle every situation. So the field of dimensionality reduction is rich. There are many, many algorithms, just like for classification and regression, there are many, many algorithms that you use to be able to classify data. So there are many techniques for dimensionality reduction. It's a very rich literature. There's this whole field of manifold learning and so on and so forth. We learn it and they're beautiful visualizations, the TSNE, the UMAP and so on and so forth, which you'll get very hands-on experience in the next class that we have. But here we won't have the time for that but I'll stop here what we will come to now is we will see how we modified a PCA to get dimensionality reduction we can actually modify KNN to make it a little bit smarter we'll use distance kernels so the idea is this see I told you that if you have a point here, where was I? Let's go back to our motivating example, the cows and ducks. So suppose you have your ducks here and now you guys are familiar with this example so I wouldn't draw too many points and let us say that you have cows here in the feature space and obviously the way we motivated this is by saying this is weight x1 is weight and x2 let us say is the size of the animal way intuitively obvious that ducks are small and lightweight relative to cows and cows are rather big and heavy creatures way out there in the feature space obviously spread will be more so I'll lightweight relative to cows and cows are rather big and heavy creatures way out there in the feature space obviously spread will be more so I'll spread it out okay something like this so the question is this renders itself very easily to birds of a feather notion if you want to find out what is this point this guy it is you look at the neighbors or this guy, you look at the neighbors, it's pretty obvious that things this size and this weight are much more likely to be ducks rather than cows. You look at this point, you don't have data for this point, you get an animal whose weight is that and size is given by that value. And intuition says that it must be a cow because things similar to it are cows in the feature space the neighboring points in the feature space that is the intuition between K and n K nearest neighbor here nearest neighbor but then I also pointed out that this has a limitation for it so the limitation comes from the fact that suppose I bring in some things that are bigger in size but not as heavy. I don't know. I cooked a giraffe, but biologists will please excuse me if this analogy is wrong in terms of weight and so forth. So let us say that this is it. So how do you know whether something is a duck or a giraffe? You can say, well, I'm going to look at the neighboring points. But then the one problem that remains is how many neighboring points? If you take k is less than one, I'm just recapitulating what we did. If you take k less than one, sorry, not, what am I saying, silly. If you take k is equal to one, you can often get it wrong, isn't it? Accidentally or by chance, more likely that the nearest neighbors may be of the other kind isn't it so let's look at this example suppose you are here which point should I take at this point do you see this point that I drew with the small where my mouse is guys. Yeah. This point if you ask what is it Is it a giraffe or is it a cow, you'll be if you use K is equal to one. The answer would be, it's a cow. But if you take K is equal to let us say 3 now you'll say giraffe isn't it and let us say that you have in this particular meadow you have a lot more ducks around let's say that in your data set there are hundreds of ducks so now suppose you take K is equal to 200 let's say what do you think your answer will be it will be the entire data set and what is the majority here no duck so you you will end up with an answer of that so you notice how your answer varies as you go from here to there this is the traditional journey from high variance to high and so let me connect it to this concept of high variance to high bias right at k is equal to 1 you are in a situation of high variance overfitting to a situation of k is large let me just say k tends to infinity high bias in fact in k is equal to infinity obviously you won't reach infinity because you have finite number of a sample point but it's in practical terms whatever the sample size is and you will end up with a situation of bias and it will degenerate into the baseline or the zero-hour classifier DJ yeah no terrible spelling I have a degenerate rate into the baseline or 0r classifier what is the baseline or 0r classifier say its answer to every question is the majority right whichever is the most common so in this space cows ducks and giraffes it turns out the ducks are most common so a baseline classifier doesn't care about anything it won't even care about the neighborhood it will always answer the answer to any question which what animal it is it's a duck okay the number of neighbors yes indeed thank you for asking that if I didn't clarify the case then a number of neighbors so it is the hyper parameter in this model so I? No high variance. You will see why. It is the decision boundary is very complex and very sensitive to and if you take another sample of data your answer will change and I will illustrate that point in a moment. This is something you should know actually. This is it. So this other end is easy to understand isn't it? If your answer just simply doesn't change, irrespective of what you do, you're in the state of obviously an extreme bias and there is no variance at all, then you answer, isn't it? Zero variance. So it's, I remember if I may just say, there was almost a silly joke. Once I encountered a fairly silly book that somebody had written, and the title of the book is Quantum Explanations or something like that. And I was very fascinated because I used to love theoretical physics. I picked up the book, and in the book, the explanation of every phenomena was this mythical thing called quantum form. So everything happens because there's a quantum. Obviously, rather a silly book, I was disappointed and I felt like a fool spending money on it. But that's that. So a high bias situation is like that. The answer to every problem is just duck. You know, I have an animal this is the way this is the size this is it this that all the attributes and was the answer is duck right so it is your point it's one extreme is the zero our classifier base doesn't depend on the data absolutely doesn't depend on the data that's why it's called zero R it doesn't look at any of the features at all one answer for the entire feature space So now what is the journey from K is equal to 1 to K is equal to n and how do we illustrate this? So I'll illustrate this with an example and in the notes I give more examples, but here let me take some simple motivating example I'll make a different picture here two three four five six Right. And now let me take some points one two three four five six seven eight nine and let me just bring in some errors this and while we are at something like this and then blue points also let me throw in something here and a couple of things here so guys let's try to buy build a decision boundary let's take an arbitrary point suppose I take a point here what will this be this little point what do you think it will be but clearly it's a yellow point isn't it it seems suppose any we are taking k is equal to one one nearest neighbor so what is the nearest neighbor to this point guys it's yellow it's the yellow and then it will be yellow yellow and the nearest neighbor to this point is blue you would realize that if I take the perpendicular bisector of this the decision boundary has to be here isn't it then you look at then again here any point that you take here between these two once again you'll build a decision boundary somewhere actually somewhere close to this triangular space somewhere here the decision boundary will go then it will go through here right and then it will go through here do you see how we are doing the decision boundary right and it's getting complex but okay I will sort of and then it gets even more complex I don't know how to deal with this but we'll deal with it at some way it goes up here it goes through this and then it goes through comes back come here and then goes through like this something like this you get your K is equal to one decision boundary keep in mind I'll just make it a smooth curve even though it's actually made up of little many perpendicular bisectors exactly sort of those lines those edges so this is your decision boundary now keep the same thought in your mind and I'll use a different color let me use the green color perhaps for K is equal to let's take K is equal to how many points are there three seven K is equal to three let's look at K is equal to three go back anywhere you realize that up to here right very close to it it will actually be the three nearest neighbors will be if you look at this let's ask about this point what are the three nearest neighbors one two three so it will be declared as blue isn't it this point is blue what about this point it will be declared as three nearest neighbors are one two and which one do you want to say this one three nearest neighbors so what will you declare this point to be This one, three nearest neighbors. So what will you declare this point to be? Blue, right? This will still be blue. What about this point, this point itself? Any in the vicinity of this point would still be blue. Do you realize that even if you're sitting on a yellow point, if you take the three nearest neighbors, it would still be depending upon neighbors you're taking, these neighbors. It is still a blue point So what is happening is that the decision boundary is going much more smoothly Right. Now, let's look at this point. What are the three nearest neighbors? It is yellow. So it will go something like this Right so far and then here these are the blue neighbors. This is yellow. Let us look at this point. Where are we three nearest neighbors are yellow. Right. So it will turn around, it will now be asked this question. What about this point is yellow. What about this point. This point is blue if you notice okay the three nearest neighbors are I don't know depending upon how you look at it let me cheat a little bit to simplify that you would say that this is blue this is blue do you realize that okay suppose looking with the problem a little bit more to make the meaning clear and so for add some more point so you realize that you i mean obviously i'm cheating a little bit here your decision boundary might be like this what can you say about that versus the white decision boundary do you notice that the white is far more curvy isn't it it's more complex yeah now let's take a is equal to six let's take another okay is equal to six then what happens the problem begins to simplify quite a lot actually I would like to believe that if you are here most of your neighbors let's look at the six nearest neighbors one two three four these two would be the nearest neighbor five is and whether this is the next neighbor or this is the next neighbor depends upon where you are let us say that this is it so this point is still yellow and if you draw this decision boundary perhaps it would be even more smooth actually how would it look like i don't, how would it look like? I don't know. It would probably look like something like, let me just simplify it, right? Or pretend, I don't know if this is something, but I'm trying to just develop the intuition and perhaps cheating a little bit in my point, but we'll do it actually in code and you'll see it much more realistically. So what can you say of the decision boundaries? Do you notice that the decision boundary keeps getting less complex? It's getting less complex, isn't it? In the asymptotic limit of K is equal to, let's say that K is equal to the maximum number of points, whatever it is, let's say 20, what would be the answer, majority? Which points are more? I clearly see blue points are more. What will be the decision boundary? Here. It will say that all points are more. So what will be the decision boundary? Here. It will say that all points are blue because I have more blue points. Are we together guys? If you look at K is equal to 20 and you take 20 neighbors or 25 neighbors, your answer will always be that all points are blue and you have a very simple decision boundary. What is the decision boundary? Everything is blue. You can just put any line beyond all the points and put all the points on one side and that becomes your decision boundary. Actually, it is even more extreme. Your decision boundary can be anything like something like this. Oops, sorry. Let me use the right color. It could be anything like this so all the points on this side of blue is this is this obvious guys is this are we making our seats clear so decision boundary becomes less complex and so now what happens think about it this way if I take another sampling of points this decision boundary will not change but on the other extreme k is equal to one will change you can you would agree that if i took a different position of the blues and yellows the decision boundary will immediately shift is this point obvious guys yeah it's a different sample we'll have this blues and greens a different place. And so the decision boundary will change. So there's very high sort of a Variance of the decision boundaries as you keep taking different samples. Your answer keeps changing. It's a classic example of what is it a classic example of it's of example of Oh, variance. Variance, high variance. High variance or overfitting. This k is equal to 1 is listening too much to the data. It's a very, very overfit solution. So that is the journey from overfitting to underfitting at the other extreme. So now one way that you could remedy this is you can bring the kernel to the rescue. You use a distance kernel. Let's bring in the concept of a distance kernel. So remember, what is the notion of distance? Distance between two points is what? It is anything that follows these properties. it is anything that follows these properties d x x prime is equal to zero if and only if x is equal to x prime x and x point prime are the same and just recapitulating what we said is the definition of distance and furthermore we said that there's a triangular inequality d x to some other point so you take a detour plus d, obviously let me put this as vectors in some space or something like that, dz to dx prime will always be more than or equal to the distance from, direct distance from x prime. So in very practical terms, when you go to work, quite often your journey to your desk always involves a Z, which may be the coffee machine, the water cooler or something like that. But your journey home is always a straight journey back, and which feels shorter, obviously the journey back. Anyway, I'm joking. is this defines the concept of distance a typical distance says that we talked about at the minkowski distance distance and just as a recap what we talked about is the distance was that there is the d the norm the dn of x between x and x prime right so let us define x delta x is equal to n 1 to infinity is defined as Delta X I so the component wise you could look at the component Delta X I is I the I earth component right so the component y is delta xi to the power n summed over all of them in other words it is delta x1 n plus delta x2 n and the whole thing summation over i and the whole thing to the power one over n you have to take the nth root of it 1 over sorry this is and it should be here and it's power and 1 1 n you remember that D is equal to 1 if you recap each lake was the what kind of a distance was it Manhattan distance yeah yeah n is equal to 1 sorry the n is equal to 1 and people often use the word Ella so let me use the L L is equal to 1 L depending upon which community of crowd you come from but machine learning crowd people tend to use the word L L norm the word people use is L norm they when they do that they always are referring to the Minkowski distance this is your Euclidean distance your Euclidean distance and go ahead so one thing is I'll give you an intuition of how it looks in space take a unit what does a circle look like in euclidean distance l is equal to 2 actually let me just put it here l is equal to 2 is here l is equal to 2 is what you realize that if you make a all points so let us have a definition of a circle circle is definite defined as all points and let me call it the unit circle so that I don't have to worry about radius radius is one all points at unit distance from origin so this is the definition right we will define the circle Distance From origin. So this is the definition we will define the surplus that anybody would like to argue with their definition is that self evident. That's how we define circles. And yes. So now let's think what happens when we do it for L is equal to one. The one norm so points which are at the same distance are here. But the surprising thing is because we say that delta X, so there's no X1 norm, X norm 1 plus X2 norm is equal to 1. What does this equation look like? This is the equation of a straight line. It actually looks like this. And let me give you the intuition. See see you can go unit distance in this direction right or you can go unit distance in this direction or you can do this way you can go halfway here and halfway up or you could go three-fourth way up here and one-fourth way up or you could go one fourth way you could go three fourth way up but ultimately if you if you are a taxi driver with only let's say that gas in your tank for one unit distance but this is the this is pretty much the envelope of how far you'll travel do we see these guys yes yes we say that a circle for L1 norm is actually diamond shaped it's not it's not round it's diamond shape you see this diamond shaped circle right this is how it is and that raises a very interesting issue oh well the first time you see this you're a bit surprised right because a fundamental notion of a circle has always been round, isn't it? But in a different norm, the circle looks differently. In fact, what happens is if you go to L is equal to half, it will become like this, much more pointy. And we will need this intuition when we do regularization. It will begin to look like that on the other hand if you do so this is a lazy people to put the word here L is equal to 1 this is L is equal to 2 what does L is equal to 3 and so forth look like so remember that we had a unit circle for L is equal to two, the L is equal to three will develop a little bit of a bulge over this. Will look like this. Are we together? It's bulged out a little bit. This is L is equal to three. And then when you remember, we talked of the infinite norm. L is equal to infinity right and that devolved to the largest value the largest Delta Delta X max right so that looks in this triangle looks like this so it looks actually like a unit square actually the circle devolves into unit let me explain why that is so the the L is see we talked about the L nominate the L is this L to the power L so let me give you a looking okay recap this what L is it's a recap. L in Minkowski norm Minkowski's definition of a distance I think goes to norm. It is called the norm. norm it is called the norm what it means is look at this expression the ellet norm of a between two points distance between two points is defined it's a generalization of euclidean distance it is defined as this you take the x 1 minus x1 prime absolute value to the power l plus x2 minus x2 prime to the absolute value l and you keep doing on but let's stick to two dimensions and to the one by l the square root the lth square root of this where l can take values from one, two, three, all the way to infinity. So now, come again. Like l is dimension? No, l is not dimension. l is the measure. It's a way of measuring distance. That's why it's called norm. Your data may be in two dimension, but you may still take the infinite norm. In fact, let's look at two dimension. So if you look at it, L is equal to d To what does it become be to x x prime is a familiar Euclidean distance x one minus x one prime square plus x two minus x two prime x2 minus x2 prime square square root right now you realize that for even powers you don't even need the mod so this is nothing but x1 minus x1 prime square plus x2 minus x2 prime square this you remember from your basic high high school uh pre-calculus or algebra what is this this euclidean this is your euclidean distance now d1 the first norm of x and x prime is equal to the same expression becomes x1 minus x1 prime mod plus x2 minus x2 prime mod. That's it. And because there's the 1th root of a number is 1 itself. What does this look like? This looks like your Manhattan distance or the taxi driver's distance. The idea being that in Manhattan cities are on a grid. So you can only go along this right, so This was from a previous lecture. So I would say it may be helpful to Watch the video of the last lecture which I just haven't posted Let me the YouTube recording is there the live recordings are there you should do that and I'll put a cleaned up portion also but it's all there guys are you aware that all the lectures like including this one it is being live broadcast on youtube you can actually watch this entire class sitting comfortably on your sofa and watching it on your television and not only that after life those recordings stay public all the class recordings are at this moment public and present on the YouTube support vector site if you haven't gone there please go there subscribe to it watch those videos and you will get that so this is a recap of what we did before so this was who was asking this question sonal was that you who or come here sorry who was asking this question about the l yes yeah that was me amir as in sonal okay good so uh did you understand this idea now? Yeah, I did. So this is, so now the peculiar thing is, see what mathematicians do is they take intuition from real life and then they abstract it. You know, they make it more general. They go beyond that, try to go beyond that. So that is what we are trying to do. Give me a second. So this is basically just a different distance formula and guys one second the recording somehow got paused has the recording pause I don't know I'll just pause and start it again. Okay, so this is just another distance function that's absolutely correct. So mathematicians keep generalizing from simple terms. So then it leads to beautiful consequences. For example, the D is equal to two we are familiar with. It's a nice round circle. That's a practical day to day intuition. D is equal to one. This is, sorry, not D. D is equal to one. This is, sorry, not D. L is equal to two. The second norm is familiar, Euclidean. The L is equal to one takes a bit of a surprise, but then if you think of taxi drivers in Manhattan, they can, so suppose they have to go unit distance. They can go unit distance here, or they can go three-fourth of the way and one-fourth of the way up, or they can go unit distance here or they can go 3 4th of the way and 1 4th of the way up or they can go half distance here and half distance here or they could just go along this axis all four units right so if you look at the unit distances their former they will form a diamond shape or rhombus shape right so okay so uh sorry uh this picture i already do it will form this shape right and if you this is l is equal to one and if you can even go to fractional values of l and when you do that you's say one half looks like this at the other end if you keep on doing it do you notice that is beginning to budge the bulge increases this straightens out it begins to develop a bud and at the other end of this extreme it becomes this square how does it become a square think about this point when you are here well the distance is of course equal to here what about here when you are when you are here like what is your distance at this particular moment you will realize after some time that this point is at unit distance because this distance the y distance is to be the x2 distance is to be ignored the L is equal to infinity just looks at the big bigger of the two distances so which is the bigger of the two here this distance so it is still this there's this, till you reach this point, at which point now you are equidistant. Both these two distances along this, along this are the same. And after that, it is the distance along this. So the unit circle becomes a square. It will take you a moment to think it through. So this is the Minkowski norm. Well, why are we bringing this? We'll use Minkowski norm a lot in machine learning but here we'll use it in using something interesting just keep this thing in your mind and then worry about it and now distance let's bring it to our K nearest neighbors neighbors oh it's K nearest by the way do understand Minkowski norm because we'll need it in regularization today I was going to do regularization is 10 I want to let you go in another happener but so we'll keep the topic for for next time actually are for an extra session by the way way, I'm ready to give you guys an extra session on the, on what, on boosting, right? So maybe we will need two extra sessions in this workshop to finish off things. But all right, let's look at this. Care nearest neighbors. You realize that suppose you have neighbors which are far off. Look at this. Where are we? Where is our intuition? Yeah, look at this. If you look at this problem, you agree that the further going far, a little bit makes sense. You know, taking three neighbors, four neighbors makes sense. But if you go too far, you again begin to lose it because far off neighbors are not representative of the local situation isn't it if you take a k is equal to 200 all you get are ducks isn't it so the question is can you bring some intuition to improve upon it so one intuition that you can bring is to say hey you know what people who are my close neighbors are more representative of me than neighbors which are far off are we together so you can say that something somebody who lives on my street give give that person twice the weight age compared to the guy who lives in the next city when you want to know something about you does that intuition make sense guys yes yes so so something like your attention model right oh yes yes it's like that but don't bring that here at this moment okay so so at this moment so simple intuition that the further off you go the less those neighbors are representative of you so what should we do it's okay to go to those neighbors but we just need to weigh it in we have to take a weighted average let's say that we take the vote you take their votes a bit less value than the words of the people immediately to you right or if you're doing a regression you're taking the average take a weighted average then weigh it by neighbor so suppose you have three neighbors w1 X 1 whatever the value for this is right let's say that the value the y1 actually the value let's say that you're trying to predict y hat and based on the average of three of three points x and there are three neighbors to y2 y1's value y2 values y3 value does this make sense guys suppose you're trying to find the weight of a duck you look at three neighboring ducks and see their weights those are the uh well weight is not the thing but something okay whatever it is but you want to find the weight based on their size right so you take three neighbors and you three ducks nearby and you take the average of the weight does that make sense but the thing is you weigh the nearest is nearest is one, farthest is three. So, for example, you could say, let me weigh this four times. Let me weigh this two times and let me far off by. Let me weight once. Distribute the weight a little bit. So that the value that I come to, and then of course, divide the total weight by seven. So a four seventh, two seventh and one seventh, right? Is the weight that you associate with each of these readings. So then what do you get? You get a weight that is more representative of this than this. Are we making sense guys? Yeah. But at the same time, you're not only looking at this guy's weight. You're looking at other people's weight also. So when you do that, what you're doing is you're doing a distance aware or distance weighed nearest neighbor, k nearest neighbor. And you may still take k nearest neighbor, k is still there, how many neighbors you take. But one thing you realize that suppose you take k as very big, k is equal to 50, but your 50 58 neighbor is very far away what will be the coefficient here it will probably be zero zero one something very small value the idea is that you make the far of neighbors very small they influence very small so that they don't they don't have as much vote as the first guy right so what you do is the intuition is that you take the votes are the weights over this regression or this thing and you create a weightage function of weight function. Actually, you call it the the kernel. So actually that also has a K value. So I won't use the key. Let me use the word weight. That also has a K value. So I won't use the K. Let me use the word weight, right? A weight that is a function of a distance between two points, WD, or more formally, it's a function of distance function you need, something that decreases. It may decrease like this. It may decrease like this. It may decrease like this. Do you realize that each of these will serve the purpose? Right? You just say that the further off you are, the distances, the less the weight you will give to that point in making a decision about thousand ducks or taking the weight of the denim and so forth is this intuition clear guys you need a distant any function any function which fulfills the criteria it is monotonically decreasing with distance right is decreasing with distance it cannot just go increase like this will this function will not do uh this is not a good weight function this is meaningless right yeah emphasis like it goes up and down etc it won't do so we won't do that actually let me just put it here and this is not you need something that decreases it may not even decrease it may decrease very slowly or it may not decrease at all it may just remain so if you notice that if k if the weight doesn't decrease at all with distance it is your care nearest neighbor without a kernel without a distance awareness do you realize that guys that the standard KNN is nothing but in this terminology distance awareness in which the weight function is constant is this intuition making sense guys yes straightforward but you want something better than that so sometimes people even use a step function they say that if you are within a certain distance I will consider you are the uniformly after that I won't consider you so what happened is that soon people began to ask these weights were called W's are called distance kernels and it's the same idea of the kernel so I won't go into more mathematics so the thing is so long as it is any function that decreases monotonically or at least decreases, never increases at all monotonically, is a valid distance function. So there was an effort to find all sorts of distance functions. What is a good distance function? What is a good kernel? And there was a cottage industry trying to prove that this kernel is better than that kernel is better than that. And so there are many kernels that people began to use anything could do a sine wave, you know, the, the, what is it the cosine goes like this. That is a perfectly good. Again, it is decreasing. that you want that decreases you could use a bell curves this this too is a good kernel right so people came up with a lot of kernels and they began to investigate which kernel is better than the other at the end of the day they found out actually surprisingly that the choice of kernel doesn't matter much it does matter a little bit but in most situations a changing from one kernel to another gives you a tiny bit of improvement in your classification or regression it doesn't make a tremendous difference so at this point what i'll do is let me just move on to the other computer and i'll show you examples in from the notes by the way there's no cell give you of what i mean notes by the way there's no cell give you of what I mean in this context give me a moment so distance we can and so I'll give you guys this chapter now it's from this chapter how do I minimize this okay so guys look at this are you all able to see my screen yeah suppose we are writing a classifier that distinguishes between the blue point in the what color would we call it orange points orange points is the close orange points here. Let me zoom in a little bit and go to a single view single page view page now single page okay guys so are you able to see this picture clearly there's a lot of noise here points the orange and the blue are mixed up but still you can say that there is a separation so we will take different values of K and see what happens. If you treat it as a simple linear regression problem, a line would just go through like this. Now what happens when you take K is equal to one nearest neighbor, one nearest neighbor? Do you see how complex your decision boundary is? And I'll give you a moment to stare at it and convince yourself that it is the right decision boundary obviously I did it programmatically so it happens to be right code to generate it so in this situation you have all these points and you have the skin nearest neighbor decision boundary you see all these islands and peninsulas and so forth and this can this sort of backwaters and whatnot. If you look at this, if you treat blue as the sea and orange is the land, you see all sorts of geography features. You take five nearest neighbor. Do you notice that the decision boundary is simplifying itself? Does this look simpler guys? Yeah. Yes. is simplifying itself does this look simpler guys yeah yes right then you go to K is equal to 15 and it has simplified itself even more you take K is equal to 100 and now it's the straight or not straight line but okay much more smoother much more stretched out line. So do you see the variance decreasing as you increase the number of neighbors? And the question is which is the best value of K to take? And the answer to that is whichever value gives you the best predictions on the test data, isn't it? On the validation set, whichever gives you the best predictions, that is your value of K so proof of the pudding is in the eating you have to validate it against data see what the predictions are how do they come out and base it on that now I just mentioned that we have two kinds of I mean I'm just taking the impact of norms how does Euclidean norm in the Manhattan norm affect for the same set of points actually the Euclidean norm in the Manhattan norms do you notice that they build different decision boundaries they will break it up differently it's just something to know that you can just play with the notion of distance and change the decision one one question that people ask is hey a k nearest neighbor assuming that you have a notion of distance assuming that you have an the dimensionality of the problem is small and you don't have the curse of dimensionality that we talked about last time isn't this a lovely thing won't it always outperform other right yeah sodium chloride yes so i take this example of sodium chloride actually it's not true but because if you look at a lattice structure in an atom in a in this lattices so salt is a lattice structure you will always get it wrong k nearest neighbor will always come out wrong irrespective of what value of case because the nearest neighbor of a sodium atom will be a chlorine atom on this and all the other for clearing would be sodium atom and if you think whatever K you take you keep getting it wrong right again goes to the no freelance theorem that there is no way out you know nothing is perfect everything has its flaws now when you get this norm, so to review what I just taught you about the P norm, by the way, instead of L, I've used P. Mathematicians tend to use the word P and the data science community, we tend to use the word L, just trivial distances. But if you notice this diagram that you have here and those marks, and some of you are repeating the class already have this note but do you notice how it goes from very small like concave like structure to convex diamond and convex in circle and finally to a square that's how it behaves now how do you this distance function that you have what is a distance function it follows this inequality Now this is an example of the kernels people have one easy kernel is just one over our Simple isn't it another popular kernel is just take the positive half of the bell curve. Distances are positive. This is the positive half of the bell curve. By the way, what is this constant in front? It is just a normalization constant so that the integral comes to one. This is called the parabolic kernel. It was associated with this gentleman, Ipanichnikov. I can never get the pronunciation right my apologies to this great man so this is this kind of yeah parabolic kernel so it's it's like this a simple triangular kernel it just goes like this right well people are creative you can use cosine as a kernel. You can use a tricube kernel, which is this, which goes to the third power. Then a weighted tricube kernel. So you can see that at one point people went crazy trying to come up with better and better kernels. And they thought that the kernel, the actual shape of the kernel mattered a lot. Today, we know that it matters a little bit, not too much. Right? And so here are all of these kernels, you know, put here, throw away the negative side, just look at the positive side, you see how they decay. So the bottom line is, it doesn't matter much, but it does matter a little bit. So now you have two hyper parameters in your model k the number of nearest neighbors to take and which kernel to pick generally what happens is if you use kernels k doesn't matter so much the number of neighbors it gets muted it is not something that affects the model in a huge degree right It doesn't matter, but not too much. It doesn't matter that much. So something to remember. You tend to take the rather large case, but let the kernel take care of them. So this is it. This is just saying whatever I said in words in a more formal language. So how do you do regression with it? Very simple. You just take the nearest neighbor and take the average of the output average of the predictions so you can do that if you apply it to hill dataset you'll realize it makes a pretty good value this is a lab which is done in r so i'll hand out the solution in r if you apply to the river data set let's see how well it does and i have the prediction mapped out for different values of k when you do that you realize that it is the maximum accuracy that you get is for eight neighbors for the river data set you should take eight neighbors so we did a search for the best model eight neighbors turn out to be best by the way if you rerun it and for different people it is sometimes seven or sometimes eight or something like that so and the accuracy is 87.3 which is what we expect we remember that the maximum accuracy you can get is around that 89 or something like that right so it's pretty good model isn't it the nearest neighbor algorithm especially with the kernel then how does the accuracy of the model vary with K you can see that you know afar in the beginning accuracy improves as you increase K and after that it starts decreasing as you take more and more neighbors you're considering further of neighbors and so your accuracy tends to be free we turn your point of best accuracy is around eight you get that one way to simulate higher dimension of course you can't see the tie dimension is to take sparse samples of the data to learn because in high dimension data becomes sparse so with the river data set we could just take very few samples to learn from and so it will simulate high dimension when you do that you realize that once again the more you go to higher dimensions the accuracy is better like is like the bigger the sample the more the accuracy which means that the lower the in lower dimensional spaces uh knn works well in high dimensional spaces you begin to get into trouble it's the curse of dimensionality that we talked about right all right so this illustrates that the same thing for the flag data set i'll post this notes by the way i'll put it on the website all of these notes i know that slack is getting a little disorganized there are too many things here i'll post it to slack also into this so we have the flag data set here once again the same accuracy curve that you see it peaks around what is the peak value here you you get a peak around k is equal to around k to 5 to 10 in this plateau range you get maximum accuracy 57 percent the hill data set what does it do hill data set gets the predictions right with very high degree of accuracy 99 percent r square i mean coefficient of determination the r squared very high so these things work now I give this as a problem look at this problem guys I can I can give you the data if you guys want the challenge the code to generate the data is here you look at this data and you realize that building a regression model for this can be pretty hard especially a linear regression module do you agree guys because the surface is anything but Building a regression model for this can be pretty hard, especially a linear regression model. Do you agree guys? Because the surface is anything but linear. Do you see that guys? Anybody? Yes. The highly nonlinear surface. And yet if you have rich enough data, it fulfills the criteria, low dimensions. If you have enough data points, care nearest neighbor can do extremely well. And I leave that for you as an exercise. I'll give you have enough data points, KNN is neighbor can do extremely well. And I leave that for you as an exercise. I'll give you guys the data and give you the formula. This is basically a modified form of what is called a sync function, I call it the flower dataset. It's along with your thing, there's also the flower dataset, I'll release it. And so you can try it out. You'll be surprised. KNN works, kernel KNN works like a charm, right? But it's very hard to model a parametric model like a linear polynomial equation to this. It's a very complicated polynomial. You notice that. Looks like a flower. So well guys, today we had a long session I'll end with that Thank you.