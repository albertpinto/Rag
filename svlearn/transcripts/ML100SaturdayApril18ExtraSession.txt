 So, folks, one announcement is that EDA file, exploratory data analysis file, has become too big and people are having difficulty viewing it. So ignore that file now. I'll delete that. Instead I've broken that book, that file, into two different files. One of them is called tabulardata.pdf and the other is called network science. At this moment, network science is very much a work in progress. You guys can watch it evolve, but we'll stay a little away from it. Focus on the tabular data, that is the standard structured data, machine learning on structured data. i have shared the links with all of you if you have not received the those documents or cannot view the documents let me know at this moment those documents are evolving on a daily basis right every two three days i keep putting a new version there so new newer content keeps showing up. Now, I'll just show you what is there in this document. We have, just to give you the things from the very beginning, you're familiar with this. At this particular moment, it is... We can see your screen. You can't? OK. I thought I'd share. OK, we'll share my screen all over again there we go can you folks see my screen now yeah okay yes so this document tabular data has 293 pages at this moment quite a bit of it you are familiar with let Let me give you the chapters of this. This has, for example, What are the chapters? The exploratory data analysis chapter, which is something I hope by now you have done all the exercises for. Next section of the book, part two of the book, is about regression and classification. In the last session, we did univariate linear regression. Today, we'll cover a lot of territory, actually. We'll do nonlinear least squares and we'll do multivariate regression. Next Saturday, we'll do classification. Next Saturday, we'll do classification. So let us go to the linear models and try to cover our territory there. So if you go to univariate linear model, here we are. This is what we need to do. So we need to do these exercises, folks. So you take this data set i've given the solutions in both r and python so this is just loading the data all these data sets are available in github you all know it is mentioned in the resources at the very end so please do these notebooks and as you do it take my help and reach out to me and take my help. So this data set we talked about, we discussed last time. We talked about the residual analysis. Just to recap, we did all of this. We found that, you know, just because you get good numbers in your summary of the model, it is not enough. You have to look, you have to do the residual analysis just to see if there is any pattern in the residuals, whether the residuals show homoscedasticity or heteroscedasticity and that is or any other pattern in the data. And so that we did, we realized we needed a better model, a polynomial model, and a polynomial model was a good fit. Then we took another dataset, which sort of is, in hindsight, looks like a bell curve. This dataset, again, we did a linear, direct linear model. It was a total disaster. Adjusted R square was 0.007. In other words, it was practically zero disaster adjusted r square was 0.007 in other words it was practically zero no improvement but then we did polynomial regression in polynomial regression it turns out that it is a good fit but we do have one problem we have this wagging tails you know a lot of oscillations in the periphery you see this oscillations in the periphery or the borders You see this oscillations in the periphery or the borders. That is the ranch phenomena. And when you see this, it gives you a hint that perhaps polynomial regression is not a good approximation to this or not the right way. It's a good approximation, but it is perhaps not the right way to model this data set there may be a better way to do that those better ways will learn in due course of time in fact today itself will learn and there is a topic called regularization which we will start with in the workshop number two how to deal with situations like this so by the way uh one more thing that prachi mentioned. Those of you who are going to take ML200, please drop a word to Prachi. And I will open up the registration today itself. It will continue right after this workshop. There will be at most a one-week break, which I will keep for people to have doubts. I mean, after I officially end this workshop i'll keep one week i will continue to hold just help sessions so that people take help with their labs right for whole one week at two sessions will be just for help and then right away we'll go into ml 200 intermediate data analysis so we did this data set right i hope you guys have finished the first one so this is the python version of it uh the best i would suggest is do it and then reach out to me as you need help what we are going to do today is a different topic we ask this question we are going to do today is a different topic. We ask this question. We are approximating the data as polynomials. For example, when we get this data, data set 2, we approximate it with a polynomial. But what if you can just stare at it and say, you know what, I know what mathematical function this looks like. It reminds me of something. Now, this phrase, it reminds me of something, is perhaps one of the most potent tools you have in machine learning, especially in univariate machine learning. Because we can visualize the data very easily in two dimensions, it often gives you a hint on what the function could be, the generative function behind it or the relationship could be. When you look at this relationship, it may remind you of trigonometric functions, sine wave, cosine, and so forth. So you might ask, why are we doing linear models when what we should really be doing is asking this question, what is the amplitude and what is the wavelength of this sine wave? Because if I can tell the amplitude and wavelength of the sine wave, I can essentially, I would have essentially characterized the relationship, the function. When you ask that question, when you say that it reminds me of something, and let me try if I can fit that function to this data, you are doing something called nonlinear least squares. In other words, you're going beyond linear. You're saying here is a sine wave, for example. If you look at this, what is a sine wave? It is this equation. I hope, well, I made it too big. This is the equation of a sine wave, isn't it? A sine kx plus, and then of course the epsilon is the error term. Do you guys remember the sine function? Anyone remembers the sine function? anyone remembers the sine function the a here is the amplitude a k is the sort of it's the wave number it is the reciprocal of the wavelength times two pi and x is of course the x-axis y is therefore a sine kx. Now, this usually happens. You infer it if you have some recollection of your mathematics, you know, basic mathematics and algebra. If you remember, then it says, you know, this is reminiscent of that. So this approach of nonlinear least squares, while it is very powerful, it is, I would say, optional. It's very powerful, optional. If you get it, it's great. If you don't get it, no harm done. We will, in machine learning, learn a lot of topics so that this approach of just guessing the function is only one of them. But there is a pleasure in being able to guess it. And when you guess it, this thing, analysis becomes very straightforward so how does it become straightforward let me explain what this function is here I've explained a is the amplitude case the the wavelength is associated with wavelength it's called the wave number we leave it now both in R and in Python there are there is a support for nonlinear least squares. It's actually one of the things that most machine learning books don't talk about, but it is one of the oldest tools in the arsenal and heavily used by physicists and applied mathematicians and so forth when they deal with data and they make the models, they fit models to the data or functions to the data. Let's see how it starts. The way it works is this. Are you guys able to see my screen? Sorry, fit width. Let me look at it one page at a time. with fit. Let me look at it one page at a time. If it page continuous mode, single page. Yes. Is the text quite clear and visible to you guys? Yes, as a for me it does. Okay. So there is a library in r called nls tools there what you do is you prepare a formula you say this is my guess y depends on a or y is a function of a times sine kx now x and Y are the data a and K are the model parameters the amplitude in the wave number and therefore the point of this machine learning exercise or this modeling exercise is just to find what is what would be the best a and the best k value so here you prepare the formula. Sine wave is the formula. Then what do you do? You try to fit. You create a model with this data, like X and Y are given as data, of course. Then you say, let's go and model it using this formula. And you start with the initial guess for A and K. Those guesses may or may not be right. But it is worth having a guess so you say a is equal to 1 k is equal to 1 setting them to 1 is usually a pretty good guess in this situation that is it in two lines you do this and after that the model is built and then you can view it and it's very very powerful guys that you can build a nonlinear model in just two lines like this the thing is let's see how good it is once again we say that a it turns out was approximately 1.32 that's good and k was 0.99 pretty good the p values look good the t value is the t value good or bad the t statistic here would anybody like to comment is the t statistics here good or bad remember guys i'd mentioned the t statistic or-value above 5 is generally beginning to look good. Our t-statistics above 10 is pretty good. Here, our t-statistics or t-value is 56 and 189, 190. These are very, very good numbers. Now, in this, you can't have the null hypothesis. There is no concept of the R squared. But what you can do is you can see the correlation of Y to Y hat. How close is Y to Y hat? You know, the prediction and the reality, how closely correlated they are, it's important. The other thing that is worth observing is that if you look at the correlation matrix, it would be bad if A and K are mutually correlated or they're mutually dependent on each other. So what you want to do is in the correlation matrix, see that the off diagonal elements are small. The principal diagonal is of course 1, 1. A is perfectly correlated with itself. A is perfectly correlated with itself. K is perfectly correlated with itself. But A and K have barely 16% correlation. That sounds like noise. So low correlation across the parameters is a good thing. One other thing that you observe here is that this converged very fast. It just converged in three cycles. Usually when an algorithm converges in just three cycles or three iterations, it means that it is doing pretty good, right? And it found the right answer. And after that, it wasn't budging from the right answer it was one part in million you know 1.96 or two parts in a million the the variation from the right answer was this tiny bit after a little while so it is a pretty good thing now when we look at this value, it is beginning to look good, but let us see the residual plot analysis. First thing, how do the residual plots look? When we look at this, do you guys see any pattern in the residues? Is there any pronounced pattern in the residues, guys? No. No. So that is a good thing. Do we see very outlier residues? So remember, residuals, if they have large outliers, that again becomes a thing of concerns. Now, you remember that outliers are defined as anything beyond three standard deviations so in this picture which is on the top right you see that most of the residuals are below two so in other words there is not a single residual that seems to be beyond two so nothing's certainly beyond three or minus three and so there are no outlier residuals either, which again is very good. Then you look at autocorrelation. Autocorrelation I have not explained, but let me explain. Autocorrelation is when a value is correlated with its own previous value. For example, if you keep on measuring temperature during a day and you take the temperature measurement every minute outside under a tree or something like that you would you would agree that the temperature at any given time at noon for example would be highly correlated with temperature just before noon you know one minute before noon at 1159 and the temperature at 1159 would be very close to the temperature at 1158 so a value is correlated with its previous values and that is called auto correlation called autocorrelation. Now, autocorrelation in residuals is a bad thing. And so here is a plot of a residual value and the next residual value. So you're seeing the next versus the present value. And you don't want to see any relationship between these two. When you look at this, are you guys seeing any relationship? I hope this looks like noise. There is no pattern in these residuals this looks like noise. There's no pattern in these residuals, isn't it? There's no obvious autocorrelation. The next plot, and by the way, these are famous plots in linear regression. They're very important plots, and it is always worth finding these out whenever you do your analysis. This is the next plot, Q-Q plot it is always worth finding these out whenever you do your analysis. This is the next plot, Q-Q plot it is called. At this moment, it's called the quantile-quantile plot of standardized residues. What it means is that, I'll just give you the intuition without going too much into the Q-Q plot. that, I will just give you the intuition without going too much into the Q-Q plot. Basically, what you want is that this axis, this line that is there for the Q-Q plot, you want most of the data points, residual data points or this quantile points to be hugging this diagonal line. If it is not hugging, except for in the periphery, except in the corners, outer edges. In the outer edges, there will be some deviation you can't help it but if the bulk of it is hugging this line it is a good situation so if and when you contrast that with the the qq plot in a bad situation then i believe in the previous chapter we had, it was not hugging this diagonal line at all. So, in this one, it seems to be hugging the diagonal line pretty closely. This would be called a pretty good QQ plot. So, all of it confirms what? All of it confirms that our model is probably not wrong,'t it after residual analysis what do we do one final thing we do we need to plot our prediction onto the model let us go and the same thing we will plot our predictions onto the model and when we do it how does it look like a good faithful fit, guys? Would you say this is a pretty good fit? You look at this and you think, yes, this is a good fit. It's a pretty good fit. And now instead of R squared, the best you can do is you can find a correlation between your prediction and the reality. Why hat is your prediction? Why is the reality? And it has an agreement of 98.5%, which is very good. It's a very good thing. And so what did we learn from this exercise? So what we learn is, if you can guess what the underlying function is, what the generative forces or the dynamics is, it you can guess what the underlying function is, what the generative force is or the dynamics is, it is almost the gold standard. It will beat anything. Its residual plots will beat your polynomial regression. It will beat anything in the linear model family. This really will take the cake. Not only that, you have advanced understanding. When you say that what i am seeing is a sine wave something like this you have actually said that i i believe that the generative force or the laws behind this data are this equation and so you have just discovered an equation that describes the data very well and that is a very good thing are we together now this is important guys because these days and this is obviously a little bit going tangentially one of the big debates in this field is and you can't believe that it's happening. People are asking, do we still need science? The concept being that today, the AI algorithms, the AI machinery of machine learning, the whole machine learning apparatus can take data and can make very good predictions. Considering that it can make very good predictions, why do we need scientific theories at all? So one gentleman, Anderson, he wrote a fairly influential article once. I will forward you guys the article if it interests you. And it was really talked about and it spoke essentially of the death of science or the lack of need to do science at all. And it's an interesting perspective, I'll let you think about it. Then obviously a lot of people began to agree with it, right? And it became very fashionable to say that the new science is data science and all other sciences forget about it. Then not everybody agreed, people like me don't quite agree. We still believe that it is just not enough to make accurate predictions. You should understand why, what really is going on, what is the dynamics behind the data, what is the equation explaining the data. So the old way is, of of course you try to derive an equation or approximate it and when you do that you learn something you learn something about nature about the forces or about the financial markets or biological systems or whatever right so and again people disagree so one of the lovely books that recently came out was called the AI Delusion. It sort of deconstructs this whole belief that all you need is prediction models and you don't need science. And I'll let you guys form your own opinions if you want to Google that up and check it out about this. But these things have profound implications, guys. Do we build only models that predict or do we want explainable machine learning a machine learning that we can explain as human beings we can say this is the reason why or we believe this is a hypothesis which is explainable with the equation or with something and is that even needed I fall into the camp, which is strongly in favor of explainable AI. A lot of people feel and have been feeling for the last 10 years, many years, that explainability is nonsense. All we need is good predictions. So you can form your own opinion, you can read all these articles, you can, and there are people who are halfway between, you know, it's a whole spectrum of beliefs. You can think about it, but as we move along, I will emphasize interpretability and explainability of our machine learning models very highly. It is also important from a legal perspective. Let us say that you have an algorithm that hires people. It gives them a test of a barrage of questions, 100 questions, and based on how they answer those questions, it goes and associates scores with them, makes a machine learning model, and it predicts whether they would be good employees or not, or competent, high- be good employees or not or competent high performing employees or not if you build a model that cannot be explained that is a black box the problem with the black box is how do you know that it is not discriminating against any of the protected classes for example it's not discriminating against the blacks or the indians or the asians or the hispanics and so on and so forth in the United States or against the Whites. You don't know. You don't know whether it has a gender bias, whether it has a bias against disabled people. And these biases are very subtle. Quite often in the words we use, our biases show. There are tools that will tell you whether the paragraph that you wrote has a strong male gender tone or it has a strong female gender tone. And these things are effective in our responses to them. And so it would be worth knowing that did you reject this person simply because the paragraph that the person wrote overall had a female gender bias, but the machine could not explain it. It just went for the fact that it likes people with a strong male gender, male tone or something like that. It could be, I'm just giving a hypothetical situation. situation. Increasingly, it has happened that as people are studying a bias in machine learning models, it has become a huge issue. We are discovering more and more that these black box models are a huge problem. They are a huge problem when it comes to bias. And we really need to watch out. This is a very powerful tool. If you don't use it carefully, it can be very dangerous. So interpretability and explainability is one of the core things that helps you understand whether your system does or does not have biases. Because your underlying data may have bias that it has learned from and biases are often very subtle which is why they are sort of discovered a posteriori you know you build a model you unleash it onto the world it does its damage and then somebody notices some researcher notices that you know it's having an adverse impact on protected classes. One has to watch out for that. Now I'm going to do the same analysis in Python, not very different. All of these, by the way, guys, do you notice that this entire modeling was just three, four lines of code? You include this library, two lines, three lines of code here, that is it. To generate the residual plots again two lines or five lines produces your complete analysis this speaks to the power of this modern machine learning tools whether in our or Python how absolutely powerful and elegant it is or try doing it in a low-level language like C C++ Java or whatever it is and you'll realize how difficult or how much of territory you would have to cover to create something like this it's a long journey well it's it's a lot of code it will be a few hundred lines of code so that way get used to these libraries guys machine learning is not so much about coding as about understanding because the code is very very small always the same thing we do it in Python. We write the equation in the Python version. I also added one more extra term. It's called the phase lag. Y is a sine kx plus y, right? One more parameter we added. So now it is a three parameter model. Once again, what we do is we define the sine wave here to be this, right? It is a times sine kx plus phi. If you compare it to this equation, this equation, this equation, y is equal to a sine kx, you will realize that all i am doing is quite literally writing the same thing in code in this particular line do you see this guys the line that i have highlighted is exactly the same as equation 2.4 that way both r and python in this respect are very very expressive except for the star and the word np. This thing, it looks almost verbatim the math. I hope you can see the simplicity of this, guys, how simple it is. And so if I were to zoom out, if I were to again start with some initial guesses, and then I say, go and fit the curve, it will fit this data. Once you have that, what are the things that we want to find out? K, Phi, and A. Now, this is just some pretty printing options when you print the thing. A standard Python, I could have just printed the value as it is, but then it would be too many arbitrary decimal places. It looks a bit ugly. We don't want to print so many decimal places. So Python gives you a way in which you can set the printing options. You can say that for floating point numbers, for numerical values, use a precision of five. And this is standard string formatting syntax. Those of you who are used to it, can use it. See, it is not necessary. You can just say print A or print K. That would have been good enough. But if you are not used to the fact that in Python, you can pretty print text quite well. This kind of a syntax, curly brace, colon, less than five, and so forth. What you're basically seeing is that the first one limited to a precision of five, right? That sort of thing. And for all of them, the three of them. So now, at the same time, this returns you two objects. One is the parameters, the value of the parameters. And the the second is it returns a call called parameter covariance matrix are they correlated very much similar to r so what are the values of the parameters a is 1.32 k is 1 and phi is close to 0 practically which is exactly what we found in the r code are we together by the way guys I am holding out on giving you the solution notebooks you know the Python notebook with the solution in the R notebook at the solution simply to give you guys time to I work on these labs yourself before I release solutions let me know I'll eventually release all these solutions but when you guys have all done it, done the lab, or tried to do the lab on your own, let me know. Then I'll release the solutions. Just as I release the solutions for the exploratory data analysis, I'll release the solutions here also. So we can see the covariance matrix. Once again, it's mostly filled with zeros, which is absolutely lovely. Zeros. Now, in Python, it turns out that things like the confidence interval, et cetera, et cetera, are not computed on your own. So you have to sit down and compute it yourself. So how do you compute it? You go and open the ISLR book. You look at the definition of the confidence interval. How is confidence interval defined? And then you write it up as code. As simple as that. And you can do that, though I would say that's not necessary. Most people these days, they don't worry too much about confidence intervals. I mean, it is a good thing to do that. People often skip it. skip it the biotins of course believe that it shouldn't be the confidence interval it is misleading but one way or the other the 95 percent confidence intervals are pretty good so what what is the last thing we want to do we want to also create a correlation i mean the scatter plot between y and y hat right are they correlated or not so we get this plot would you say y and y had to have a very strong correlation guys strong positive correlation again 98.5 correlation very strongly correlated means we seem to be heading towards a good model so far nothing, nothing is saying it's bad. Let's look at the residuals plot. Once again, if you're doing it in Python, you'll have to create it on your own. Residuals are nothing but y minus y hat. And so we can create a scatterplot of y hat against residuals. Do we see a pattern here? No, we don't see a pattern here. This looks pretty good. There is no anything. Now, we don't see a pattern here. This looks pretty good. There is no anything. For example, there is no heteroscedasticity here in this model. Looks good. And then finally, we plot the predictions against the data. And when you plot the predictions against the data, it's again a pleasant thing to see that the agreement is pretty high. It's a high-fidelity model. Would you agree, guys, that it's a pretty high fidelity model that we are looking at isn't it maroon line is the actual model and these green and blues are the confidence intervals so far so good guys that's that and this is the code like plotting code is always a little bit more than the actual machine learning code it's one of the things you'll realize that in data science there are two aspects what is the machine learning itself and that generally the code is very scarce it's pretty brief and quickly you get to the result. But then when you try to visualize the data, then art comes in. Art is always verbose because you try to make the plot look aesthetically pleasing. You want to give it some good colors, axis, this, that. You end up writing very straightforward code that you have to get used to. You have to learn python enough to create pretty plots and actually the plotting code tends to be a little longer and more complex than the actual machine learning code itself which tends to be very straightforward in general whether you're doing deep learning or you're doing simple you know linear models as we are doing or you're doing simple linear models as we are doing. Generally, machine learning code is very, very short. So because the subject is not about code, it is about understanding data and how do you approach data and learn from it. All right, guys. So that was that example of nonlinear least squares. What I would like to do now is just talk about this thing. Right. About R versus Python. What I would like to do now is just talk about this thing, about R versus Python. So I'll read out this thing. You may or may not have read it. I'll just read out this paragraph that I wrote. See, in repeating the nonlinear least squares curve fitting once in R and next time in Python, we observe the close parallels between the R and Python code. As we have seen in the previous labs too, this is a recurring similarity. In general, once you have become familiar with the syntaxes of the two languages, translating data analysis code written in one of the languages into the other becomes more often than not a straightforward exercise the exceptions to this are those cases where support for an algorithm happens to be in one of the languages itself so for example you know there's a new algorithm and the creator of the algorithm has just written a python library only for it then well of course you better go and use python because there is no r equivalent. Or there are quite a few libraries at this moment still, which are done in R and they don't have a Python equivalent. People haven't gotten the time to translate or create an equivalent in Python. And so you have to go and use R. It's a fact of life. So in order to develop fluency with data analysis in both of the languages, it is instructive to repeat the lab exercises in both. Simplicity is in the eyes of the beholder. For common data science tasks, there is brevity in R code. And if you believe that brevity is the soul of it and has more simplicity, then R code will certainly appeal to you. Those not familiar with it it on the other hand tend to gravitate towards python since it is a general purpose language in widespread contemporary use for all sorts of programming situations therefore it is often the case that many have past fluency with python which makes it easier for them to analyze data using python libraries despite the resulting code being quite often a bit longer you guys saw that right python code tends to be a little bit more verbose a bit longer our code is once you master it it is almost exemplary in how brief it is and how much it achieves in just a couple of statements but the downside of python is you have to learn python whereas most people already know pi i mean sorry if for downside of r is you have to learn r whereas for python most people already know python so they ask the question why bother learning a new language when I already know Python and I can do machine learning in Python? Well, I would highly encourage you to learn both. R is pretty elegant language. And the libraries in that are very, very elegant. And of course, we have been seeing examples of that in the exploratory data analysis chapter. There were excellent libraries in R and there were excellent libraries in Python. So data analysis chapter. There were excellent libraries in R and there were excellent libraries in Python. So pick up both. Since both of these languages are in widespread use and each can be learned without excessive effort, the author would strongly urge the reader to ensure that she comes to be fluent in both. When a data scientist describes either a new algorithm or the results of an extensive data investigation, it is customary for her to show the analysis details as code snippets written in only one of the two languages. Whichever happens to be the data scientist's favorite language becomes her language of analysis. It is assumed that the reader would be familiar with it. The community is broadly split in equal measures between the two languages. Therefore, not knowing one of the languages becomes a significant impediment to understanding the work of the other practitioners in the field. There will be quite a few significant books and articles that you will struggle to understand if you happen to know only one of the languages and the analysis is expressed in the other. So that's the thing. And you know, more informally, Murphy's Law will kick in. Rest assured that the moment you hear of something some very exciting development in machine learning and you want to go and read the paper or the article on that particular topic invariably it will turn out that it will be the examples the code examples will be in the language you don't know so if you happen to be a r expert you will stumble across good research that you really want to understand, but it will be done in Python. And if you only know Python, sooner or later what will happen is you'll come across excellent articles that you want to master, techniques and libraries, and it will turn out that the only coverage for that is in R. So you should know both. And the code is so similar between the two languages. They're so close when it comes to data science. There is no reason not to know both. I hope that is one lesson I have been emphasizing to both of you, to you guys. Now let's look at this other example we did with sine wave. If you look at this data, what does your instinct tell you? What am I looking at? wave you look at this data what does your instinct tell you what do i what am i looking at if you're familiar with curves you would say this to me looks like a bell curve approximately would you guys agree that this sort of looks like a bell curve if you say that it is a bell curve then why are we approximating it with a linear or polynomial model? Because when we build a polynomial model, if you recall, what you end up with is the wrench phenomena or the wagging tails. So these oscillations in the peripheries, not a good thing. So polynomial regression has its demerits. So the question is, can we do better than that? We can do nonlinear model. We say, hey, it is it is to me looks like a bell curve these are the different shapes of a bell curve for and in a bell curve there are only two parameters one is the mu where is the tippy tip hilltop of a bell curve or the center of the bell curve it is given by mu people call it the mean of the bell curve, mean value, which is the center. And the other is the variance. How spread out is the bell curve? So let's observe this. This green bell curve, where is the center of it? It is at minus 2. And what is the variance? Well, it seems to have a pretty significant variance. It's a little bit spread out compared to some other ones so this is it green one is centered at minus 2 and it has some variance contrast that with the blue one which is positioned at 0 but how much is the variance it's very low it means it's very peaked at the center as opposed to let us say the yellow one yellow one is also centered at zero but see how spread out it is you know it is uh you have to go pretty far before you get to most of the bell curve and so the variance is five that's the intuition of a bell curve bell curves are more formally called a normal distributions and they're a multi-dimensional representations i usually call a bell hill informally but the more formal term would be again normal distributions of gaussian distributions okay so um and i would encourage you you know when you have these words normal distribution etc have one word a formal word in your mind and also i tend to find it easier if i have some very intuitive or may not be perfectly accurate but sort of very intuitive name that is evocative a bell curve is very evocative you know what the shape of the curve is you'll remember it much more so what does a bell curve look like oh goodness bell curve is a transcendental function and why it is written like this there's an amplitude part of course how high the bell curve is there is it is written in terms of mu and sigma this equation looks complex and the nature of transcendental functions is they do tend to look complex however if you remember from the lecture notes i explained what these things are it is X X square what it is a mean and Sigma so I'll leave it amazing thing is while the equation looks so complex the code looks exactly the same almost no change see what did we do we wrote a formula the same formula we converted to this here you notice that we did that and the rest of the code is exactly the same we build an nls model with this formula we make initial guess for the values one one one is pretty good to start with why not right and uh sorry in in in a, we have a model. That model once again looks good. The T values are very good. Eighty four, sixty, seventy two. The P values look good. Right. And the confidence intervals are pretty tight. It says that A is between point nine five and one, which is a pretty tight value. The correlation matrix again the off diagonal elements are not very strong though little small values are there then what do we do we go and look at the y versus y hat plot you know prediction versus reality plot and when we do that it does seem to be it seems to be having an even better correlation. Do you see 99% correlation? Now, remember, guys, even with polynomial regression, we did not achieve such a high degree of correlation between y and y hat. So this model trumps even the polynomial regression that we made. Now let's look at the residuals. How do the residuals look like? So there is again the same function. You look at the residuals. Are you guys seeing any pattern in the residuals here? Not much of a pattern. You don't see heteroscedasticity or anything here. Do you have outliers in the residuals no it is between minus 2 and 2 remember outliers are beyond 3 or minus 3 you don't see that likewise autocorrelation do you see any autocorrelation no this looks like noise pretty good what about the QQ plot well the data seems to be having hugging the diagonal axis for most of the places, right? Except that it's flying off at the very end, which you expect. So all things considered is beginning to look not wrong. at least it seems to be somewhat it's so effective model i wouldn't say not wrong but it's effective model now let's put the plot the model against data do you guys see what a neat graph you get how lovely it looks it is way better than what you got with polynomial regression so for example let's look at the graph that you got with polynomial regression this is the graph you got with polynomial regression. So for example, let's look at the graph that you got with polynomial regression. This is the graph you got with polynomial regression. Do you see that it has a problem of oscillations in the end? Do you see that, guys? You have these oscillations in the end, right? Yes. Yes. Or whereas, when you look at this new graph that you have it is it is i mean this is what you would call a beautiful model completely nice smooth curve with no unnecessary oscillations at all and that is when you feel that you have done a good job in modeling this data right and remember where did we start with this data when we started with a naive linear model it was a complete disaster our r square was zero practically like 0.007 or something like that which essentially when you get a value like that you throw in the table so systematically by using better techniques we have now ended up with a model which from having no predictive power to such a high predictive power now such a good model right which again speaks to the fact that when you deal with data and you're not getting results it may not be so much in the beginning because there is no signal but it may to a large extent be because you're not looking at the data in the right way behind data there is always a generative force it is quite possible that the predictor has nothing to do with or the target has nothing to do with the predictor the null hypothesis but generally if the data has if you believe there is truly a relationship you should make every effort to try to build as good a model as possible. If in spite of all your best efforts, you still can't build it, then you begin to suspect if the null hypothesis is true. Namely, there is truly no relationship between input and output. So that's that. Now I'll redo this thing in Python. Once again, this is the equation. How do you do that in Python? Again, just a few lines of code. This is it. You define the function and then you pass it to the curve fit. Very similar. As you can see, Python code is much more verbose. There we could do it in two lines. In Python, you have to write, frankly, much more code to make the whole thing work. But not too much code you know it's just still fairly reasonable and when you do that it works let's see if it works the same results are achieved pretty good of value i won't go more into it i haven't plotted it and so forth because it seems redundant at this point we are getting the same level of correlation. Likewise, we go and visit dataset 5. Dataset 5 is like this. It's an interesting dataset. You ask yourself, what does this dataset remind me of? And by now, remember the question in nonlinear least square is this, what does it remind you of? So you might ask this question, how in the world would I know? What does it remind me of? The way to that is, generally, you have to become friends with a lot of functions. There is a, I would like to illustrate this with a story. there is a i would like to illustrate this with a story uh at one time there was a problem posed in europe by the bernoulli brothers right there were two brothers called the bernoullis and am i talking about the bernoullis i think so okay the two bernoulli brothers. I hope I'm not conflating it with another mathematician brothers. But anyway, so imagine there are two brothers, which I believe are the Bernoulli brothers. Now the trouble is they were both brilliant. But people didn't know that there were two of them. And so they would write some great mathematical result. And then praise may accidentally, half the praise may go to the other brother or credit may go to the other brother because people used to keep getting confused between these two uh brothers and to make matters worse they even had a nephew who too was a great mathematician after some time and so after a little while people got completely lost which of the brothers are we really which of the people three mathematicians are we really talking about so it again speaks most of most people didn't even know that there were three of them right we only know the last name and you remember that if you remember if you know their names at all now what used to happen is between the brothers they used to be a pretty healthy rivalry so they used to pose puzzles at each other for the other to solve they should challenge each other so one of the challenges they thought they feel was a interesting mathematical challenge I would go into it it's about how fast can a body fall what is the best path for a body to fall from a place a to place B where a is higher than B, but not directly above it. So what is the fastest path and so on and so forth? So shortest time path and so forth. So that puzzle was interesting, and a lot of people got interested. They started trying to solve it. Then Newton heard of it. Newton obviously was very very competitive of of the european mathematicians he used to just dismiss them as used not to be taken seriously so when he heard about the puzzle he got interested except that publicly he was saying you know this is nonsense so then people submitted the solutions and one solution came which was very elegant and was it jacobi brothers by the way barnouli i keep saying maybe it's not barnouli it's the jacobi brothers anyway uh we need to go back and look at the history i might be completely uh mixing up the names but okay let's let's take jacobi brothers maybe so one of when he looked at the solution he immediately said something famous he said you can always recognize a tiger by its claws he and what he meant is he could recognize the genius of Newton in the solution that was presented even though Newton's name was not attached it was obvious that it is that genius who has submitted the solution and it's a classic statement in history and I like to refer to it because it's a it's something like that these functions that you see in mathematics they are not just dull facts they are elegant they're beautiful they're marvelous you know they're they're like just i mean absolutely marvelous and beautiful functions and if you could just take one family of functions, let's say here the gamma distribution, and just say that this month, you know, I'll just look up the Wikipedia article. I'll read up a little bit to give half an hour, one hour to understanding what that function is so that you remember it. You will be quite surprised by the time the year ends, you would know 12 families of functions. And so you will see them all around you. You know, it becomes very interesting when as a mathematician, you're just going for a walk. All around you in nature, you will find those functions. Literally in front of you, written large on the landscape. And it's sort of, as a child, I used to always wonder, will I be able to understand the world? What does it all mean? What is it behind all of this? What's the feeling of it? That's what brought me to science. And I must say that the mathematics behind physics and behind computer science and behind artificial intelligence is sort of the core of it. I mean, it's sort of, it gives you a very deep understanding, not just of the individual subjects, but of nature foundationally itself. It's very elegant. very elegant. Somebody wisely said, I think it was Einstein, but I mean, again, we're mixing up the name, when he said that when I look at mathematics, when I look at these things, I can read the mind of God, you know. The God is essentially a mathematician. The whole world has a mathematical foundation. Why does it have? Nobody knows. It need not have, it need not be explained by mathematics. But as you see the world and you try to look behind, under the covers of what is it, how is it held together? How does it work? You always find beautiful mathematics there. And these gamma functions are again, like all of these you see, if you can spot the function and say, here is the day here is what i believe is really going on you will do far more powerful machine learning than most people can dream of you can actually walk into a room where people are struggling sit down write an equation quickly write code fit a model to that and have people in occasionally not always but occasionally when it works you can have people absolutely amazed it feels as though you just pulled a rabbit out of your hat because they may have been trying using all the traditional tools and they would never have thought to think about it more abstractly so there's a value in that kaizen which is why i'm explaining these things to you so there is such a thing as a gamma function it has two parameters k and theta so you can see that when k is very high which is the black line it almost looks like a bell curve theta is small when k is low and theta is big it is more like it completely looks like a and theta is big it is more like it completely looks like a reciprocal distribution but then in between stages are there this K is equal to 2 and theta is equal to 2 makes it look a little bit like a log normal distribution skewed bell curve and so on and so forth you see these distributions so when you look at this guys does it remind you of? Would you say that if you knew the curves on the right and you look at data on the left, your first reaction would be, aha, I might know. I might be recognizing the tiger by its claws, you know, I might be recognizing an old friend here. Would you say that? Look at the curve on the left and would you say that it has a close resemblance to the curve on the right? Anyone, guys? Mostly with the value k equals to 2 and theta equals to 2.0. Yeah, that's right. It begins to look quite similar. And it, you know, at the end of the day, you're just guessing. But this is how this field works. You hypothesize. You say, OK, let me try this and see if it works. Remember, guys, in machine learning, there is no such thing as the right answer. There's only such a thing as an effective model that makes good predictions. So let's try that. Now, the gamma gamma function you'll have to open a book and see now what is the equation of a gamma function well you don't need to it turns out that the libraries for example this Python library it it already comes with a gamma function built in so I defined a function called gamma like because I also put a amplitude part there gamma like that's all the gamma function i multiplied it with the amplitude once i do that i took the input and did that this is again the formula rest of the code is exactly the same all you have to do is use this gamma function and then ask now ask the system to fit the values and see what it produces oh it produces immediately a value of k is equal to 10 a is equal to 2 b is equal to a and b so what does that mean right let's go and see what it means a k is the amplitude so we can ignore it and so it is saying a and b are 2 2 let's go and look at this graph and look at 2 2 where is 2 2 yeah is it the green line 2 and 2 are the green line isn't it guys this green thing is this now i let you compare and see is this green line a close approximation to your data would you guys say that it is guys are you there yes any such things more than happening right it's pretty close approximation and so you say voila you know you got it so nice and you look at the correlation matrix, everything. What is the correlation between Y and Y hat? A perfect correlation. I mean, it can't get better than that. Well, it turns out that this data set is unusual because when I was preparing it, I forgot to add noise. There's always noise in the data. I was trying to illustrate the point. And so I did not add noise because there is no noise, because this data is pure signal, obviously the correlation is 100 percent. You have literally hit the nail on the head, absolutely on the head, you have hit the nail. Let's try to plot our model on the data and we plot it. Look at this. Do you see a perfect match between the model and the data? Isn't it, guys? By the way, it is not bell curve residuals. It should be, OK, the legends are wrong. It is not this. It is a match of prediction versus data. What do you say, guys? See, there are occasionally situations like this that you will be struggling. If you remember, linear regression was a disaster. Polynomial regression worked, but it has range phenomenon. It had oscillations and so forth. It sort of worked. But, you know, when you make a guess like this and you end up hitting the nail squarely on the head it is exhilarating it's just absolute pleasure to look at the data that you didn't understand just a couple of hours ago and to reach a point at which you you feel that you really found out what's happening now you say why in the world would I worry about gamma functions and things like that these things you know the sine wave the bell curve the gamma function they all have one thing in common can you guys tell what is common to the sine wave the bell curve and the gamma function anyone these are called transcendental functions transcendental functions are functions that cannot be represented as polynomials of finite degrees you know polynomial or third fourth fifth degrees or something like that if you try to expand it in polynomials it will have infinitely many terms which is the real reason why you have the wrench phenomena those oscillations when you try to do it by nature transcendental functions cannot be represented as polynomial now you say well so what why are they so important it turns out that transcendental functions to a large extent is the alphabet in which much of the world is written you go back and look at the equations, the quantum equations of electron clouds around the core and you will realize that... What's happening? Are people dropping off? We seem to have some network issues. Okay. Is there anybody here in the meeting at this particular moment have we lost everybody uh i think most of them no i can hear you we can hear yourself oh you guys can be and you can see my screen also yes okay guys in between do keep giving me some feedback anyway what i was saying this is a purely personal person today i'm giving you a lot of my personal perspective or how i look at the world to me uh i'm sure once i can do it and i've got a quick demonstration here about i've always wanted to be a teacher i've been teaching yeah so to me these transcendental functions are like the alphabet the language in which much of nature and dynamics and anything you do, even the financial markets or man-made things, any piece of engineering, if you ever model it, very, very often you'll find that your model will contain, your theories will be, nothing will be made up of transcendental functions. Think about it you drop a pebble in the water what do you get you get ripples those ripples are sine waves right those are trigonometric functions circular sine waves going spreading out those ripples you can see that they have troughs and crusts and troughs and crests now you also notice that the amplitude of that keeps decaying with distance. That, too, is a transcendental function. I believe it's Legendre function. The drum you play are, again, given by transcendental functions. I'll let you discover which one. The electron cloud that is around the core of our atom you know the all of life is described by orbital structure and chemistry and biochemistry that orbital structure the spdfs etc those are actually the notes of a spherical harmonics those are the spherical harmonics of yet another transcendental function called the Bessel's function. So if you master transcendental functions, just pick one a month, you'll be surprised that you can enter so many areas of science and engineering and medical biology and chemistry and find yourself in familiar territory. You can look at what people consider the hardest part the math of it and to you it will look very familiar it will look very easy to grapple with and learn right so it's a very powerful thing and i won't say more about it but that is it so that is the context of transcendental functions so guys i would like to take a small break and then I would like to move on to real life data system examples, multivariate data systems. Let's take a 10-15 minute break. So here's the thing, before I take a break guys, do you want to call it a day? Like keep the nonlinear, this data sets, real world data sets for next week or would you rather that we do it today any voices as if I would want to take a like to do it next week so next week is okay this has already been quite some time we have been talking for more than an hour over a half. So it's all right. I'll stop the recording. Before I stop it, guys, I hope you guys were all unusually quiet and I can't see you. So I'm not getting any visual clues or audio clues on whether you're understanding it or not. Can you guys tell me, is it clear? Are you guys understanding this? The code will take some time to digest, right? You'll have to repeat the code, look up some documentation. But the foundational concept that you can guess a function and then try it out and see if it describes your data. I do parameter fitting. That's the core idea. Are you getting that idea? here yes sir very good so with that I'll just stop the recording guys and obviously I'll post this recording to the YouTube right after this let me stop the recording