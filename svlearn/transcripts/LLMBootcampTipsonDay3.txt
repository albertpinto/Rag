 All right, folks. If you are able to attend it on your laptop through the Zoom, you can do that. To people who want to relax back on their sofas and watch on the TV, it is being live streamed now. So you can watch from the comfort of your sofa what i'm going to do is after observing different teams and thinking what i should do to help you guys i thought i'll give you some parts of the solution remember some parts of it not the whole of it so any questions before i start or any the context see um one of the questions the two parts to it one is where do i run my ear once you have to ask this question if i write let's say any one of the aips however simple it is let's take the simplest one some of you realized that breaking up text into chunks is not as simple as you think. It needs more careful consideration. The brute force way of doing fixed chunks can lead to a sentence finishing abruptly in the middle of a word or in the middle of a sentence, a context window or a sequence finishing that way. You could then say, all right, i'll retract a little bit either i'll end at the previous sentence or the next sentence or something like that you can do such simple tricks like that and allow for some degree of overlap between the chunks that is uh one way of chunking the other way you could do is you could say that all right what if i look at semantic boundaries of places at which the meaning of sent the one sentence what it's talking about and the what what the next sentence is talking about changes significantly you could try that in theory it sounds good in practice you will have mixed. There are no perfect ways to do that. It's a very active area of research. And in a way by asking you to do a semantic chunking, I wanted to give you guys a window into the complex world. Guys can you please mute yourself? Dennis, can you please mute yourself? Yeah. So in this complex world that AI really is, when you try to solve a problem carefully, it gets significantly harder the more you think about a problem. It just is like people have told me that searching through the documents these days is just three lines. Go to Hugging Face or Langchen, three lines, you can do it. Yes, you can do it as a proof of concept and the germs of the solution are there. But to scale it up is a different thing. We have talked about it many times. Today, what I would like to do is take you guys through some very simple sketch of solutions on that front. very simple sketch of solutions on that front. First, I'll walk through the core idea, and then I will also talk about what are some good practices on hosting it. Is it the right practice that you just run it as a Python code? Should you run it in Flask? I noticed some of you are running it in Flask. What are the considerations that you have to go through so towards that i will first start with how i've laid out the directory so um it is audio yes yeah so if you notice there is a llm bootcamp project uh i think i've i will be releasing parts of the solution today i'll release the inference part the maybe one spark job also i'll release so you have an idea the way this code is structured and i apologize if the fonts look a bit too small i'll try to increase it in size is it reasonably visible on most of your big screens guys all of you are sitting in front of large screens i believe in your different conference rooms anyway Yes, that's right. Okay, so I'll start with something very simple. I have the Commons, which basically contains some basic IO aspects, like for example, saving a model and loading a model, right, and the ability to do that in such a way that you i mean just some basic utility uh some i create special data types instead of using str for classifier i tend to use um for url i've created some types uh Python types, URL, classifier label, classifier name. You don't have to do it. These are all sort of think of it as syntactic sugar. Exceptions. It is always a good idea to have your own exception classes. I have a few exception classes. My root is the SV error. And then you have the missing arguments error, unexpected direct, unspecified directory, because quite often, you know, people may launch your job without specifying the directory in which you would find data. It is helpful if you throw a specific exception named after it, unspecified file, things like that. So some very basic things. some very basic things right and then uh one of the things i noticed that people keep doing that and i've been walking from room to room and i noticed this very common is that people do always you know or open file and so forth defensive coding says that you should not presume anything you before doing anything the the and i would if i may say that good practice is to have preconditions and post conditions before you do anything ask under which circumstances should i do it if you are told to open a file surely you would agree that you need to fulfill two preconditions that the path the string is not empty it points to a legitimate thing and that it is genuinely a file it's not for example a directory or some other thing isn't it it and the third is that obviously it's readable at all if you're asked to read a file you need to see whether it's readable at all now these things these are cross-cutting concerns this will be true irrespective of whether you're writing code for ai or whether you're writing traditional code and so on and so forth so it makes sense a little bit to make them into utilities and obviously i encourage you without going into what i have put in my utilities i would strongly encourage you to think about your library, your code, and ask, what are the cross-cutting concerns and what can you put into a utility library of your own? In my case, I check whether something that claims to be, that is supposed to be a directory is really a directory or not. Right? And so what is it? You check whether it exists, whether is a is truly if the path exists it's not a file for example it's truly a directory is the directory readable is the directory writeable because if you're asked to write output to a directory you have to check whether you have permissions and whether you can write to that directory yeah check if the directory is empty ensure that the directory exists right if it doesn't create it file exists right likewise delete a file file is empty these are basic utilities so some so i wouldn't go more into it i will then go into the config you realize that in your case like for example in this boot camp projects series, you will encounter or you will need to create a lot of configuration files. I decided to use the ML and I'll spend a couple of minutes talking about it. I don't know what is off your invalid child element in block. Okay. We are trying to create a schema for it. Something is off. We'll look into it. So the way, oh no, this is a is the schema sorry where is my bootcamp config let's look here this is it so there are many ways that you can store your configuration file you can store it in the any format you can store it in the json format you can store it in the yaml format all things considered yaml in some circles yaml is as good as any other let's just say that which one it is gets to be a bit of a religious war but they're just different stylistic ways of storing configuration do it in json do it in any do it in the database do it wherever you want so long as it makes sense so what are the configurations that we need in this project basic things for example what is my database i took the simplest one for which i posted instructions in the course portal this is the default port username passwords a schema documents this is exactly for the tips that i'd released jdbc driver no surprise then why do I need JDBC driver? Because if I do PySpark, Spark is essentially Java and Scala based. So it will need JDBC driver. What is the format of the JDBC connection? So is JDBC specific within the database? Document directory, which directory are we going to use? So these are all no brainers. Spark, PySpark will need Spark. Where did you install it? Models. models now one of the most common things i notice which is very endemic in english-speaking countries it is a little hard for people to remember that the world has many more languages than english and documents come in all sorts of languages we all do that obviously most of us don't have English as a native tongue. Nonetheless, when we write code, we sort of switch hats and we almost always code exclusively to English. I would say this is a great opportunity for you to rethink that, not do that. possible use multilingual models unless you're sure that you have to only work with english in that case take a language specific model english french german whatever it is because a language specific model would be more accurate for data in that language and of course hopeless for other date for other data whereas a multilingual language will work for about 100 and 180 languages let's say but may not be as good in any one language as a language specific model so those are the trade-offs that you have to make right now i notice that even with basic things like sentence encoder, most of you are using. The mini l six and so forth, which is fine, but no it's limitations it works only with. English so long as your data corpus is only English and you acknowledge that fact that you can only function in English is fine. Otherwise, if you are going to encounter data in multiple languages, then you should take a multilingual language. One hybrid approach that helps, but it's sort of a complication, is that if you have data predominantly in English and a very small percentage of data, let's say 5% data across other languages, then take a forked or hybrid approach. Sense the language of a document first, detect the language, and if it is english use english embeddings create a separate index only for english and then create a separate index for all other languages using a multilingual multilingual encoder and that approach will be actually highly successful if you do that. Consider doing that because then you have the best of performance for your dominant language, as well as you have support for other languages. And that's important. Right? Spacey is a library that we are going to use and you'll see how and why I use it. Spacey is a pretty dominant natural language processing language it has been written in c python is blazing fast it runs quite respectively fast on non-gpu machines also so consider that now QDA quick question do you plan to share this with all of us oh yes of course i'm going to release it all but do not but i will deliberately poke lots of holes in it so that you don't just copy it and say oh i'm done in other words i will make sure that your learning is not impaired you still have to put an effort to integrate it into your system does that sound fair absolutely Absolutely. Thank you. Yeah. So that's that a chunk size. I took 47 I noticed that the chunk size why why is there a limitation. Why do we need a configuration parameter called chunk size guys anyone. Because there is a limitation on the model you can accept. Excellent. Yes. If you use a bird-based model, typically they'll have 512 tokens. Some of them have large 1000 tokens. Some of the newer language models have 2000 tokens and sometimes 8000 tokens. And now if you go with one of the commercial models, you might be talking in 100,000 tokens. But remember, any commercial model is out of bounds for this course. If there is a lesson to learn in this bootcamp, it is that open source community is doing impressively well. It might not be doing as well as the commercial models and commercial houses. If you want to see AI not completely monopolized and become the private beach estate of these rich guys, participate in the open source movement and contribute to it. Support it, use its models, and wherever possible, give back to the community. It's a basic common sense. So Elasticsearch, no-brainer. You put it somewhere, you create an index all of you are familiar with elastic search now what are the services that i create i'd like to talk about the services right so uh this service is let me add some comments to that uh this service is a Vanilla fast API service. The reason I'm distinguishing it is a service does not do AI, does not need AI hardware resources. Now you may say, how is that possible? Would somebody like to volunteer and answer how is it that in an AI app the main doorway the main microservice that receives information can run without AI resources anyone well the only way it can do that is if it delegates all the AI work to other services, helper services. Isn't it? So it's the main service follows a delegation model. It's also called the chain delegates. So what you do is the main services job is to route, to break up a job, a breakup, a task into smaller parts and decide which part needs AI, which doesn't. The parts that do need AI, then invoke another service that is actually mounted upon AI hardware. That way you save. Because in real life, what will happen is this service may be doing typical things, fast API, et cetera. You may be writing a lot of react i mean react js web front end and there must be a lot of business logic and the standard enterprise application development none of which needs ai hardware use and but they they did do a lot of traffic you can scale them elastically in a continuousized orchestrated world very very efficiently and cheaply if cheaply without putting them on gpu hardware gpu hardware is expensive so it is worth doing that and making sure that on the gps you do only the work that is absolutely necessary otherwise you don't do it right so that is one consideration you must have. The world of microservices, typical microservices and business apps world, if you look at the profile of the activity, they are heavily IO bound. They are waiting for data from the database. They are waiting for data from Elasticsearch. They are waiting for this to happen, that to happen. And the CPU utilization actually is not that brutal you don't put a a thousand core or a ten or a hundred thousand core or a like a peter flop machine to serve your microservices it's never done right it's done on modest hardware very cheap easily scaled scaled out elastic on the other hand the moment you talk about AI services, you're talking about very expensive hardware. GPUs are, they have one GPU these days costs practically as much as a house costs in America. You could in the price of a GPU, you could easily go to some of the states of US and buy yourself an estate. So that should give you a sense of expensive GPUs are right. And they're getting only more expensive by the day. So you want to limit, you want to have very focused and tight services on the AI hardware. So what are the AI services that we put? We put these services. One is to clean and chunk the data because cleaning is such a trivial thing we just do basic concatenation etc but the main point is chunking before chunking we prepend a tiny bit of cleaning of the data because when you take data and you run it through let's say ticker apache ticker which most of you have standardized on you'll realize that a pdf comes with a lot of empty lines i don't know if you guys notice that and i don't know if you guys put some measures to clean it out but it's a very good idea to do that right as much as possible clean it out by cheap means like for example regular expression expressions work wonders for non-ai transformations of text regular exp expression should be a go-to thing you should be very conversant with it guys there there's no reason why anybody doing ai should not know regex their books on regex their entire websites devoted to regex their regex helper guides their regex youtube uh videos and the regex is an old old thing whose value has never diminished we'll use some of that but after that we do chunking as you'll see we use a little bit of ai to do chunking semantic chunking now the semantic chunking we do is not too fancy it is still i would consider in in the class of simple ai ideas but it is there elastic search actually i i apologize elastic search is not a service so i shouldn't put it here i should put it in the family of non-vanilla services now sentence vectorizer as you know we use a sentence but to do that a sentence transformer to do that therefore it's a ai service we want to do we want to have a microservice that does nothing but that and who's scaling we scale it in such a way that we designate gpu nodes only for scaling those uh those things right those sentence vectorizers not only that we might want to control the concurrency you realize that unlike vanilla services a transformer has only that much bandwidth for a concurrency you put too heavy a traffic through a transformer the whole thing the gpu will fall on its face however strong it is right so you have to control the concurrency you have to manage how much compute throughput you you you can accommodate and so forth, so for that reason is the first search again and fast index builder, these are the four Ai services so today i'll walk through. The specifically the Ai chung Ai services in the next hour. If that's okay and if i'm going too slow guys let me know right so the first i suppose is the chunker chunk vectorizer uh here is how i do it i hope it's um let's go and do that. Oh, sorry, I went to the wrong directory text. I have to go to text. Yeah. A text extract. Okay, let's first go to text extractor. Just to show you, which is the non-AI part, but as a warmup, because you'll see the structure of the way I've put the code here. Once again, and obviously you can clean out these inputs. We are using ticker. We use ticker for three things. Text extraction, right? Detecting what type of file it is. Is it a PDF? Is it a Word doc of file it is, is it a PDF, it is a word doc what it is, and the language. In which language has the text been written. I created a specific again exception dedicated for this text extraction process nothing fancy. So here's the text extraction class. As you notice the text extraction class is a something that expects in its constructed the path right and uh this path it will try to do a two string on to get the text first thing and then a dot type Etc Etc now when you notice what we do in the two string um what do we do to toString? It's a static method here, but in there, what we do is we check if the file exists. If it doesn't exist, we throw an exception because you can't text extract from a non-existent file. If you find the text, and see guys, I mentioned to you a simple rule. If you are doing IO, if you are doing model inference, or you're doing a remote procedure call, RPC, in these three cases, definitely sing a song, which means do some logging, and wrap up all code that can potentially fail in try-catch, try-accept blocks, accept blocks right exception handling blocks which is why you'll notice that and as I was walking through your teams most of you have this line right right this you all have but but I put a little bit of ceremony what are the ceremonies I put? This is what we people, like I was taught long, long ago, 20 years ago, we call it preconditions check. This literally almost in the first week of my working for a NASA project, I was told every function must have a preconditions check, which is what I'm doing. You can't convert a file, extract text from a file unless you know that the file exists common sense it should be more than that i would encourage you to decorate it further with the fact that it must also be readable not just exist but be readable right so or if not file readable that too is important isn't it so like this so this gets a little bit better right then you log it you're going to pass this file then you go and pass the file and obviously one message that i missed is i should say then that successfully law the successfully parse the file and by the way i say it as a info i was debugging in reality there should be a debug message in other words you don't want to slow down your parsing when you're doing millions of files with the info level at this moment everything in the code sample is at info level so that you can see it happen as you run the code yeah so the other thing is this is a little bit more controversial i tend to give type hinting i tend to mention that if i create a variable result out of the parsing of the file what what in the world is that result out of the parsing of the file what what in the world is that result right and then for example and then from that if i extract the content i just say that it is str this is called type hinting or type hinting guys goes a long way in finding bugs in your code and in the id to help you write better code because if you type into your code then your id whether it's visual studio code or whether it is pycharm or whatever it is you are telling it that this is what it is so later on when you use it it will make sure that it can give you the appropriate hint so it will warn you if you are doing the wrong thing now whenever you parse a file there will be a lot of trailing and beginning spaces, lots of empty spaces, empty lines, you can strip them off, that's it. Document type sensing, etc. So this part is easy, text extraction. Now, as I was going from room to room, I noticed that obviously, none of you have done it with this much ceremony, most of you have just said, this much ceremony most of you have just said parsed from file now it's a subjective matter whether you feel one way the the virtue of just having this is a simplicity right the virtue of my style of doing it is that you'll have better clarity on what went wrong if things do go wrong right you can pick your style next comes the text chunker let's talk about the text chunker what am i doing in the text chunker uh can i pause you yeah sure go ahead yeah if you do if you do the type hinting yeah then you can mono and compile your code into very high speed Python it's compiled very good yeah that is uh amongst the great things yeah um you're talking about mojo isn't it yes mojo yes so yeah so guys obviously now this and Prakash many thanks for bringing up this very interesting topic as you know people are people love python and one day and the next day they hate it they love it because it's so easy to learn they hate it because type hinting is missing and it's just a nuisance to figure out what exactly does a thing produce right some people are annoyed by the indentation but the one gross problem is performance and it turns out that if you can actually you can convert in some sense do a much further optimization or translation into high performance low level code if you type in there is an initiative called mojo which is trying to say let us make python into c because the we can bridge the gap if python is type hinted and it can run at the execution speed within ballpark of c language not if not completely there then in within the ballpark of that right so that's that a very good point so i'll not talk about the sentence chunker what does the chunk text do uh guys again one thing to mention you notice that i tend to write object- code i am told that object oriented code is now over right um it's a failed philosophy and so take it with a grain of salt we all have our personal styles perhaps i'm a bit it's a bit too late for me to change my style i tend to like object programming and functional programming in fact in the j world, we used to do a lot of object functional programming, like use lambdas and map reduce and so forth, filters, but do it in an object oriented way. So some of that has sort of spilled over in my doing Python. And you may have your own style. So this is a class. It uses a mixin configuration. Do you notice that there's a mixin? Why in the world would I use a mixin? So a little bit about mixins, if you don't know what mixins are. Mixins are interesting things. They are reusable functionality, and they generally tend to follow a concept or a word that I use. Maybe I haven't seen that used that often. It is a single responsibility pattern. A good mix-in is something that does only one thing. And many, many people need it. Right? So, for example, in this project, you would agree that reading the YAML configuration file and getting something out of it is a is a is a functionality that many pieces of code many many it's a cross-cutting concern and something wants to read read from the configuration they shouldn't have to reinvent the wheel they should instead delegate it to a configuration. So therefore, configuration class becomes a great candidate in my mind to be a mixin, right? And again, these are, see, none of these are hard and fast rules. You may differ. And I would like to hear about that. So, but I've created a mixin. It has a method called load configuration. What does it return? And now a little bit about this. When you use YAML, many of you use PyYAML. PyYAML has a couple of defects, which is that it completely erases your comments in the YAML file. So if you look at a YAML file, which was somewhere here. Not this one. Configuration, Py. Where did the YAML file go boot config yeah you notice that we put a lot of comments to make it more readable or sensible all of those comments if you read from a pi yaml and then you modify use pi yaml to modify the configuration and write back all of these will disappear but so somebody has split the library or forked that library and created another library called ruamil which honors and protects your comments it keeps them that's why it creates a commented map yeah just for you to know and it also by and large preserves the order of your entries so this is it i read the config file once again the moment you have a config file uh my my way of thinking is preconditions post conditions what are the preconditions first of all reading a config file is io the moment it is io therefore i will do both logging and exception handling right so you will see a lot of log statements and you will see the whole thing opening of the file the inner core of it is just two lines but you notice that there is a lot of ceremony that i've wrapped it up with yeah and so i'll just walk a little bit through it config file is none then complain just say hey you forgot to specify a config file right but this is called configuration by default you don't have to specify the config file if you're fine with the default one and the default one that we have kept is obviously the boot camp yaml right boot camp yaml file right now when you write a yaml file this is a question some of you raised i believe um i think satyam raised that he is worried that how do you validate that all the necessary properties or entries or configurations are actually present in the yaml file there are multiple ways of doing it one way for example is to use pydantic class and then validate it data validation of the bean make it a required field and it's a good way the my thing is that it tends to i mean you lose comments and i tend to like emphasize readability of code quite a bit so i had to forego that instead i took the different approach. You can validate the YAML file against a schema. You can create a schema, which is what I've done here, but it seems to have a bug at this moment. But once you create a schema declaring what must be there in your file, then in your config file, at this moment, I've commented it out because obviously there's a bug there. You can make sure that the YAMl file that you read and the configurations that you read from it or do adhere to the schema in other words it has all the necessary fields that you want it to have right so this is an alternative mean i prefer this mean simply because of the overwhelming importance i put to commented code. So here we go, you can read this if the file doesn't exist at all, if neither the default file exists, not as specified file exists, we of course throw an error, we raise an exception. Otherwise you say that all right, I found the file. Again log it, sing a song, check for the schema validation file if the schema validation file exists if not exists again blah blah blah and then go read the file this is the part that almost all of you have done right do that uh i said can you go to line number 62 line number 62 yeah it's deliberately commented it out as false so that it doesn't run because at this moment 62 i am at 62 okay yeah if false yeah it's deliberate because at this moment in my schema definition there seems to be a bit of an issue there's something not quite right here most likely an indentation or something is off right i'll figure that out or i'll how about this better still you guys figure that out and then mark it then remove the if false this part looks standard guys i hope the one thing that i would say is so in contrast to that if you had done something like this like f is equal to open like this 44 is what i would call um anti pattern don't do this why should we not do 44 why should we do this long see this is a this is the opposite of everything that I've done isn't it guys why should I not just use this because I saw it in some of the solutions anybody would like to chime in what's wrong with this um we don't know what's the what happened with the open exactly the file handle is a dangling file handle number one number two we don't know whether the file has been uh file even exists we don't know whether the file is valid right and so you will just get a nasty stack stack trace if the file doesn't exist and even if you have successfully read it you're basically at the mercy of the operating system to go clean up the file handle after you so while this is this is great for code snippets on the web but it's not great for production grade code try not to do that right and see people see here's the thing a valid criticism is doing it like that in creates verbosity in code. And is it necessary? You guys have to reflect on your own what you think about that. But it is my style. Next, let's go to the simplest of them. Oh, should we first go to sentence and code? No, let's go to the text chunker. So now we went to the text chunk. It will the text chunker need to read the configuration file? Yes, because why should it need to? Oh, look at this. We are picking up some AI models. Do you notice that these things, all of these models and parameters are being read from the config file? Generally, you can tell a lot about what somebody has done in code by simply reading the imports. From the imports, you can say spaCy ah sentence transformer aha both of this and then there's a configuration mixin just reading the imports you can conclude many things you can conclude we'll be doing some linguistic processing we'll be doing some sentence embedding and we'll be reading something from the configuration file so never overlook a study of the imports whenever you're reading somebody's code, right? And then go straight to the init and you would know pretty much the lay of the land. By now, I would argue that if you know what you're looking for, just this much is more than enough if you had a good shell of homes and you may not even need to read the rest of my code. But for what it is worth, here it is. Given two vectors, alpha alpha and beta does anybody notice something very funny about the a and b there arguments 38 yeah it's a greek yeah yeah so one of the things that i delight about python and julia is that g Greek letters are a first-class citizen and mathematicians love Greek. That's why whenever mathematicians talk, people are saying, it's all Greek to me. I'm joking. Anyway, so this is the cosine. As all of you know, it's the dot product normalized. Create chunks. So given a text, how do you break it up into chunks? Once again, the way we do that is um and by the way why am i putting all of these uh comments in here because it will as you will see they become part of the read me uh read the docs in a second i'll show that to you this is your spacey being used what it does is it takes your text and creates a nice linguistic model out of it that model will tell you what the sentences are what the tokens are what the nouns are what the verbs are what everything is what the punctuations are and it will do it in a language independent manner right so if you think that full stop is the boundary of sentences you are mistaken Right. So if you think that full stop is the boundary of sentences, you're mistaken. Look at, for example, Hindi, Sanskrit, anything. Full stops are nowhere to be found at the boundary of a sentence. And so forth in different languages. Clean text, what do I do? Now, this is the use of regular expression. Whenever you find this sort of a thing, pattern, this, what I'm saying is that if you find more than one space, replace it with a single space. If you find a lot of new line characters, replace it. But when do you do that? See, the trouble is if you get rid of the new line characters, it can be bad. Why? New line characters are good indicators of paragraph boundaries, isn't it? So why am I doing it, guys? Can somebody tell? Why am I removing the new line characters? Why am I removing characters? Because fundamentally, you're using the spacey sentence logic in there to be able to parse out the uh or make the paragraphs later on as a result you kind of don't need this this new line characters anymore yes almost completely there see the trick is notice that I'm inside a sentence not I'm not talking about the whole text when I'm just iterating over sentences when i'm looking at one sentence do you really appreciate that uh how after four words of a sentence there are two blank lines followed by the rest of the sentence makes sense yeah okay you don't want that so you you would rather that your whole sentence is one long line which is what this does right makes your sentences into single lines it's just a very basic cleaning and then also if append the text and clean the text and append it only if the final length of the clean text is greater than zero in other words there is still something still remaining your entire paragraph may be made out of blank lines and spaces or tabs. Then no point in appending it to clean text. Then comes the question, how do you chunk it? Choose. You need to do two things. First is the obvious one. You break it. You encode that sentence and you see what is this cosine similarity to the previous sentence. This is the most basic one you do. A better one, perhaps I could say, which I suggested suggested to some of you is to do mean pooling but i didn't do that because the code began to look a bit more complex so i thought let me not confuse people but so here it is just comparing it to the previous sentence when you compare it to the previous sentence actually it becomes quite imperfect, but code looks simpler. The point that I'm making is do something. Do something interesting to find that out. It's not as easy as you think, though I would encourage you to use something a little bit more sophisticated than just cosine similarity to the previous sentence. But when you do that, if the similarity is greater than a similarity threshold, take it. Now, what you'll realize is that sentence similarity threshold is very context dependent if you are looking at a scientific domain or specific literature about a topic you will see that you would want to set the threshold as like you know close to 0.7 which is the cosine of 45 degrees if on the other hand you're looking at general purpose new york time articles you will realize that the i mean it's it's something interesting i was looking at one author style and i found that every sentence differed from every other sentence or most other like most of the text were self-contained. And for effect, for contrast, for whatever, that author's style was that its semantic content would be different from the previous sentence. So what do you do then? Then you set the threshold very low, right? Even setting the threshold to zero is okay because, one second guys, because thresholds can go all the way negative. Two sentences, you know, are different if their their similarity is negative so remember the the cosine of an of a theta goes from minus one to plus one so even zero is reasonable though i never find zero quite useful i set it to at least 0.1 0.2 even for general general newspaper text or you know opinion text in new york times or something like that yes somebody had a question yeah i i think this is this is like you know deeply subjective um are there examples that you can show us that you know we can also relate to because um we're comparing like somebody's subjective opinion to numbers so there's qualitative and quantitative is there something that you know it since it seems sounds like you've done the research um is there something that we can look at because you're sure i would like to share i can share my jupiter notebook findings it was very interesting actually uh certain domains where things are very tightly argued and well argued i you will notice that the sentences they have a lot of cohesion they are proximal and very similar to each other whereas i noticed that political opinion statements, they go more for effect. And I'll show you my Jupyter notebooks. I've analyzed texts from many domains going all the way to history and anthropology and so forth, and politics and so forth. So for whatever it is, and so therefore, I consider such thresholds more a hyper parameter of your model to be decided judiciously based on what people perceive as semantic separation and it also depends on the corpus right as a yes yes very much corpus is that's what i said the domain as well yeah and like shalini was saying this is i think subjective because i had a similar experience i i was comparing exactly the same thing, a Bloomberg article between two passages. And the net effect was that some of the like, where there was a natural paragraph boundary, according to the author was not necessarily what this algorithm would have you believe, you know it would say oh this is the next sentence or this is the you know yes there were some false alarms so to speak as well yes that is very true so shana was there first of all your findings are very correct though with one caveat a paragraph boundaries sometimes are arbitrary. Sometimes editors will say, hey, your paragraph is too long. Can I break it here? Right? So to that extent, a good semantic chunking should outperform such editorialization. So there it could make sense, but you're absolutely right. See, it's a very primitive measure and therefore not the best, but for whatever it is worth, it's a hyper parameter of your model and you can sort of improve upon it with all its caveats and limitations ask if i have one question associated with this no no no guys one second i have prakash raising his hands for quite some time prakash go ahead yeah sorry is this uh feedback is Asif, the question is on the text, which you have splitted in the top by, let's say, 400 characters or 500 characters. And then each sentence, you are trying to identify its similarity. Yeah. And then in the next line number 71 if they are similar you are adding it again on 71 yes what i'm doing logically and let's see i'm trying to make chunks so think of chunks as bucket okay here's the thing imagine that you have you're filling these sentences are boxes, right? Or clothes, something. Imagine that you have five boxes for packing. You need to fill the first box or the first chunk. What will you do? You'll put a sentence. Then you need to decide that you put a cloth there, you pant there. Then you want to bring in another sentence. Let's say that that second piece of cloth happens to be not a cloth, but it happens to be a kitchen utensil. You're packing. You certainly don't want to put it in that box. You want to start a new box for it, isn't it? That is why you check the similarity. But you also know that even if you get very similar things, like you keep on getting clothes and you keep on putting in the box, you can only fill the box so much after which it will be full so there are two conditions that you need to check one is if if yeah if the token the tokens that you will add will cause the cost to overflow and if it doesn't overflow there is still a second condition to make sure that what you are putting in there is similar to what what is already in there like you're not mixing kitchen utensils along with your clothes got it but uh question is even though after that entire text you are doing a vectorization of entire text no chunk of the chunk so what happens is this method doesn't do vectorization at all. This service just returns chunks. Given a text, it will break it up and return chunks. I see. That is it. Yeah, go ahead, please. I have a question. So here we are making sure that each chunk has a good context. You know, it doesn't have noise it doesn't bring in another uh context you know we are discretizing the the uh you know the context right and how does that help when we load it into the model uh Meaning, how does that help in semantic search on the whole? See, Jay, that's the main point, right? See, ultimately, when a chunk of text, a piece of text, becomes a point in a vector space, you want it to be a semantic embedding. Means, if it is about atoms and orbital structure of atoms, it should be in other subatomic physics statements but if you look at for example i illustrate that to you why we should do this suppose we go let's go to wikipedia yeah because there were some techniques that involved chunk overlap as well and that is true yeah when there is overlap and that helps and when there is this kind of discretizing the context and that also helps that is where i'm trying to uh you know confusing a bit let's say that we search for something what would you guys like to search for? Let's say transformers. Right. Well, it turns out that Wikipedia doesn't quite work with AI at this moment. Transformer. Let's just say. Transformer machine learning model. Look at this. This is about the transformer, what it is, what its history is, et cetera, et cetera. A transformer is a deep learning model. It must propose blah, blah, blah. It is notably required blah. Then the second paragraph gets to its use but then right after that it goes to the history of it isn't it and so predecessors etc so if you take the entire text of this article and you make it imagine that it becomes a vector as a semantic vector what happens it becomes a mix of the semantic vector for this the semantic vector for this history and the semantic vector for this and in a way its overall value is diluted isn't it because now you don't know what it is about there's too long a texts you're lost in the land of text there are too many too much semantics in there whereas the idea was that given a piece of text there should be just one semantic concept in it you you realize that i very provocatively called the title of this chapter of this project the metropolis of meanings isn't it one vector should have represent one meaning or one context or one semantic but if you look at this article so much is said here and that is the value of chunking the reason the real reason for chunking is not so much the transformers take only 512 tokens or they take only blah blah blah tokens even if you let's say that you use cloud and hundred thousand tokens are fine but when you're doing search indexing it is still a good idea to chunk because you want to focus each piece of text to one meaning one idea not many ideas in the text right that's the point so paragraph one uh let's say it talks about you know like we are reading uh you, let's say, talks about, you know, like we are reading, you know, let's say a famous cricketer, Sachin Tendulkar's Wikipedia. And the paragraph two actually talks about some other person, right? Like say couple days. So in a semantic search, you know, like when paragraph one is different paragraph two talks about something else uh how does it help uh it helps because if you break it up into chunks like that then when you're looking for kapil dev or a spin whatever he was supposed to be a great spinner in bowling or something right i don't know what what it was but let's presume for a moment that that was his forte so then you're searching for that it would show up in the search results and my apologies to cricketers who are disgusted with my ignorance but and suppose you are looking for such an amazing style you get that what you don't want is you search for something and this particular article which is very relevant to kapil dev it doesn't show up because it also or shows up low in the search results because it also contains talk about you know other cricketers right which dilutes the vector moves it away from where it should be. That's the point. Yeah. So that's that guys that's that's the real reason why it's a good idea to chunk especially do semantic chunking. So I wouldn't do I wouldn't say more about it. You can optimize the NLP loaders. Like whenever you create a model, you can decide what things you're going to do, what you're not going to do. Like in this case, we don't do token to vector named anti-recognition. Now this gets a little bit into what I would call linguistic structure. Those of you who are aware of it. And obviously those of you who took my, the SPACI course last year, an NLPp course know what i'm talking about, but for the rest i'll just skip it for now. But if you remember, I mentioned the optimization that from spacey load only that which is necessary, in fact, I have myself had forgotten and it's my pleasure actually to see that one person, one of you guys did it right and remembered it. guys uh did it right and remembered it and i noticed that and i said aha yeah yes good that you did that so this is it this is chunking so now i'll take as chunker as a thing it's a code it's a bit of code that will break up text at the end of the day and return a list of chunks of the text right based on certain parameters now the is, who should do it? Where should it happen? It is certainly using AI models, it should be on a GPU driven thing. So for this, you don't want to just use fast API or things like that. There are structures that have been built on top of fast API, but which are much more designed or infrastructure to serve AI models. So model serving is a whole cottage industry of open source and closed source projects. There are many such things, and depends upon what trade-offs you need. For example, a PyTorch serve will only serve PyTorch models. Tensor serve will only serve TensorFlow models, right? Or Triton will only work on NVIDIA cards, but can give you a lot of performance and speed and optimization, right? And so there are many, many models serving frameworks that serve the model at runtime. I would strongly encourage you to get introduced to one of them and use that. One of the relatively easier model serving technologies to use is RayServe. A Qflow is another because it containerizes it. If you are within the Docker and the sort of Kubernetes world, Kubeflow is a great choice. MLflow is there, especially if you're very close to Databricks technologies and so forth. So there are many such things. For the sake of this bootcamp and for educational value, I took RayServe because its learning curve, as you will see, is close to zero. In fact, I claim that in five minutes now, I'm going to introduce you enough to RayServe that you'll be able to use it immediately. So let's go and look at the service part of it. How do you serve this? So this was Chunker, right? Let's go and clean Chunk service, right? See, one of the things I do, you'll notice is that in my so-called infrastructure code like you know microservices this that i never put heavy duty logic i always delegate heavy duty logic to delegate actual functionality core code right here the core class was chunk text so here we go let's look at this class called clean chunk model it uses the chunk text when it gets a request it will extract the json from the json it will get the text it will call the chunker to chunk the text and it will return the chunks does it look dead dead obvious guys assuming that the chunk text class does its job this class does its jobs which you just saw uh they did respectively well would you agree that this code is dead simple now once again when you are receiving a request remember is this does it follow into one of those categories io rpc and model inference does it come under any one of those three categories anyone guys are you yes it is io it is at the receiving end of our rpc and guess what you are calling uh ai service along with it right it literally has that so a good idea i'll put it wrapped it up with log log right a better thing would be if i had wrapped it up as a further improvement if i should have wrapped it up in a try accept block also okay so something to remember but i for simplicity i left it like that just to show that when you write your core classes don't write it in your rest code i i've sort of been moving from room to room and seeing that the microservice code or the actual service code that you write is too heavy with actual logic don't put logic there this sort of code takes time to debug you want to where things are unclear things are coming over the network there will be network latency ports may be wrong it client may be calling the wrong server god knows what may be happening keep your this code dead simple you will be doing your ml ops and your devops guys and your system operations guys a big favor and because they will know where it failed they won't have to muck through your very complicated ai code to figure out what happened they just need to know that it failed at line 45 if it it failed at line 45, it was not a network failure. They are happy. It was not a server being the wrong server being called, whatever it is. They know specifically that you, the dude who wrote the AI code were screwed up. And so he can just pick up the phone or send you a slack message or email that oh gosh you need to fix this or file a jury ticket whatever so it makes it makes separation of responsibilities much easier if your code is clearer like it's separated out like this so please do that so now comes the part what exactly is this racer what you do is i'll give you an idea of how it comes you can install ray serve it is a simple pip install so how would you do that if you guys are seeing my console at the bottom are you guys able to see my console at the bottom you do pip install ray serve that's all one simple pip install will do the job i'll do it again even though it's sitting on my machine it's done how do you start a ray job i'll do it again even though it's sitting on my machine it's done how do you start a ray server i'll start it and show you ray i'll first stop whatever i have of course there you go so ray has stopped well it none observe something it had 42 ray processors what in the world are 42 processors doing this is where it does it does a lot of parallelization distributed computing scale out scale in and so forth it does all the heavy lifting for you so i'll start the ray again you notice that ray has started it is inviting you to monitor it in a dashboard. We'll go to this dashboard, see what in the world is going on here. Here we go. Oh, look at this. It has come up. It's in a healthy state. There are no jobs waiting. What are the jobs? Nothing. What is it serving? Nothing. Yeah. At this moment, nothing is. Oh, in the past past it has been serving quite a few things maybe but now it is at this moment empty metrics well we haven't integrated many things it's a basic one so now let's go to the code and say just a couple of lines very basic lines you're going to say this this code the service that you wrote the way you think of it is that you write a code a rest endpoint code let's say then you ask a ray cluster you reach out to a ray cluster and you say hey you know what why don't you manage my service for me are you are you understanding the language you're you're saying hey let you are this ray cluster i'm giving you my code bind to it and take my entry point bind to it and then expose it at a certain port but and give this is the route part slash chunker so you will go to local host slash chunker and so on and so forth but you do the scaling so in other words if you want run it on 50 different machines but you do the traffic right routing from client to to this so that as far as the client is concerned what they see is just one ur. You do internal traffic dispatches. You do internal scale in, scale out. You do everything, basically. You want to do things like that. You want to use a model serving framework. You don't want to home grow these things. Are we together? And to do so is very easy. Here you're saying, okay, I want to connect to a ray cluster that's running locally. What do I want to connect to a ray cluster that that's running locally what do i want to do i'm going to expose an entry point coming from this class like this class where is this class coming there is a decorator and now i'll show you the part that i put on top of the class i decorated it with a serve deploy when you look at the serve deploy i have put certain parameters here one of them is reactor option. Number of GPUs. How many GPUs do I have on my machine? Well, I happen to have one. Zero is perfectly fine. Number of instances to deploy it on. So I'm saying, okay, you can have for this particular microservice, you can have up to two instances, like two different processes running it and you can do the auto scaling minimum replica zero when nobody is using it scale it down to zero when somebody is using it well scale it up to a max of two and but and then comes a factor that sort of is important i think through it how many concurrent requests can you do and for that you have to ask yourself your gpu what is your max batch size that it can take right or what is the max number of computations it can run in concurrently usually it is not millions it's very small number even on a 4090 even for something which is just text chunking 100 is really pushing it right you could have you should have for typical hardware it is good to do this see if your chunking takes let's say just 30 milliseconds you realize that if you do 10 things it effectively you're doing uh close to 300 uh requests per second which is a very very high throughput number right so the max concurrency don't go don't go aspirational or too ambitious about it right this is enough actually yeah 100 is a bit of an overkill for something like this so you have a chunker this is it so guys does it all make sense async is always a good idea async await to do it asynchronously but if you don't do it well then you have a synchronous call no big deal guys am i making sense uh question is go ahead ray is equals to ray column local host port number so it is trying to distribute this service on multiple machines and everybody will have a running port on localhost 1001 yeah so what happens is that the cluster has a exposure cluster is saying I'm available and listening at this machine if you want to submit a job submit it to me it will go submit it to you I see right and it will spread out the job everywhere and it will take care of the dispatch I said got it yeah it's a pretty powerful feature isn't it I got it. Yeah. It's a pretty powerful feature, isn't it? As if so, architecturally, how is this array kind of working in this picture? So it's sitting on top of the cluster where all the chunking processes are taking place? Yes. No, ray is the cluster into which your chunking processes, chunking and things can be deployed. And what you would do typically is you would deploy ray on the gpu enabled systems i see and as if yeah adding to that previous question this uh service will be called through the pi spark cluster right yeah you can call this in fact i'll show you an example how you can call it through the PySpark cluster. Okay. So then parallelism makes sense in this setup. Yeah, it does. See, what is happening is, so I will make a little bit of an editorial comment. And people, especially you, Prakash, you come from Cloudata, big data world. I want to hear your perspective here is what i find over the years yeah guys uh prakash maybe you should mute yourself for a moment oh guys please mute yourself you're getting a lot of echoes it's gone now thanks okay uh so see echoes it's gone now thanks okay uh so see big data movement that started with the famous paper map reduce the fundamental call and big statements in the architectural Community saying disk is the new tape yeah hard disks have become extraordinarily small let's not take data to the compute by running SQL queries let's take compute to the data, right? The whole NoSQL movement came about. You would take computations, make it as local as possible to decrease network overutilization or clogging bottlenecks. All of those made a lot of sense in the early 2000s. The world has changed since then. if you look at today with the ai world some of the things that comes about is some things are not parallelizable massively parallelizable except for big companies you and i like normal jews we don't have access to 10 000 gpus right and so it is not that easy for us to spread a computation such that 10 000 gpus can in this in a distributed way come like go fit into that other thing hardware wise network has become insanely fast these days a hundred gigabit networks and 400 gigabit networks in the data centers are very common now right 400 gigabit is the norm almost even in support vectors with our pretty much shoestring budget this entire building is wired with a 40 gigabit wiring but 10 gigabit switch because we can't afford 40 gigabit switch. So networking has become way faster than it used to be. People have moved away from mechanical hard disk to SSDs. Nobody would have thought in his right mind to run big data clusters on SSDs. But now more and more you're seeing such deployments. And so data itself, memory has become so cheap that a lot of these data centers i mean people have created layers on top of data storage caching layers right near data they create so the world architecture world has changed in quite a bit and obviously some of the topics in big data have to be revisited especially with the growth of ai workloads so i have have used PySpark, but I must say I've used it with some concerns, with some reservations. Because think about it this way. Look at this chunker. If I have one GPU, on one GPU, if I run a parallel, I have actually the chunking job, the pipeline, the text extraction from thousands of documents becomes after the text extraction, thousands of documents becomes a, after the text extraction, it becomes a chunker. I'll show you the extractor. It runs just fine. After that, it goes to the chunker. But what will the chunker do? You can run it in PySpark. You can run a massively distributed parallel computation, but there is a choke point. The choke point is the number of GPUs you have have and so the size of the ray cluster will never be never be like a hundred notes usually not not for normal joes like you and me right and so it is it is a thing to think and i think databricks has been trying to make some dent at it of what does it do uh in view of and they try to literally create their own gpu cloud as you know for databrick hybridizing spark with a big ai workloads unfortunately it's all very proprietary solution it's not in the open source but what is in the open source is the plane spark on which is the pi spark layer and so is it the right way to do things it is not a bad way of doing things but is it absolutely the right way i don't think i don't think anymore i used to think but not i don't think so anymore all right um but what are your choices at the end of the day it boils down to what are your choices. And so i'd like to hear what you guys find. Let it I would really love to learn from you guys if you guys have other findings other considerations to talk about let's let's collectively find out in view of modern realities, how do you hybridize as a massively distributed computing world. When it makes calls into another world ai world where resources are at a premium and rationed how do the two come together and what is the best architecture one is the one that i've written i i am not saying by any means this is the best but what is the best let's find out i mean nowadays the the GPU is the most scariest resource available. So from previously the disk drive network, and now it turned out to be a GPUs. Yes, yes. I mean, I'll give you a perspective, guys. In the cost of one GPU, one H100, you can buy how many CPUs? Make a guess, randomly 10 cpus 20 cpus you can buy you can actually buy quarter million cpus in the cost of one GPU, you can buy a quarter million CPUs. The cost difference is that extreme. It is ridiculous, ridiculous. Almost am I right? No, no, no. I must've gotten my zeros wrong. Maybe two 50, two 50 GPU, sorry. CPUs one GPU costs $80, eighty thousand dollars one thousand dollars gets you easily five cpus these days so 80 times five four hundred yeah four hundred cpus i exaggerated quarter million four hundred cpus so the price difference is one is to four hundred right it is just unbelievably large price difference. And we need to and again, at the end of the day, software architectures are driven by hardware considerations. We need to be mindful of that. Think about it. Anyway, so guys, did you get a sense of this? If you see my chunker, you can you can understand the sentence and sentence embedded encoder which will be even simpler again sentence encoder needs to know the configuration file why because it needs to know which models to load whether to normalize embeddings to do what whether to go on gpu or not and so on and so forth but the actual embedding is very easy you just encode by now all of you are familiar with this now whether to normalize the embedding so not to normalize them weddings. It is a question. Yeah. Once you question, can you repeat a little bit about models of framework and when to use this model so frameworks. If you if you have AI models that you're serving you should always use them. The only question is which one you need to have a clean boundary right you have AI models that you're serving, you should always use them. The only question is which one? You need to have a clean boundary, right? You have, in this case, since we are getting JSON as an input, yes, fine. I can use the model for chunking and get a response and pass it to the next service. But if I had to call another, like, third-party service to do my model model processing, then Ray might not be a good option, right? Because he said, I haven't looked into it. So I might be wrong here. What I heard from you is raised based on clusters. And this clusters are capable of scaling, but they won't scale the DB or something that you're connecting with. Yes, yes. So what has happened is that the world is fragmented into little corners, each with its own optimization. For the AI world, you need to use some model serve framework. Raise the, all of these are there, tensors, you know, PyTorch serve, TensorFlow serve, and so on and so forth cube flow ml flow many many many such frameworks at the end of the day pick one but at the but the general consensus is pick um don't just go with fast api because then you'll have to essentially reinvent the wheel you'll have to do your doc scaling through docker's you'll have to essentially reinvent the wheel you'll have to do your doc scaling through docker's you'll have to do management of traffic you'll have to basically bring in the kubernetes etc which is why qflow by the way is is a simple alternative because that's exactly what qflow kubernetes does so if you leverage qflow kubernetes if you are on qflow it is a very thin layer of ai on top of Kubernetes cluster with the constraint that the cluster only runs on GPU enabled machine. That's an alternative to it. But go with a model serving framework. There's a lot that happens here. Don't just write your own fast API. That's a lesson to learn. Asif, what about Dask? Dask is great. See Dask is an amazing framework that is at the intersection at this moment of data science and data science at scale. So first of all, I must say that Dask is something I periodically get excited about and then forget about it. I haven't seen very too many people running transformers on Dask. And since we are LLM-centric course, maybe we should explore it. I don't know what the latest state of the art in Dask is, what those folks are up to. Maybe they have started serving LLM models and transformers very efficiently. But the last I looked at it, it wasn't like that. So check it out. Ask. Last I looked at it, it wasn't like that. But so check it out. Go ahead, please. Is Ray a single node cluster? No, no, no, not at all. Ray is a cluster manager that manages traffic across many clusters, many, many servers. So guys, play with it. Now, see, I'll tell you the strengths and weakness of Ray. Ray doesn't do any model optimization at all. It says bring your own optimization. For example, if you use TensorRT or Triton,on those guys they will take your code and they will optimize it for the nvidia hardware because they are released by nvidia now what happens is that when you write code these run on their hardware they have to be transpiled into machine code a lot of the things in nvidia are completely secretive they have never really released the internals of the architecture so open source community is never able to really optimize things that run on nvidia to the same extent that nvidia drivers do they release cuda they release their stuff but even then uh taking something that comes from nvidia will certainly work absolutely wonders on nvidia will it do the same thing on your neural engines of apple probably not will it do the same level of performance on amd's upcoming uh what is it called rocks rockham chips probably not in fact not the simple answer is not right so the world is fragmented at this moment ray's one benefit is that it gives up some optimizations but is the Switzerland like it's very it's portable you can run it on anything you can run it on your laptop you can run it on a server you can run it on Windows with some caveats I noticed a couple of you running Windows were having let us say very uh very entertaining experiences with it let us say very entertaining experiences with it, then it can run with, I mean, let us say that it runs on all genuine operating systems, right? Very well. It doesn't give you any particular optimization. And it is a legitimate piece of software. It's not written by bozos. It's written by some really, really smart people sitting at UC Berkeley. The same lab that produced Spark, the AMP lab, the AMP lab had gotten renamed to something else, I forget what, but it is the product of the same lab that came out with this project. What's that? Rice lab. Okay, yeah. It's the product of the rice lab. Have they all become rice eaters? I'm kidding. So after the biryani, it's hard not to make the joke. All right, guys, so there you go. Ray, see guys get introduced to Ray because his learning curve is zero. And I wanted to I sort of wanted you guys to do your project, the first project your own way, so that you you struggle. And after the struggle, you learn the value of ai serving ai specific serving systems right these technologies pick ray for its simplicity then graduate to something that is your hardware specific that works better on your hardware right does that look like a fair bargain on top of also, the way you do that is you remove the scheduler and then use the underneath ray scheduler. But most of the ML framework comes from dust. That's also possible. Oh, okay. So there's a dust on top of ray. Ah. So the dust scheduler is not fault tolerant, whereas the Raze one is made better because they are the Spark guys. So it's something like bringing Spark to Python world. So that is what we can create. And then getting the data frames across is the Dask framework. Yes. Yeah. And you brought up a very good point, guys. Raze's one great strength is this amazing scheduler, a very mature scheduler, right? If you ever look at these guys who created Ray, they pretty much lead the conversation by talking about the scheduler in those presentations. Now, how great it is. Thanks, Sachin, for sharing that. So, guys, you notice that once you see one piece of my code the others all begin to look the same right this is your sentence encoder this is your index searcher what what do you do in the index searching do you see how simple my code is the init will be obviously reading the configs but the actual do part will be index.search that's it right this is already implemented more or less correctly the only thing missing here is that try catch why did I say that there is something still to be done we forgot to log in do the usual ceremony logging and index logging and singing and try catches what is the device in that CPU versus GPU I see right uh likewise the first indexer what do you do indexing is a little bit more complicated because you may have more complications which particular indexing algorithm you do oh yeah indexing has a little bit of a caveat see when you create an index there are many sort of indexing when you create a fast uh ann search system or a file there are different indexes you can use that use different geometrical or data quantization approaches the flat l2 is the easiest because it's brute force it just looks at euclidean sort of a distance it is also the one that defeats the whole purpose of using ann right why would you do that then right so um it is good to do prototype to start with but gradually as you become that you use something else now the second thing that i would say is um hnsw on the other hand is graphical it it is computer heavy some people found that it was way too heavy and ivf uh inverted the the thing uh file index gave almost the same accuracy or performance and some people felt that no it doesn't so your corpus based on your corpus you have to choose one thing people worry about, almost all of you worried about is, FAS will just tell you how many rows have been indexed so far. So you guys were taking that and storing that in your database. You don't need to do that. You can wrap it with something. See, the first thing is that this is the property of indices. This is these have been very carefully thought through all of this approximate nearest neighbor people are people mathematically trained and mathematicians love commutativity and additivity. Right. The whole things that keep talking about associative algebra and things like that. So in other words, an index is useful only if you can create many subindexes in parallel and then merge them together. Indices make sense only if, for example, I give you an ID and you respect that ID rather than just taking it in and then giving me a number of items I've stored that you have to use as an ID. So remember, what you need, look around. Will millions of people need it? Is it the most obvious thing? Everybody else will need it. If they need it, it exists. Don't go reinventing the wheel. And so in this particular case, there is index ID map. Wrap your index with the index ID map. Then you can search it by the id that you give because when you add things to it create ivf save index forget it add look at this you can use add with ids you can give it a vector and you can give the id and or list of vectors and the ids those ids being whatever database ids you uh you use to identify that row that chunk right it is as simple as that guys i see a question on that so sorry i'm okay so the question is uh you are uh the chunks sentence vectorizer i uh one at a time we are indexing or we are indexing a list of it multiple vector list you notice that i'm vector list chunk multiple chunks they embeddings remember indexing you don't do chunks you do it on the vectors. The question is, is it happening on a bulk of document or one document at a time? It doesn't matter. In my architecture, the way I did it is I took a document, all the documents in parallel, and I text extracted it. I saved the extractions into the database. One job done. This is the one compute job, if you look at it. So let me walk you through that in a moment uh so hold your thought but okay i'll give you a high level and then i'll walk you through the code i have i always divide my workload into small chunks remember single responsibility pattern is my mantra so one job just does text extraction and saves it in the database the second job then takes the takes the text and cleans and chunks it and saves the chunks into the database the next job takes the chunks and each of them converts it into a vector puts it back into the database and the last piece of the puzzle last part of the pipeline of my Jenkins pipeline is the indexer i gave it in my design doc if you notice and the question is that you pick up all the sentence all the database rows and try to convert into the phase index yes of course but the question is you don't do you send it sequentially do you send it in yeah do you send sequence yeah exactly incrementally yeah what you do is this is where the thing comes see you have to do good design that's a good very good question actually you have to do good design on two fronts you have to do good design on the sending front you have to be respectful that uh the gpu might get completely overwhelmed you can't send it a million chunks right simultaneously there is no point most likely the the receiving end the server is going to queue it but if you have no choice go do it if you if you trust that the receiving system will anyway queue it no harm just go send it in parallel one second one second and the receiving side what you need is once you have the queued what do you do now but systems like ray will do that for you they will maintain the queue all right and a reasonably good queue though to be bulletproof it is far better to have your own kafka or such queues in front of those just if you want to be absolutely bulletproof but from the queue you want to consume batches of data let's say that your gpu is 4090 you can take a batch of 32 you know vectors at a time 32 chunks of text at a time and vectorize them so you want to send chunks of 32 size through the through your gpu right are you getting that prakash right are you getting that prakash yes i said but i wanted to know is it uh doing compaction at like uh incremental indexing and compaction or is it what like see incremental indexing is a norm right data keeps coming in but what i usually do or i can tell you my my practice i maintain two replicas of our index i don't ever do simultaneous indexing and searching generally i don't consider that a great idea what i do is at every x number of rows let's say 10 000 inserts or thousand inserts based the traffic, based on other considerations, volume, etc. I will materialize, I'll save a copy of the index. I will have a listener job, you know, the one that is doing the actual search. That process will be listening for the latest copy of the index. The moment it finds a newer copy of the the index it will load that one and replace replace its current index with that actually in my implementation i have a read index and write index good so that's what for a query i use read index good and the only thing you need to make sure is the guy who's serving your read index is listening for new copies of the read index yes and that's why i do like a minute base uh re rotation now don't do minute based rotation because uh it may be unnecessary uh make it insert based after x number of inserts yeah you realize it right that way you don't waste i mean right now it's just ready very early so i just tried a minute base yeah but that's a pretty respectable yeah made good yeah good that you're thinking about all these things it's really good so guys do you notice that the rest of it is very straightforward you you you index with the id uh here and i won't go into that i mean if you want i can talk about more and more service first and query results re-ranking sorry one second no i think mosme was waiting thank you so we talked about passing 32 chance to the indexer or n number of chips or n number of chips. Yeah. Read it from the DB, pass it to the indexer, create an index. I didn't quite catch what you said about something to maintain the number or something. No, what I'm saying is, Vasiv, for example, look at your code. You remember that you are trying to store the, you know, insert row i if the row id matches what is the row when you see fast index when you search for something it returns the capital i in your code remember you see that capital d and capital i the capital i is what it represents the number which represents the insert order into the face right that this was the 10th element inserted and this was the 106th element inserted that's what it so you are you are forced to store that into your database isn't it what is the point of having a face index then no that is not the index first index is it is for extremely fast searches of vectors so imagine that each vector is a page in your book now what happens when you read a massive textbook you're looking for some concept where do you go looking for it you go looking to the back of the book where there is a small index a few pages of index pages right in the index that concept is there and it tells you which page contains that isn't it so a fast index is not a collection of vectors. It is a collection. It basically tells you that if you're looking for something, which vectors are relevant. Just like an index of a book textbook tells you, if you're looking for a concept, which pages of the textbook are relevant. That is the meaning of index. Remember, always have the mental picture of a textbook in your mind. That will clarify the meaning of index. Remember, always have the mental picture of a textbook in your mind. That will clarify the meaning of index. Masmi, I know we are getting a little bit worried. Database indices actually arose from the same concept. I'll now relate it to what you think of as Oracle Index. Oracle Database Index. Since you come from Oracle background. See, suppose I was looking for Masmi. And the world has eight billion human beings and i have no idea where it is i would be searching everywhere you know i would go to every continent and say is mosme here but suppose somebody whispers to me one oracle i go to an oracle a magician and the magician just says i don't know but i can tell you which continent masmi is on masmi is in the u.s is in the american continent well you come you you limited the scope you come to u.s now you get another oracle there and you say oh i've divined that masmi is in California. Now you zoom into California and then somebody tells you that, oh, she's in Fremont at this moment. You zoom into Fremont and it becomes much more precise, funnel targeted search for Mosme in the database. That is why indices were created. Indices bring optimization, even in the Oracle database. That is the real meaning of index and the same thing is here a fast index is a it just gives you a extremely fast way of zeroing in and finding what you're looking for the vectors you're looking for yeah but in oracle database i'm not saving the row number in the table no you're setting you're saving something equivalent just saving the offsets the the row you know the row source offsets yeah locations on this from where to start looking for from where to do sequential scan or sectors to do a sequential scan of now see in a index you in an article index either you are pointed to the next oracle the next index sub-level index or you are pointed literally at the leaf level to a row source to a collection to an array and your array of rows and you are told that your answer is there in the small area pros that is indexing yeah it seems to be doing a lot more work yeah i mean it does a lot of work obviously these are mature workhorses of the industry but i'm talking at the level of a concept guys guys please mute yourself yeah so so did i answer your question you got the idea right the the first index is basically a way to search for a search for vectors i got that yeah good so what happens if i i mean i i'm understand what i'm understanding is there's a heavy lifting that's going to happen first time you load up 30,000 documents, then it's going to be incremental, right? Yeah, I'm just not getting two or three documents. Which is why I told you that don't don't like for example, Prakash said he's going to make a copy of the index every minute or three minutes don't do that instead do it for x number of inserts yeah eight hours whatever yeah or number of inserts so use some heuristics some meaningful heuristics okay guys so this is the re-ranker do you notice that all my code after a little while looks exactly the same right and that adds to keep your code base simple keep it dead simple to read and dead simple to understand and most of you are familiar with my code if there is one thing you i hope you would agree it is usually dead simple i mean there's no there's no magic here there's nothing hard this entire project was not so much about doing deep AI, but learning the infrastructure to build the infrastructure so that you can do interesting AI in the subsequent projects. Like, for example, this week's project builds upon what you have built so far. When you do the image search, by the way, I must say that when I say image search it means both ways i can give you a dog and i and say search for the documents relevant to this dog or i can give you a query and a straight text query and in search results i expect both images and text to show that's your project but i think if you follow this sort of a template or a way it's one way of doing it you may have better ways or you may have other ways good question yeah go ahead yeah so the phase index are they distributed or are they on only available on the server no no face index is a file remember but the index of the mathematical property that they have some associated properties they're additive so you can maintain like you you i think you or somebody was using the word sharding of the index and multiple things don't shard but you have the opposite of sharding you have the ability to merge indices right you can maintain sharded sub-indices doesn't quite help you very much but merging index is very easy you can maintain sharded sub indices doesn't quite help you very much but merging index is very easy you can because indexing takes time you could create a micro service architecture the you have a micro service so index available only on that service or index is distributed it depends upon how you serve it whether you use a distributed file system or a local file system but in when you are just reading an index and loading it in memory remember index is an is an in-memory object you read it from a file and you're serving it from memory so as you said for that generating the fias index i understand it goes into one single file but you are adding it among distributed workers in your model, or it's just one worker? No, no, no. In my case, what I do is, see, one thing that you could do is create, if you create distributed workers, let's say that you create three workers, what will happen? They will end up with three different index files. That is fine, but you'll have the task of merging them no first gives you a very simple method to merge the indices you can add another index to a current index so then you have to follow it up with a merge process in in this sample code that I gave you I kept things to extremely simple situation I made the insert into a single index okay okay that Okay. That makes it simple. Yeah. And then partition. Asim, quick question. So, let's say if you have distributed service. Yeah. So, the sentence vectorizer embedding goes into different, different indexes, right? There is no one index which contains all the chunks. In my case, I maintain a single index that contains all the chunks. Why would I maintain different indices? And if there is a single index, sir, then there is a potential of that service will get a DOS attempts. Absolutely. That's what I'm'm saying that see your choices are this at this moment i i just as i mentioned i've kept things very very simple so let's go to the rest service and see it where is my indexer service uh fast index builder service do you notice that i've set max replica to one yeah right suppose i didn't do that it's a it's a choice if you get heavy traffic heavy data what you could do is you could have let's say three replicas then what will happen you you should be aware that you will end up with three index files each containing different data you would agree right because data is being distributed across three of the replicas for indexing means a will go to maybe replica one b vector will go to replica two and c vector will go to replica three right now at inference time you don't want to be searching three indices what you want to do is you want to merge them into one single search index so what will you have to do after this job finishes indexing somebody has to pick those index one index two index three and do what merge them so that for inference time for search time you are searching a single index yeah but you don't have that service at present in your i didn't i didn't do that i didn't do that i didn't do that yeah but if you want to do that by the way it's a one-liner guys it's one line yeah i mean that won't be a service it will be a distributed file system not necessarily it's not like that prakash you don't need a distributed file system all you need imagine a three files have gotten created it could be on a single file system it could be on a distributed but what you really need is a way to merge them and first gives you a simple command a simple api to merge indices okay yeah why would you merge them as if because you have to load the index if you have a large index you'll run out of memory right indices are never large i mean you can put the entire wikipedia and i believe the wassersteins have done that and i'm sure they're delighted at how small the index file eventually why do i still have to merge i can still search against in indexes yeah you do except that see here's the thing uh must we think about it this way if i told you um that arunesh knows where you are and i have to just go and ask arunas hey where is mosman he'll tell me uh it's easy quick on the other hand if let's say arunesh has no idea, and you, because you happen to be at work, and you could be in any one of the 10 departments roaming around, right? Because your work takes you across 10 departments. I want to search for you. I will have to call the head of all 10 or some contact person in 10 departments and ask each one of them, is Masmi there? Is Masmi there? Right? So what happens? There is wasted effort. When you maintain 10 indices, you remember what will happen is when you do a nearest neighbor search, each 10 of them will come up with 10 nearest neighbors. Right. Then you will have the problem of merging the search results at inference time that increases, significantly increases your runtime inference cost or search cost and search is done at volumes big volumes right high traffic systems you want to minimize the amount of computation you do at runtime how can you do that by having just one person one index that can answer who are the nearest neighbors to a given query vector am i making sense guys that is why it is good to opt like spend most of your computations during the pipeline you know put it in your jink in your pipeline data pipeline you create multiple indices then you merge the indices it's a job it's a tiny job one line job but at the end of it the merge will take time let's say it will take five minutes but at inference time you have lightning fast searches especially because this index files get loaded to memory and you have to realize that today's processors run at six gigahertz and at six gigahertz and they have a massive massive bandwidth to the cpu memory and like memory is near when you search something in local memory it is lightning fast and you instantly get the results yeah so uh don't you have to coordinate the elastic search and the index switching when you do? Oh, yeah. So, okay. So now, Sachin, good question. Remember, what we are building is a hybrid search engine, right? The state of the art is not vector search or elastic search. It is hybrid search, which is what you guys are building. What does it mean? So now let's go to the real search, the endpoint user search. building. What does it mean? So now let's go to the real search the the endpoint user search when an endpoint user searches for something and I've deliberately left it as empty for you because I was going to share this code with you. What do you need to do? First of all, you will take the query vector, you will receive a query vector from the user, the user will say send me the top 10 results. What will you do? You will go to Elasticsearch and say, give me top 20 results. You will also go to FastIndex and say, give me top 20 results. Now you have 40 results, right? You will mix all of those results up and give it to the re-ranking service and say, hey, find me the best results amongst the 40 right the cross encoder the full transformer cross encoder will then apply the full power of a language model but it it's a very expensive job but that job is doing by comparing a query vector against just 40 other vectors 40 others sorry not vectors you're getting 40 other text pieces isn't it and that is a price worth paying to get better accuracy better recall better precision and that's why these are the steps get the results from Elasticsearch, get the results from FAIRS index, then sent it through the re-ranker, return the results. That's it. Merge. Sachin, is that making sense? I think you answered this question, but my question intent was slightly different. When you have indexed the same article in the first and elastic then only you will when you switch the indexes right say the first it just switching don't you have to coordinate with the elastic search also like making sure that the document is there in elastic and first at the same time see uh what you're getting at is every time you insert, you have to make sure you insert in both places, elastic as well as fast. That absolutely is correct. So when your data pipeline, every document must be indexed twice, elastic as well as fast. It makes sense, right? Otherwise, both of them should get a chance to look for it so that is done now your secondary question is what if i change the underlying algorithm behind first instead of going with for example flat l2 i go to hnsw well then that's your internal problem you need to rerun your first indexer right on all the documents so all the documents must exist in elastic search all the same documents must also exist or be indexed into the first index did i answer that sachin yes good so that is it guys so this is it um i'll release this code at some point today but i hope you got the idea of how to go about it and now i will take a half an hour break and then we would like to start start the presentations from each of the teams. So, guys, bring your notebooks. I want everybody's votes. And remember, you are voting on the rubric. The rubric is completeness, quality of the models or the way the thing has been done, beauty of the AI, performance and scalability. performance and scalability. Well, I know that most of you haven't completed but at least we can speak to that and other people can then ponder over how well you have addressed those issues. Right. So the rubric is there on the slack. Please use the rubric be ready in half an hour. We'll start this presentations. Any other questions guys before I close this help session.