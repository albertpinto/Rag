 at the end of it so it almost feels so obvious like it's brilliantly brilliantly done brilliantly thought out actually it's a pleasure to read papers like this at the end of it almost feels so obvious uh now guys can you see my screen also can you see my screen also? So we will start with the concept of a manifold. What is a manifold? See, I could give you the mathematical definition of a manifold. A manifold in, fold in let's say that you have a space you have any n-dimensional surface or hyper surface right or space any any space people would in topology call it any space in which locally it looks Cartesian, it looks flat. So does our world look flat locally? It does. For example, in this room, the floor is quite literally flat. But you can say, well, the steps are not flat. Yes, that's a discontinuity, but in the middle of the step it looks locally flat, but then you have a discontinuity. And so I mean you do have not a discontinuity, if you have a sharp bend, it doesn't look at the edge it doesn't look flat anymore. But surfaces that locally look flat, how do we bring that concept down? And so there's more to it you say that so think of a surface of the earth everywhere you can put local patches there that is one reason because human beings are like ants on earth for the longest time everybody was sure that the earth is flat except that it left the question what is beyond the edge was sure that the earth is flat except that it left the question what is beyond the edge right so and it's it's a testament to human faith that there is a pretty strong community that still believes that the earth is flat so if you believe for example in philosophy somebody would say that you have to completely believe what your common sense evidence around you is telling. But if you really look at what your common sense evidence around you is telling, the world is flat. But still, the world can be ellipsoidal. The Earth is ellipsoidal. And so it's a good example of a manifold, roughly an example of a manifold, because locally it looks flat. So now we generalize it to higher dimensions. Well, not really. They're hills and mountains. So if you smooth out the hills a little bit, so that a tiny little creature everywhere finds a flat, you are looking at a manifold. So in a very rough sense, the surface of the earth is a manifold. Now, imagine a bed sheet. When you are about to, you go into a hotel and the bed sheet is all flat, the sheets are all flat, isn't it? And to an ant and to you, they look flat. Why? Because locally, the ant can only see locally, it's flat. To you, with bigger vision, also it looks flat. So you say it's flat. But now, you get into the sheets, and next morning when you wake up, how does the bed sheet look? Yeah, all crumpled up, right? So but to you it doesn't look flat, but to an ant it still looks flat. So that bed sheet is a manifold. What is not a manifold is something in which you tore a hole. Right? Because now there is a discontinuity. Or where, like when you put a crease, when you take a sheet of paper and you fold it and you crease it, now what happens to the edge? The edge doesn't look flat, right? It doesn't look like flat paper, sheet. It's literally the edge of the sheet. So therefore, that is not a molecule. Those things are usually called, I believe, orbeefolds, if they're still continuous. You just created an orbifold. So my way of thinking about manifolds is a somewhat cheesy one, but I think it will serve a purpose. It is exactly that. I'm an ant and the manifold is the bedsheet. Right, in some higher dimension space. Doesn't have to be in three dimension space and the bedsheet will not be two dimensional. And you realize that the bedsheet is two dimensional, whether it's nicely ironed out or whether it is crumpled up. To the ant, it's a local two-dimensional space. So that is a manifold. Now, if you have been attending my courses, you notice that I put a lot of emphasis on the manifold interpretation of machine learning. What I say is that data, generally, especially for the purposes of classification and regression, they are always proximal to some manifold and the job of a machine learning model is to discover that manifold, right, the function, a manifold. And remember there is also a duality between geometry and functions and algebra. Namely, what is geometric has an algebraic equivalent. What is the equivalent of a curve? Curve is a manifold, isn't it? Locally it looks flat. What's the equivalent of a curve in the world of algebra? A function, a function of x. now generalize it to higher dimension a two-dimensional surface in three dimension is some function isn't it in the xyz space some function of this so a manifold is a function now relate it back to your machine learning algorithms what What are predictive algorithms? What are supervised learning algorithms? Their job is to find that function which makes the best prediction. The function of x is y hat. Oh, where's my writing tool? Did I walk away with my writing tool? Give me a moment please. Did I walk away with my writing too? It seems so. Give me a moment please. Might have left my pen in my office. Let me see this. Fortunately, Okay. So we agree that i'm sorry not this this curve is this is a manifold manifold a 1d manifold because a curve straightened out is a line. It's a 1D, right? There's only one direction you can move, an ant can move, a 1D. And this is literally y is equal to function of x. For example, it could be sine of x from the way I drew it. It looks almost like sine of x. sin of x from the way I drew it, it looks almost like sin of x. Does it make sense guys? In two-dimensional, you can take x1, x2 and this is your y hat. What are you doing? Data, if something is a function, you expect that it is some something for any given x, for any given point x, which is made up of x1, x2, the coordinates, let's say that some point this p, this p the coordinates of that point you agree that up there there is a y hat associated with it you would agree with that and that y hat is some function some vector function of the x vector make sense and now this is two dimension and you can generalize now to arbitrary and dimensions n dimensions. n dimensions. So far so good, guys? So let's take the problem of regression. Regression is what? This is, suppose I scatter points here in the first one, you realize that y hat is literally the prediction for x. How much ice cream did this entrepreneur sell based on the temperature, whatever it is, this is it. Something like that. On the other hand, if you are doing a classification, then for every point of the input space, there is a probability. So what is y hat? y hat then becomes a probability density function. That function is a probability density function. In other words, it is bounded between 0 and 1. Are we together? And with the additional constraint that the y hat, or which is equal to probability of x. If you integrate it over the entire space, entire feature space, let's say that the feature space is p-dimensional, d x1, x xp what should you get one in other words if you toss a coin there is 70 probability that it is heads 30 probability that it is tails and you add up both the probabilities what should you get one otherwise something is off probabilities what should you get one otherwise something is off take it to the continuous domain when there are many probably infinite number of possibilities and each possibility has a finite of probability density then the integral is nothing but summation right over very tiny bits so the integral of that probability density has to be one so So that's it. But at the end of it, do you see, and this is just a reminder of what I keep saying, so what happens with supervised learning and in particular with classifier is you're searching for a probability density function at each point, right. And as you do that, there is something else that is true only for a classifier, which is that you end up discovering a place of equi-probability. That is your decision boundary. Remember, we talked about grapes, and let's say that we take the weight of grapes and the size of fruit, size of the fruit, and then you have grapes, grapes, grapes, grapes, grapes, grapes. They fall like this. Grapes are tiny, except the one in the break room. They seem rather large to me. tiny except the one in the break room they seem rather large to me and let's just take strawberries right what color is strawberries uh let's give it a nice strawberry color strawberries and oh they're beginning to look like strawberries. So whenever you find a probability function, you're sitting on the fence. That fence, sitting on the fence is your decision boundary. This is just a review of our basics and I'm just saying it because I'm building an argument here. This is your decision boundary. So in our way of looking, because we are dealing with continuous functions, your decision boundary in two dimensional is a line or curve, right? In three dimensions will be a surface and n dimension will be a hypersurface. In other words, it will be a manifold. Would you agree? Now comes the. Now, let's take it to our situation and ask. What is this paper trying to say? So now we've got the concept of a manifold and we realize decision boundaries themselves are manifolds. Let's take it further now. So decision boundaries are manifolds. Manifolds of n minus one degree. Right. So if you're looking in two dimensional space, then the decision boundary has to be a manifold of what degree? It can only be a curve, some complicated curve. Right. So that's what a decision boundary would be. With that thing. Oh, by the way, you know, because I keep saying these things in my courses over and over again, if it looks too easy and repetitive, let me know, we can move faster. But now we come to this paper. It is saying, so look at this paper, what's the date of this paper? And this will give you a historic sense. 1st of June 2022, you realize that much water has flown down the bridge. Isn't it? Or flown under the bridge. The adversarial attacks were discovered in 2013. The last paper we talked about came out in Aeon Goodfellows was 2014, in revised version in 2015. That paper was a good contribution. It showed a way to create adversarial examples. was 2014 and revised version in 2015. That paper was a good contribution. It showed a way to create adversarial examples, which is common sense. But the question is, Ian argued, and it turns out that plausibly, I mean, it would look plausible, but today we are not so sure, that, oh, neural networks are too simple. all machine learning algorithms are too simple, even in patients and maybe the piecewise linearity is a problem. Right, because that is what seemed to him common to all the algorithms. So then came other schools of thought that had all sorts of interpretation, but in the meanwhile much history, much of history happened. But in the meanwhile, much history, much of history happened. People became creating adversarial examples became in a small cottage industry. A tribe of people started doing that. Then some startups started coming up or companies started coming up and say, oh, we can help you protect your models against adversarial attacks and see here is a technique and there is a technique right and then they went to conferences, set up booths and Albert met them. So they all do that and now comes an interesting question, do those defenses actually work and how are they able to defend except by some empirical methods they say oh if this is true so for example if you have an adversarial example add it to the training data so make it more robust that should fix it right it is in this yeah exactly it looks simplistic but then you can take another example which is neither this adversarial example nor that. And so it poses a very interesting puzzle. It turns out that in the feature space, next to a cat are all sorts of things. There are dogs and horses and snakes and God knows what all there are infinite number of them lurking in the vicinity of the cat isn't it so any small deviations of imperceptible deviations from the cat which human beings can't see will make it into a snake or something like that and you can put the example of the snake image and tell it, no, it's a cat. But then there are infinitely many such points lurking around it. How many examples will you add to the training data? So it's not quite a battle that you can win. Do you see that? And that was puzzling people to no end. Why was it right and people initially hope many hopes that all the adversarial examples would occupy regions of the feature space you could cluster or surgically excise them out just blacklist those regions of the feature space right or where they're all mixed up where nothing is is clear. So that didn't work out. So people have tried many different hypotheses and with different degrees of success. The reason I like this paper quite a bit is because it argues in a way that I find very elegant and fairly plausible. It's one of the most plausible interpretations of why adversarial attacks are so very easy to create and so very hard to defend against. In fact, it goes on to show that most of most of the so-called adversarial defenses so far, many of them are rather futile right so now before i go into that i would like to have so now that you understand what this is what a manifold is can somebody enlighten me or what is this paper really trying to say It goes to the heart of what may be causing adversarial points to lurk around. It's basically making a point that trying to explain how these adversarial attacks can happen and points to this wind-filled version of the manifold, which is some sort of a distorted lower-dimensional version of the manifold. Yeah. But this paper I'll go over carefully, guys. And because we'll go over carefully and read it, I really want to, because it should train you in really how you should think about machine learning in many ways, or at least the way I like to think about it as a mathematician, in terms of geometric and mathematical ideas. So if you have the patience, instead of rushing through this paper, go through it slowly. Is that okay? Because clearly you guys haven't gotten it. It is technical, but actually when I explain it to you the sheer simplicity and beauty of the idea is quite nice so we'll read it let's start reading this so question asks if that when they tried to i don't know if bound or curb the feature space with different classes right why didn't that work the feature space with different classes, right? Why didn't that work? You said like the cats are close to the dogs and then they tried to kind of put them all, like club all of the similar ones together. Why didn't it work? I mean, if you- Okay, there is a very simple reason. The spaces are not separable. The set of all points that belong to cat that can be interpreted as cat and the set of all points that are interpreted as dogs are so closely interleaved that between any two points of a cat you can find infinitely many dogs if you want simply put yeah that's the that's sort of the argument. You'll see why the explanation, Sachin, is in the way that I'll explain to you in a moment. So let's read this paper carefully. The extreme fragility of deep neural networks. That's a rather damning statement, isn't it? The extreme fragility of deep neural networks. Many people will like, suppose I said that about your work, the extreme fragility of the code base you have created. So the extreme fragility of deep neural networks when presented with tiny perturbations in their input was independently discovered by several research groups in 2013. It's a fact. Early in 2013, people realized that with tiny perturbations, perturbations means, what does perturbation mean? Small changes, delta, delta axis. Epsilon size delta axis. They are very small in the input. Input x is the input. This fragility of the neural network doesn't mean that your neural network model itself, I mean, the code doesn't break. The inference is very fragile or unstable like a very small deviations of input leads to vastly. Different outcomes or predictions. Kiosk theory has that. Very small changes in the input can lead to vastly different outcomes sometimes. It's something nonlinear systems do that. So to me, actually, when I was reading this paper, a lot of that was in the, I was trying to relate to some of those ideas. However, despite enormous effort, these adversarial examples remained a counterintuitive phenomenon with no simple testable explanation. That is true. It's been a decade now. We are looking for some right or at least convincing explanation that will address all the issues of explaining why they happen. First, you have to know why they happen. It's somewhat like you're sick, you're running a fever. People have to know what germ that you have before they give you an antibiotic. So first comes understanding. So despite tremendous effort, these adversarial examples remain a counterintuitive phenomenon with no simple gestable explanation. In this model, we introduce a new conceptual framework The the the the the the the the the the the the the the the the the the the the the the the the the the end of the training, you end up with a manifold, a decision boundary. But it is what happens in how the decision boundary is cooked up, is created during iteration after iteration, how it forms. Something happens there. And that is the explanation of why adversarial attacks are so easy or adversarial examples are so plentiful so so this is the dimple manifold model they call it today we call it the dimple manifold hypothesis right in particular because they're making a hypothesis that this is what happens and they give fairly plausible arguments to say this is why it happens and go through that in particular we demonstrate that training is divided into two phases the first phase is a typically fast phase the clinging process in which the initially randomly oriented decision boundary gets very close to the low dimensional image manifold so here they speak in terms of images but we can generalize it by the way it doesn't have to be but image makes a good case image manifold which contains all the training examples. So think your fashion, enlist, enlist or something like that. Right. Or anything. Next, there is a typically slow dimpling phase which creates shallow bulges in the decision boundary that moves it to the correct side of the training examples. This framework provides a simple explanation for why adversarial examples exist or why their perturbations have such tiny norms and why they look like random lois rather than like the target class. Like the target class, this explanation is also used to show that a network that was adversarially trained with incorrect label images might not still correctly classify most test cases, and to show that the main effect of adversarial training is just to deepen the generated dimples in the decision boundary. That, I hope, is worrying and it pretty much puts water on a lot of effort that people have been doing. What it is saying, and it is I think right, that training it with adversarial examples, it looks so sensible, right? This guy is calling a panda as a gibbon tell it that it's a cat it turns out that not only does it not learn from that it actually makes it worse it's some form of overfitting yes that is the deeming of the mood dipping of the impulse in a way. So finally we discuss and demonstrate the very different properties of on-manifold and off-manifold adversarial quantizations. We describe the results of numerous experiments which strongly support this new model using both low dimensional synthetic data sets and high dimensional natural natural datasets. So I read this entire abstract because it contained a lot of information. So the mental picture that people have had is that, oh, there's a decision boundary. And if from a point you cross the decision boundary, then of course you'll have an adversarial example. Then it would mean that all the adversarial examples are in the vicinity of a decision boundary, isn't it? Because you're not moving too far. If you move too far, the picture will start looking different. Isn't it? So let us say that you have these islands, red and blue islands, So let us say that you have these islands, red and blue islands. To go from make a red cat look like a blue cat, you would have to go across the decision boundary and some distance. That is on manifold. A bumblebee flying around is off manifold because it is off the surface. It is in the larger space. So or more obviously put, suppose I have line. All points on the line, these are on manifold. Whereas X, Y, these are off manifold. These are off manifold. They are not sitting on the, located on the manifold. Are we together? And observe one thing. If you're off manifold, there is a perpendicular journey that you can take from some point on manifold to that off manifold point. Isn't it? And that is actually part of the crucial argument that there will be. Yeah. And oh, we have almost five o'clock. We have to do the other. Let me do at least finish the introduction, guys, and cover a few sections, because this is very interesting. Can you guys please, I know that you haven't understood the paper, could you give it a second reading before we meet next week? Because it is, these things matter, Dyson. If these models are so easily attackable, It is your ethical obligation to think seriously about protecting it before you unleash it upon the general populace. So one of the things that, okay, I'll say in 2013, in this paper of of the Getty, which is the Ian Goodfellow paper. No, actually not that. So ZD and Biggio, by the way, these guys are really big in the field, independently demonstrated the surprising fact that even the best trained deep neural networks are extremely fragile when presented with tiny adversarial perturbations. We don't know. This discovery naturally attracted a lot of interest, and many attempts to explain this phenomenon have been proposed over the last nine years. That DNNs are too non-linear, that they are too linear, that they were trained with an insufficient number of training examples, that adversarial examples are just the rare cases where DNNs are, that images contain robust and non-robust features. Is there any interpretation that has not been made? So just what variety of explanations are there? So you can see what are the things you can say. Either the model is too simple, piece piecewise linear or it is too non the two opposite points of view you could argue that you didn't train it with sufficient amount of data because if you had trained it with sufficient data what does more data do it reduces overfitting it is a regularizer right so if you say that this model has overfit and that is the only cause of this problem, then you can hypothesize that let me go and get gazillion images. Let me go and get the Leon dataset. What is it? 400 million. Oh, 400 million is like... There is a 5 billion. Yeah, 5 billion. Pass it through. Maybe then it won't be susceptible to adversarial attack. And yet it is. So insufficient number of training examples doesn't work. The adversarial examples are just rare, where DNNs are not true. You can literally open a factory that will infinitely keep producing adversarial examples. That images contain robust and non-robust features, etc. Now you're going into the images that some things are squishy. Right. Some features are clear, but some are squishy. And so, well, you know, ponder and given they're both mammals, they both have a face, nose, eyes, right? And some features are not that robust for classification purposes, right? So these are all the explanations that people have given. And robust and non-robust is also somewhat interpretation of how you interpret what is robust, etc. However, none of these qualitative ideas seem to provide a simple, intuitive explanation that can be experimentally tested for adversarial existence. Would you conquer with this statement, guys? So this paper aims not to propose new adversarial attacks or defenses. So not yet another method to attack, nor a method to defend, but an explanation of what really is going on. Maybe going on. It's a hypothesis. So this paper, but to propose a new comprehensive framework for thinking about adversarial examples. Numerous papers and talks about this subject use some variant of the highly misleading 2D image on the left side of figure one. So he's saying many papers are using this image as their argument. Why is it misleading? We'll discover. In other words, it is positing a simple explanation on one side of cats, one side of dogs. Just find the shortest journey from cat to dog that will necessarily cross over the decision boundary and cross over a little bit. Something to that effect. And in this mental picture, the square, so it is unit square, contains multiple clusters of training images from the two classes denoted by red and blue so suppose the only thing you have to tell apart are cats and dogs all red are dogs all blue are cats right islands are cats training aims to create a 1d curved decision boundary, denoted by the gray line, that splits the input space into two not necessarily connected parts. So in this particular case, you notice that it has split the space, but at this moment the two spaces are connected, but in general it is not. um that is that so its goal is to place each training example on the correct side of the decision boundary and as far as possible from it in order to maximize the confidence level in its provided level subject to the limited expressive power of the given dna in this mental image adversarial examples are created by moving the given image along images along the green arrow towards some kind of centroid of the nearest training examples with the opposite level make sense right you you take a journey towards the essence of cattiness from doubt. Right? And cross the training example. So, for example, this is a point that people have made. And oh, it states that Jan Goodfellow has made in a lecture at this particular time. I've not watched that lecture, but I would imagine so. For simplicity, we consider in this paper only two class classifiers for images that are represented at points in n dimension and use L2 norm. L2 norm is what? Euclidean distance. Simply put, we will just use common sense distance measure, Euclidean distance measure. This input space is split into two complementary not necessarily connected n-dimensional regions by the curved n minus one dimensional decision so where did the n minus one come if you are in n dimensional space decision boundary has to be n minus one or in a page the decision boundary has to be a curve yeah that's it. So far so good guys? Yeah. So let's entirely forget about, should we forget about the related work? The new mental image of adversarial examples. The well-known fact which underlies the new conceptual framework is that all the natural images are located on or near some low dimensional manifold. As shown by a huge number of previous papers going all the way back from 1994. What does it mean? See guys, let us say well known. Did I know that? I claim that you all knew that. How did you know that? You have learned something in the past that would not have existed if this were not true. How did we know that there is a low dimensional manifold on which the images really live. Come again. Yeah, see there are many many arguments you can make. For example, you why do auto encoders compress an image from thousand by thousand pixels down to, let us say, just 10 dimensions or 20 dimensions and still are able to restore the image. But what does it mean that there is a latent space which is just 10 dimension, which means that this space is somehow in a very warped state in the full high dimensional space right are we together just like the surface of that earth is existing in three dimensional space we say is three dimension but you realize that you can actually flatten it out if you take one not pull out then you can sort of do a projection into a plane a plane is two dimension right So surfaces are two dimensional. So in other words, what it means is, and what do auto encoders do they find the lowest dimensional representation or no dimensional representations of what you have. So, in other words, people have and people have done experiments that they have seen that when you take images with high pixel dimensions like you know thousand by thousand pixels etc the actual existence of points is not in all those high dimension it is actually on a low dimension space so I'll give you a mental picture that this is coming to imagine that you're in a three-dimensional world right things could be floating in the air or just imagine that you're flying in an aeroplane right there are things the cloud here cloud there stars here moon there and so on and so forth but suppose or suppose you notice just miraculously you happen to notice and it happens to be a fact that these monarch butterflies when they travel they occupy a very narrow band of the sky at a certain altitude so i'm told i may be wrong but for the sake of our argument let us go with maybe in precise bio but I'm told that they're up there, and you see them as a band. So what happens? They're approximately, they're not occupying the entire vertical space from here to there, here to infinity. They're occupying a plane, isn't it? And in fact, a patch of a plane. So Monarch butterflies are here migrating maybe another community of monarch butterflies somewhere else also migrating as a little band somewhere do you see what i'm getting at guys or just look at clouds clouds themselves well clouds can be pretty thick so you wouldn't approximate it as just a sheet of clouds. But if you take a little bit of liberties, compared to from the distance from here to the moon, you would agree that all the clouds are in sliver. But depending on the type of the cloud, they are at one level. If you look at the Cirrus cloud, they are all at one level. Exactly. That's good. That's a very good statement, that all the Cirruses are at one level. That's a very good statement that all the citizens are at one. So in a similar way, imagine that in your room, you're searching for where are the kids, where are the kids, they could be jumping around, they could be sitting, they could be doing, but for whatever reason, you happen to notice that the kids are all sitting on the ground in clusters. Right? There's this cluster of friends here and there's a cluster of friends there. So in reality, where do the kids exist? They exist on a two dimensional plane. Approximately to a two dimensional. They're there. Now what happens? So imagine that now look at the implications of this. You're in a very high dimensional space. Right. And it's a very subtle argument. Hidden in that high dimensional space is a plane. And your data points are on the plane in a way. Am I making sense? All your images are as little points on the plane. Well, plane would be an exaggeration, a surface. Imagine that all the images are like little patches on a bed sheet. And they are in your room. Now, what you're saying is, what is the, how do I distinguish between the cat and the dog? The thing is, OK, between the cat and the dog, here's the cat patch and the dog. The thing is okay, between the cat and the dog, here's the cat patch and the dog patch, you may put a decision boundary. But there is a third dimension. Right? And what happens in the third dimension? They are neither cats or dogs, isn't it? But if you create a decision boundary like this, they will, it will rise up as a wall, sort of. And that is a crucial'm getting ahead of it. But OK, so now I'll ask you a question. Can you create lots of examples that are neither cats nor dogs? What happens if I take a cat and don't stay on the manifold, go off manifold, perpendicular to the manifold? If I go perpendicular to the manifold, how many points perpendicular to that manifold and therefore a cat going at on top of a cat sitting. So in other words it's like a totem pole right. How many things can I put on top of that, on top of a cat, off axis or off manifold along the perpendicular direction infinitely many and would you agree that none of those points are actually cats or dogs yes and yet a decision boundary necessarily will classify it as something isn't it why are they not cats But why are they not cats then? Because all the cats are on the bed sheet. What is up there? We don't know. So, Asif, is that right? Basically, we put a decision boundary based on some of the training data that we got. We said if everything is on the left, it's a cat. Everything is on the right, that's a dog. But you're saying right and left on the bed sheet. Exactly. No, I'm right and left on the bedsheet. Exactly. No, I'm saying in terms of the decision boundary, let's say whatever is the decision boundary, like the wall that you're talking about, everything on the left of the wall is a cat in a 3D space, is a cat on the right is a dog. But we never talked, we never told these guys about a lot of things. Could be anything. Yeah, exactly. The main point is that, so we'll go step by step, but observe the fact that whenever you train with data, meaningful data always lives in a lower dimensional submanifold. See, this is the thing I've been saying since ML 100, isn't it? Always do, not just for images images data will always be proximal to a lower dimension right otherwise it will be meaningless i'll give you an example see guys take take the example of regression right if i have a relationship between sorry um between x and y like this and it may be a complicated curve x and y like this and it may be a complicated curve do you think there's a relationship between x and y is there a relationship you would look at it and say yes non-linear but there is a relationship what does it mean geometrically geometrically it means that there is a embedded sub manifoldifold that data is proximal to. The first approximation is sitting on it. Isn't it? On the other hand, if I would you would you have any confidence that there's a relationship no isn't it and look at the data is it sitting on a sub manifold yes no no no no it's occupying the whole space isn't it it's just diffused isn't it so what does it mean when when there is really a relationship, what is a relationship? When you say I can predict Y from X, it means that Y is some function of X. It means that it is a manifold. And therefore, by definition, and obviously mathematically, what it means is all the data points are proximal to a hidden submanifold. Your job is to just go discover it isn't it so all of machine learning is a process of discovering that sub manifold am i making sense now right and the moment you have a sub manifold an embedding manifold you have to know that there are always perpendicular directions where there are points that are not on the manifold i can go take this point so fly off into space and what do you call this guy p of manifold point you get that right and so one of the arguments that it will make is that that's a fertile ground for adversarial data why and how and there's much this paper is very nuanced guys very well argued and i love it but we are way past our time how about i stop at this point i've given you enough geometric intuition guys can you please try to read this paper yeah hang on uh go ahead albert so it has to be part of the function it's not a part of the function it's an adversarial problem yeah it's basically you're giving it a point that doesn't make sense. So, so one more thing, but before I won't I will explain these things in detail. But at this moment, take it as a fact that what happens is that, let me give you a very high level understanding. When you're training the thing, the example, this, what happens is, suppose there are two problems at that on the bed sheet. Imagine you have a patch for cats, you have a patch for dogs, and it's a curved bed sheet. to discover the bed sheet now imagine those cat patches hanging in the air dog patches hanging in the air and you know that they are on a bed sheet because that's a otherwise they wouldn't be it just wouldn't make sense right so the you have two problems you have to first find that embedding manifold on which the data sits and then go and build a decision boundary on it. So that is the critical part, right? How do you figure out that some manifold on which the data exists? Exactly. That is the point. That's it. So here is how we would go about it. Let's say that very simple example. Let us say that I take in three dimension space. I want to draw a decision boundary. The decision boundary can be, what can it be? It's a sheet. It's something. It tells that this area is cold, this area is hot or something like that and there are examples all over. First thing is it's a flat sheet so you can start with a random plane somewhere. Imagine a foil, a metal foil, metal sheet somewhere, flexible metal sheet. It's your plane, it's a linear decision boundary. So the first thing you need to do is bring it close to your data isn't it that would be your step one right first bring that happening close to your data so that it even makes sense you randomly started there you need to bring it close to where your cats and dogs are sitting there in the feature space that part makes sense but now here is an example here's a crucial a tricky part you have the foil and you need to now tilt it gradually and bend it in such a way push it below let's say that you want to say right the above are cats, the points below are dogs, because this is your decision boundary on one side has to be cats and dogs. So what you have to do, you have to now decide what parts of that you need to push below the dogs. So now comes the slow part of pushing it. Once you have brought it close to the data, which you can do pretty fast, which they're calling the fast phase, then comes the argument is, and this is where I'm like, it's a very plausible and beautiful argument. I'm not 100% sure it works, but it's argued that see the decision boundary, the neural network structure is, it has to make small changes. And in small changes, it has to sort of step by step cross over. And then bend the sheet of metal to now have this red point above it. This red point above it. Yeah, I mean, it's the learning thing. The trouble is, in a way, it's overfitting. You have enough data, you have this cluster but the point is even if you had lots of data like this you would still have that because you are forced to dimple the manifold the moment you dimple the manifold it leaves potential it leaves potential for you to create adversarial examples isn't it because the when you when it learns think about it this way here here are the cats and you bent it like this the manifold it will bend only enough it will learn only enough such that its loss function decreases but what it won't do is tilt it like this completely it will learn like this isn't it and the images are here that is all that is practically needed because you're training it with real cats and dog images they're here it serves its purpose and what they're saying is adversarial examples are through this right just go off the perpendicular off, and now you can have plenty of adversarial sort of of the more the number of dimples, the greater the chances. The greater the chances because see the more dimples also means that it is pretty close to and in some sense like September see the the the general feeling that Satyam is going to intuition is correct that it sort of looks as though it's a form of overfitting except that it is hard to regularize it is baked into the structure of the neural network the way it is trained that you there is no very clear way out of it right so once you bend it down the network will stop and so the the whole thing is uh it opens up the possibility of two things one is adversarial attack by creating points off manifold but on manifold also because at this point the data date on the data plane itself there will be because it's a non- thing, the ordinary, so those ones are easy to understand, you know, just cross over the decision boundary on the image plane itself. And but if you do that, then it might begin to the dog might begin to look rather cat ish, you have to be careful. But when you go off diagonal, you know, off manifold, you realize you have no danger of looking like a dog you're a cat you're going perpendicular to a cat yeah right so uh so anyway this is a well argued paper guys how about we take it up later that's why they say that if you have the small very small perturbation that's when it gets in place yes it will be easy for on plane for on manifold perturbations large perturbations will be observed but the point here is that off manifold perturbations perpendicular you can go as far as you want so so anyway the points that i want to do and close this way are the explanations of the counter intuitive property of adversarial examples section five i will do guys and i'll conclude with that the biggest difference between the two mental images is how we think about adversarial examples in the old mental image adversarial examples were created by going horizontally towards the nearest training examples with the opposite label, isn't it? So in the new language, you would say that is the on manifold movement from one cluster to another cluster, right? Yeah, the cat group to the dog group images, and which are all very far as it was the nearest training example to the opposite labels, which are all very far away. It was a nearest training example to the opposite labels which are all very far away. In the new mental image adversarial examples are creating by going vertically a tiny distance towards the dimpled decision boundary and then continuing another epsilon on the other side because your decision boundary is dimpled see if you're uh hang on where is it like see look at this point guys the old picture would say go like this you see where i'm writing on the picture figure two go like this cross the decision boundary a little bit right maybe the human being will still see a cat but you know it's a dog but actually when you try that it will look rather doggish right on the other hand in this picture think about it this way guys what happens if you go in this direction vertically up you're going perpendicular to the plane of the data. When you go this way, you cross the manifold. You get to a point that looks very... So let's say that the reds are cats or dogs, I forgot. What did we call the reds? Cats. So cats. It has no danger of looking like a dog, isn't it? In fact, or take this example, anyway I'll go a little bit far off from this. Along this line, keep moving up, you'll cross the decision boundary and after that you have infinitely many adversarial examples to choose from, isn't it? Do you see the point? Let's see, okay. to choose from, isn't it? Do you see the point that's here? Okay. So in the new mental image, adversarial examples are created by going vertically at tiny distance of silence towards a temporary decision boundary, and then continuing at silent distance on the other side. Once you cross over, lovely, right? This model provides the following simple explanations. First question is, why are there adversarial examples at all? How can it be that next to any cat image there is also an image of a guacamole and vice versa? Right? The answer is that all the real cats and guacamole images reside on a tiny image manifold. You know, there's a very low dimensional bed sheet onto which they're clinging but there is not just in three dimensions there's just only one more axis right perpendicular but in thousand dimensional spaces there are thousands of perpendiculars to that bed sheet in all of those directions you can go right the answer to that is that the real cats and guacamole reside on the tiny image manifold. However, below and above the manifold, there are vast half spaces of pseudo cats. And pseudo images recognized by the network as cats and guacamole, even they they do not look like one isn't it they would have looked like it if it would have been on the manifold data manifold right but they are off there you're creating something that in real life you wouldn't see am i making sense yeah that's where so the manifold was created on the basis of the training data, right? The initial part of the paper says that if you do have more training data, you're still not going to be able to solve it. You won't be able to solve it. Because, like, I mean, if you had a lot more cats or dogs, like, wouldn't your manifold be more accurate? No. So look at this picture, I can go on creating lots of dogs and cats and cats and cats and cats and cats. The line will still, the manifold will still be there. You have to come up with some manifold. Yes, exactly. So it speaks to the complexity of the data itself and how it resides on the manifold. It's also probability, right? right I mean no matter what model you kind of come up with you'll never be 100 right oh yeah yeah that is that no having said that with see in many situations the models are fairly like for example let's say that you try to distinguish between a grape and a strawberry example. Weight and size are the examples. There is almost no adversarial attack you can create. Because every grape has a weight, every grape has a size. So they fill the manifold. They fill the page. Any point you take will be a legitimate grape or strawberry. Isn't it? Now, it may be an impossible strawberry like a hundred pound strawberry but out there still you it's not an adversarial example because it will be classified as that so the problem has to do with the fact that data is not residing on a sub manifold it's residing on the whole manifold the whole sheet and decision boundary can be fought but didn't you cover us in this the clinging phase and the dimple creation is the manifold thing right like when you get better and better at it right yeah you you get better and better at it but remember it gets better and better on what it is learning from its job is during training see imagine that you're on a plate on the bed sheet right on the bed sheet the neural network will learn to make the most beautiful and correct decision boundary between the cat images and the dog dogs right because all the cats and dogs are patches on the bedsheet. But what it is not learning and has no clue about is that what to do with the points that are off manifold. Because they inherently don't exist. They only exist in the world of adversarial examples. But the dimple starts to form only when you give a larger training set right that's that's when they start to create it right that's what no no no with sufficient let's see dimples speak to the complexity of the patches being spread out here and there no i thought i read it the different way like that's why i was questioning what satyam was saying he said that you give more training examples right if you go a little bit above no no okay so sachin i will go there um let me draw bring a few pictures i i want to illustrate it this is the paragraph for all that just thinking about thinking about yes yeah there thinking about the decision boundary as a thin sheet of pliable metals. A little bit above, go a little bit above. Which paragraph, section got and which paragraph? This one? One and two, the one above that, right? Oh, training at DNN. So we recall this conceptual framework and note that it is based on two testable hypotheses about how decision boundaries evolve during the training process training the dnn processes in two distinct phases the typically fast clinging process see what is the clinging process data is sitting here you are making a manifold here so very quickly it will come at least proximal to the data right that part should be a easy fly because that's just straight gradients and no no obstruction we did it a different way though right like you you normalize the data remember you used to talk about like bringing the cross x y right in the middle of the data so you assume the data is all normalized so imagine that this is all in a unit case right still a typically fast clinging phase which brings the decision boundary very close to the image right so followed by a typically slow dimpling phase which creates shallow dimples in the decision boundary that try to move the boundary to the correct side of the trading. So why shallow this thing? Because learning, deep learning will minimize the loss. Imagine that suppose I have a red point here. Well, this doesn't look like red. Let me take a red. Suppose there is a, well in this case they bring red or blue above, I keep forgetting. Where is that picture? Oh, red is below, blue is up. Okay, let me just take this middle one. Okay, red is here. Oh my god, this is troubling. But okay, we'll go with that. And then I must have a blueberry or something like that. Great. Let's say that the grape is here. And you're trying to, initially, you're trying to create this. Initially, just imagine that you created a, in the beginning, you came to this, quickly you came to this surface. And then what you try to do is you try to bend it this isn't it you realize that after you have bent it to this dimple the machine learning algorithm has no reason to go like this make a deep dimple yeah there's no reason to because data just doesn't meaningful meaningful data just doesn't exist there. It is empty space. So, to your argument, if I have a off manifold point and I change the model to say that that is an adversarial data, who in the hell? No, no, it makes it worse. I'll tell you how. So what happens is that currently, you take a point like this, this point. Let me just take this point. And you take a point like this which obviously looks like a grape but this to the to this decision boundary but you tell it no no no no it's a strawberry no i'm telling the universe this green line is your manifold and decision then i'm saying anything that is beyond it oh that is off manifold i'm saying anything that is beyond it oh that is off manifold i'm saying no no so i'll tell you no no your intuition is right how far off points never sit on the manifold they're proximal to it otherwise it will be over yeah you have to remember that there's manifolds the mathematical constructs they're infinitely thin So this thin metal sheet is infinitely thin. So by definition, every point is proximal to this decision boundary, thin metal sheet. But when you say off manifold, what is the qualification of an off manifold? Yes, so the qualification of the off manifold is, let's say I'll give you an example. Like here you are there, right? You go from this point towards the manifold and just any point that crosses the manifold is off manifold. This is an adversarial example. And now the risk is if you go too far, God knows it might not even look like a strawberry. So you don't want to go too far. Even visually might not look like a... That is right. And that is sort of the main idea that these people are... No, but it's the other one if you go too far see if you look at this green line or whatever if you look at this green line in the manifold as a classifier right you have to classify as a strawberry or a grape right so anything on this side of the green line is a grape it may be off manifold it could be very further out but it is that green manifold is saying everything on this side is a strawberry everything in that so you're going to agree no no no so one way off manifold means it's adversely yeah yeah it sort of gets see the whole idea is that think about it guys here is a sheet just imagine it's flat like in this paper here they should their patches are red and blue cats and dogs right so you bring a manifold it will be it will stay very proximal to your to your data manifold decision boundary is its own manifold but it will stay proximal to it right and what will happen is, so that it will dimple out because it will just in necessary areas, it will flip over. Now, so far so good. Now comes the question, can we create adversarial examples from any point, just go in the direction of the decision boundary and cross it a little bit. Now the question that you are asking is so we'll come to the defense part later, right? I know that you guys are getting lots of ideas on defense, hold on to that. But you realize that the reason, in a way, the core reason is that there is a perpendicular, in fact, there's so many perpendicular directions to go. Whereas when I made this simple example of actual data, like weights of grapes and strawberries grapes and strawberries there is no other direction to go you can't create an adversarial example above the boundaries because you're not sitting flat on the region but beyond that is that whichever direction no but i'll just point out look at the middle picture whether you have a bunch of blues and reds what is the manifold that will separate out the blues and the reds yeah it is the see you look so okay let me explain this picture guys so what he's saying is and by the way this is a deliberately hard constructed thing they did it for a reason so you see that is a, this is literally looks like the salt lattice. But is it right, as if that the manifold through those reds and blues, which can classify red or blue is going to be incredibly hard. Yeah, yeah, it is deliberately a hard problem, I say. So you agree that the data, where is that? So, okay, guys, let's get it clear where is the data manifold it is literally the plane the plane of this lattice this lattice plane is your data manifold so now you realize so intuitively you ask yourself if in a high dimensional space data is clinging to this plane then that's the first observation to you which means there is a lot of space which is nonsense and much of the risk comes from there yes yeah they are arguing that that is because then the nature of the deep learning is such that it becomes a risk i got your idea just hold on to that what you're saying is sufficiently far from this manifold right right? You will. But then that puts an interesting constraint. See, we don't explicitly compute the decision boundary in a closed form, right? You don't. If you knew that closed form equation, you could do that. But what you instead have is this very complicated neural net, which is you trust it is this function and this lovely decision boundary but you never ask it in close form you just say that okay you know your decision boundary tell me your prediction that's what you do yeah if we knew that function absolutely right we could then therefore have a notion of distance from the decision boundaries. Right? But we don't have that question accessible. The other observation also is that as they are saying that given this is a very simplistic example of a red envelope. You know, on one side we're looking at avocados and cats and dogs and whatever. So if you look at a visual model which has been trained on infinite number of things yeah the way i understand is that the manifold itself is oh no no you can never train on infinite classes i mean no no you have to explicitly state the finite possibilities so if i look at it like a if i look at like a lava yeah and i say, describe this image. Oh, that it will do. It can be generated. Because it's a generative model, it's not a discriminative model. This is a discriminative model. A discriminative model is classifying the same. And classifying is from an already declared set of possibilities. So in other words, you have to say probability of a cat, probability of a dog, and probability of a horse let's say they add up to they must add up to one always right there has to be closure now like what exactly is this paper talking about because for the classifiers that you have? Yeah, yeah, this paper completely focuses on the image classification. Classification. Because it's a hard problem and it's also intuitive. So, guys, what are they saying? So we know. So, guys, remember, in this simplistic example, data plane is flat. Data manifold is flat. True, looks flat to you, right? It's flat, or in fact, it looks horizontal plane on which the data is sitting. Is it occupying a submanifold of the entire space? The entire space is three dimensional. It is in a submanifold. So this begins to, so this is, it's parallel to the real world. Images are sitting on a submanifold. And then you start to bring a decision boundary so what would you do you take a bed sheet first you wrap throw it all over the points so it comes very fast phase now there it is but there's a problem but one side is supposed to be red one side is supposed to be blue so what do you need to do this bed sheet is sort of uh what is it it's like a knitting netting right so you can sort of push it down so that the red points stay up and the blue points oh no red points stay up and the blue points stay down or something like that so how will the blue point stay down and in those areas you pull the sheet up so the blue points will stay down and red areas you push the sheet down so that because the points are in a plane so to keep the red points up you'll have to push the sheet down and to keep the blue points up down you have to pull the sheet up so you will end up with a dimple sheet to pull the sheet up so you will end up with a dimple sheet do you see that intuition and what you're seeing is because you have that happening it's a hypothesis remember guys these are not completely bulletproof arguments these are hypotheses it's a mental it's a model of what is happening and therefore you see that much mischief comes from the off of manifold direction right so any like off manifold direction when i go now you realize that there is uh the thing so we will discuss it in more detail guys and later on but let me finish this section five i wanted to these four five points so that four points that stays there so uh now guys with this mental image let's go back and say the answer is that all the real cats cats and guacamole images reside on a tiny image manifold namely in the case of blue and red points on the plate however below and above the manifold plane. However, below and above the manifold, there are vast half spaces of pseudo images. Things that are basically nonsense spaces. Recognized by the networks as cats and guacamole, even though they do not look like one. The point is a decision boundary has a jaw. It partitions the space into two halves. Isn't it? It will call one half cat, one half dog, right? Whether you like it or not, whether it's nonsense or not. So given a non-cyclical point, it says, well, you know what? It's a cat. You got it, right? That's it. So that's the one point, isn't it? The adversarial examples we generate are such pseudo-images. Note that when we consider multi-class classifier in an n-dimensional input space, they have multiple n-1 dimensional decision. So that we can do pairwise, forget about it. Second point, why are the adversarial examples so close to the original images? So this question comes, right? Adversarial example, when you look at the panda and the gibbon, right? Pandaversarial exam, when you look at the ponder and the gibbon, ponder that was misclassified as gibbon, they look so close. The amount of noise you introduced was so minuscule, so utterly tiny, right? And it was hard to explain why that little perturbations could cause such vast outcomes. As explained above, DNNs prefer to have large perpendicular derivatives in order to have shallower dimples. So what happens is. Look at this point, guys. At this point. At this point. This is a point of high curvature. In fact, a related hypothesis is high curvature hypothesis. You agree, right? To have shallow dimples, it will have high perpendicular derivatives in order to have shallow temples that make it easy to undulate the decision boundary and the training examples. The tiny distance is a direct consequence of this large gradient since it suffices to move a short distance to significantly affect the confidence level. So if in the vicinity of the decision boundary the gradient of the loss is high right so to change your answer right change make a big change in your probability of your answer you need to go really a tight if this is big then you need to take pretty small steps right the tiny distance is a direct consequence of this large gradient since it suffices to and this is as simple as this you know this is referring to this right this we encountered in the last paper remember this is the last reading why don't the adversarial perturbations resemble the target classes this is this used to be a question then those adversarial examples why don't they resemble the target when we use adversarial attack to modify a cat into a guacamole. Why doesn't the perturbations we use look green and mushy most adversarial perturbations look like featureless small magnitude random noise. We saw that in the newspaper random noise. In the new mental image, we are moving roughly perpendicular to the direction of the guacamole image. So can I use dog instead of guacamole? We are moving perpendicular to the dog image. For example, if a unit vector towards the nearest guacamole image is this, then a random unit vector perpendicular to it is is like if a vector is going like this, right? Then you realize that in a three dimensional space, there are infinitely many vectors will go perpendicular to that vector. Right. And. vector right and which each is a tiny positive or negative value around this given such an adversarial perturbation looks like the random salt and pepper perturbations we see in the standard demonstration so those perturbations will not have structure they are small random perturbations salt and pepper right so it makes sense it has been experimentally demonstrated that more robust networks tend to be less accurate, right? Why do robustness and accuracy trade off? Things that are robust to adversarial attack, they tend to be less accurate. In the the new model each of the training and the and the existence of nearby adversarial examples are two sides of the same point when we train a network we keep the images stationary and move the decision boundaries around them by creating difference right that's the thing right when we create adversarial examples right we keep the decision boundary fixed because you know the model is frozen, right? And move the images to its other side. Allowing a large perpendicular you have a point here. You don't want to bend your decision boundary like this. In fact, that would be overfitting. Right. You don't want to make such weird things, you want to have a nice shallow like this dimple minimum dimple like this so that also uh yeah uh where am I in um it has been in the new why do yeah okay when we train a network we keep the images stationary and move the decision boundaries around them by creating dimples when we create I would say by allowing a large perpendicular derivative makes the training easier since we do not have to bend the decision boundary around the training example sharply. However, such a large derivative also creates a very close adversarial example. Isn't it? So see, the gradient of the loss, when it is is good helps you learn better but once you have learned the trouble is it also helps the adversary learn faster right so that remains a fact because see what is happening to learn you're moving the decision boundary for adversary you're moving the decision boundary. For adversary, you're keeping the decision boundaries similar, but you're again doing the gradient of the loss, but with respect to data. See, here's the thing. In learning, you do a gradient with respect to the weights. Isn't it? Or you can say theta with respect of the losses with respect to the parameters, changing the parameters, changing the, moving the decision boundary in adversarial attack you are instead doing this right that's pretty much what they're saying now um doesn't look like a fixable problem uh yeah i mean see it needs first of all, there's the thing, is this hypothesis true? Right? They make really strong plausibility arguments is one of the leading candidates at this moment. The strong curvature hypothesis is the other way, the sharp curvature hypothesis, which is quite related and similar. And other point that it makes is adversarial training just deepens the dimples. What will happen? So think about it this way guys. Suppose you give it a point here and remind it because it came from a dog and it still to human eye looks like a dog. You tell it it's a dog, all that will happen is the new decision boundary will now go like this. It will just sharpen the dimples, deepen the dimples. Isn't it? Common sense, right? That is it. That's what they're saying. So I'll let you guys read the paper. It's a beautiful paper. Read the rest of it, guys. It's worth reading. Because it's a serious problem with neural nets and people are trying to understand it. Is there a paper that talks about how to avoid this? There is a paper. Yeah, of course. I shared it in the resources. The very next slide is that. It's a survey paper. Where is that? In the very next year yeah the next one yeah this one there's an underfitting see underfitting leads to robust models but they generally don't help you with that then the predictive yeah yeah but it will help with this so you can have it robust but inactive. That will be robust but accuracy goes for a toss. It's a double-edged sword, yes. But also the thing is, even if it is robust, it will have lesser dimples but you can still attack it. Yeah, yeah. That is the hypothesis here. Any occupies a very limited subspace or embedding sub manifold that leaves a lot of dimensions for mixture unless it's very dense right what about a very dense yeah yeah sort of but images inherently have this problem right they're pixel dimensions right or is raw data so see thousand by thousand pixels is a million dimensions yeah right and from there you're trying to reduce you know that a cat doesn't need a million dimensions to be expressed yeah yeah. One thing that. makes sense. The same. is making not. yeah. that from one network to another just make it null if you don't know this x or y just make it not yeah that was the only one that makes sense because the other ones all had some actually it has a weakness because null just introduces a code if it's not a cat or a dog is something else so it becomes a three binary classifiers dog or null cat or null bag or cat right and ultimately a decision boundary will be a composite of this so what basically right i do i don't think it will quite solve it and make it better that they have given them like the distillation or you know features using or any of the defense yeah none of them make sense yeah yeah exactly distillation is the approach that operations it's like let's take a more robust simpler mode yeah right but that's not going to solve that because it will come more immediately yeah so guys here's the thing when you think of problems from a mathematical perspective do you realize that you gain the power to see what will and will not work in some sense now i don't know whether this hypothesis is true it's a very very intriguing paper. And I thought I'll introduce you guys to the same. Of course. All right, guys. So that ends the paper reading section for today. And now first is Masmeen Banerjee is going to talk about on this topic. And then Satyam is going to do his demo and then we'll have the Cambrian. Patrick, you're ready with your Cambrian? Or next time? Next time. Next time. Next time. OK, so we'll have Mosby and Satyam. No, no, I won't do that. People will forget. We are moving past this topic. So talk about it. I'll stop this team. Or you want to talk here? No, I