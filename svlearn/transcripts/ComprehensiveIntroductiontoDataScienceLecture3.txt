 Last week, we covered regression, linear regression. We covered the foundations of linear regression. If you remember, our lesson plan was to understand machine learning first and then how it applies to regression in particular. We learned that machine learning, the learning part of machine learning is built out of two concepts. One of them was who would like to volunteer? What were the concepts on that form the foundation of machine learning? How does the machine learn? Gradient descent is one. Yes, it is the yes. Excellent. So between yes, both of that both two legs are. I'll just say it aloud. dantik said is errors first you need to quantify how much mistakes a machine is making what is the quantity of error okay then once you have quantified the errors now you have to find a way to efficiently make the machine reduce its errors learn it learns by reducing less mistakes, just as we human beings learn. And the evidence of learning is that we make less mistakes, right? So, and you ask a child whether it's a cow or a duck, if the child is just guessing, then the child will have a high error rate when you point out to an animal on whether it's a cow or a duck. When the child has learned, the evidence of that learning is, how would you know that the child has formed a concept of a cow or duck in our mind? What is the, you know, I suppose the duckiness of a duck or what's the essence of a duck or the essence of a cow? Once it forms, the essence of a duck or the essence of a cow once it forms the evidence of that the empirical evidence is the error rate goes down when you ask this child what is it cow or duck the child makes less mistakes they come in child may still make mistakes it may never have zero errors it makes far less mistakes and that is the evidence that the child has learned and exactly the same thing applies to machines so we need now an efficient learning engine right a way to efficiently learn or optimize the learning so people call this the optimization process the technique of optimization and the workhorse in the industry is the technique dominantly used there There are many techniques, but the dominant technique is something called gradient descent. The essence of gradient descent was that, and we will do that again in this, was that you look at something called the error surface in the hypothesis space. For every hypothesis, for every set of parameters that you choose, people often call it the parameter space. I like to call it the hypothesis space. For every hypothesis, for every set of parameters that you choose in the, in the param, people often call it the parameter space, I like to call it the hypothesis space, you can quantify the error. Then that is basics of machine learning. Then we come to regression. Regression is the process in which you have some inputs, and what you're predicting is a number. are we together let me write it i suppose correlation yes so so what is the measure of learning it is that is the process of systematic reduction in the errors. We did that. Again, to review, we took the example of regression. So regression, I will just recap by saying that in the case of a line, we took the example of a line without me actually defining regression. So today, let me just review that. Suppose you want to find the best fit line that goes through the data to do that in the what you do is a line is uniquely defined by its slope and intercept isn't it so you say that the slope and intercept are the parameters which which you are searching for when you say i'm searching for the best fit line to the data you're searching for the best parameters beta naught and beta one beta naught being the intercept beta one being the slope to fit the data right and once again to recap our convention and by now you must be familiar with it we use roman letters for observables for data x and y so those are roman letters we use greek letters for concepts everything that is not observable we use greek letters for for example slope is a concept we have a hypothesis that this data is best expressed by a straight line. That line is your hypothesis. The defining features of the line, the parameters are its slope and intercept. So therefore, we'll use Greek letters. And that's where we differ from high school notation, which is mx plus b. Instead, we say beta naught plus beta 1x. It's just a change of notation. There's nothing fancy about it, right? So that is what we do. So then the question comes, how do we quantify the error? We realize that at any given moment, you're making some errors, the gap. You make a certain prediction, and this is where my mouse is, is your thing. And we do this residual error. For each prediction prediction there is a gap between the prediction and the reality of the data right that gap is the error from this gap we create the notion of two ways of quantifying loss so one way that we don't quantify losses we don't add up all these errors why do we not add up all these errors do we remember that add up all these errors why do we not add up all these errors do we remember that negative yes yes because some errors will be positive and some will be negative you can see from this picture and let me exaggerate it so suppose this is the line of prediction and some data points are here like this so let's look at this data point this error is let us say y and this is y hat i right so this is positive on the other hand this error this is y j and y j hat what can you say about this two point this error is negative isn't it y minus y hat is negative y is smaller than y hat so the positive and the negative errors can cancel out and so you won't really get a sense of a good um that you're learning so you need something that doesn't look at the sign of the gap but the measure the the magnitude of the gap magnitude of the residual error right and so there are two ways that we can do that the two ways are well you can take the absolute value of so there are two ways that we can do that. The two ways are, well, you can take the absolute value of them. There are many ways, actually. I correct myself. There are infinitely many ways. But among the simpler ways, you can just take the absolute magnitude. Right. When you do that, it is called the, what do we call it? Sum. Right. This is the sum of absolute errors right and the other one is some squared errors you square the error if you square the error it becomes positive and therefore it begs the question well why not if i can take the absolute magnitude i can take the first power the second power the third power the fourth power i can take any power power, the second power, the third power, the fourth power, I can take any power. Why? Which is the best answer? It turns out that there is a theorem by the great Gauss and Markov, Gauss-Markov theorem, which basically says that of all of these, for linear regression, the optimal one happens to be the squared one, the sum squared error. It is the blue theorem, B's best linear unbiased estimator, that sum squared error is the best linear unbiased estimator. unbiased estimator, right? And remarkably, if I get my dates right, 1793 is when Gosse published a paper in which he didn't reveal that he had used the least squares, but he was solving the problem of that astronomical body that we talked about, predicting its location. And then 1805 is when Legendre wrote his famous paper, literally called least squares method of least squares where he introduced this concept and i believe it was 1812 when the goss markov theorem formally made its way right but it is the best linear optimization so just to give you a sense of history that this subject is older than you think. The foundations of this subject were laid a couple of hundred years ago. So almost 200 years ago, remarkably, and we are using that. Now, that is that. And so we say that if you look at two lines, in anything, we also have the question of the null hypothesis. What is the null hypothesis? Who hypothesis. What is the null hypothesis? Who would like to explain the null hypothesis? Which means the X and Y, they are not related. Exactly. That means X will not have any impact on Y. That is right. So in other words, Y doesn't respond to X at all. I'll give you an example. We took the example of a young entrepreneur trying to sell ice cream on the beach. So let's say that temperature would be a good predictor of how much ice cream would sell. On hot days in California, much more children show up on the beach demanding ice creams. And so there's more sale of ice cream on a hot summer day than on a cold winter rainy day in California. So if X is temperature, Y is the amount of ice cream sold. You see that plausibly Y is related to X, the temperature. On the other hand, if X happens to be the value of stock, some stock that has nothing whatsoever to do with ice cream, let us say a Caterpillar stock, or I don't know, Microsoft stock, or whatever it is, or stock of something else, it would be hard to to you would not really expect the amount of ice cream sold on the beach by this entrepreneur to have much of a relationship whatsoever with the stock price right or the stock of something in let's say some asian stock market some company you don't expect a relationship between the two of them, or a direct relationship. So then the null hypothesis, or another example that I took is how many penguins jumped off a particular ice sheet into the sea out of fun on that given day. One is not related to the other. And so the null hypothesis holds. Y is not related to the other and so the null hypothesis holds y is not related to x so whenever you get data do not immediately get busy trying to find a relationship first sit and ask is it even plausible that there is a relationship take the null hypothesis seriously that there may not be any relationship whatsoever. This is actually more practical than you would imagine. People have often ascribed all sorts of things to be related to, for example, people have created the notion of IQ, people who try to measure intelligence. Much of the concept of IQ is essentially debated or disproved. But one of the things was that IQ is, let's say, positively related to whether you are a male or a female. Men were supposed to be brighter, right? And often the reason given was that men have bigger brains, right? Until somebody pointed out that elephants have even bigger brains. Right, so, right. So obviously before anybody looks at the data and makes such a statement, it's worth asking, what if the null hypothesis is true? And of course, today we don't believe in such hocus pocus anymore, isn't it? Right, for example, if you go to the hocus pocus anymore, isn't it? For example, if you go to the STEM fields into the university, you'll be pleasantly surprised. The world has changed. There are far more women in STEM fields, in particular in computer science, than there are men making their way through graduate school or making their way through college, actually. So the world has changed. And these old things are no more believed in. So, well, still believed in by some people. So, the null hypothesis has to be taken seriously. Right? Ask yourself, is it true? And so, any relationship that you find between X and Y, remember, must be weighed relative to the null hypothesis. It must improve upon the null hypothesis in its predictive power, in its explanatory power. And then you say the hypothesis, the theory has some weight, that model has some weight. And these words we will use interchangeably, hypothesis, theory, model, or interchangeable models, at least for the, interchangeable words, at least for the case of it. So now, one related concept of sum-squared error is the mean-squared error, which is just nothing but taking the average of it. So you can have sum-absolute errors, average is mean-absolute error, M-A-E, and sum-squared errors, mean is mean- error, M-A-E, and sum squared error, mean is mean squared error. So you find all these interrelated words, and it is purely stylistic which you pick. So long as you square the errors, which is the norm, you can take the sum squared error, you can take the mean squared error, you can take the root mean squared error. Right? Does not matter. Right? Does not matter. Right? So now we came to the concept of the, and this was this was the question that sort of relates to you, Sanjay, to the question. Suppose you have this data, we have the three models, hypothesis A, which is the yellow line, hypothesis B, which is the red or pink line, and hypothesis C, which is the green line. If you look at the data, visually you can tell that hypothesis A is simpler than the ground truth. It is positing that the relationship is a straight line, which is an oversimplification isn't it you say that hypothesis a suffers from bias errors right bias errors come from oversimplification and we will be more mathematically precise about bias and is related to ingredients in a later session but not today but this is your bias the second error if you look at the green line it is hypothesizing that the relationship between x and y is this complicated uh green curve what can you tell about the green curve it is more complex than the reality isn't it so you say that it suffers from variance errors variance errors happen when typically you overfit to the data or things like that this is a variance errors generally when you make a model more complex in the ground truth and it happens when you start listening to the noise all data has, but it also has noise. It has errors in it. When you start listening to the noise in the data, you're overfit to it. You have variance errors. And then if you look at the hypothesis B, which is the pink line, I hope you would agree that pink line seems to be the most sensible of the three. Isn't it? It sort of resonates with the ground truth. So our job in machine learning is always to find that model which in some sense resonates with the ground truth. Are we together? You cannot a priori tell which model it would be because data when it presents itself, it is like a little bit of a Sherlock Holmes story. You know, a crime has been committed. The data are the clues that are left behind. You need to do some detective work to hypothesize, build theories of what has happened, present some scenarios, and then work out what exactly could be an explanation of all the clues. Isn't it? So, given a data, a priori you don't know which model would fit best. So never be under the belief that some model is always better than some other model and things like that. You find a lot of that language in machine learning. It doesn't work, right? It's sort of data rules. Data always determines a ground truth and different models may or may not work. And your job is to find the best model. To find the best model to find the best model is the world of hyper parameter optimization or the biation search of the best model there are a lot of words today in modern language i would encapsulate it as automated machine learning what is the process of discovering the best model that would fit to the data right i will learn about that so as if if you go back so if we do the r square that will not give the which model is good no r squared would tell very good question so what does r square tell r square just tells this particular model that you build whichever one it is how much better is it than the null hypothesis that's all okay you can pick a model you can pick for example the the yellow model the hypothesis a straight line all that r squared tells coefficient of determination tells is how much better is this white the the yellow line the yellow hypothesis than the null hypothesis but the fact that it is better than the null hypothesis doesn't mean that there is no scope for improvement clearly the pink the pink curve the pink hypothesis is significantly better than the yellow hypothesis isn't it so if we get the r square for all the three the pink and the red and yellow and say they are a and b and c we can't correlate themselves yeah so the answer to that is quite interesting actually the let's say this way r squared is a clue in machine learning r square is a very powerful clue that you may be heading in the right direction but one of the lessons you learn in today's lab sanjeev is that r square cannot be the sole determiner it can be misleading right sometimes you get very good r squared but you still are off base right but r squared of the error generally is a pretty good indicator whether you're in the right direction or not. See, in science, things can, or your results can disprove a theory. They can never prove a theory. Remember that. Science is about falsifiability of hypothesis. No theory is ever proven, right? So I will sort of borrow that general tourism to data science and say, whenever you build a model, any one of these indicators like R square, and then today I'll teach you, and the residual analysis and so forth, they all each can disprove your hypothesis. Say, not disprove, basically say that there is a scope for improvement, right right they can give you a clue that you're you're probably off base but not one of them can prove that you have arrived at a very effective model are we together it is that you can you take the cohort of or you take the collection of evidence and it is the preponderance of evidence from multiple angles that makes you believe that you may have arrived at an effective model so see if i'm moving from the abstract map concept to let's say what how do i look at this practically right i would assume the business context i trying, let's assume I'm doing this regression to solve a certain business problem. There would arrive a point at which the model that I've come up with and the associated R-squared and the incremental improvement that I can achieve is not giving me any more significant business. Point of diminishing. Absolutely. See, another way, i keep saying the word effective model see and let's now relate it to business premjit's question i'll repeat is in reality when you bring it down to the ground when do i stop my exploration my hunt and the answer to that often is is the business that decides the reality reality that decides. Suppose you're using, I'll give you an example. Suppose you're using it to determine your advertising budget. You're saying, how much sale will I have if I advertise using this, this, this and this factors, what are the factors that affect the sale? Now, if the needle has been moved you have a model that has given you sufficient lift that your business is actually benefiting from that right you say it's a great start you immediately deploy the model into production it might not be the optimal model you go to market with it that buys you time to improve upon the model now what happens is that you arrive at a point of diminishing returns very quickly right the the reason for that is your data usually we have reached the level of mathematical sophistication now that most of the time in fact invariably the bottleneck to making more effective models is not the variety or richness of algorithms. It is whether there is enough information in the data set that you get. See, the algorithms can only learn from the signal that is actually present in the data, information that's in the data. Once it has been squeezed out, there is no further scope for improvement. And that arrives pretty soon. The algorithms are pretty smart and you guys will become pretty smart. So that typically very quickly in a week or so, once you have cleaned the data and done things like that, you would have reached a point of diminishing returns. And you would know that it is almost pointless to go on trying more algorithms or more effort because you have probably squeezed the lemon enough right so that's how it works in practice right and this is a point that you miss actually that you cannot find what is in there and you're limited by what is there in the data that answers your question premium yes i was kind of making a statement question yes yes yes yes. quite a retarded question so back to the frame gist situation. So you get a whole bunch of data and that's the confusion that we have everywhere. Or you want to analyze something and if there are real numbers like here numbers, then you can start off with linear regression to solve your problem. Yes. The path you follow when you start. Start simple always. And that goes to the heart of something called interpretability. One of the key things you learn in this workshop is explainable AI or interpretable AI. See, let me put it this way. Suppose you are a, I mean, I'll just sort of make it a story of sorts. Suppose you have diabetes and you go to a doctor and the doctor gives you a very complicated set of instructions. Make sure you wake up exactly at six in the morning, go for 23 minutes of walk, right? Come back, then you eat this and then you do this. In the evening, you do that and add this thing, you are pretty much lost on what you need to do to manage your diabetes. But if a doctor comes and says it's simple, lifestyle changes are important, right? Amongst other things to manage it, make sure you walk a lot, right? And you manage your diet. Right? Or make sure you exercise a lot whichever way you want to do it like burn a lot of calories and cut down on the calories you're taking to the extent that you can and that is it go do it figure out a way and do it now which doctor's advice are you more likely to follow the simpler one right and so it goes to the heart of and it also relates to the Occam's Razor principle. The simplest effective explanation is the best one. Now complicated algorithms tend to produce very complicated explanations. If they produce any explanations at all, most of them, they don't produce any explanation. So you throw a deep neural network, it will make it will become like the Wizard of Oz, it will make predictions, but won't tell you how. Right. But the simpler models give you an interpretation of what's happening. You always want to go with the simplest explanation. So you start with linear regression, always. I do, because for me linear regression at least forms a baseline. The null hypothesis and then the linear regression. And you build upon it and you see the improvement in your results. Is r or r squared prioritized over ? No, don't get fixated on any one measure. See, you could pick something mean squared error, r squared, or you can look at the residuals and so on and so forth. Look at everything. People do tend to get fixated. So for example example there was a time not long ago when everybody just talked about r squared and in statistics there was when medical literature and psychology especially they got fixated on something called the p-value right it's a measure of statistical significance it was abused many many many papers have been written in which they only claim that they are seeing a relationship is p-value. If they get a good p-value, below 5%, they say, oh, you know, there's something interesting, some relationship. And people found all sorts of relationships that didn't exist. There was a strong backlash actually. Somebody wrote a book which was a little bit harsh, I would say. It was called the Cult of Statistical Significance. And of all things, it completely attacked the whole fixation on p-value, right? Many people just dismiss p-value as an invalid statistical measure. At one point, some journals even said that if you mention p-value, you won't publish it. Today, we are not that. P-value is a valuable consideration. You can use it. But you have to use it judiciously. Don't make your entire evidence p-value. In the same way, don't make your entire evidence your r-squared. Because you can easily build models which have perfect R-squared and are still completely garbage. So never trust one metric. That's one of the lessons of it. See, you're dealing with reality. Data comes from reality. Reality is nuanced and complicated. So you have to learn to think in an advanced way. Don't think in an overly simplistic way. So that's that. So this is r squared, which is r squared, and better r squared is better. r squared is the improvement, the coefficient of determination is the improvement upon the null hypothesis. This is the main equation. r squared is how much your hypothesis phi the error of your hypothesis sorry null hypothesis your hypothesis h improves upon the null hypothesis e phi error of the null hypothesis right and the degree of improvement is the coefficient of determination it is what it is by the way now gradient descent we talked about gradient descent the the gist of gradient descent is simple if you are on a surface if you are given any function think of a function as a real hill a slope how would you get to the bottom it's simple you take a step against the slope You take a step against the slope, right? You realize that at point A, the slope is positive. Here, slope, oh, sorry, slope is negative. Sorry, somebody should be on the watch out, guys, whenever I make absent-minded mistakes. Slope is positive here at B. At A, it is negative. What is the slope at c zero so at a if you want to go towards home the you need to make a step against the slope slope is saying negative don't go negative x go positive x instead take a small step in the positive x direction at b the slope is positive if you take a positive step you'll move away from minimum instead go against the slope take a negative step so always take a step against the slope and this is now today we did it for one dimension today i'll show you the same mathematics is true for two dimension so today's topic will be um perhaps in this entire workshop the most mathematically heavy so i hope you are well caffeinated and i brought your mathematical hats and I brought your mathematical hats. But I assure you, after this, the rest of it won't be as involved mathematically, but we'll go to higher dimension now with this. So this mathematics, I hope, is very clear. This is a big mantra. This is the gradient descent step. The next step that you take is against that. Now, when you bring it to the world of hypothesis, what happens is that the step that you take is against that now when you bring it to the world of hypothesis what happens is that the the step that you make is not in xy plane but it is in the beta naught beta one plane so this is a real world the two worlds are talked about the world of data empirical data which is x and y axis then there is a world of the hypothesis space or the parameter space also called in the parameter space beta naught and beta 1, right? For every line in the real space, which is a hypothesis, there is a point in the hypothesis space, isn't it? It is uniquely defined by a beta naught beta one. This is the crucial observation to remember. And if you remember that, then the only question is at every choice of beta naught beta one, there is a certain amount of errors that your hypothesis will make, that your line will make the sum squared error, right? So you take any one line and you will end up with a particular sum squared error for that hypothesis and then as you work it out you realize that in this equation x and y is data they are fixed in the learning process you can't change your data right so in other words you don't change your data to fit the hypothesis. You change your hypothesis to fit the data, isn't it? That reminds me of an experience I had in college. Sorry for the digression. This was one of the things. We had just gotten into, I come from IIT, so we had a classmate. We were supposed to be doing physics lab in our first freshman year, and there was this unavoidable pendulum experiment, and we were supposed to find the value of g. Right? So, you know, you swing the pendulum and everybody comes to a different value. There was one student who, by the way, is a very successful CEO today, but he came up with a value of 9.81 for g, gravitation. So even the lab technician, he was looking at the lab manuals and he said, this is not correct. So he says, why? This is correct. He says, no it's not correct now by the Professor will give you give you a zero. Right and the student didn't agree and the Professor gave him a zero can you guess why he gave him a zero. He went backwards, he took the hypothesis that he is 9.81 and he manufactured his observations accordingly he rigged his observations you can't do that because but and because you hype and by the way the hypothesis was wrong 9.81 is true only if you're sitting on the equator at sea level right it's not true way up there so uh that's something to remember. So if you keep so given that, so if the data is fixed in this error equation, if you look at this error equation, hang on, what is the variable? The only thing you can change are the hypotheses, isn't it? Beta not beta one, parameters of the hypothesis. And so error is a function of beta. This is the key observation. Error is a function of beta. What does it mean? That in the beta space for every point, there is a certain error. Y-axis, if you make it, the vertical axis is the error. So there's a certain error. The other observation to notice is this equation is quadratic. It is quadratic in beta naught beta one, isn't it? How do quadratic equations look? It's like a parabola, isn't it? So this is how it looks. So it is a parabola that's rising up over the parameter space, beta naught, beta 1 plane. If you think of this floor as the beta naught, beta 1 plane, hypothesis plane, it is this, your error is a ball rising above it. Now can the sum squared error, sum of squares ever be negative? No. So it means this was never really touching. At most, it can touch the ground, which would be the case at which minima, there's no net error left. In reality, because there is irreducible error, remember we talked about it, this bowl will be lifted above the hypothesis plane, isn't it? So any point on this plane, any point that you pick, right? Let me let this five fingers be different points that I picked on the hypothesis. The place where it is touching, my fingers are touching the bone, it's a measure of the error, how far lifted it is from the ground is a measure of error that that hypothesis, each of my fingertips being a hypothesis is making with respect to is the error that it's making isn't it so what do you want to do how would you do a learning you want these fingers each to gradually move down to this you see that right look at this you want to see oh my goodness yes so let me do this do you see my fingers here if you imagine that this is your error surface rising above the plane the hypothesis plane beta naught beta 1 plane this is your surface error surface parabolic or the convex error surface. Now, if you are at this point, how would you go to the minimal? You will take the path of steepest descent, isn't it? Literally like here to the center. Can we see those remotely? If you are a little ant, you would literally tumble down the path of steepest descent. And the path of steepest descent, steepness is given by the gradient, the slope. You walk against the slope, means you don't rise up, you fall by the slope, you go. And so if I have these five fingers, each point representing five different hypotheses, each point representing the error for five different hypotheses on the hypothesis plane how would you make these how would you make these points go to minima sort of like this walk down to the minima isn't it and that is what you're going to do walk down to the minima, isn't it? And that is what you're going to do, right? So you take the path of steepest descent and that part is obviously the path of gradient descent that we do. And I should have written this here. Let me do that. The equation that you want to do is any parameter, any beta, whether it's beta one or beta zero, you would want to say one second, whether it's beta 1 or beta 0 you would want to say one second either whether it's beta 1 or beta 0 is the next value of this is whatever its previous value is minus alpha the learning rate remember we talked about the learning rate small step how big a step you take and the error with respect to beta i. So whether it's beta naught, but you go ahead. So the actual equation of the model doesn't affect the hypothesis. What say I do beta naught minus beta 1 x? See, those things don't matter. So for example, if you said beta naught minus beta one x you can write it with beta one prime there could be a beta one prime where beta one prime is equal to minus beta beta one right so long as it is linear it doesn't matter it has to be linear right because if it as we will realize when it is nonlinear, bilinear includes polynomial, because in linear algebra, polynomial is actually a linear equation, right? So polynomial is perfectly fine. What happens, one second, if you go to higher degrees of polynomial, what happens? Your hypothesis space is not two dimension, but three, four, five, whatever dimension, but you mathematicians think of a hyper surface or hyperplane. All of these are still the ground, the floor, but they are higher dimensional floors. Above it rising the error surface, the error axis. Premjit. Graphics question is something that I have probed in my mind a couple of times. I'm gonna repeat it to make a a connection so when trying to understand the inclusion of how gradient distance is working you don't need to actually worry about the actual model the actual model is not at all used here because all you're looking at is when the hyper parameters change how does the error function respond to it? That's basically what you're visualizing. That's right. The actual model is how the picture looks. All you're taking from that is the parameters of the model. What you're saying is correct, but the terminology is a little off-base. See, the class of hypotheses that you are taking are linear models, right? In this case, using beta naught beta 1. Each model is a choice of beta naught beta 1. So in this space of infinitely many models, you have to pick that model or that hypothesis, which gives you the least error. And the search for that best model is the search for the best beta naught beta 1. And the search for the best beta naught beta 1, or let us call it in a more formal language, the more perfect beta vector. The perfect beta vector. Right. Let me call it the beta star, because it will be in this plane, the best beta naught beta 1. Let me just put stars here. Suppose this is where the minima is, right? From the origin, you can make a vector here. This is your beta star vector, right? This is your search for the best vector such that this parabola here achieves its minima right at this point, isn't it? So this is your error surface. So you could write this in a more, Raja Ayyanar?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam?nilam the model right you can call it best model best hypothesis best parameters whatever you want to call it is that is that which Armin beta, it is that value of beta that minimizes the error. Do you see how the language, oh, by the way, argument, the word used is argument. Maybe let me write it more clearly because this is a crucial equation. You're saying I'm searching for that beta, which. but beta which this is the one that i'm searching for and i'm using this vector notation just because i don't keep writing beta not beta when i get in again and i want to generalize to polynomial regression because it can be beta naught beta one beta 2 beta 3 beta 4 and so forth are we together this is it so as if i'm not able to visualize this parabola so the parabola is the graph of the all possible e of the all the hypothesis or all hypothesis see see once again let's go back to one dimension right suppose it was just beta let's just look at it with respect to beta naught. This is the error. Let's say that this is the error. What happens is with respect to beta naught, we are keeping beta 1 at this point fixed. So this will be a cross-section of that surface. A cross-section of the surface will be a curve. Each value of beta naught is a potential hypothesis, right? So each value here in this curve, and now go to higher dimensions, beta naught, beta one, each point in this, you see this plane, the ground plane. Okay, yeah. Each point here is a particular value of beta naught, beta one. So each point here is this particular value of beta naught beta one so each point here is a hypothesis the question is how much error does this hypothesis make isn't it this is the error of this particular hypothesis and so what are you finding you're trying to find that hypothesis trying to find that hypothesis this hypothesis which makes the least error this is the best hypothesis coming from let's say a point here this is your best point isn't it are you getting that it are you getting that so each point on the error surface is the error of a certain hypothesis kyle your question uh so he had written that uh the next beta is uh beta next beta one is beta one plus alpha times. Is that, shouldn't it be the next error? No, no, no. Yeah, okay, I got your question. See, when I say beta next is equal to the previous value of beta minus alpha times, and now let me write it in a more compact notation the gradient right this is the gradient grad notation that i introduced last time when i write it like this it is true that if you compute the error of the next beta beta next right it should be less than error of the previous meter right these two go hand in hand okay are you getting it right but remember the learning has to take place by going from a bad not so good hypothesis to you have to go from here to here you see this from a yellow point to the ping point you're trying to go so this is the path on the ground in the hypothesis space that you want to follow and here in the error surface this is the path that you want to follow that you want to follow. Okay. Right? That is it. But it is the, see, what is the variable that you can change? You can't directly change the error. Error is the response to the hypothesis. So you can change the hypothesis. What you're doing here is you are changing the hypothesis. You're saying, let me improve the hypothesis a little bit. And this is, let me improve the hypothesis a little bit. And this is the way to improve the hypothesis a little bit. In the hypothesis plane, let me make a small step in a direction against the gradient. Right? That's what it is. So we're not worrying about the beta 0, we're worrying about the beta 1. And if we have beta 2 2 something called a different variable then there would be one more parabola yeah yeah so no there's only one parabola so let me be very clear suppose you had beta naught beta 1 beta 2 three dimension right what would happen is you would still visualize it as like this beta naught beta one and it is hard to visualize here but imagine that these are all 90 degrees somehow it is hard to imagine but this which in your mind see if you think about it if you make three axes all perpendicular to each other to you it looks like a volume isn't it three dimensions but mathematicians think of it as a three-dimensional surface this is a little counter-intuitive now we have three parameters imagine the ground to be made out of a three-dimensional surface which is actually a volume but we call it a hyper surface every point in this here every point here is now made up of beta naught, beta 1 and beta 2. It's a point. Beta naught is equal to this. Are we together? Rising above each of these points is the same parabolite. But what is the dimension of the parabolite now, the surface now? It's a three-dimensional surface right so it becomes so that is why you know a human intuition begins to fail here hyper surface right this is the error hyper hyper surface and this is the error this is the parameter hyper surface parameter hyper surface that's why it's best to think in terms of just beta naught beta 1 and generalize it to higher dimension so that reminds me of a joke at one time after go ahead so in the three dimensional space that you've uh drawn for the hypothesis um so the gradient seems to be a three-dimensional vector right exactly it will be a three-dimensional so in that case um if we want to move beta one we move it yes this would be made up of it's a vector which has three components i'm sorry this is a three-dimensional vector beta 2 this is your three-dimensional vector for the gradient. And so the learning is beta naught next is equal to beta naught minus alpha d e d beta naught beta one next. So you get it now. You take a step in all of the three parameters. Are we getting it? And now you can generalize to beat 100 if you want. Right. See, one second, I will come to to you why don't you ask a question residual square of residuals yes yeah parabolic the surface yes Yeah, parabolic, the surface, yes. Is it like, is our assumption is this is always the case? For linear regression models, it's always the case, right? So I will jump a little bit ahead and say, see, one of the reasons linear regression models are so powerful and so popular, and you should take them so seriously, is because they have a property called convex error surfaces. Convex error surfaces are surfaces that has a global minima right it is not true actually always and one of the pursuits has been in this field for the longest time we thought that if you don't have a convex error surface you're in trouble right people always like a convex error surface but then it turns out a class of algorithms like the neural networks their error surfaces are not convex they have hills and valleys right lots of hills and valleys and so one of the deeper questions is how do we find the best minima right and there is a tremendous body of work and thinking and marvelous thinking on how to go searching for minima in this very complicated, these are called loss surfaces or error surfaces, like that, how to go about finding the minima there. And every time there is a breakthrough that finds, because it's a complicated and unsolved problem, right? People are still finding more and more creative ways to find minima in a complicated loss surface. But we are starting this workshop and this field with one of the most fortunate situations. In all linear regression, which includes binomial regression, error surfaces are convex. And so we can descend to the bottom right so what happens is that all the technology we learn here remains the same in complicated loss surfaces gradient descent works same last thing but the the change that happens is the fear that is what if i get trapped in a local happens is the fear that is what if i get trapped in a local minima more practically speaking suppose i fall down and land in lake tahoe right well i'm still not at sea level still high up in the mountain if your intention is to come to sea level the base error the minimum error how do i make sure that i don't find gradient descent will make you stop? Just naive gradient descent will make you stop at a local minima. So how not to stop at a local minima, right? So we need to bring more technology into this, and we'll learn about it as we go along, right? But so far, guys, is this a good review? Yeah? So this is all a review of what we said, guys. And the other last observation that we made is that, look at this. A straight line. This is something to remember when you do polynomial regression. How many bends does a straight line have? It doesn't have a bend. That's why it's called straight. Right? So zero bends. You represent it with the equation of polynomial degree one, isn't it? Right? So it's a basic fact of linear, of basic algebra. Suppose I make a parabola with one bend. Right? What is the minimum polynomial I need to represent it? Quadratic, right? You need a x squared term. You need a polynomial of degree two. Then suppose I have two bends now. No zero bends, one bend, two bend. How many, what's the minimum degree of you can convince yourself quadratic won't do. You need to bring in a cubic term. Right? So do you see a trend here? Do you see a lesson here? Right? And so suppose I have three bends. I need a polynomial of degree four. This was, by the way, one of your homeworks, and we mentioned that. So this is the minimum you need to solve this. It's a basic fact of linear, a basic fact of algebra. In fact, it goes to the fundamental theorem. Why? The proof of this is this. If you have an equation, algebraicic equation polynomial equation of degree n how many possible roots can it have right it is a measure of how many times it can cut the x-axis right n times right so remember that every time so suppose it cuts the x-axis once Raja Ayyanar?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro?nilro Right? If you make it cut the x-axis thrice, three roots, how can you do that? You'll have to bend it twice at least, isn't it? And therefore you can generalize. It's a fact that actually I find it very useful to think. A little observation in mathematics goes a long way actually when you're doing data analysis. Some basic fact that you learned in algebra long ago is that so all right guys that is a recap of what we uh what we had the last time the last time okay that's that any questions guys was this review useful all right so let's stop the recording and we'll start one question that i'm having is yeah i understand the geometric aspects of this but when it comes when it comes to actual beta not better one values which are translated from your data space right so it's coming from the data space right the beta one beta two and all that no no beta no data data drives your see what the data dictates is a there is an error surface your job is to find the minima of that error surface in the hypothesis plane so think about it data exists data exists the moment data exists if you take a linear equation you know that in the hypothesis space there is a parabola that exists there is a paraboloid a surface that exists because the existence of data coupled with the fact that you have taken a linear hypothesis automatically produces a error surface in the hypothesis space your job or the machine learning is the search for the best beta there okay that's beta not beta one that is the learning part of it you start with some random hypothesis and then your gradient descent to the best one okay so any linear including data not x1 x y x square x cube whatever you have it on this beta naught will still no sorry the hypothesis suppose if it's a straight line you think you'll still get a parabola as your yes yes you will always get a parabola because a straight line is given by beta naught beta one isn't it so think about it in the hypothesis space you have a plane made by beta naught beta one and the y-axis rising i mean the the vertical axis rising above that plane is the error surface so you will still have a parabola over that plane right where did i draw um so as if uh what happens when we increase the say the number of features in uh like the number of features in our data and what happens when we increase the degree um say we use uh we we are not using a linear model but we're using a polynomial model see a polynomial model is a linear model you just say that x squared term that you add is a new feature that you add see here is a thing think of the house price house price may depend on let us say it depends on the square footage. That is one variable, X, the size of the house. The second variable is the distance from the downtown. The further you are from the downtown, the cheaper the house becomes, let's say. Less expensive it becomes. So there are two factors. So what you can do is you can say the X1 x1 x1 is the square footage x2 is the distance from the downtown. So then you have to pronounce three parameters beta naught beta one beta two, but then you realize that actually the relationship is more complicated, you want to add x1 square term also. Right on x2 square term also you go on adding you treat them as new features so you treat them as though they were x3 and x4 right okay all right so whenever you add polynomial terms each higher degree is you inventing a new feature and adding it to your linear equation so as if i'm going back to the same because i'm also finding difficulty to visualize this. So let's say if we have only one variable. So error is yi minus beta naught minus beta 1 xi. So if I need to plot, then what I'm going to do, I am going to calculate the error for every point and then plot the graph. Then it will come like a parabola. See, you don't actually do. You need to understand that this is what happens in reality as you coded the the libraries will do the gradient descent for you and it will find the best beta for you right you just have to set up your hypothesis the whole point of this machine learning libraries is they do the hard part for you they they compute giving your hypothesis the error and then they do the gradient descent in the error they find the best beta and they tell you the answer but this is what is happening under the covers but if i'm trying to i'm trying to understand the con so that it is taking my mind so how do you like what makes you think that it's going to be the parabola if i have if i take the one feature and if i want to plot the parable manually how do i do that like oh okay how do you plot it here it's a very good exercise actually why don't i do thing i will post the code for the because i wrote that code i haven posted it. Let me post the code for a parabola, a linear regression and the parabolic surface, paraboloid, and the whole contour, the whole gradient descent. I have that code, a visualization for you to see it very visually. I'll post it for you, just like correlation, you could slide and play with. You can play with that. That will help you play and understand what's going on. All right, now I'll tell you about higher dimension. One independent question that you guys asked before, sorry, it's been a long session, is that what happens if there's beta naught, beta one, beta two, beta three, beta four, beta five, or 20, 20 variables are playing, 20 features are there. And so there are 21 parameters in the model. How do I visualize? it's still the same your plane is the hypothesis, the beta space. The vertical axis is the error surface, which is a higher dimensional parabolic are we together, so here is one thing i'll tell you from personal experience. personal experience. When I at one point, I jumped from engineering, to do mathematical physics, theoretical physics, and I was working in quantum field theory and general relativity and so forth. I was doing a doctorate there. So, early on, I encountered people talking about, so quantum physics exists in infinite dimensional Hilbertian space. It's hard enough for me to think in three dimensions. Right? In fact, it's terrible. Then I thought, okay, well, listen. Then I looked at general relativity. General relativity, Einstein had already, in a way, made it harder. He mixed up space and time. And so now everything was in four dimensions. Right? And then there was string theory, which was already super string theory, which was in four dimensions. Great. And then there was string theory, which was already super string theory, which was in 10 plus one dimensions. It was getting a little hard for me to graph. So I talked to a senior researcher, senior graduate student. I said, how do you do that? He said, don't even try. No human being, we don't have the mental equipment. We haven't evolved the mental equipment to think beyond three dimensions. So what do you do? You develop all your intuition in low dimension. You say two dimension. So when you're doing this air surface, for example, two dimensions, beta naught, beta one, y-axis being the third dimension, you can still visualize it, isn't it? But when you write your paper, or when you talk to the colleague, you say, right, let the parameter space be this hyperplane in n dimensions, right? Where n can be any number, 10 or whatever. Let this be the this, right? And with a very straight face, say, as you can clearly see, right? This is true face say as you can clearly see right this is true obviously nobody can clearly see right but the point that people are making is the generalization is correct the intuition when you generalize even though you can't see that higher dimension that generalization mathematically holds and your intuition remains valid. So to this question, this problem that you guys are having, now, how do I think if there are three betas, right? Or four betas, how do I imagine the surface and what will the error surface look like? You can't visualize it except to say mathematically, it is some generalization of this two-dimensional surface that i can imagine to higher dimensions and the plane is a higher dimensional parameter space so that is something you have to get used to right mathematicians say that all the time without batting an eyelid right it's the part of the language and part of the visualization in this field. No computer tool will help you visualize five, six dimensions. You always visualize in two and with some techniques it begins to look like three dimension because your screen is two dimensional. So that is something to remember. So anyway, that's that. I will take a break here for 10, 15 minutes. And this was all a review of last time. See, this topic is a bit hard. And frankly, it takes a little time to develop this intuition. I've tried to make it as simple as possible. I could have entirely skipped all this mathematics, but the reason I did that is because I feel, well, knowing the foundations is important, important part of being a data scientist, to really know it at the foundation level. And in that spirit, we'll continue on to understand regularization at the foundational level. It'll be a, one of the, this is one of the harder sort of sessions. Believe me, after that, it gets easier. The rest of the workshops will actually, the theory parts will be easier so the hard part is pretty much up front we'll do regularization now after this all right guys all right guys welcome back so we have reviewed the last session which i hope uh reinforce some of our learning from the previous week. We're going to continue this. The topic that we'll deal with is effectively model complexity and a bias variance trade-off. So what is bias variance trade-off? And we'll go a little deeper into it. It's a very simple idea. Before I continue into this, in this book and in common literature, we talk about bias and variance trade-off. But I want to caution that it is not really a trade-off in many situations. Bias errors are there, variance variance errors are there and you could have both a lot of both right so it isn't that because you have high bias errors you don't have variance errors and so forth so what are what are all of these words we will learn about it now so we'll talk about this the key topic for today is a bias-variance trade-off. Let me write it down. And I won't speak of it from a very mathematical perspective, but from a sort of intuitive perspective. And remember that even though it's called biased variance trade-off, I will put this caveat, the need not be trade-off. Now, to frame this problem, to understand it, I will deliberately take the form of linear regression you're familiar with polynomial regression in a polynomial regression how do you get more complicated models how do you get more bends in your hypothesis in your model by adding higher degree terms isn't it so we can so we will use So we can, so we will use this, by the way, so we will use this as a simple frame of reference to think about bias variance. Now, bias variance goes beyond that. It need not be just binomial regression. The frame of mind that you have is how complicated is the model and, or simple is the model, and how does the errors that you have, the error that remains after all the learning is done after all the gradient descent is done the optimal model for that class of hypothesis or that complexity of model how does it compare with the different complexity of the model are we together so the way to frame this problem, go ahead. Since you're opening this topic, here is an intuition that I've been using, I will say that, let me know if I'm too far. And if you can connect back to that. Yeah. Choose the wrong model, you'll have high bias. Overfit to a model, you'll have high variance. Is that a... No, the way to put it is you could be wrong either by being overtly simple or overtly complex. If you are wrong by being overtly simple compared to the ground truth, you will have high bias errors. If you are wrong by being overtly complex compared to the ground truth, you will have high variance errors. And of course, you could be so wrong as to have both high. So choose the wrong models with the wrong intuition. Yes. See, given data, you don't know the ground truth unless you investigate it. A priori, you don't know what the complexity of the appropriate model should be. What is the complexity of the data? And so you start with some hypothesis. But the whole point of machine learning is we learn. We figure it out. And I'm creating a frame of reference that if you take the simplified world in which the complexity is determined by one parameter, the polynomial, the degree of the polynomial that you fit to the data. polynomial that you fit to the data right so now the the degree of the polynomial n is the hyper parameter of the model so we introduce a term now which is not parameter it is hyper parameter hyperparameter model. Hyperparameters are not something that can be learned by gradient descent. Gradient descent will not tell you what is the right degree of the polynomial, isn't it? For example, in dataset 2, you could fit a linear model, you could fit a quadratic model, you could fit a cubic model, and so a quadratic model you could fit a cubic model and so forth third degree four degree fifth degree model at each value of the polynomial there is a best fit model right gradient descent will take you there but what gradient descent won't tell you is what is the best value of n the the hyperparameter. So hyperparameter search still remains an exercise to be done. How would you do that? Here comes an interesting part. The more complex the model, the more flexible it is. You know, it has ability to have many bends, right? And so what happens is that the classic example that is given and by the way this is given in the great classic amongst many many places uh i obviously first saw that in the great classic of christopher bishop a machine learning book but it's everywhere and so i'll give you guys uh his visualization is available publicly so uh his slide, so I'll give you. So he takes this example, which I find so appropriate that it is almost like you can create lots of other examples but it's the one that keeps sticking in my mind. It has to do with your dataset too. In the dataset too, suppose your dataset, if you remember the dataset was, if I remember it right, it was something like this. Isn't it? So with this, so now don't take this, this is this hyphenated line, dashed line is a secret. Imagine that a data doesn't come like this. A data comes to you in blue dots. One, two, let's say three, four, five, six, seven, eight, nine, 11, 12, 13. I'll just take these points. This is your data set. What would you like to fit? And so let me deliberately blue is the data on the x y axis x axis is here y axis is here right the simple way that you look at this this is your data you're trying to say find a relationship between Y and X. You could make this hypothesis that the answer is this. This is hypothesis one, right? This seems to be reasonably the best fit line. I think so, like this. You could make a hypothesis that goes like this. Let me call this hypothesis two. You could make make this hypothesis that it is. I'm just taking an example. Now, do you notice that the blue hypothesis, let me just call it hypothesis four, it has a singular point that it seems to be going through each of the points in the data. Do you notice that it seems to be going through every point in the data? notice that it seems to be going through every point in the data right so what would be which of these hypotheses would have the least error residual error hypothesis four it will have exactly zero right residual error yet just looking at, how many of you would trust hypothesis for for making future predictions? You would not. Why would you not? Why is hypothesis for not good? That is right. So in other words, see, there are two ways of explaining it. Data always comes with errors, noise. What this has done is, so wherever a data point is, the appropriate generative does is that it listens not just to the signal the information but it has fit to the noise also it says the exact location matters isn't it and so what will happen is when you take another data set same data remember this is a sample from the population remember you have a you can go and take more measurements but when you go and take more measurements let us say that you take more measurements and what color should i use for more measurements let us say that i use this color more measurements come up and those measurements are like this, this, this, this. Right. These are the new measurements. Maybe I'll mark the measurements with the X. This, this and. This. Let's say this, right? And maybe this, this, this. Do you see a problem emerge? What is happening? When you look at these X-marked lines or these colored lines with this color, the new data, this hypothesis, which is in H4, begins to fail miserably to fit with the new sample of the data. Isn't it? New measurements. It is a very poor model for measurements that you haven't seen. But which of these hypothesis holds up well? You realize that H1, actually the hypothesis H1 would not even blink an eye, right? It says, oh, just fine. But what is the right phrase? Bat an eyelid or something right it wouldn't be affected at all it says that's fine this i'm still the same right it wouldn't change much it would stay rigid on its place what about hypothesis two or maybe it will perturb a little bit hypothesis three would maybe begin to adjust a little bit more but it will still fit to the data it will be a good hypothesis hypothesis four would abandon its old position and draw a new curve for itself isn't it it will say oh shucks this is completely wrong let me go and redraw myself into a new hypothesis and it will again try to go fit to the data. It will just change its shape. Are you true? So the way it manifests itself is that if you look at the variance errors, the residuals, the variance of the residuals, you find that the worse the hypothesis, the more you find that the variance has a wider spread. I mean, the residuals has a wider. Another way of looking at it is you'll start seeing bigger residuals. Do you see that for H4, the residuals could be big. Now I need a color that I can use. OK, green is not used. And let me just use this with big bold ones. We will focus on hypothesis 4 and look at the new data. Do you realize that this, look at the residuals that I'm making and compare it to the size of other residuals. This one is small. And this. And this. This is the residual. And this. And this, and you pretty much are getting the idea of what is happening. They are small residuals, they are big residuals, and then they are huge residuals, right? These residuals are huge with respect to the blue line. But do you see how some residuals are small, but they're also bigger residuals? If you look at the residual, for example, of the yellow line, they don't have, like, it sort of remains whatever it is. And what about the good line, the H3? do you notice that the residuals of hc they don't show that sort of variance they remain more or less small right a little bit worse maybe and i will get worse with that so that is a clue that is a clue of what that you have a model that's very overfit one another another manifestation of this will be that your R squared or the sum squared error, let's just look at the sum squared error, the sum of squared of residuals for, when you give it one dataset, it will be very small. You give it another, the dataset you train it on, it will be very small, but you take another sample of the data to check whether it worked or not and then there is then the sum squared error explodes isn't it you have a huge risk here so now we will from this we'll learn a technique first is how would you know given that you're trying to fit polynomial models how would you know given data how would you know, given data, how would you know what is the right degree of polynomial to pick? Unfortunately, if you pick sufficiently high polynomial, you'll get and you train on the whole data set, you will get a perfect answer with high degree polynomial. Zero errors, isn't it? So what happens is is let's look at this the so from this we have a crucial observation that on the training data the data that the algorithm actually sees while learning right on the data that the algorithm sees while learning this observation will be true which observation will be true hang Which observation will be true? Hang on, where are my colored pens? Why are my colors disappearing? Okay, yes, much better. I will draw the axis. I will draw an interesting axis. This is a... I will take any measure of error, mean squared error, sum squared error pick one let me just take some i'll just use the error function e is that fine and i will see the best error given a degree of the polynomial n goes in this axis what is the degree of the polynomial right and i will look at the error that remains when you have done all the learning at a given degree of the polynomial you would agree that the learning error when you're learning the error would be given by in the beginning the error would be high when your model is too simple compared to the ground truth then by the time you reach the third degree of polynomial, it gets good, but it keeps on improving, unfortunately. So what happens to your error? It goes, sorry. And at some degree, it will just hit zero. Isn't it? Some degree of the polynomial, your thing will just hit zero. Isn't it? Some degree of the polynomial, your thing will just hit zero. One second, let me just mean the best model that you build the residual error the error that remains it begins to keep decreasing right it fits better and better and better and better but too much of a good thing is bad when it completely fits the data it's actually not a very good thing. Rantik. Is this for one data point? Yeah one data set you take one sample of data. It's called a training sample. Whatever you train the algorithm on. Right? Yes. So let's say in the sample size, we only have like 100 data points. Yes. And our hyperparameters are 101. Wouldn't that make... Hyperparameters is just one. And degree of the polynomial. Degree of the polynomial degree of the polynomial is 101 100 actually if you have 99 degrees it's enough because you'll have a perfect fit so but my point is given beta naught because beta naught is there so you'll have a perfect fit but shouldn't the fit be getting worse with like more uh polynomial data set no no no no no there's more curves the error you're just looking at the error so look at this let me reiterate what i said okay look at this blue line that i talked about let me re-emphasize it look at this big blue line are you seeing this blue line yeah what is it doing when you look at the training data the initial data set it has gone through all of them it is so flexible it has gone through all of them when it has gone through all of them what is the residual error there is no residual error so the error is decreasing training error is decreasing you got that yeah but at some point it should start going no it shouldn't go up training error will keep on decreasing right so hold it so now the problem is that doesn't help us because we know that later on it will fail so we do a trick we simulate the later on what we do is you take the whole data set the learning the data set that you have been given and you borrow you take a little bit of it and you hide it under your pillow. You don't show it to the algorithm, right? So you say, go learn from this. Are we together? Asif, can you repeat this one again? So let us say that you're training, somebody gave you that, you know, you have to build a model and you have 200, like, I don't know, you have 100 data points, right? Or 1,000 data points, whatever, pick a number. What you do is while you're learning, you don't give the algorithm all the 1,000 data points. Let us say that you take, I don't know, you give it half, pick a number, pick a proportion, you give it half. And the other half of the data point you pretend, you don't tell the algorithm about it. You pretend it doesn't exist. You hide it under your pillow. And then you give it 500 and say, well, here's your data, go learn from this. You don't tell it about the other 500. Hold on, hold that thought. So now what happens is your algorithm will learn from the 500. The worst that will happen is it will overfit. But then what you do is something interesting. You look at the error that your algorithm is making on the test data. You build the algorithm and now you bring that data out from under the pillow. You know, now you're looking at the real, the test comes. You make your algorithm predict the values, the Y values on the test data that you never showed it. Data under the pillow. You make it make predictions and then you compute the net error. So when you do that, when you do that, you observe something quite remarkable. Look at this hypothesis. At hypothesis one, you have higher, hypothesis two, you have less error, hypothesis three, you have even less error, but hypothesis four, it blows up. Well, actually this is not the four degree point. I mean, this is whatever 10 degree point, I mean, it blows up. So the real trend that you will see in the date, when you do it on the test data, the data that you hid under the pillow, is a completely different curve. What you will see and what color is more suitable for that. I should pick a color. I am always indecisive about it. Let's say, oh, I use H3 here, or maybe I'll use this color for whatever reason, this. What you will notice is that first of all, the test error generally tends to be more than the error. And so it will go like this. go like this test error that is the error on the test data set are we getting this test data set so what it means is that the minima of this here in this particular case it would be around three let's say that this is 123456. i'm taking an example, it could be three it could be five, whatever it is, but at some point. The enemy decrease and then it will start increasing. blowing up. Right does that make sense guys from this picture is it evident that it should do that right there's a few who are remote and you would like to volunteer is it looking pretty obvious yes yeah yes it is and that is the that is a lesson to learn that the way you train an algorithm is you don't ever show it all the data because you have to do hyper parameter tuning also algorithm is you don't ever show it all the data because you have to do hyperparameter tuning also. The only way you can do hyperparameter tuning is you hide some of the data under the pillow and you tell the algorithm this is all you have. Give it a small training set of data and you pretend that this is all you have. The algorithm goes about it and learns the best. But then you will have many models. One model for each value of the parameter, hyperparameter. And now you have a way of comparing the errors of each of those different models. Each of those different models have tried their best. They have used gradient descent to find the best possible model they can build for that value of the hyperparameter, that value of the polynomial. Now you can compare. You can build a graph like this, you can compare. And thus the lesson that whenever you get data, split it into training and test set. First split you do, we split it up into your training and test set right this is a very basic way of looking at it there are more nuances later on we'll do something called a cross validation right a cross validation will come in and so so then i didn't introduce cross validation i've sort of taken a more simplified way of looking at it when we do n-fold cross valid k-fold cross validation and so on and so forth and bootstrapping etc things we will divide data into multiple sets not just training and tests but for simplicity's sake at this moment we will just take training and test set right so remember you must hide some data under the pillow and whenever you measure the effectiveness of an algorithm always do it on the test side never on the training set because training set can misguide you guys i'll take questions but before that let me also point out something do you notice that when you have variance errors one interesting thing that comes out is that notice this the gap between a training and test errors begins to blow up so when you notice in your code when you try try it out in real situations and you see that your training error is much lower than your test error what is it giving you an indication of high variance you have overfit you have overfit to the data that training data you're basically listening to noise are we together you're basically listening to noise at that moment quite attentively you don't want to okay variance uh when we say variance we mean the variance in the residuals is it variance in the residuals yes so the real expression so now that brings me i will summarize it with one equation you say that given a model at the end of it you would be left with so you take any degree of polynomial you would be left with error that you can't gradient descent has done its job. You still have error, right? The error that you are left with, the error finally of the best, like after gradient descent, is made up of three terms. It is bias bias squared think of it as bias and variance ignore the square a square is because of the historic reasons the way the formula was initially declared and so on and so forth but people have decided that there's a certain formula for a bias variance variance but there is another ghost in the room. What is the other kind of error that we missed out? Unresolved error. Irreducible error, excellent. The one that comes from many factors like instrumentation error, measurement error, but more importantly, that which is not there in the data, missing information causing. Like for example, you're looking for sale of ice cream on a beach, if the only thing you're looking at is temperature, you missed an important predictor, namely whether it's a holiday, weekend, or it's a workday. Workdays, parents bring children to the beach and children eat ice cream. On workdays, parents are working. So very few children end up on the beach so it's a significant determiner of how much ice cream you would actually sell but because you weren't given that data that will manifest itself in the irreducible error the part that you can't get rid of no matter what you do so with hyper parameter tuning you can try to optimize the bias and the variance are we together but you can't do anything about the epsilon the irreducible error right so the hardest thing is to know that i mean obviously this is true for human life also, we all know what we know, we often know what we don't know, but sometimes we don't know what we don't know. We may think that we accounted for everything but there are things unaccounted for beyond our thoughts. So this is a very crucial formula. Go ahead. So question, so that's so this is this is a very crucial formula yeah go ahead yeah so question so so let's say the first step i did that i took the training the 50 or x amount of the total and then we plot and which we got the h4 and then the n came to happen number seven in this case so now i know the n is seven then i give you the remaining then i have a test one test one and i'm going to run it again so it will give me the n equal to three no it doesn't work see suppose you plot out the error for the test error. So your test error, what will happen is when you do it on the test data, the data that you hid under the pillow, you'll get this curve, the test error curve, right? So you keep on doing it for n is equal to 1, n is equal to 2, n is equal to 3, n is equal to 4, 5, to three n is equal to four five six so suppose the optimal answer was like you said six or seven right suppose this answer three once you come here and you notice that your test error has begun to rise right is now like beginning to rise definitely is beginning to rise what does it mean do i need to go to n is equal to 100 anymore no you know that it's not going to turn around we have already started overfitting so we can stop now sandeep okay right so you have to be sure don't like at n is equal to 3 and n is equal to 4 4 is more than 3 that is premature to stop that may just have been an accident of the test data the way you split the data right so what you do is you go to five six wait till you're confident and then you stop right now i must also mention one thing see i said that you split the data into training areas so there are certain nuances guys the nuances are this data when it comes don't trust the data the foundation of analysis is randomness it has to be true the data has to be randomized right so that when you split the data the two data sets represent the same reality to the equal degree isn't it one of the words i taught you the last time was iid independent and identically distributed each of the data points should be independent and identically distributed means one data point doesn't require the other data point or have any influence from the other data points they're independent observations and in identically distributed they're taken from the same reality and this applies both to the test and train data set they should collectively also speak to the same reality that the test data set is speaking to. So what you should do is when you get the data, you should just in the interest of caution, do what? Stir it up. Put the data into a pot, stir it up, randomize it. So that once you have randomized the data and now dip into it and say training and this this is test. Right metaphorically speaking. Yes. Not to generalize, but you become, see, your test error curve is more robust, right? It speaks to a reality, whereas the yellow curve, the training error, you know, will inexorably tend towards zero. More and more complex models will always lead to lesser and lesser residual error, which can completely misguide you the only way to not be misguided is to but keep your eye on the test error but even when you randomize even when you do this there there is a word of caution here see when the data sets are small right what may happen is that the split may be in such a way that the test, because the test and a training split may not be optimal, they may be speaking different realities. Right? And so there is still danger, which is why we'll go to more steps of cross validations, etc. But today, I will just simplify it down to say, never run your experiment only once. Run your experiment again and again, to be sure. Run a few times. And the other thing to say, how would you do that? So one of the things you notice when you see a code on the internet is that they will, and this is a programming thing, is that, and the lab will do it you set the random number c to 0 or 42 right 42 is very popular because of the hitchhiker's guide to the universe it's supposed to be the meaning of life right so people often do that as i inside joke in data science so trouble is if you keep running the notebook over and over again what will happen the randomized split is every time it will be exactly the same split isn't it so what you should do is disable your disable your seed let it truly be random right then only you every repeat of the experiment, right, will be a different trial rather than the same thing over again. It's one of those little things you have to remember to do. Go ahead, Albert. So that pink line goes, right? So I was just wondering if it was not overfit, then the pink line would have been going down. Going down. Yes, yes. Going down. Yeah, parallel to the yellow line. See, ultimately, it will rise up. The question is when, right? Today, anyway, before we break out for lunch, I'll give you an idea. Today, we deal with a world of linguistics, natural language processing, right? And computer vision. The dimensionality of space here is very high. The number of words commonly used is in the order of 100,000 to 300,000. Some companies, when they do analysis like Google, who have access to ginormous hardware resources, and where frankly almost more money than many governments or many countries right they often run spaces of I don't know like one million two million words and so forth huge ginormous spaces so you can imagine that actually you have to deal with that those large spaces so a lot of this discussion that we do still holds true, but the numbers are not as equal to. So we will talk about it differently. What does it mean? There the hyperparameter is not polynomial. There the hyperparameter is the sheer number of nodes or complexity of the neural network and its architecture and many things. So what you call hyperparameter it changes meaning there are many hyperparameters there is how much of dropouts you do how much of this you do how much of layers you have how many nodes are in there so hyperparameters is not one but many which is why searching for hyperparameter in real life gets terribly complicated think about it one of the state of the state of the art models these days is something called GPT-3. A GPT-3, I'll leave it as an exercise for you to discover in the break. How many neurons or how many parameters does a GPT-3 model have? What's the dimensional space? So collectively, see if you can figure it out. You just, if you Google it up, it'll immediately come up. And I think the answer will surprise you. What terribly high dimensional spaces we're talking about. One of the big mysteries is that really does it take that complicated model to do that? And there are certain theories like, for example, the lottery hypothesis and other things which hypothesize that actually it's not the entire neural network that acts what happens is that the random initialization preferentially selects a subset of the neurons which do the real work of getting to the minima right and it depends upon the initialization because the last surface again has lots of hills and valleys and you basically end up preferentially picking one right so there are all sorts of interesting research going on, we'll come to, but this is a good introduction to an important topic. I'll end with one final precaution. Bias and variance are not opposite. So here what happens is, in this polynomial thing, now let me make another curve. If you just break out the error in terms of the bias term and the variance term, the yellow line, if you break it up into two separate parts and the pink line, then how does the thing look? The way it looks, and it's very interesting to see that, is as follows. And this is, again, these are the classic curves in this field. So now I'll use different colors just to differentiate between this. What happens is that the bias error keeps going down by a setter for the degree of the polynomial right complexity model complexity and errors. So this is the bias error. The variance error on the other hand, if we can use another color just to be sure, goes the opposite. It goes, this is not fat enough, let me make it equally fat. Okay, let's try this. This is not fat enough. Let me make it equally fat. Okay, let's try this. It goes up like this, somewhat like this. So the more complicated you make a model, the more this will increase. And so the total error for which we use this color, and we are looking at the test error here, what happens to this the the total of these two will go like this isn't it this is the total test the total test error right because the tester is the one that matters it goes like this but now what happens is that it reaches an optimal point somewhere right that is what you're searching for and that is at some value of the complexity but remember that as you make your model more and more complicated bias errors go down and variance or overfitting error goes up and the total error is made up of bias variance and the irreducible error which you can't do anything about right so this is so let me give it names i didn't give the name to this a green one this is the variance errors noise guys please mute yourself okay so um the total error is this so this is again uh one of the celebrated curves in this field these two functions these two curves you should remember really well this curve that with complexity the test error will begin to blow up again it will go down and then blow up and with complexity the bias errors will keep going down and the variance errors will keep rising right and to the extent that you're looking at the sum minimizing the sum remember just to reiterate our expression was yeah literally you see the expression here bias squared plus variance plus epsilon epsilon ispsilon is invariant, but to the extent that you are putting these two together, you will get a point of just the right complexity. And when you observe, when you observe in a bit, you will see that that model is very close to the ground truth. If in lower dimensional models, you can actually visualize it, you'll see as we are doing in our labs, that the visualization puts us very close to the ground truth, right? So I'll take questions in a minute from everyone, but I want to recapitulate at this point. The thing that we learned is bias variance trade-off. First thing is it's not a tradeoff. It turns out there are situations where you can really have high bias errors and high variance errors, right? And for reasons, because you can have regions, certain regions of the parameter space, of the data space where you make bias errors. You have overly simple hypothesis. And there are certain regions of the data space where you're making very, very quickly, like lots and lots of complicated curves to fit the data. And then again, you simplify. So what will happen is you have both high bias, high variance errors. So both of them can happen. It's not really a trade-off, but for simpler situations, quite often it is a trade-off, but for simpler situations, quite often, it is a trade-off. So remember that this bias-vein straight-off search helps you determine the best hyperparameter of the model in many ways, right? It will not help you to find the best parameters. The best parameters comes through, you search for the best parameters through, of a model through? Gradient descent. Gradient descent. Gradient descent, error minimization. Excellent. This is the celebrated equation. The total error of a model is made up of bias and variance and irreducible error three times. Right. For intuitiveness sake, ignore the square. The square is because of a mathematical reason and the way the bias is defined yeah go ahead albert you had a question variance is not square only the minus think about it variance already has a square built in you remember sigma squared that's the reason while trying to choose the set of features that you want to use in the model, let's say you pick sine, cosine, some function like that. Why would you pick one of those as transformationism due to a variable? You essentially introduced, in terms of degree of the polynomial, a significantly high degree of polynomial. Yes, sine is a transcendental function. You just agreed to use infinite dimensional space, but you don't look at it like that. You say that you go to another space. The way to think about it is that trigonometric space, the sine space is another space. And actually that space is the space of Fourier transforms. So you look at the frequency part of the sine wave. So anyway, this is a little bit of a stretch, but let me explain what it means. See, what happens is that when you use this sine wave or something, if you notice a wave-like structure, you could say that it is a simple wave. There's one frequency here. When you do the Fourier transform, what in the world is a Fourier transform? Fourier was a great mathematician who studied this undulating or wave-like functions, or any functions for that matter. And he said, what if we could think of it as, to speak it very metaphorically, as different notes of music. Each note with a certain frequency. So unique frequencies. What are the frequencies that make up a shape? A simple thing like this is one frequency. So sine, the way you look at it, sine omega. So suppose it is a, if it is k, you look at it as mod kx in other words what is the wavelength what is the wavelength of that wave right and so you could have another as you relax and increase the wavelength you realize that you can have things like this and so any curve that is like this, you can actually break it up into its constituent frequencies. And Fourier transform is one of the beautiful, beautiful, and I mean, I always feel it's pure magic. What it can do is, and I hope I up to a proportionality constant, it is essentially, and i hope i up to a proportionality constant it is essentially and i'm just fx e to the minus kx dx right typically people write it in terms of time ft e to the minus omega t in the in the time domain or dt for a transform and what this transform does is it makes this a function of omega or a k whatever it is but we won't go into the mathematics of it for your transform extracts the frequencies so when you say it is made up of sine one sine wave you are actually saying that in the frequency domain there's only one frequency involved so the only thing I need to worry about when I'm fitting into data are amplitude, what frequency it is, and what is the phase shift from origin. In other words, the thing that you look at is that same data, it's a data set 2, when you hypothesize that it is a sine wave you will say y and by the way it's very good that you ask this question because you'll see it kx plus phi right let me just mention this is the so-called amplitude of the wave just basic amplitude this is wave number right yeah wave number i'll just leave it as that and this is the phase shift right this is so there are only three parameters to the model so in effect you're saying i'm looking at a three-dimensional model, but there is a problem here. Hypothesis-based is three-dimensional, but this equation is non-linear. It's not linear in the three parameters, right? If I write it as, in a more conventional way, beta naught sine beta 1x plus beta 2. There are three parameters, right? If I were to write it like this, do you see that they are combined in a nonlinear way? So we are looking at a nonlinear model. Nonlinear models don't have this beautiful parabolic structure, which means, and we will do that exercise today. Actually, you anticipated one of the things we were going to do. The problem is you have to start pretty close to the real minima you have to start with a guess because if you're off the guess you'll get trapped in some local minima somewhere right so there are certain cautions that you do now there are techniques to make it more robust and so on and so forth and that's the whole fun of machine learning it's a very creative field but anyway since you asked this question in fact this is one of your labs so see if you're going to summarize what you just described here the understanding and intuition to what was in that earlier phase yes just not you cannot use that completely no it uses yeah you can use it yeah no there i deliberately took a polynomial equation. If I were modeling the same data with polynomial, as we did in the lab, you realize that a third, because the data set is in a range. See what happens is that a true sine wave goes on from minus infinity to plus infinity, right? So how many bends does a sine wave have? Infinitely many bends. but in the domain of the data that we have data set to you have only two bends so for practical purposes because if you are going to model if you are sure that you will not make predictions outside this domain then a polynomial regression is good enough right that's the way to think, but can I instead use. Ramanujan Kulkriji, A nonlinear model that also you can do so, whatever model you use everything that I talked about bias of variance and everything remains the same. Ramanujan Kulkriji, Right, the only thing is there, it would be in the frequency domain, how many frequency terms do I want to take and that will keep on adding parameters to your model okay yeah yeah yeah in our case it will turn out that a single sine function is enough but it may be two sine functions and more that's why in other words the foieses may have may have more a lot of sine functions. And you can model it. See, okay, I'll stop with this a little bit abstract statement. If you understand, it's good. If you don't, see, when we think of data, any data, any function, and you mark it as, you represent it as xy, in the real observable space, you can look at data in the observable space it is called basis the basis is cartesian basis you took right the cartesian coordinates are there but you took the basis as implicitly that but you can do a different job you can look at any function and expand it up when you write a function complicated function in real x y space you can write it as a series this is one of the things you learned in calculus you can write it as a four-year series four years just made up of many many trigonometric functions right that's called four-year series you can write it in terms of uh you know those bell curves lots of bell curves expand it out in terms of bell curves many many bell curves so think of it that any function can be drawn by a collaboration of sign you know sine waves or a collaboration of bell curves or a collaboration of many things so the word that you use is these functions they are they form a basis for representing this function all of this either sine waves or the exponentials when you use explanations as a fancy term you use you call them the radial basis function means any function can be represented as a collection of bell curves when you represent it as a bunch of sine waves you say that you have done a four-year expansion four-year series of that function and you can find the four-year series of the function and there are many bases and that leads to another profound results we'll look at in deep neural networks for example a result is that the sigmoid there's a function called the sigmoid you'll learn about it in the next session right sigmoids form a perfectly good basis for any functions so these functions all of these functions which form a basis and they can you can prove that they can represent any function any function complicated shape can be expanded in terms of these basis functions. These are called universal approximators. Universal approximator means any known function, however complicated, you could approximate it well using a finite number of these. Generally, see what happens is nature, especially as a physicist, I keep going back to physical data. When you look at the structure of the universe, I mean, the structure of the universe, the structure of everything, everywhere, you see when I look around, for example, in my life, I see transcendental functions everywhere. I see the light coming out of this. It's electromagnetic waves, sinusoidal, right? I look at a bridge, a cable, electric cable on the side of the road. That's a sort of approximately hyperbolic function, transcendental function. Everywhere you see transcendental function. So it makes sense that data that you observe, underlying forces are transcendental functions. So generally, expanding in transcendental functions is a more appropriate way powerful way of modeling data always so, which is why you don't see too much mention of polynomial regression. In. Like see when you read some of the harder machine learning textbooks without statement they just start using radial basis functions. statement they just start using radial basis functions you do something like support vector machines and so on and so forth we will come to those they use radial basis functions and things like that why actually if you ask most people why they wouldn't be able to give you an answer but this is the real reason real reason is if you want to capture reality, reality is often best represented as expansion in transcendental functions. So just to recap this stuff, from what I understand, let's say you have a polynomial function of a model. Gradient descent would be used to actually optimize the actual value of the whole machine yes whereas this would be if you optimize this you're optimizing the number of terms the number of terms very good the degree of the polynomial exactly it's hyper parameter tuning right so uh anyway to finish that thought like for example polynomial why can't i represent a sine wave sufficiently long with a pointer you can except that it will have infinitely many terms, right? So it will become a horribly, horribly, horribly big space to model. And so, which is why, I mean, sort of, it is part of the mathematical tradition, but we'll develop all of that power slowly in this workshop. As you go through this, guys, there's a 14, there these 12 series and some extras and then there's a deep learning which is another like 26 30 sessions all of it put together when you come out of it trust me you'll be mathematically far more sophisticated when dealing with data okay but we are learning to sort of walk before we run yeah one question that i have is so this this transcendental versus polynomial those kinds of functions are they relating to static versus dynamic data by any chance no no data when you are learning from data data inherently is static data is given to you you don't fudge around with the data there's nothing for example right like if you're doing well for dealing with streaming data like for example i'm analyzing some log analysis coming from 1000 machines all the time and predict what's going to fail tomorrow, for example. Or I'm getting every minute data from the time, and you are getting say milliliter data points every minute, for example, how do you model for those kinds of sequences? That's a wonderful question. Let me answer it like this. First, I have taught you at this moment how to model with static data in your language. Now what happens is there is a way you build a model with some data that you have. What we are doing is called batch learning, you take a batch of data, sample of data and you learn from. There is a whole field, there's a whole area and in this workshop we may not, most likely we won't get time to get through it but at point, if you stay with me, support vectors, you learn, these are called online learning, or real time learning. How do you continuously learn from data, the trouble with batch data is, suppose you take some data from yesterday, and you model, that model may not be applicable today. Data, the patterns in the data has changed. Remember, the fundamental word, IID, independent and identically distributed, you learn from data that is there, the distribution of yesterday, the underlying reality of yesterday, if it has changed today, your model, what you say is that your model has become stale, right? No more applicable. It's as simple fact as that. So there is a whole branch or there's a whole way of looking at building models. It's as simple fact as that. So there is a whole branch or there's a whole way of looking at building models. It is called online learning or basically real-time learning or incremental learning in which you continuously keep learning from the data. So as the data pattern changes, you adapt to it, continuously keep adapting to it. So that is what you do when you have streaming data. And by the way it's a beautiful field i mean uh if enough of you are interested i'm totally willing to give a one-day session completely to that it won't be in this thing because this workshop is really packed i hope you guys are feeling that every day we learn a lot of new things so it is completely jam-packed it'll it'll have to be separate maybe in de to be separate. Maybe in December, we can do, or in January, we can do a day session, a separate workshop for real time learning, right or streaming learn, or what is called online learning. It's a beautiful topic. It's a beautiful topic. And there's a ways of doing it. And a lot of interesting ideas and techniques there. Alright guys, so with those words, it is one o'clock now, three hours. Let's take a lunch break. Is it a good time to take a lunch break? Yes, we have not though finished the theory. The main topic I wanted to teach, which was regularization, I haven't covered. So when you come back from lunch, we will do one hour of theory before we do laps and so we are a little bit behind schedule so uh one basic thing i'll do guys um when we come back from lunch and do theory let us do this for quick recap and a very quick one we are going to finish on our theory with one couple of topics we missed but they are important topics i would like to finish those before we continue and what in the Yes. So we stopped here as a key lesson. I have to share my screen. I'm sure you have to apologize. There is the sharing. All right. Are we seeing it now? Okay, for folks here, I think, should I switch off the light? You won't get the glare. All right. So the total error of your model after the learning is over is made up of three terms, bias, variance, and the irreducible error epsilon. Bias errors go down as you make your model more complex. Variance errors go up as you make your model more complex. Remember, we are looking at the error with respect to the test error, error on the test data set. And the total error therefore, because it's a sum of two parts moving in opposite direction, there is an inflection point somewhere in between, which gives you a model which is closest to the ground truth and which is the one you're pursuing. Along with it, we also should remember that training error keeps going down the more complex you make your model. That is not a proof that you have a good model. In fact, too good a number for training error usually is an indication that you are overfitting. What you need to look is test error. And the gap between the test error and the training error is often an indication of the amount of variance error. So overfitting, right? You get very good numbers for training data and you get poor numbers for test data. Classic sign, you have overfit to the data. That is it. So we are going to change topics now. And there are two more actually important topics that I want to mention. One is a smaller topic. Let's get it out of the way. It's a little bit of a digression, but I think we'll make it. It also relates to the paper that we are going to read this weekend, Sunday weekend, Sunday paper reading, which I, oh, actually that's tomorrow. So one question that comes, where did the word regression come from? The word regression is quite common. You know, we use it in machine learning so often. But actually, it's a word that has a negative connotation. Those of you who are all of you, almost all of you are here in software engineering, when there is a regression reported in your application in your code, what does it mean? Is it good news or bad news? It's pretty bad news, isn't it? Regression, the word regress is the opposite of progress. So regression is the opposite of progression and not something to be desired. So how did such a big algorithm, some very important thing, a regression. So now let me just formally state what we have in set. A regression is a black box in which X vector goes in and Y hat, prediction comes out. Prediction comes out and Y hat belongs to the set of numbers what it predicts, what a regression algorithm algorithm a regression model predicts is a number are we together the input is a input vector or more like if it makes you feel better so we can think of it as x1 x2 all the way to some n features go in right collectively they are the x vector the feature vector you call it or the input vector what comes out is a number here that's the way we have been seeing it so far actually what we have been seeing is only one input going one output come out but you will generalize it now to many inputs output is a number but there is there is a subtlety here actually the output need not be a number more generally regression more more generally regression mode more generally y hat it can it can predict a vector are we together so what happens is we say that x vector goes in which belongs to some n-dimensional feature space right if they are n predictors, and what comes out is not just a number, but it could be two numbers, it could be three numbers, and so on and so forth, right. For example, if you are predicting the velocity, wind velocity based on certain inputs, you realize that a velocity will contain, it's a vector that contains a direction. So it could be actually a vector. It's one of the points to remember. More generally, it is this. So in other words, it could be a vector. I will just write it down. It could be a vector. That is what regression is. Now, the origin of the term regression, there's an interesting history. We'll mention it only once, but it is worth knowing because it has implication. This word goes back to quite an interesting data scientist, what you would call in modern terms, a statistician named Galton. Galton became quite influential actually in the field and he was a very, I mean there's a lot written about Galton we won't go into, but he observed He observed something interesting. He observed that the children of very tall people are not so tall. If you take outlier tall people, their children are not so tall. And he wrote about it and so forth. There's a paper and so forth. The opposite is also true. If you have very short people, then the children tend to be a little bit taller than the parents. Now while this attribute is true for height, it is true for many other things. Most kinds of random variables are things in which there is a... Like outliers tend to be pulled in in the next generation if you can think of a next generation in some way you can look at the people often say it's true for iq i don't believe in the concept of iq terribly but the ones who do they keep saying that you know outlier people they tend not to have outlier children in many ways they support whatever i pick. Pick your attribute and the statement made is outliers don't produce outliers. Now remember the equation that I said, let's take a simple example. Y is equal to the linear regression equation beta 1 x plus epsilon, right? This is a regression equation isn't it for linear regression could there be a connection between these two statements can we can we infer that suppose let's take this example suppose it's a null hypothesis y doesn't depend on x at all let's take a simplified model many times the y the output is just and i will ignore beta it is only proportional to epsilon there is no relationship to x simpler case let's take a simpler case what does it mean x simpler case let's take a simpler case what does it mean number one simpler case so in other words the response just depends on some noise that is there some like causations or things beyond your control right and so what you are saying is that if there is a value that is far out next time it may not be true i'll give you an example see suppose you could jump or you're doing a long jump. I'm just contriving an example. You do a long jump, right? And you will fall a certain distance. Let's say that you jump from here and in this sand pit, sometimes you land here, here, here, here, here, here, here, and you keep on jumping. Broadly speaking, what will you notice you'll notice that your jumps have there is a peak median value most of your jumps are this distance let me just call it some average distance and around the average distance sometimes you'll fall a little short of it sometimes you'll fall a little further than this together so far right now let us say that for one time you happen to fall extraordinarily well let's say that you achieve a result great you achieve a result great you find that you have jumped that far do you really expect that the next jump will be even further generally if you're learning you just picked up a technique maybe but once you have saturated how far you fall becomes a little bit random, isn't it? It's sort of, it's somewhere around the maximum, the mean of how far you can go is a mean, and you will fall around that mean. And occasionally, I don't know, something propels you, a wind comes and propels you, I don't know, some factor helps you that you don't know, you to jump very far but the next jump you do will it be what is more likely will it be further or will it a or b which scenario is more likely for your next jump b we expect that right so the idea is that the the causes of this because you don't have a technique to it was not technique that took you to that furthest distance your technique has already saturated you so then close to your mean distance that you're jumping every time you'll jump a little bit less a little bit more than that and sometimes significantly more if you jump significantly more next time you'll be pulled in right this is an example this is an example of the fact that the forces that cause the small variations are too many and not under your control right which is why it's happening so therefore isn't it a very obvious fact that if this is true then the outlier the next generation of the outlier use the next time you jump after the outlier you'll be pulled in isn't it the the same will be true at this end of the outlier also the opposite end of the outlier like suppose you for whatever reason you don't put enough energy and you ended up with this value what is likely to happen your next jump is it likely to be this direction a or b b you're going to do better than that right now this is an important lesson the it is as simple as that the reason I'm mentioning it is the topic we are studying, from this, around this topic, there's a lot that has been said and written, and sometimes very wrongly said and very wrongly written. It's perhaps one of the poorly understood things amongst the scientific crowd. So I will write it here. The word that I will use, or the formal word that is used is regression towards the mean. Galton who wrote the first paper on it, he did not use this word. He used the word actually mediocrity in his paper, regression towards mediocrity. Mediocrity is movement towards the means. The same thing that outliers tend to produce, he was looking at the positive side, the high end gets pulled back in in the next generation right now when you look at it in this perspective i hope that this looks like a self pretty much a self-evident that's how it looks isn't it but it is quite poorly understood and uh and we'll speak a lot about it so now let us say that Y number, case number two, Y actually does depend upon X to some extent, right? Case number two, let's say, Y does depend on X. So even when Y depends on x what will happen y will be up to a proportionality it will depend on x and so you're ignoring beta naught maybe let's bring in beta naught beta 1 x it will proportional to x a plus still a random fact will be true isn't it right so whatever value so suppose you take a value let's let's fix the value of x fix of x right so and let it be very large let's say that x is what is x and y here x is in this particular example height of length of well let me go back to galton's example height of parent example, height of parent. Y is height of child. And by the way, it's more complicated because you have to factor in the height of the father and the mother and so forth. There are multiple predictors. But let me just simplify it into a toy thing and just say height of the parent. And you can pick. You can either pick the father's height or the father and mother's average, whatever it is that you want to do. And then there's the height of the child. What Galton did, and for reasons that I'll explain later, he took the father plus, I think, 1.08 times the height of the mother right height this was your x sort of now you look at y the height of the child so think about it suppose you take an outlier tall person right the outlier tall person is outlier is taller than his parents or her parents right why do you think that is some part is genetic right the causative factors are some parts are genetic and maybe some are environmental you really give him good food or whatever it is some are explainable causes right x let's say it's genetic the height of the parent that very tall guy let the tall guy be henry right and let's let his father be paul then a father is tall that contributes to henry being tall isn't it let us say that once again according to your equation the height of the child of henry henry is depends upon height of henry depends upon the height of paul approximately let's say there's a direct relationship, but you also realize that you can't get rid of the epsilon, the noise factor, right? The normal noise. The noise will do what? Will it guarantee that the height of Henry will be exactly the height of Paul? What will it do? Now, it will cause some change to that, some deviation to that, isn't it? If this is distributed like a bell curve, let's say that the height of Paul is here. Paul is here. So the height of Henry could be anything bellved around the height of Paul, roughly speaking. Would you agree? Does this make sense? Right, it makes sense. And so it so happens, perchance, that Henry happens to fall here. So the error, whatever those random factors are, they so conspired that the next generation, the Henry, turned out to be much taller than the parent. And those were some random factors in effect. Now tell me, so Paul, so the way it is, is Paul leads to Henry, child, and leads to, let us say, John. Right? What can you tell about the height of John? If you look at the height of Henry, you realize that the causative factor was around this, isn't it? This gap is the epsilon gap so are you sure what would you say do you think john will be as tall or taller than henry that is more likely or is it more likely that his height john's height will be pulled in. Come again. He will be pulled in because if you look at the bell curve, the probability is much higher that you will fall somewhere in between here. Right? So you're more likely to be more likely than, and this is less likely, taller than henry is it can still be possible but it is far less likely because the bell curve is centered around the causative factor yes go ahead no it won't remember that the causation in that generation in that family line says that the real the real fact the causation is driving everybody's way to be somewhere here right any deviations from the real genetic cause right is moving them back and forth so obviously they can be uh hen Paul, Henry, who remains? Peter, let's say. Peter could just have ended up here, right? You don't know, right? So all over the place. It just so happens that Henry turned out to be an outlier and went there. So outliers will get pulled in because those are because of adventitious unpredictable factors unpredictable factors don't always work in your favor right so this is called regression towards the mean this is the basic idea guys and remember that because people often sometimes it has been it has almost become a mythological figure, this statement regression towards the mean, and people don't understand it. And I will give you examples of how poorly people understand it. So I'll give you two examples and then we'll wrap up this topic. So the way historically it worked is that Galton wrote the paper Reg towards mediocrity. His student Pearson, of the Pearson correlation, was the first one to realize that this relates to the regression work, all of these, you know, the least square and regression and all of these things, work that had already been done, that the explanation was there. Right? So there's a lot of history to all of that. We won't go, but I will tell you and how poorly people understand it. So let us say that you take a bunch of people, a bunch of your students or friends to gamble. Right. What is that city? Las Vegas. And you go to a casino and you give to each of your uh people your group cohort whatever group you call it group of friends group of students whatever it is and you give them let us say an amount a hundred dollars hundred dollars hardly means anything these days given inflation let's say $500. You give them that to play. By the way, I've never actually gambled, done a casino. Can somebody tell me how much do typically people spend in a day? Go ahead, Albert. Maybe even $1,000. $1,000. In a few hours, you kind of could lose. $1,000. But typically, it was a mental frame of mind when people say, I'm going to Las Vegas, I'm going to have fun at the casino. What is their budget typically? 200. 200. Okay, so let's take 200 as a plausible situation. So you give 200 to each of the persons, and they go and gamble. Some will win, some will lose, right? But statistically the win and loss, let's say that they have started with 200, what will they come back with? So let's say gambling with $200, day one. Some people, what do you think? Suppose you take a sufficiently large body of people, let's say 100 people, you give them $200 each, make them go play casino. And let us say that the game they play, there is no hole in that. It is a toss of a coin. The only game they play is a very simple. I don't think this game even exists in Las Vegas. Since I've never gambled, I can't tell. But all they're doing is tossing coins. The game approximately tosses coins, a fair coin. So they can win. They can lose. It can be heads. It can be tails. And based on that, the outcome happens. So what will happen is because the house is not taking anything, and they are just playing against people. Most of the people will neither win nor lose. They'll have small wins and losses, right? Most of this will be around here. This is $200. They will either have the $200 back if all you're doing is tossing coins, or you would lose a little bit right the amount that you come back with would you agree would have a distribution like this around 200 right there will always be some people who will have a whole streak of lucky breaks and who'll end up with let's say i don't know should i make it thousand okay thousand let us say some people or whatever the number is i don't know. Should I make it 1000? Okay, 1000. Let us say some people or whatever the number is, I don't know, some large number outlier performance. Let us say that 10 of Right. Tomorrow, day two. Day two. What do you expect these people to do? Do they expect that the 200 became 1000? So do you expect the 1000 to become 5000? What do you expect to happen? . Yeah. Right. So, I mean, it depends upon what. If they kept their money here, now, once again, it's random factors. So there'll be somewhere around here. They may even lose money or they may keep their money. But if the system has a hole in it, if the house is taking out some monies, imagine a dice in which the five outcomes lead to five participants, but the sixth outcome money seeps out. Gradually what will happen is one day you make a lot of money, the next day you're not likely to be winning, you'll actually lose more than you win because your winning was an accidental streak. Are you getting the point? So that is a point to remember, because why when I put it like this, it looks so obvious, but it is not so obvious. In the stock market, you often see that for reasons that are like just accidental, some traders or some money managers will do extremely well. The moment they do, the investment house will start bragging about them, right? And saying, oh, they're great, and they have a magical ability, and people delude themselves. Those traders will often say they have the magic formula. They have some theory in their mind. They're winning because of this, because they have been trying out some pattern and that pattern is making them win. Guess what happens typically? That pattern breaks because that pattern was purely accidental. It breaks and then you start losing money steadily. And many prestigious banking houses, investment banks, as you know, have gone bankrupt because they put too much faith in one or two day traders who had winning streaks, brought a lot of money to the investment bank, but the winning streak broke ultimately. And when they broke, pretty much the company, they bet too much of the company money into it and they went down. I mean, you guys are into the people into finance, etc. We'll have more stories to tell. I not only do I not gamble, I don't do much of stocks, but it's a classic example that people give. But I'll give you a more interesting example. See, we are told, like, for example, in education and as a management principle, that whenever you look at either your students, your employees, or whatever your direct report, you see them do something good as a parenting exercise also. Advice is if they do something good, a parenting exercise also advice is if they do something good immediately give them positive feedback isn't it but if they make mistakes don't shout at them like just try to ignore it unless it is a blunder just look the other way right but make sure you notice the positive aspects and it's your responsibility to see the positive aspects and give positive reinforcement. And that is our way, well-established way, to deal with some basic way of relationship. When you deal with people who look up to you, that's what you do. Whether it's teaching, whether it's workplace, whether it's your children, it seems to be the current way. It's through encouragement. It is the converse of the traditional way where a teacher would hold a stick and whip and the parents whipping the kid used to be a regular pastime 100 years ago. So now there was an interesting experiment. This was done in Israel. In the Israeli Air Force, when they were teaching young pilots, they decided that this whole thing of coddling and giving positive feedback is all nonsense. So let's do one thing. The day they would train pilots to fly and the pilots who did badly, who flew, the young pilots, the student pilots who flew badly, they were given, they were absolutely shouted at, right, as a policy, they shouted at that. They found the worst performers for the day, and they absolutely went and shouted at them. Then the next day, they observed that those worst performers, they actually did better. Right? Not only did they do better, they, I mean, so then these guys noted it down and they, they tried to repeat the experiment and every time they did the experiment, this was true. Just go shout at the worst performers and next day they do better. Right? So it seems that the conventional knowledge is turned on its head, right? The right way is to just hold a big stick and whip people up who do badly. What is wrong with this reasoning? They wrote a paper, by the way, on it. And a Berkeley sports team, I'm told, I don't know about that, somebody told me, I think, was it KK told me that they also did the same thing. They also ended up publicly saying that, giving a really strong negative feedback to the underachievers leads to improvement. So do you think there is a causal relationship? Is that the thing you should do? Maybe you were making a mistake, you need to go home and the moment you reach home pick up a stick and beat up your kid. It might improve him the next day. What's wrong with this reasoning? The worst performers will eventually perform better. I mean, either way. It's negative is not tested. Right. There are many flaws. Exactly it's the null hypothesis is not tested. Right. There are many flaws. Exactly. You all are hitting the right points. See, the problem is you haven't done a proper study. What happened to the people? If you really took the underachievers, you should have broken them into two groups. One group you don't give negative feedback, and the other group you give negative feedback. And then you see, did the one with negative feedback do better than the group that didn't get negative feedback? Isn't it? You need a control group for the experiment. There was no control group. But that leads, so the experiment is deeply flawed. I mean, it's silly that in the 21st century, people can actually write papers like this. But now the question is, why is it that, why did this happen at all? Why did this phenomenon happen that the people who underperformed next day did better? You see, yes, exactly. See, the point is, when you do extraordinarily bad, you're just having a bad day. It doesn't mean that your worst day or the day you perform extraordinarily bad is a true measure of your talent. Given the true measure of your talent, there will be factors, random factors that will make you on both sides of it. Sometimes you'll do better than your talent level would justify. And sometimes you do worse and sometimes you do terribly worse. Right. Many things can happen. You may have you may be having a fever. You may have just had a, you know, gotten a traffic ticket, speeding ticket while coming to the airport, right, or God knows whatever could have happened. Many things can affect your performance many, many random things. So actually, you should have expected that the people who did worst on a given day are most likely to do better the next day. The only way, the minimum that is required in such situation is to see a consistently bad, consistently poor performance. Because what you're trying to determine is what is the inherent performance, talent level of this person, isn't it? And you can only do it, you know that it will have a, the values will have a distribution around the true value, around the true value, right? A true potential of the person. And you can only do it if know that it will have a the the values will have a distribution around the true value around the true value right a true potential of the person you cannot determine that by one reading that is one flaw isn't it you need to wait for many many days see and do that secondly control group is missing isn't it the the the the second second problem with this study. So, and you can go on finding more and more. So, and also one data point that does not make a statistic, right? You shouldn't draw conclusions from too few data points. These people made conclusions from just one observation. They repeated the experiment, but more than that, it is called, remember I said sequence is not causation. People realize that correlation is not causation. But sequence is not causation. Just because you gave them a scolding, the improvement that happened the next day is not because of your scolding. Right? It would have happened anyway. And the only way to know whether scolding was the cause is you needed a control group. Here, the biggest fallacy was even more elementary. Just because you scolded and next day you saw improvement, you just assume that you caused the improvement. You see where I'm coming to, right? So this is the fact about this is the concept of regression towards the mean that Galton found and it is everywhere. It is one of those, it's become, it's taken this fact and statistics in their sense has taken up a mythic proportion and there is, and scientists in every field keep talking about it, and yet we all make the same mistake all over again right human psychology seems to be very poorly adapted to grasping that this may be the underlying cause in many many situations right so remember that which is one of these reasons see in india for example there is this exam many of you know iit example there is this exam many of you know iit people practice really hard and the ones who get in in in my time in the 1980s the band of scores that would make them get in would be very very narrow right and many students would get exactly the same score it was hard to give them ranking and so on and so forth so now what happens is when it is so sensitive the scores are so sensitive and so close and whether you get in and not get in is just so tightly there what happens whether you get in or not not only depends on your talent level also depends on the luck factor did you happen to have fever that day did you have a good breakfast were you did you get enough sleep that day isn't it and so you see a lot of very bright people we used to see who didn't make it and it it used to be a mystery why did this fellow not make it he's so bright well there's the explanation right so that test was flawed because the band, it was so tight between people who would get selected and people who wouldn't get selected that just random chance could flip you from a guy who selected to a guy not selected. The test was, in other words, a relatively poor measure of talent. It is actually a good measure. The point is that if you were average, you would be excluded. But amongst the people who were talented, let's say that 10,000 people are very talented. In those days, about 12 or 1400 used to be selected. In the top 10,000, the 1400, the people who get selected and the guy who is 5000 the gap is very narrow and from that perspective the test was flawed right things have changed now i'm talking about things 30 years ago so more than 30 years ago so that's something to keep in mind never never ignore the epsilon never ignore the epsilon right the normal distribution of errors that that affects many many aspects of life so with that i'll finish this topic of regression towards the mean would you would you folks have any question before i move to the next topic so as if a quick question when you said that x1 x2 x3 is the input why do you call them vector because they just have magnitude ah in in machine learning the way we think about it very good so suppose there are three things the ice cream on the beach depends on temperature So suppose there are three things. The ice cream on the beach depends on temperature, x1, how windy it is, x2. Let us say two factors, and let's take two because that is enough to illustrate the point. These two points that go into the black box regression, and then you have y hat coming out. Now these two points I can represent in a Cartesian plane, right? out now these two points I can represent in a Cartesian plane right x1 x2 axis and any any data point is represented by x1 and x2 and so it has a direction as well as a magnitude do you see that it belongs to a point in the Cartesian plane okay got it thank you so it is a vector the cartesian plane okay got it thank you so it is a vector all right folks so with that i will now come back to the digression i'll come back to this thing called regularization is the next big topic that we are going to do regularization regularization. So this asks the question, see, when you have data, you can build a model that's too simplistic. You can have bias errors. And you can have a date. You can have a situation model is too complex right? It is a variant centers. Let us stick to the example. This example is regularization is a more general concept. But in our simple case we Ph.D.: polynomial degrees, so what happens is. R. Vijay Mohanaraman, Ph.D.: When you take so suppose data points are like this. R. Vijay Mohanaraman, Ph.D.: A complicated model tends to have. R. Vijay Mohanaraman, Ph.D.: lot of oscillations you notice that lot of oscillations are there right so oscillations why because this curve is too flexible and it's trying to fit to every point in the data set so how would you suppress that one easy way to suppress it is and the suppression of these oscillations is called regularization because if you could have a technique to suppress it what could you do you could have you could actually take a polynomial of degree 10 so let's say the high degree polynomial poly 10 and could you take poly 10 and still get a curve that is like this i'm sorry curve that is like this. Oh, sorry. The thick curve. Could you do that is a question. You know that a normal 10 degree polynomial will oscillate. It will overfit the data. So what can you do to convert the thin blue line to the thick blue line thick blue curve right that the process of going from one to the other is the process of regularization if we could do that we have essentially like you can have the cake and eat it too you can start with a fairly flexible model and still regularize it and make it not oscillate and therefore fit well to the data, even in test for test. That sounds very tempting, isn't it? The amazing thing is you can do it. There are techniques to do it and we'll learn some techniques. The simplest technique is regularization technique so I'll just talk about techniques the first technique technique number one get lots of data Lots of data. I hope you're there. But I hope you get the point. Are we all able to see my screen clearly? So you can see this is the situation one, A, and this is situation B. Now what does your gut tell you? If you try to fit a high degree polynomial, it has to fit through the data. It will try its best to fit through the data. It's very flexible. But would it be true that that flexibility will be still maybe small variations here do you notice what has happened those little bends in the 10 degree point I will is still there it is you need to create 10 bends in there, but because it has to go through so many data points, what is happening? It is forced to go through, it is forced to stay very close to the data, isn't it? Are you seeing it guys? In fact, this is wrong. Let me draw this part again. It is, let me draw this part again it is it is being it is trying is best to fit the data that it sees isn't it and by being forced to stay close to the data which it can because it's flexible it will now look like this to the data which it can because it's flexible it will now look like this guys is this looking obvious i need some feedback yes it is looking obvious right so the lesson number one one easy way to suppress variance error by the way not bias error and so there are a few caveats here one easy way is to get more data the more data you bring the problem will be there but this to suppress be there but this to suppress variance errors overfitting errors fitting things now this led to an interesting statement that a few years when big data was really taking off somebody made a famous statement i believe it is the author of this uh was it rajaraman or somebody at stanford i think made the statement that big more data is better than is better than smarter algorithms smarter algorithms right the idea was that the easy way to improve any algorithm take a simpler algorithm but just feed it a lot of data and then it will get rid of the variance. There is some truth to it. Right? And big data, one of its claims was that we now have so much data that a lot of the techniques people used to use when data was less. And you had to run through hoops. Data was precious and you had to run through hoops data was precious and yet to run through hoops we can forget about all of that just get more data and use simpler algorithms google in fact used to make a big deal that in most of their technologies they don't use the latest used to they now they don't say that used to that we don't use the greatest latest and greatest algorithm we just use straightforward algorithms but we feed it more data to train it to do it more data will suppress variance where it is wrong is it only fixes variant centers it doesn't fix bias errors so for example if your model is this not poly 10 but if your model is So for example, if your model is this, not poly10, but if your model is this yellow line, do you think more data is going to improve that line fit any better? It's not. That line is suffering from bias errors. It has already reached a saturation point. The more data you throw at it, the more it confirms that yes, this is the best fit line. Isn't it? The flaw there is you have taken a simplistic model, an overly simple model. So this is something that people forget, that more data doesn't cure all modeling problems. It only cures problems of one kind, the variance error problems it'll errors, variance errors, not bias errors all right this is it so that is a lesson learned right with this lesson learned now what is the other technique that we can use the other technique and there are many techniques and as you do this uh go through the nine months here with this, you will learn many, many techniques. But I will talk about the second technique that's important. The second technique, which is the core topic for today, and I'm wondering whether I should do it today or I should move it to the next week. It is 3.30. So let's take a poll. I need at least two hours to do the lab we'll make it one quick question so the once we have more then we will have more oscillation right then how our goal is to reduce the oscillation no once we have more what if we have more data lesson no once we have more what if we have more data no no remember a 10th degree polynomial will have at most 10 bends so more data is is not going to increase the number of bends it's fixed at 10 right only 10 bends but more data will suppress those oscillations make those bends not as it will mute the bends okay that is the point so remember the number of bends is not or oscillations doesn't depend on the it depends on the data lack of data but the number of bends is directly you can count it by looking at the degree of the polynomial right you keep on throwing more and more and more data it then you'll mute the oscillations that is the point of big data more data this is the first technique so uh any other questions guys before i ask a housekeeping question so like i mean uh it will come to a point that the bends i mean are fully oscillated right i mean like what point should the data stop or can you say it's sufficient so see it depends upon see remember the gap between the training error e training E training and E test, right? So this is the delta, which is the gap between these two. Just look at the magnitude of that. One minus the other, usually E test is more than E train, right? Delta. So when this delta tends to zero, you know that you have reached a point of saturation are we together because what has happened at that particular moment is your model is learning a curve that is pretty good at making predictions all the oscillations have been muted your test error and your variance errors and your training errors are close to each other now you know that you're reaching a saturation point another way to look at it is that at that point you're throwing more data is not improving your test errors right you're reaching a saturation point but typically my technique is this i look at the gap see i should have written this in the opposite way actually test the error res the net error that i'm left with between this yeah this delta when it becomes small so let me use the word delta is approximately zero like very close very close practically is good enough and let's see this in the lab which is why i want to get to the labs quickly so guys any questions no questions so then i'll ask how I mean, guys, let's pause the recording. All right, folks, we did theory enough of theory today. We were overtly ambitious on how much we could cover today. So we will, we skipped a topic called regularization we'll do it shortly now i would like to do the solutions to your homework and i will move a little bit quicker but give me one minute to situate myself here. Now guys, if you have done the labs, which I hope you have, I would strongly advise you to pull it up so that you can use that as a reference. You can compare what we did and do send it. As I said, it is important that you send your labs to us we will review we'll give you detailed feedback on ways to improve and things like that and we'll tell you how you stand if you have done a great job we'll tell you so uh let's go back to where we left off last time. Remember, last time we talked about this lab was about on the, I used to call it the range phenomenon. I think one of you collected news, the Runges phenomenon, Runges phenomenon. What is the Runges phenomenon? If you recall, it is the oscillations that you see in the periphery, or more broadly, any area of the data which is sparse. So we were looking at dataset 3 that looked like this. We tried to use polynomial regression. It seemed like a great tool. Then we realized that polynomial regression in this particular case, linear regression was a disaster, of course, polynomial regression was better. But when we did polynomial regression, even a degree seven, we got a very good R squared. Look at this coefficient of determination. You would agree that this is impressive, isn't it? So while it is very good, nonetheless, we thought we had hit upon a really good model when we try to go and look at the residuals it's a surprise it doesn't look like random noise you see a pattern there and so you say well what could be going on you can visualize the model predictions and then you get some clue you realize that it is reading a little bit more into the data, especially if you look on the right hand side. It is looking more into the data than is there. It is sort of doing a strange kind of a fit here. So this is this oscillations on the periphery is the Runges phenomenon. Now today we'll do data set four. Data set four is actually more straightforward. In the data set four, I gave this as a homework. We go through the same analysis, right? And you look at the data. What does it remind you of? To me, it looked like a necklace, right? Fairly straightforward, one band. So obviously, for the sake of going through the exercise, you can do a linear model because a linear model always gives you a baseline performance. When you do a linear model, what do you realize? Your R squared is not quite good. There is no straight line that fits this data well. A 4% or 5% explanation or improvement over the null hypothesis is pretty pathetic, isn't it? So you know that you failed and it shows up in the residual plot. Residual plot, what do you see? You practically see an inverted curve, the same thing. You see a strong pattern in the data means you have not done well then your visualization of the model when you look at so obviously confirms your belief that you have a poor model because when you look at the red line do you think it's a good modeling of the data no it, it's a very poor modeling of the data. So then we say, all right, we already have this lovely craft, polynomial regression, let's try it. One bend, so I need a polynomial of how many degrees? Two. That's a minimum I can start with. You try that, and you notice that two degree polynomial, already your residual now looks pretty unremarkable, isn't it? It looks like noise. It's spread all over the place. And then when you visualize your model predictions on the data, you get a near perfect fit, right? And what is the R square you get with the second degree model? It is 99.29%. So guys, I hope you did this lab. I deliberately made your first assignment an easy one so you would get a quick result from this. So this is it. There is no new learning in it. It is a practice one. It confirms your belief that polynomial regression is a powerful tool. You should at least try it given data right now this polynomial regressions and so forth it gets very interesting there's talk about interaction terms and things like that we'll do when we go to multiple multivariate regression many variables present but this is it so i'll be posting this of course to the course website today i was waiting for you guys to all try it out. Now there is something I did. You notice that all of my notebooks seem to have a repetitive section at the very top. Right. That now, data set three, data set three. So if you go back and data set three, you notice that there is a little bit of a formatting section. It's clutter in the beginning, right? And then there is also these imports. By now, all the imports, you know, you almost know them by heart, isn't it? So what I have done is I have put these into a separate common Jupyter notebook. This is very common. You want to run something in the beginning of each of your notebooks, just put it out as another jupiter notebook so i put all of this in a separate jupiter notebook right now what i do put it in the same directory as your code and now for data set four do you notice what i did in the beginning is it data set four or five? This one is for you. I miswrote it as five. Okay. Right. So what do I do? I write it like this. And when I run it, automatically the formatting, etc, kicks in and the imports kick in. Isn't it? So this is the way that you can run another notebook from this notebook and it just removes repetition right just like you would do in normal programming you don't want to repeat you call other functions which is this this is the equivalent in jupyter notebook so this is it we did four now let's go to data set five once again one quick question one you can use some little reference. Oh, yes, if it is in the same directory, you don't have to do dot forward. Otherwise you can or you can give the explicit directory name and so forth. Right. Right. Relative names and so forth. So treat it as this is more a unix command kind of a thing data set five data set five once again now guys all of these data sets are online if you notice in the code i tend to read it straight off the web right then you don't have to store it locally you want to copy it locally by all means do it we can load it up from local by now this part this part, the initial head-tail sample, by now must have become cliche. So I will stop. I've stopped repeating this part in other notebooks because by now I assume that the basics of exploratory data analysis has become second nature to you. Can I assume that, guys? Can I skip descriptive statistics in future? I will be doing that. So you have descriptive statistics you have two of these variables there are no missing values so this whole missing value analysis is sort of pointless funda's profile I haven't run but you can run and you'll see whatever it is uncommon this line data visualization when you visualize this data you see ah this is this so the point the reason I made it out to you is that uh do you notice that there seems to be no noise in the data? It's a perfectly clean check. Right. So it happens rarely. The data is so clean, so very clean that when you build a model, you may get a good model, but your residuals may say an entirely different story. They will point out two big mistakes. So that's the other cautionary thing. What happened in this situation actually is when I was creating the data long, long ago, four or five years ago, I forgot to introduce noise into it. And because I forgot, later on I looked at the residual. It was strange. To me, itself, it was a learning experience to see a pattern in the residual. And I learned something. What I learned is that when the data is too clean, right, you are applying a model, and I'll give you the hint about it. What happens is that it's a gamma function, right? Is it a beta function or gamma function? I think it's a gamma function. Yeah, it's a gamma function. And so if you do it with polynomials, a polynomial is not able to's a gamma function yeah it's a gamma function and so if you do it with polynomials a polynomial is not able to capture a gamma function it can capture it within limited range right but what will happen the fact that it is not a polynomial function will show up where in the residuals it will show up right so residuals are still trying to tell you something but uh you have to know what to do. So here you have this data, you obviously linear regression is useless I won't even go there. This is useless so actually not useless this is still pretty good you know it depends on your business use case. If this much error is okay or acceptable for your business use case the straight line is not bad how much is the well i called it terrible but now in second in hindsight i wouldn't say like that what was the r square i got or any one of you got how many what's the r square you guys were getting about 60 percent 60 61 62 percent right yeah you all were getting so guys did you all get residual analysis like this? Things like this. And then what happened? Yeah, and then you fit the model. It's not bad. Actually, I should correct by now failed. I wouldn't say failed, as you would have expected. It leaves scope for improvement. I would say it leaves scope for improvement right polynomial so then you're interested in trying polynomial and you can try polynomials of various degrees right but when you do that you find that you get a r squared for six degree this to me sounds like perfection if on test data set you're getting you remember we are validating it on the test data set. On test data set, if you're getting an R square of 0.9999, what does it mean? It seems to have hit the nail on the head. Would you say that? It seems to be a very good model. And yet, when you look at this, what do you make of the residuals? Do you see a pattern there? Unmysticable pattern. So how do you interpret this, right? Now let's just say that, wow, what's going on? Isn't it? Your reaction should be, I have two contradictory pieces of information. R-square is very good. Residual, scarce. So what really is going on? You plot the prediction on the model and what do you notice? It's still perfect. So ultimately, the proof of the pudding is in the eating. You have a model that is making perfectly good predictions, isn't it? Good R squared has high fidelity to the data, does represent it well. And so what do you do with the residual? You come to the conclusion that this polynomial is not the real truth behind the data, but who cares? It works. Right? So I can continue with it. In real business terms terms i can just go ahead with it that is what it is saying to you right yes oh residual analysis see residual analysis says it is many things actually a lot of the insights of regression is in the residual analysis that's one of the things that in fact many text most textbooks forget about it the point is if you see a pattern in the residual it means you have left some signal uncaptured in your model right or your model is not a true reflection of the underlying generative force like in this case it's a gamma function but you're trying to do a polynomial and as you know know, gamma being transcendental, you can never represent it with a polynomial, unless you do infinite dimensional polynomial. So the point is, infinite degree polynomial, sorry. So the other point is that it also shows you, the other thing that it shows you is I'm going to come to. that it also shows you the other thing that it shows you is i'm going to come to certain a specific pattern of something called heteroscedasticity means something it gives you very interesting clue on what you can do to make a better model it can point you to the way forward to get better that is why residual analysis for to get better that is why residual analysis for regression is a very crucial thing that people must do so as if a quick question let me take the that is the nuances i'm trying to bring to you so here are the situations your r squared can be good but your residuals can be can show patterns which means that there is another model that better model somewhere right then the third thing remember three legs of the journey your predictions when you plot it over your data in low dimensions you can do that you can see how well it is representing the data how well it's capturing if it is capturing so the situation is this sometimes your residuals your r square is good but your residuals are bad and your plot of the data shows But your residuals are bad and your plot of the data shows it is off base, then you need to build a better model, sometimes residuals are good, I mean R square is good residual has no pattern and in that case generally your model will follow the data, it is a, it is the best situation. Then there is also a situation in which r squared is low but residual show no pattern and it is a point actually we'll do in the next lab that that also happens and actually you have the best model that you can possibly build because you don't have enough information to do better so r square value shouldn't be taken as absolute this is one of the lessons for next time for example in advertising and sales in sales for example none of us can make no there's no technology that will help you make perfect sales models otherwise you would have trillion dollar companies easily right the thing is you you bring in the various factors that will increase sales or bring about sales, and you model it as much as you can. But you still get off an R squared of 23, 24%. Now, 23, 24%, if you're not trained in this, you may consider that, oh shucks, this is bad. But actually it's not bad because bad is a relative term. No matter what you do, you can't build a better model. That was the best you could do. And you can see that in the residuals because your residuals are not showing any patterns. So the residual analysis is very important. It should always qualify whatever interpretation you're giving to your mean squared error or to your R squared and so forth. That's the way to look at it. So that's the learning here. Now what happens if you go to 12th degree? Things begin to get complicated. And what you don't see here, but you'll see for some reason, it is not. When you take this tail forward, keep moving it forward, you start seeing oscillations. I haven't shown it in this picture and i need to at this point here there would be oscillations which i have not shown but i tried to show okay this work is incomplete i will show it to you i want to show you that the beautiful actually it seems to follow the data well but the moment it goes beyond the data it wiggles and it's also a lesson when you use complex models never extrapolate right extrapolation is a very dangerous thing never extrapolate from the domain on which you learned right so anyway that is that this is for data set three now i'll go to univariate two. Univariate one was simple, I hope, right? Do I need to explain univariate one, guys, just as a recap? Univariate one was your first homework, or was it even a homework? Or was it an in-class thing? In-class thing, right? So that is where we established all our processes, the systematic way to analyze data we did. This is the best established all our processes, the systematic way to analyze data. This is the best of all situations. You have a linear relationship. You go with the linear relationship, it works. It works well. And this is your residual plot, as pretty as can be, this is what you dream of seeing, right? Especially on your first attempt. And then this is your prediction line through the data. Of course, this is as easy as it can be. The trouble is real life is rarely linear, right? And does all of the things, interesting things that we do. So now what I would like to do is go to univariate 2. Univariate 2. Now again, I have just run the thing. Let me run this so that the formatting kicks in. Okay. Univariate 2. At this level, you load this data, the usual data, no missing values. It's almost redundant. Now look at this data i thought this was also easy when i threw at you how many bends do you see there are three bends now you remember guys that this is just a little bit more than the data set two isn't it i just added an extra bend you want to do it as a polynomial of degree, how much? At least four. At least four, right? So obviously, if you try linear regression to get a baseline, you know that you will see pattern. It's still a good exercise to do that. You can see that what is the prediction line? It is practically the null hypothesis, isn't it? If that is true, because it's practically the null null hypothesis what is the r square you would have expected actually is worse than zero it's minus 0.0166 well by a little bit but practically it is it is the null hypothesis you haven't learned anything whatsoever from the data. A linear model wouldn't do because the best fit straight line is what it is. Now, let's try polynomial of degree. You can try four, three, four, five, it's up to you. Here I deliberately took polynomial of degree nine because when you do it, you'll realize that you'll do it. So I'll tell you a trick. Why degree nine, because when you do it, you'll realize that you'll do it. So I'll tell you a trick. Why degree nine? See, anyway, you can forget it. It's a mathematical fact. You see a sine wave. If you do a Taylor expansion of sine wave, it's something that you must have in the very distant past, in the midst of the past, you must have done early calculus it happens to be x minus x cube over three factorial plus x five over five factor in other words it has odd terms it's an odd function it can only contain odd powers of x because there are three bands and you want to take four polynomial degrees so you should take x x cube x 5 x 7 minimum isn't it that's the way to Raja Ayyanar?n Yeah, see guys here's the thing, data science is an interesting thing. If you remember your calculus, linear algebra and probability, you can pretty much run circles around when you do the analysis, if you really remember it well. Unfortunately our generation of engineers, we didn't need to use it in the workplace long so most of us have forgotten. But if you keep it alive in your mind it is amazing what wonderful connections you can see and use when you're doing data analysis and we'll see more and more of that well okay here's the thing what do you say to this looks good right so here your linear regression and so this was a simpler one now let's go to univariate three regression and so this was a simpler one now let's go to univariate three how many of you did univariate three what is the polynomial that you went to what's the polynomial degree that you went to abhishek anil yeah anyone on oh by the way i stopped somebody on remote sanjeev nine you also went to nine okay sanjeev what was your polynomial degree i'm sorry not your point what was your question actually my question was saying that why how do you choose the n equal to three or four or nine so yeah i answered that okay thank you so so for a univate three, what was the degree? You took nine, is it? OK, eight. So what happens with univariate is when you plot it, so the usual analysis, when you plot it, you get a bit of a surprise. I put a little snake out there with lots of twists there or a path kind of thing. So you have a hard time counting the number of bends here so you know that if you are going to do it you have to eyeball it carefully and count the number of bends so i don't know depends upon how carefully you look there is a bend here a bend here a bend five, six, you know, seven, eight, nine, 10, 11, right? 11 banks. So you can go to, based on how you look at the data. So the linear, first you must build a linear model, even when you know it won't work. Why? It will give you again, a baseline. What was the baseline we achieved with a linear model? a baseline what was the baseline we achieved with a linear model 79 are you not impressed it's pretty good right 79 oh why is this actually i have not run this notebook okay here's the thing what yes what do you see this residual is correct actually residual shows a pattern definitely you know that linear model is not the appropriate model it doesn't capture the underlying reality but guess what when you plot it over this for some business situations that may work isn't it you may not be looking for the perfect answer. This model, this linear model may be good enough for certain business situations. The business team will say, oh yeah, this works for us. Let's run with it, right? So that's a value of a quick and dirty analysis and getting the client going, giving early results. You try it with polynomial of degree six here. What happens? It looks like this, right? Six bands, one, two, like if you can count it one two three four or where is my there's a gentle bend here five six and seventh bend garden yes seventh bend is right at the very edge you know five bands actually five bends to get the point but anyway uh think about it what happens if you go to degree 12 because remember we actually saw 11 bends right if you look more carefully does it help you by the way at six bends how does the residual look a little bit of a pattern is there isn't it if you look carefully it's not uniform random so you realize that you're leaving signal over the table the sixth degree polynomial is really not a good capture when you go to the 12th degree what happens you're surprised because your r squared is now what 97 percent 96.6 isn't it that's a tremendous improvement over the null hypothesis but when you look at the residual pattern you're like oh goodness there's still a little bit of a pattern left and when you project your model over the data do you what do you see longest phenomenon there at the end beginning to show up you are beginning to overfit the data and still you have patterns in your residuals so what does it leave you what is the conclusion and that longest phenomena you can see if you just extend the domain you see that in the extrapolated domain it's a disaster the 12 degree polynomial is a complete disaster isn't it and that is something you have to be very very aware of that in complex models never once again the message don't try to extrapolate. Now, from this, what do you come to the conclusion? Polynomial linear also did fairly respectably. Polynomials did quite well, even though we know that polynomial is not the right interpretation of this data, isn't it? We shouldn't be using polynomials, but for practical purposes, it sort of worked, but we have to use it with caution because it starts showing Runge's phenomenon. So this was your assignment, guys. This is it. Make sure you practice. If you haven't done this, please do it. Please practice. Data science is all about interpreting your findings, not just writing code and running it. It's about being able to understand what you found right now we'll change topics have i been speaking since did we have a break recently or not we did have a break okay so i will continue next topic so uh kyle could you please stop and start a new recording? The new topic that we are going to study is again one of those great topics for data analysis that people often miss. It is called the power transforms. And again, in statistics, there's a lot of work that has been done on this. I will in particular focus on one particular transform, which is called the Box-Cox transform, quite important. Now that Box-Cox sounds funny, and it was meant to be. It turns out that there are two great statisticians, Box, who lived in Wisconsin, and his brother-in-law, another great statistician, Cox, I think Jeffrey Cox, who lived in England, who was a Britisher. So one fine day, the Britisher flew over the pond and came to meet Box in Wisconsin. And so the story goes, the story may be a myth, who knows, right? But it's still a very entertaining story. And the way I heard it, surely with a lot of embellishments, the way the story goes is this, that they decided apparently to go hunting. And you know what happens when mathematicians hunt, they didn't get anything. Anyway, so apparently they came back one winter and they decided shouldn't we write a paper it will have both our names and it will sound so funny boxcox and learn beyond this sat down and wrote that paper that winter in wisconsin and that paper turned out to be a landmark paper so we will use this box-cox transformation. So the story is, by the way, I don't know how much credence one can give to it. There must be a grain of truth, but many people consider it a myth. So with that entertaining story, what is this transformation? It asks this question, the question, the fundamental question that it asks is, when you write it like this, let me bring it here for the box cogs. Suppose you do this. So let me say power transforms in particular we'll talk about box cogs it asks this question see why let us say is a linear function of x by linear i mean and you can add polynomial terms etc but i'll just leave it simple what if what if it is one of these y to the n right or log y is equal to beta naught plus beta 1x right or y to the 1 over n square root of the nth square root of n right is equal to beta naught plus beta 1x. Now, in all of this, you notice that the right-hand side remains the same. Isn't it? It's the same thing. It's still the linear equation. But the response variable, it says that what if nature so designed that it is square of y or it is log of y? Is that possible? Actually, logs are very common. I'll give you an example suppose so forgive me for taking an example in physics but that's what comes very common easily to me so i'll use it look at all of you know the law of gravitation isn't it what is it force is equal to what is it force is equal to m1 m2 over r squared up to a gravitational constant isn't it so suppose this is your y right and your mass is and let's say that the mass that you're looking at the gravitational force are faced by an apple or things so there is the mass of the earth which i'll just mark it as a constant right mass of earth or let it be there now what happens is is this a linear equation if you consider this to be x1 x2 x3 is this a linear equation it is not so suppose you're given data of x1 x2 x3 and y and you try to do linear modeling what will happen you would have a disaster isn't it actually i shouldn't have told you that this was one of your homework but anyway so it would be a disaster simply because there's no linear relationship but what happens when I take the log of it? Look at this. Log F is equal to log G, which is a constant, right? Plus log M1 plus log M2 minus 2 log R, distance between the objects. Isn't it? Now, what does this equation to you look like guys what is this would you call this is non-linear what about this equation it is linear let us give an interpretation to it Let's say that y is equal to log of f. x1 is equal to log m1. x2 is equal to log m2. x3 is equal to log r. And so this equation, and this is beta naught is equal to log g this is the this is your coefficient this is your intercept isn't it and so what you're looking at is the equation of a plane three dimensional plane which is that y is equal to beta beta naught plus beta one sorry x1 very simple equation x2 minus 2 x3 does this look like a linear equation now and so what has happened is a very simple and ubiquitous data namely gravitational data which was not amiable to a linear regression suddenly became because you did a log transform isn't it so the question therefore is that can we know whether a transformation is called for how would we know how would we know that we should do some transformation before we proceed so the clue come and that is the question that Box Cox answered. So it is one of the transformations and now there are modifications of that. Right, which improve upon it, but we'll stick with the Box Cox. It basically says I will go to the exact formula but there is a y to the end. Basically it asked for this. Actually there is a minus one and so on and so forth is equal to blah blah blah lambda it uses the notation lambda i won't use that i will just think of it as power which is being very very sloppy forget the minus one and all of that and i'll say what power do i raise y to that it will make it into a linear equation. So to do that, there is a lovely idea, and we'll do it by practice. The clue is in the histogram. Look at the histogram. What is a histogram? Frequency plot of values of a variable bar chart of bar chart of frequency so let us do that so what happens is here is a data you visualize this data and when you visualize the data what does it look like you you notice two interesting things first is highly non-linear and the other thing that you notice is look at y the y-axis do you notice that for any given value of x the y the variance of y keeps on increasing as you go to increasing values of x variance increases this word is called hetero skidasticity right it is heteroscedasticity whenever you see heteroscedasticity it should give you a clue that a power transform is called for are we together remember these two things whenever you see heteroscedasticity in the plot you should ask for you should you should you should trigger this thing in your mind maybe maybe i need to do a power transform right normally you wouldn't think of applying a linear regression to this would you right but i mean the thing is many many times actually very many times in practical life when i analyze data i notice that power transform helps immediately in simplifying a problem people will apply deep you know throw complicated algorithms xg boost deep neural networks and whatnot right not called for because those are not interpretable models those are black boxes you want interpretable models so obviously we'll go to the obligatory linear regression exercise because it gives us a baseline performance if you do linear regression our r square is 67 68 percent not bad 68% not bad. Because if you look at this data, you can sort of approximate it with a straight line, but you'll be missing out on some crucial signals like the heteroscedasticity here. You look at the residual plot of the linear model, boy, is that a pattern? Very clearly a pattern and the pattern residuals also show heteroscedasticity here right in a pronounced way see if heteroscedasticity looks to you hard to think think of a funnel is the data funneling out right isn't it funneling out it is Isn't it funneling out? It is. So like a chimney smoke or something. So now when you visualize this model on the data, the model is not bad, except that it is able to make reasonable prediction without really capturing the behavior of the data, isn't it? So what can you do? You could do polynomial regression. I won't go go through it it improves the r squared from 68 69 it goes to 83 percent coefficient of determination do you still see pattern in the residual in particular heteroscedasticity you do see that so things are getting now observe something that here the data is skewed it doesn't look like a bell normal distribution of residues. Poinomial has improved upon it, it looks a little bit better but not the skew direction has seems to have changed. When you put your model prediction, what happens this do you notice that once again while it can be used as a model to make predictions you notice that it doesn't really capture that well so what can you do that calls for power transform in particular so the clue of this is how would you go about doing it one simple way is draw the histogram of both X and Y and see if there is skew. Remember skew? We talked about skew, right, in the first session. So is there a skew in X or is it more or less symmetrical? Symmetrical. In fact, it's a uniform distribution, symmetric. What about Y? Is there a pronounced skew in the values of y? It is heavily right skewed. So when you see this, you know that a power transform is called for, right? Of which variable? Of y. x is no point. We can do it for both, but x will remain x. So it turns out that psychic learn comes with the power transformation it builds in you don't have to do a lot of mathematics us if you do it and you just apply to why right now, when you do it, you have to do a little bit of fiddling around remember that you're dealing with that is. R. Vijay Mohanaraman, Ph.D.: fiddling around remember that you're dealing with arrays numpy arrays so the shape of the numpy matters you're applying it to y so y was a data frame first you have to convert it to a series and this is the distinction between series and numpy array you have to extract the the array from the series y was a series remember when you did x y breakup so a series object has to become a numpy array you do that by calling dot values on it do you see the dot values it gets the values out of it and then you have to reshape it because it is like one row what do you want to make it like one column right so that this is the way to do it minus one says whatever number of column it becomes and what you want sorry whatever number of row it becomes and the second attribute is the column how many columns you want one right reshape the data so that whatever number of rows it becomes in one column you do that you apply the transformation and now you do a histogram so notice the tilde on it guys you know is it visible on the screen this is y tilde y transform once again a lovely thing about jupiter and python is you can use mathematical notation natively in your variable how did i get that very simple see See, I'll do it for you here. Suppose I have y, right? I say, you have to know the latent. y tilde, right? You could do anything. You could do y hat, hat comes in. So you just have to know the latent notation and these are things. And not only that you if you notice in my code, you will see that I use alpha and beta and so forth, if you want to do beta. R. Vijay Mohanaraman, Ph.D.: lovely it's much better to name your variables by Greek letters because that's how we think about it isn't it. R. Vijay Mohanaraman, Ph.D.: it so now is it looking like more symmetric to me is this histogram looking more symmetric compared to this so what are we comparing it to comparing it to this guy compared to y is y hat looking more symmetric yeah definitely more symmetric it is symmetric so then we ask okay what is the then in the power transform remember i told you is the lambda what power you have to raise it to and when you do that there's a surprise it says it is 0.01 0.01 the closest number that i can think of is what whole number that i can think of is what zero number that i can think of is what zero right so when it is zero the way to interpret it is it is the log of y that zero is special it means you need to take the log of y right and you take the log of y and you just let it do that and then what happens you plot the data the data between x and y and x and y tilde which is the log of y effectively and the first has this relationship you know this very non-linear relationship what do you see in the second plot essentially a linear relationship isn't it which means that you can essentially say essentially a linear relationship isn't it which means that you can essentially say log y is beta naught plus beta 1 x and you can start modeling using that and when you do that you expect an effective model wouldn't you right which is what happens now you're doing again a linear model a very simple linear model but in y tilde right log y and what happens you get minus one as the intercept and three as a slope let's see if that is true minus one as an intercept that looks more or less corrected minus 1.7 as an intercept it's hitting the zero here and slope is three if you go up one unit it seems to have gone in more than two oh minus two to four units almost right so the intercept of 3.37 looks reasonable would you agree the rise has been that much so now let's look at the r squared what about r squared do you notice something interesting you have a much better model but if you were only fixated in r squared you may not have been impressed r squared went from 83 to 85 isn't it so that is something to remember that take these metrics or with caution no one sometimes you know it doesn't quite tell you the whole story residuals look very impressive you see there's no pattern in the residual, whereas there was pattern before. Now there are no patterns in the residual and you can see it as almost a perfect bell curve here. That is good. Now, what do we do? So we'll do it with another example. Now, here is something, guys, that is an unfortunate fact. It is there in machine learning textbooks also. Thank you for being here. So quite often when you read even papers and things like that, people say, and in blogs in the internet, they say if you see the skew in the data, you need to take log transform. In this particular case, log transform worked. But should you think like that? It has no basis. Right? It's just a habit of people to take log transform. And it has made its way as established wisdom into textbooks, which is unfortunate. Because there is no justification why log transform should always work. You need to do a proper power transform to see the relationship. In fact, surprisingly, I came across a research paper which said, hey, we found some use cases where log transform doesn't work whenever you have skewed data, but log transform didn't work. It's a published paper, but actually, and it is not, it was in the medical domain or something like that. So the point is that if you understand power transform, you would expect it. You don't expect log transform to always work. So I deliberately took an example of another data set. Guys, I can see tiredness on your faces now and soon. I'll end soon. But I want to make this point. You take another data set, which is there. What do you see? Do you see skew in the data? You do. You might be tempted to say, hey, log transform. Not the common wisdom that's mentioned. No. When you do the power transform, let us see what the coefficient turns out to be. Lambda turns out to be 3. Right? Not 0. coefficient turns out to be lambda turns out to be three right not zero means y cube is linear in x y cube is equal to beta naught plus beta 1x isn't it y cube is equal to that this is what it is saying isn't it now let's go and look at the original data there is a bend here you also see heteroscedasticity and everything you do the power transform and now how does it look beautiful so guys do you realize that you look at a problem most people would give up right there they say linear linear regression won't work isn't it the accepted wisdom is linear regression won't work but what i'm trying to tell you is linear regression along with polynomial regression is far more applicable than you think you just have to think smart because if you can get away with linear polynomial regression you have one giant win the giant win is you have a highly interpretable model right you can tell that if you exercise this much this this is how much it will affect your blood sugar or something like that right you have a very direct interpretable model it can't get better than that. So don't give up on linear models unless you have tried everything. Because surprisingly, they're surprisingly effective if you use your brains on that. Is y cube versus x the same thing as y versus cube root of x? Yes. No, no. Cube root of beta naught plus beta 1x so as if so if linear linear present work then as a practice we should do the transformation or we should go for polynomial like what is the best practice to follow no the the way you do that is you try everything. If you see bends in the data, see, observe something. This particular case of the data, you see this, right? What you see, there is only one gentle slope, but the curvature of the slope is changing, right? Whenever I see the curvature of the slope changing, so gradually, right? It always tells me that polynomials may not be the right approach i should go with power transforms and try it ultimately you have to try these things and see the other way is the pronounced presence of in the data itself the heteroscedasticity in the data itself the histogram has a skew whenever you see his skew in the histogram in the data itself a power transform is called for remember that you may still apply both you may it may so have turned out that you needed polynomial regression after doing the power transform quite possible right go ahead so what's the power transform yeah so it turns out that it is this is the equation it comes out with it came out with lambda is equal to three so this is the relationship it's looking for this is your relation but you don't care because in the code what happens is you if you want to find out you find out, but in one line, it does your transform for you people just go ahead and use it in fact they don't even bother to understand what just happened right. So you got the histogram would you agree not quite symmetric but pretty close. right the transform variable is pretty close to it and then now one thing to know though it comes with a price guys whenever you do power transform or any kind of a transformation of the variable you have the possibility of popping up nans right you get lots of nans or to joke lots of NANs, or to joke, lots of NANs. So you have to weed them out. You have to drop the data which have NANs. So NAN treatment becomes a thing you have to do. Remember, whenever you do mathematical operations, you have to fear the NANs. Yeah, you just drop some NANs. So what I do is that I noticed that there is there are NANDs present, true. What I will do is I will just drop those records. I won't make predictions using that or build models using that. That's all it is. Nothing, you drop any. Here's the sentence. How does it perform that? Oh, because underflow overflow. What happens is if a number is too small and you take its square root or you take its inverse or things like that it either it will blow up or it will underflow the remember computers are not capable of capturing infinity or infinitesimal there is a minimum bound the minimum size of a number below that it's underflow there is a maximum size of a number above that it's an overflow i assume it's something like the negative data and the transform of the room yeah and that also it can so you have reasons why you'll end up in that go ahead question here like how about negative lambdas negative it could have been negative so it would have been a cube root then rather than cube negative means y to the minus three power for example oh no sorry not cube root it would be one over y cube whatever right okay that's it that's all it is sort of and by the way i am if you look at the actual formula box cox uh you you will realize that i've taken liberties uh there's a minus one there there's a divide by lambda there there is there's a lot that i've lost but it's hand waving generally if you just think it as y to the power lambda it's fairly close yeah automatically figures out which formula to use yeah that is the power of that power transform does the whole analysis and finds the right expression you don't have to think that you do a square root or no you don't have to this tool does the thinking for you yeah you usually online readings like they are providing like it should be like taken in into consideration when uh positive is that the case also like or no no no so here is the yeah that is a very good question who is asking this question okay thank you for asking this question see in the scikit-learn there are two uh power transforms one of them is box cox and the other one i believe is called your johnson yeah so the box cox has a limitation if your domain the x values i mean the input values say nothing sorry whatever variable you're looking at if it has negative values in it here it is y if you also have negative values box cox transform has a limitation. It won't work. So what can you do? Use the Ewe Johnson. Whenever you have negative values, use the Ewe Johnson. By default, this power transform, and you'll see it here. Let's go and see. By the way, do you notice that Y, okay, here, Y is only positive. So I could have used, in this situation, I could have used either this, I used both of them. The first example I did using box cogs. The second example, I just left it blank. You see that I haven't given any argument to the constructor. So u Johnson is assumed by default. Generally, u Johnson works for any data. So people tend to use it more often as the default. Generally, a Hugh Johnson is works for any data. So people tend to use it more often as the default. If you try this example out with the first one, you will notice that for the first case, because all the values are positive box Cox is applicable, not only is it applicable, the transformation that it gives leads to slightly better model. the transformation that it gives leads to slightly better model okay so that's it that's it so that is that so this is it guys now let's look at the model over the data what happened here what was i doing okay something i'll have to fix this and give it to you you rerun the notebook and give it to you so would you say that this model is a fairly good capture right it captures the bend also maybe it could have done a slightly better job and we can play around with it but it seems to be doing a pretty good job right so that is the power of power transfer that's an unintended one the power transforms are an effective technique, I should not use the word power. I think you can define it right. So power transforms. Think in the complete reverse way rather than looking at why, from the point of view of x that look at x from the point of view of y would make x linear yes see that also can be done ultimately and this is a question actually in the next lab we can ask this question can we look at it backwards right and it doesn't matter you know it becomes a square root for example if the nth power turns out to be half the lambda tends out to be half what does it say the square root of y is proportional to yeah right so you could look at it this way so you can say that x is proportional to y square isn't it that's it so ultimately this transform does work and it points the way and you can use it this transform does work and it points the way and you can use it now guys today i taught you to the lab two techniques polynomial regression most people and in fact most of your books won't have it residual analysis is very important you must do it power trans power transforms are a powerful technique use it right it is always better to build simpler models to explain reality. Don't give up on simple models till you're sure that they won't work. So that is the lesson for today. And now there are a few more topics. We'll keep doing the labs. Of course, it's a long journey. Looking at your faces, should i do more labs or enough you look tired yeah okay we can do so i can look into the housing data which i may or may not have downloaded but frankly today Currently today, everything I'll put on the portal I haven't done that I'm a bit behind. So guys now the solutions when you run make sure that the support vectors common yeah first put it in the directory. Before you run all them because they will all need this. Are we together. Now I wanted to show you a thing let's go and see what i did in reality but so i told you that the log transform turned out to be a good one well where did the data come from of course i generated it what did i do to generate the data you can see the equation i started out with the equation log y is equal to this and generated data off this so do you think that the foot the the exercise succeeded power transform exercise succeeded so likewise i believe for the second one also you can go and do yes so here we go and power one third and so on and so forth so so when you say y cube is proportional to beta naught plus beta 1x you can say that y is proportional to cube root of beta naught plus beta 1x right one third so is that the answer that you're finding power transform found one time pretty accurate isn't it so that is it that is the power of that and so i'll i'll put this data generator also for you and so see guys no one technique always works the reason machine learning is such a rich subject is because you need to know a lot of these techniques the more you know the more likely you are to succeed. And the more likely, the more talented you are, the more likely you are to build good models, simple models. I like to tell this story. Did I tell you the story of the photographer? Have I bored you with it? Not yet. Let me end today's session with that story. Not yet. Let me end today's session with that story. So when in Silicon Valley, of course, every once in a while, some company will go IPO and it will instantly produce lots of millionaires, young millionaires, right? Now, one of the first one of the common favorite purchases amongst them is to go get I used to be nowadays it's just iPhone, but it used to be to go get a really nice camera. Anybody knows, has done that from experience, bought a DSLR? So, well, you do that, and when you get the DSLR, you go about taking pictures. Usually you can spot them because the camera will be on full automatic, which defeats the purpose of having a DSLR, this big fancy camera because the whole point is to take complete charge of it manual charge of it control the aperture control the shutter the right lighting and everything but they'll run it full automatic there's another telltale sign so after a little while they discover that pictures don't look good so there are two choices you can either learn how to use the camera or you can buy more equipment right so then you you see them buying expensive lenses because they realize that photography they realize the word photo is light right the word photography stands for painting with light so you better get a good lens to capture good light and they will buy this long big lenses expensive lenses right the canon l series and so forth i've seen and you find you can spot them because in broad daylight they'll be taking pictures of people right and you can always tell these people because you won't see photographers ever do that broad daylight sun is up there casting shadows onto your face everybody looks terrible right you don't do that so they'll go to a distance and they'll do all of that so anyway i'm deliberately making a caricature of it but it's sort of like that right then what happens is they realize that well even that didn't work and then it dawns on them what is called the enlightenment. They realize that it is not about the camera. It's not about the lenses. It must be about the tripod. So they buy the heaviest tripod that their spouse is willing to carry. Because they are lugging all this equipment. I'm embarrassed to say that I belong to that class. That's my history. because they are lugging all this equipment. I'm embarrassed to say that I belong to that class. That's my history. Then you learn photography gradually. So when you learn photography properly, you don't carry, you don't use fancy equipment. You just carry one camera with, you know what lens you will need, and you just take one lens to one scene, to one spot. And usually it's enough one or two lens so the great photographers people who do national geographic and so forth they go into cultures and forests quite often with a beat up camera good camera professional great camera would beat up old one with one lens and they come back with zingers perfect pictures that make their way into national geographic. Right. And so what I'm trying to say, guys, is be like the national geographic photographer. Don't be like rookies like me who carry long equipment and go to the beach and harass everybody. All right. So that's an important lesson. In other words, machine learning or data science is not a game of fancy algorithm. Every day new algorithms, new things come. Yes, it is important to keep up with them, learn them, but your first reaction is what is the simplest way I can solve a problem, right? The simplest solution is the best solution. It's the Occam's razor principle. The simplest effective solution is the right solution, right go for it interpretable simple models don't give up keep trying if it fails then go to the next more complicated the next more complicated never lose interpretability why should we not lose interpretability hell breaks down in fact the biggest risk of AI is not that AI will come in and slave us. The biggest risk of AI is that we are trusting AI too much. We are feeding it bad data. It's making prediction based on really biased data, but we can't see the bias. Because we can't see the bias, it is tearing apart the fabric of society. We literally see it all around us. And it does a lot of damage. There was an example in which Amazon released a facial recognition system, Recognito or something like that, for law enforcement. And one researcher, I believe at Harvard MIT, did some simple experiment. What she did is she took the picture of every congressman in the Black Caucus, in the Senate, and gave it to that algorithm. And that algorithm promptly declared these people, identified these people as either serial killers or rapists or something like that. You can imagine that the Black Caucus and these congressmen were not amused, right? But it is a fact of life. Imagine these things unleashed. What would happen is so many of the people, minorities, will find themselves unjustly imprisoned. So these things are facts of life. Interpretable model, you can see why it is saying what it is saying. Black box models are dangerous. So use them only carefully and only if you have to. additional angry with that. I think when we say these kind of looking at the example of image recognition and so on. Yes. But there are plenty of examples in between. Yes. Now going through that the current experience of the job. You can build a model to do something simple, right? Defend advice, let's say a buyer or a sourcer in a supply chain situation in terms of where are the opportunities for low price and so on. Yes. Building a model, you'll work with one or two people and you build a model. Yes. You roll it out in a complex organization where there are 5,000 buyers. Yes. The interpretability of the model is extremely critical because there're not going to give in to something that some machine said. They've been doing buying for the last 15-20 years. They want to know why you're telling me this and how will my job be better because it comes. Otherwise they're going to shunt the model and just keep doing what they do. Interpretability becomes very important. Thank you for sharing that example. Another example that I would like to share is this. You know that I'm applying AI to the learning and HR domain, learning and development domain. So there was a famous case many years ago, over 10 years ago, I believe. There was a company called Kronos. They were doing something, I believe it's algorithmic hiring, I don't remember the whole details, that algorithm would tell you who to hire, who not to. They got sued for discrimination against one of the protected classes, I don't know, gender or race or something. People could show that yes, it was discriminating. The problem was why did they not catch it? Kronos' defense was that we are not deliberately discriminating. We are making a very fair, impartial neural network model. It is learning from the existing data and it's making predictions. But ultimately they lost the case because it was the judge said that it's your responsibility to make sure that there is no bias in the model. You cannot absolve yourself of bias and just use a machine to discriminate. Because then everybody will say, hey, I'm not discriminating, the machine is. And we didn't know that it was discriminating. So now the gold standard is you must ensure, do huge due diligence, best case effort to make sure that you don't have bias in the system. It's very hard. And therefore, the big topic of interpretability. Traditionally, in this workshop, which is a combination of the former ML 100 and 200, I would not do explainable AI. But this topic is very important. And in fact, in just two weeks, I'm giving a keynote talk in Singapore on it. But it is important because it is the central problem of AI in society. The ethics in society, the ethical use of AI is very closely tied to interpretability of models and whether it's a force for good or it can do harm. So again, a big topic, but what I'm trying to, you must have noticed that if you have taken machine learning courses elsewhere and so forth, people don't go to such an extent going deep into linear models I have spent a lot of time on linear models because I believe that you should try your hardest to see how far you can push it with polynomial terms with power transforms with this and that right do as much as you can because if you get away with it you come back with the gold standard, you come back with a very, very, very explainable model. Alright guys, so with those, I would like to end today. It's been a long session.