 ðŸŽµ Let's get started. So you remember we reload the data, we drop the null values and so forth. Remember the Pandas profiling is a great tool to quickly get descriptive statistics, a preliminary descriptive statistics. It is not everything, don't always completely rely on this. This was the flag data set. We got the descriptive statistics on it. And then it gives you quite a bit. Then remember the first thing we do is we extract, we separate if you're doing supervised learning, either classifier or regressor, the first thing you have to do is extract out the target variable from the data. And now you have a separation between the feature set, the features that feed into the independent variables and the dependent variables. Another way to say is input and response. Capital X by convention is the input capital and little y is the response. Why is that? Because x would be a matrix because there are many features producing the input and a row is like there are many rows so it's a matrix whereas y is a column vector. like there are many roles, so it's a matrix, whereas Y is a column vector. Matrices are capital, in general convention people follow, and Y, they write it as a column vector. Actually, I take that back. In neural networks, all those conventions go out of the window because everything is matrices. All right, so we visualize this data. If you remember, this was the visualization of the data. And just to remind you, this was a theory of logistic regression. Basically what we said is log odds is the distance from the decision boundary. By the way, I have a video on our website, on our YouTube channel about distance from the decision boundary. You might want to consider watching that. It is quite relevant in the context of, let me show that to you. It's a very short video for 15 minutes. Why do I call it distance from the decision boundary it is because youtube.com slash if you go back go down here you see linear classifiers distance from the decision boundary this is the video I'm referring to and you will you will realize that at the end of it let me go straight to the end yeah you will come to the conclusion as you do here that this equation or this beta naught etc etc here W it represents a distance from the decision boundary in other words this thing is the distance from the decision boundary all right so that's a little bit of theory for those of you who remember your ml100 we'll build a model if you try to naively build a logistic regression model as we realize it's a disaster it's a baseline model it just degenerates to the baseline model where the majority class is the predicted class or the zero r model next we try it with polynomial regression polynomial logistic regression we expanded the feature space with polynomial terms when we expand the feature space with polynomial terms we get a pretty complicated model. You see how complicated this model is, and the fifth degree polynomial. But then if you try to look at how good this model is, still the accuracy is only about 69% for the zero, which is the real positive test case here. There are fewer zeros here. 85 for one, but it doesn't. So then we do a more careful analysis by extracting the river, modeling the river with the line. Oh, where's the line gone for the river? Okay, seems to have not run the visualization. Look at the model diagnostics model diagnostic little code these were the model diagnostic you remember we talked about having a homoscedasticity in the residuals and the residuals must show a bell curve distribution which you see on the side all of those are good things a good correlation positive correlation between prediction and reality which again is here the cooks distance to show if any points of high influence none of them seem to have they all do well and finally we are here it is we visualize the model over the data so we now have captured the river the center of the river and so it helps us extract the feature namely just the distance from the center of the river and and we are done. After that, we just have to build a logistic regression in one variable. Just look at this. We are using a feature space of just one variable. We ignore all the input variables, and we are just using d, which is a new variable, the feature that we extracted. And when we do that, we get a model that is actually already from the confusion matrix, begins to look good. And when we do that, we get a model that is actually already from the confusion matrix begins to look good. And when we do the classification report, we suddenly see that it has very good accuracy, 84% for the 0th class, generally. Overall precision, the harmonic mean of these two is pretty good. Acc 89 percent so 89 percent accuracy is a pretty impressive accuracy even for the zero the positive test case we get 84 percent um pretty good accuracy we can see how the precision and recall is for each of the two classes uh for the zero positive classes this 85 82 83 and so on and so forth pretty good the roc curve again if you remember what matters is the area under the roc curve when we look at the roc curve the area under the rsc curve it's an impressive 90 96 percent of area under the roc curve. Pretty good. How often do we get the mixed up? So in for blue there we misclassified some blues as yellows, the sand, and some ones we misclassified. Very few number of ones we misclassified as zero. So this is it. So now the question is we learned about decision trees, isn't it? The theory of decision trees, just to recapitulate, we learned that a decision tree goes about recursively splitting the region. It uses either Gini or entropy as its impurity index or disorder measure and then it uses information gain as a criteria to split, where to split. I hope this time you all understood information gain. That reminds me, I have to send you a survey guys, but do please attend that, respond to that survey. Like you notice, if you feel you didn't understand something, I always repeat it. So that's how I discovered it. So now that theory, when it comes to practice, what does it become? It is a pleasant experience. It becomes just three lines of code. One line to this is just importing the library. This is to build a classifier. Do you notice that you build a decision tree and you fit it to data training and training and training data you make predictions you get predictions and then what do we do we look at how good those predictions are we look at the confusion matrix the confusion matrix looks pretty good the principal diagonal has most of the elements and off diagonal doesn't seem to have that many elements which is very good if we do a classification report you see a accuracy of 87% not quite as good as our feature engineering but still respectable so this model is not really as good as the previous are doing it carefully by hand but still it's a respectable measure now when I build this decision tree one of the things I didn't specify here is how deep was the tree so we can specify actually those parameters so there are a lot of parameters that we can use to tune the tree and perhaps it will make it a little better. Then once we do that, we can create, as you notice, where am I? Decision tree on the river data set, right? So this is it. Overall accuracy of 87%, very good accuracy. then you said it just shows how these nonlinear methods or decision trees are quite effective and you see the reason why when decision trees came onto the scene people they really went overboard with it you know they got exaggerated statements that this is all you need to do and she learning and so forth those times have changed of course today decision tree is not considered the state of the art by any means but it is there nonetheless so yeah yeah go ahead yellow brick classifier any reason to use data yellow bit pacifier it is a library that helps you visualize models okay model diagnostics attention so here we go we look at this so guys look at this decision tree produces our ROC curve like this for a moment just stare at it and tell me is this better or worse than what we did in feature engine compared to this what do you think guys our feature engineering ROC curves are better or the decision tree ROC curve is better feature engineering one is better yeah it's much superior and it also shows guys that if you can do feature engineering, it is the gold standard. Machines will only try to achieve that, but not always be successful. Sometimes they are successful, so I'm quite off in that, but it comes close. When you try to visualize this decision tree, remember the whole value proposition of decision trees is that they're interpretable. I'll let you decide how interpretable this picture is. How many of you feel this is a highly interpretable picture? I'll open it in a new tab. Where is it? Yeah and I'll expand it. Even in its expanded form, how many of you are delighted that this looks like a very simple explanation of that clever data set you wouldn't feel that it is almost a black box though in principle decision trees are highly interpretable in practice the moment you give it a non-trivial data set you begin to say all you see all interpretability going out of the window whereas a feature engineering it was so easy for us to interpret. We had an intuitive explanation. We said distance from the from the center of the river. That is the difference. This is why good feature engineering is the gold standard. We will do this now again with random forest. When you do it with random forest once again we studied the theory of random forest you use a lot of trees and so on and so forth you see me do that we'll grow a lot of trees how many trees will be grow will grow a forest of thousand trees when we grow a forest of thousand trees and by the way number of jobs in parallel this is the degree of parallelization how parallel you make it so the more parallel you make it the better so for example if I say number of jobs that will run in parallel is hundred right obviously if your machine has that computing resource then by all means you go and do it and if you do that notice that this decision this random forest built on my machine in 1.6 seconds but don't expect the same on your machine if you have a gorilla machine it may finish even faster if you are running it on a laptop expect it to run for a minute or so right let it how is is that a standard for thousand trees or how did you pick that? Yeah, that's a very good question. See, my basic rule is I never take more than 400 trees or 500 trees. I count the number of predictors.'s the number of predictors is two right okay i multiply 400 by number of predictors okay so it's uh see we live in computational plenty so we can sort of go overboard there's nothing wrong 200 times to 400 times to 800. So I approximated it to 1000. It quickly ran through. But do we really need 1000 trees? Probably. Maybe I could have gotten away with 400. I usually don't go below 400. The default is 100. And the reason that you could learn in all these things have a modest default is because they are trying to make sure that the code runs the library runs on people's laptops and pretty underpowered so that's that so What did you mean by number of predictors is two? Sorry, I didn't get that. The, the, the, talking about here in the random forest and so you run it this is the level of parallelization that depends upon how good a machine you have so model diagnostics is again simple the same model diagnostics that we have been doing so do you notice that guys certain things become boilerplate in every analysis you'll do that once you build a model if it is a classifier you'll you'll need your analysis, you'll do that. Once you build a model, if it is a classifier, you'll need your confusion matrix, you'll need a classification report, and you will do the ROC curve. Right? So once you practice this, you will use it for the rest of your life. That is the value of it. While this is good to do, I must also say that when you interview people and you see people from different programming houses or different data science houses come to interview with you, most often you find that people are sloppy. They don't actually do the model diagnostics carefully. So you have to just learn to do it right once and then do it. All right. So with random forest, you see that the ROC curve is, is it looking better than the decision tree guys? Yeah. Yeah. So can you, can you guess why is this better than the decision tree? What is wrong with decision trees? There is a limit to the extent or that the branches of the tree, we are limiting it. No, actually the problem with decision tree is exactly the opposite. It tends to just go and overfit to the data. Remember it fits the data hard right and so you have variance errors but a random forest because it's an ensemble of lots of trees it reduces your overfitting problem right because it's a voting machine different trees are built or looking at different aspects of the data it is very hard to overfit because each tree gets to see only a small window of reality. So even if it overfits, it will overfit to a very small window of reality, never to the whole reality. So random forests, therefore, are much more robust against overfitting. And it gives you a clue that the reason the decision tree failed is because it was against overfitting and it gives you a clue that the reason the decision tree failed is because it was an overfitting problem and that showed up right look at the tree it built when you look at a tree this complicated what do you conclude from this very very complicated tree which you which is practically impossible to read what you conclude is that it has gone in over fit to the data yes that is it see complexity unnecessary complexity we are able to explain this data in one line distance from the center of the river but this decision tree needs such a complicated explanation to explain it right so that's a classic illustration of overfitting. Then Random Forest certainly seems to do better. But even if you look carefully, you'll notice that the ROC curve of this is only 95%. Remember, hand created, I mean, your feature engineering gave you 96%. So Random forest does come close and you can tune it and try to tune it and so on and so forth right so um now what you can do is uh you can try to take one tree from the random forest right i just took the first tree from the random forest inside and said show me how does it look and you realize that each of the trees still looks pretty complicated. Isn't it guys? But because you are doing an ensemble over this comp, all these different trees, your answers are better. Now, one of the things that you get in decision trees and random forest is a overall very rough measure of how much did each of the features matter in helping you decide about make a decision whether something is river or not is a river point or not it is called feature importance how important is some feature just to give you an intuition, let's say that you are selling ice cream on a beach, right? Then you may be, temperature is an important feature. Wind is an important feature. Day of the week is an important feature. But suppose in the data somebody has given you the number of penguins in Antarctica who jumped into the sea that day. Just because you have that as a column or as a feature in your data, it doesn't mean that it's a very relevant feature. So in feature importance, its importance would be close to zero and the most significant features would trump. For example, temperature would trump and so on and so forth. So that's about feature importance in the same way if you talk about classifier we are talking about deciding whether something is a cow or duck you would all agree that size would be a pretty important factor isn't it but when you are looking at the at the at the cow or the duck right think? Think something really irrelevant. Let's just say that whether this, if they have stripes on their body or some sort of a straights or color lines on their body, whether the color lines are horizontal and vertical, perhaps doesn't matter. You can take some other irrelevant feature, like how much mud is there on their feet, as far as you can make out. Probably may not matter, because both of them may or may not go through mud. So that is the value of feature importance in determining the answer in predictive models. Now, one of the things I gave you guys is the flag dataset. And I give you a hint that flag is just a small twist on the river data set. So how many of you saw the flag data set? I did. Yeah, that is you did. And anyone else who did? You did. So I'll give, I'll walk you through actually, it was just a small puzzle. See, when you look at the flag data set, you visualize and it looks like this. Does it look remarkably similar to the river data set, guys? Yes. Yeah, it looks very much like the river data set. So now what do I need to do? Except that if you're on one one side of the river you are this greenish color if you're on the other side you are the yellow color isn't it so when we compute the distance from the center of the river the rest of the feature extraction is exactly the same but the only distance is do you notice that you don't take the absolute value this is the only one line change in the river this one was absolute here it is just as it is distance from the center positive distance will make it green line negative distance will make it yellow line that's all and when you do that you again do your analysis exactly the same analysis the same feature building of logistic regression you notice except that now it's a tri-valued thing and here is your confusion matrix looks pretty good i would say the the principal diagonal is heavy and red the off diagonals are pretty light right and the number of mistakes you are making are very very low and if you look at that classification report you have a close to 89 percent accuracy fairly impressive accuracy right so with feature engineering just this is it once you develop the art of feature engineering right you can use the intuition you develop in one data set quite often into other data sets and obviously here I engineered it to be such that you could use the intuition you develop in one data set quite often into other data sets. Obviously, here I engineered it to be such that you could use the intuition from one to the other. Let's look at the ROC curve. Would you guys agree that the ROC curve here is pretty good? Yeah. Right. In fact, it is 94%, 98%, and 99% ROC. I would consider those pretty impressive ROCs here. And then this is the beauty of feature engineering by hand. I mean, when you feature engineer, rarely do more blackbox algorithms come as close to it. But here we go. It depends on the context also. Sometimes it's hard. The reason you use blackbox algorithms is you can't feature engineer. You don't have the intuition. You can't visualize it. so you let them do the feature engineering for you so here we go the number of mistakes how often the river points are sometimes confused as green sometimes as red right and so on and so forth one and two but there's a little bit of a mistake here a little bit of a mistake of a mistake here, little bit of a mistake here. No. Suppose I try the decision tree classifier on flag. The code remains exactly the same, no change except that I'm feeding in the river data set here. When you look at this, what is a confusion matrix is here looks pretty good you you give the accuracy do you know the accuracy is only 86 percent guys it is significantly less than your hand engineered feature yeah feature engineering exercise isn't it and when you do the classification that's your classification report and this is the visualization of it once again when you look at the decision boundary I mean the ROC curve you're 85% 91 93 good but no way close to the 98 and 99 person kind of numbers you were seeing with feature engineering these are the number of mistakes and I'll leave the last part doing it with random forest as an exercise for you guys. From here onwards, do you think you can complete the rest of the exercise, guys, doing it with random forest? Yeah. Yeah. So, fill in the gaps. I've deliberately left some things unfinished. I'll be posting the solution to Slack, right? And obviously, please don't post it on the public web. This is otherwise I wouldn't be able to teach the next class they would say already all the solutions are there. They wouldn't even try they'll just take the solution and just say, Oh, I'm done. Shouldn't be like that. So alright, this is it. Now. Any questions guys, I sort of went through this. This is the way ideally you should create your notebooks. So by the way, Dennis, what method did you use to find? Was your method for the flag of what I used or was it some different method? Oh, actually, Cindy's our code on slack yeah no no but could you explain what approach you use was it the same or slightly different Oh for the flag one I first classified no I changed the label for these are one were two into a consistent label like saying like I changed all the twos to one word changed all the ones to two oh and then just did um absolute value um okay classifications and like a set of a parameter if you're lower sense of midlines and you're you know either one or two you know, either one or two. And then that way it was actually biased against, like, two's apart. It's biased towards the direction of, like, which way you initialize it as. So I did it twice and stacked the predictions together and then got like a 90% accuracy. Interesting. Okay, I'm going to read your code so guys some of you have sent me your notebooks I apologize I've been rather busy now fourth of July is coming and I'll be reading it over that and I'll give you guys, one quick question on when you do random forest on river here, how many subset of features, one like there are only two features here. Did you? Yeah, that's right. So what happens is that in a situation like this, it will pick up one feature at a time. It will just build a tree of one feature and uh uh same question with a different uh part so when we select like thousand then uh thousand uh decision trees for the random forest uh method how those trees are different like when we have only two features oh yeah the reason is that even if 500 trees pick one feature another 500 trees pick one feature another 500 trees pick the other feature remember they are being given different subsets of the data bootstrapping is happening right okay they're getting different parts of the data that's where it is so it was a big problem it was there just to teach you guys that the importance of feature engineering it was just a didactic data set i created to bring guys the importance of feature engineering. It was just a didactic data set I created to bring out the importance of that. How do you prune the tree for decision tree and code? Yes. So the pruning of the tree is built in here. Let's talk about it. The next example, which will be more complex next time, next week, let's come to that. What you do is you force it to limit the depth. You say max depth is equal to, let's say, four. Actually, in this problem itself, I'll do that. So you'll see it. You'll see it. All right. Just give me a minute, and you'll see this happen. Oh, what did I do? Where did i suddenly come here okay guys uh at this moment are we done with the river and flag is it looking simple like are you are you feeling more comfortable with the solution guys guys i'm getting a lot of uh I'm getting a lot of sound in the background. Yeah. Okay. So any feedback guys, recording on pause for the time being. Pause. All right. So I'll go from once again, quickly. so we're going to do the California data set Much of it is familiar territory. The one new thing that I introduced is how to style Table any table these are CSS properties beautiful thing is that the funders Data frame has an integration with CSS times DataFrame has an integration with CSS styles. You can style the data set and it looks different. It looks pretty, I suppose, than the vanilla version without any styling. Well, my styling is not particularly pretty, but if you're really good at CSS, you can make this table, I suppose, even prettier. Yeah, although it all the cool colors. Yes, you can do it with all sorts of cool colors i put salmon here but you can do uh some different colors light blue or something yes so it's not a jupiter specific command right as if i mean uh is this like a html table that the python is giving out with that command exactly so it is emitting out html table okay see one thing is there jupiter and colab colab is based on jupiter are pretty much the d factor standards in data science right if you do the the one thing that you may graduate to is pycharm right pycharm has a support for notebooks, this style of notebooks, the computer style notebooks. So all of them will respect this, this HTML output and so forth. They will all respect it. So now moving forward, one of the things you do is, this is a new thing I'm introducing you to. See, when you get data guys, you don't know that data is clean you should assume that real-life data is not clean this is a real-life data so a little bit of a background on the California housing data see in California one of the perpetual obsessions especially in Silicon Valley's are the house prices going up or down? Because most Americans and certainly most people in California, their biggest asset or their principal value or retirement money is often stored in the house. it is the house itself so as the market goes up and down the individual people they see their fortunes go up and down right and people worry if the house price is crashing people get very excited if the house prices are going up unless you're on the fence wanting to buy a house if you want to buy a house of course you love a crash and but the moment you buy a house you want the house prices to boom so a real data set from the 1990s i think 1990 uh a data set of california you see how the house prices were then now 1990 is just 2025 i think it's 1990 or 1995, I forget. This data was gathered. And what looked like an expensive house then will look like, I mean, for the same money today, you could probably buy yourself a shed or something like that, California houses. The prices have really gone up. So this is not a time series data, temporal flow, it is very specifically a snapshot in time data for california what were the house prices then this data set is available in kaggle i must have taken it i'm guessing i took it from here it is a data set that is published actually and it is also mentioned in this book that i uh recommended you all to read in this book that I recommended you all to read so it is the same in your textbook the Python one this is discussed so you can go back and read a chapter there and see and actually I have not for embarrassed to say that I haven't really looked at how he has done perhaps he has done some things better you can go and tell me about it so well this is a command what it does is it finds if data is missing in some of the rows and columns it is real life whenever you gather practical data you will find that find that some rows will not have all the values present. They will be missing values. It's the nature of it. So suppose you're taking the weather, your temperature, pressure, whatever it is, wind, and let's say that the guy is doing it manually, just forget modern instruments for a moment. So you can easily imagine that if it is too rainy, the guy won't go out and take all the measurements. He'll just run back or some measurements may be wrong. Somebody will delete it and so forth. So things like that. Sometimes you don't have all the data for all the rows, all the feature values for all the rows. So you need to inspect that first. Unlike the river data set of flag data set which were didactic data and so there were no missing values. In real life data you will get noise, you will get imperfections. So you need to acknowledge those imperfections and study it. First thing is just as an exploratory data analysis. Let's look up here. What are the columns we have longitude latitude, it gives you the coordinates of a house. Asim Kadavig, Dean of Students & Career Services at Boston University of Georgia. Actually, it's not at the level of a house, but it's at the level of a city block, but it gives you the coordinates of a city block house, excuse me, median age. As the name suggests, it gives you the median age of the houses in that block. So some house may be very recent, some may be older, but generally in a block in US, you know, most of us live in row homes, right? Cookie cutter homes. So quite often, almost the entire neighborhood sort of comes off the ground at about the same time. So median house price, median age is a measure of what the middle age is. Sometimes houses have a huge variation, new and old. Total number of rooms in the house again is a pretty good measure, it's worth looking at. Now it looks very odd, the total number of rooms is minimum is two and maximum seems to be a huge number. Why is this possible? Because it's not the number of rooms in a house, it's the number of rooms in a block. And if the block has a lot of apartments, high-rise apartments and so forth, tenement housing, you may have a lot of apartments high-rise apartments and so forth tenement housing you may have a lot of rooms this is the total number of bedrooms available typically number of bedrooms is a pretty good estimate of the number of people living there right because you see the number of households is also too many. That's right. Yeah. That's how you configure them. Right. Households. And yeah, so you can see that and then you have the households, population, median income, like you know, what is the median income of people living in that block. And the target variable is we want to use all this data to predict the value of a house. What would be the value the selling price of them or the estimated value of the house median value in the whole block city block. This ocean proximity is important. As you guys know, in California, it is pretty expensive to live on the beach. California it is pretty expensive to live on the beach we have a lot of coastline but the coastlines tend to be more expensive than in life inland and so ocean proximity matters if you live on the island for example Catalina Island or somewhere it could be quite quite expensive it's worth looking at so those are the features just looking at this value can you tell which of them is a categorical right so next thing you do you look at the missing values in missing values if you see a white horizontal line it means data is missing so which of these features has some rows in which there is missing data guys for bedroom bedrooms is one and if there isn't something else it's barely visible how many rows of data there are and so on and so forth it's a measure then so it turns out that there is a total bedroom and total bedrooms that's the only thing where any data is missing right and if you have total bedrooms missing then sometimes it affects all of these that's about it doesn't mean much now i taught you guys about this thing what was it about profile the pandas profiling so you can use the pandas profiling good thing with pandas profiling is some of the things that i did by hand for example missing value analysis in an abbreviated form it does it so when you look through this pandasarnes profiling is pretty good. It established that most of these are real valued, but it was smart enough to indicate that ocean proximity is categorical and there are five values. What are these? You are less than an hour from the ocean. You're inland. You're near the ocean, near the bay, or you're in an island. So there are five homes or five roads or blocks that seem to be on the island. So which is the island in California? Alcatraz. Alcatraz is one certainly. Yes. But I don't think anyone, I don't know if anyone lives there. Catalina? Catalina, exactly. So then you can look at the interaction, for example, how does longitude and median income, latitude and median income vary, right? Clearly you notice that at, well, latitude. Asif, can you please quickly recollect interaction is that collinearity no it just is showing you some sort of a correlation between correlation on the okay that's correlation because there is correlation below so I'm that's right so from this it may not be very obvious but you can see that San Francisco Bay Area is filled with people with a certain income value and then there are people in Los Angeles who have that income if you go longitude man you see that also it's a little harder to see longitude wise but at certain longitudes are the most hang on median income medium income here, longitude and medium income. Again, you see this value that day area. So the two big income areas are Bay Area and Los Angeles. You can sort of see that. This is the correlation. You can see correlations are there. Very prettily it draws out the correlations. Right. And if you want to understand what these different correlations are you remember that i taught you guys pearson correlation all you have to do is toggle this and it will explain to you right what these correlations are kindle towering and so forth if you want me to explain it i'll be happy anytime to do it on saturday ask me but today i want to move a little bit faster so you can see that it gives you some sort of a missing value analysis pretty much it says that none of the rows are missing except for well why is did it not show total bedroom see there well this does not look right okay it gives you a few sample rows last row this is it then let us so now I want to introduce you to some techniques one of the things is when you get geospatial data it's always a pleasure to visualize the data right and to visualize the data is actually very easy do you notice what do I do this let's go through this code I got the minimum and maximum longitude of the latitude of the the longitudes minimum and maximum latitudes using this what can I do I can find the center of the map isn't it yeah yeah it does there's this light these three lines make sense it's very simple all I'm trying to do is the center So what I will do now is I will create a map and folium is an excellent library To create maps you see in one line. I'm creating a map. I'm just giving it the central location Longitude latitude and I'm giving it the zoom level You can start out with the zoom level, of course you can zoom out. So if you notice that with my mouse, I'm zooming out. This is interactive map, these are live maps. It's a pleasure to actually be able to create it. You realize that with just a line of code, we are able to create it. One line, right? And on that map, which will be an empty map. Now I need to project our points. Each of these points are projected it. Our data points are projected it. Longitude. That color has to be like in hex code? No, you can put whatever color you want. You. You can put red, blue, green, whatever. I must have been fooling around with something. I like this color. These days with coronavirus, this color is a dangerous color, right? Things are not going well. Anyway, so this is it guys. Do you see how nice it is, how quickly you can create maps in in in data. So you know, this is these are all the tools of data science people use. It is good to know how to make maps if you have geospatial data. But you don't need to make maps, you can just draw a simple scatterplot if you just did a scatterplot of the data you would realize that you would get exactly the same thing as if I just made the scatterplot of points and what let's look at the scatterplot I'm making a scatterplot X and Y are the latitude longitude I'm coloring it based on the median house value which means that if the value of the house is high it will be more red if the value of the house is low it will be yellow so yellow orange red color maps again let me remind you what color maps are whenever you create scatter plots or plots in the code, you have to be careful of multiple aspects. One is the aesthetics. Together they should look pretty. They shouldn't be jarring to the eye. The second is disability. A lot of people are color blind. So people have sat down and they have created this colour map of good colour, the colour, you know, the colour ranges, which fulfill both the needs. Contrast. There are enough contrasts, they have the colour blindness sensitive, at the same time they give you the aesthetics and the beauty. So this is nothing but a scatter map, scatter plot. And yet, and then the size of each point is how much population there is and the color is how expensive the house is so if you look at this map can you see guys that places so deep red is expensive places they also tend to be high population centers isn't it which makes sense cities are densely populated and the median how home values is very high there is it making sense days we all know about homes we all know cities so it makes sense can you tell very San Francisco in ways where is Los Angeles in minus 122 yeah that's right you can literally see see this is not a map this is just a scatter plot of data and yet you can see a lot here you can see the san francisco area you can see a little bit of the santa barbara area you can see the los angeles area here and the san diego area not only that you can also see interstate 5 run through it do you see it guys effectively this line here those of you who have been now let's correlate it to the map and see whether we are right so look here guys do you see this yeah right all the cities are along this. And actually it's not, I don't know if it is interstate five. I think it is. Yeah. But yeah, there you go. I think there's also a California route that goes through this, that may be more through the cities. I think five tries to avoid the cities i think five tries to avoid the cities you can overlay the map and the scatter plot right yeah yeah you can do that and you can do you can do so anyway i give you some at this moment i was just trying to give simple code so you see this is your standard scatter plot i've just prettified it a little bit yeah see on the right hand side that 100,000, 200,000, that is basically your C. Dr. Raghuram G. Median house value. C, right? Okay. Dr. Raghuram G. Yeah. So in those days, in 1990 perhaps, half a million used to be the cost of houses, very, very expensive homes in Los Angeles and San Francisco. Today, what do you think 500,000 will buy you? You have to go to Tracy. Yes, that's right. And then you need to literally add a zero there. It's close to 5 million in those places. I was just looking at the house prices in Atherton. I think they all are six, seven million and so forth. So, by the way, any rich participant here who lives in Atherton or something like that? So, all right, so I put this side by side now, huh guys? You can literally see the scatter plot in this map they look so close to each other isn't it now the other one thing I realized that the ocean proximity is a categorical variable so I'm explicitly changing it to categorical picture of a string it's just a good practice to do that. Any surprises so far, guys? Anything that needs explanation? Does this statement look very easy? These two things. By now, is it looking very straightforward and easy, guys? It is a categorical, so I'm making it a categorical. Yeah. Yeah. Then this is the describe. By now now you must be familiar with it we are describing it then what do i do i i draw histograms of each of the variables when i draw the histograms can i just shrink the font size a little bit so i want to see the histogram in one single shot yeah so what is the histogram in histogram you can choose the number of bins what does alpha 0.4 mean guys who would like to enlighten me what does the alpha stand for is it like how transparent it is exactly the transparency transparent it is. Transparency. Transparency. How saturated it is. Alpha one means completely saturated opaque. Alpha of zero means completely transparent, invisible. So 0.4. Why did I do that by the way guys? Why did I not have it? Why did I, if I don't put alpha, what will happen? Yeah, these will be very saturated colors it adds aesthetics then now why did I do this X rotation is 90 degrees why did I do that? Can you tell me what it is doing? X-axis is, you know, like the numbers are rotated 90 degrees so you can read it well. Yeah. If you don't do that, then it will overlap. Yeah, otherwise never. Exactly. And figure size 2020 means I'm giving it a rectangular shape. Just enough big. You can decide what your figure size is 2020 here was good maybe i could have made it a little bit bigger now the pairs plot is interesting this is again uh those of you who did ml100 with me now what pair plots it It is nothing but the scatter plot of two variables taken at a time. So the way to read it is a little more complicated. Let's take this. What is the relationship of longitude to population? Do you notice that there are two humps here? Can you explain why there are two humps? Why is this peaked at two places? San Francisco and California and San Francisco. Exactly. San Francisco and the area and the LA area. So the bimodal. Bimodal distribution. California is bimodal in most of the things. So it's quite interesting that most people of California live in these two cities and the rest of California is empty. People often say California is very expensive, very expensive. Actually, if you go away from these two population centers, California home prices can be quite, quite reasonable. Yeah. Yes. So in case you have an urge to run away to the middle of nowhere, maybe in the middle of Arizona or something, remember you don't need to run that far. There might be people might do that if this trend continues like this, people might even do that. Yes. So you can see once again near the sea, how often are people, this is a box, this is a color category plot. Given latitudes, how much of values there are. Once again, you see that there are people sort of distributed all along inland at certain inland people are at all sorts of latitudes near the ocean people are at they're closer to this latitude the 35 degree latitude which is closer most likely to San Francisco area I mean the Los Angeles area. Near the bay is of course all San Francisco population and so forth. So these are the plots guys I'm introducing you to in case you had not introduced to that. What I would say is as you study this notebook, and those of you who did ML100 of course, have gone through this exercise that are more leisurely based but those of you who are joining ml200 directly these are the landmark or that's stereotypical or ubiquitous plots that you make in exploratory data analysis so become familiar with the syntax in the beginning it will all feel rather magical that you're invoking some magic mantra or some obscure shlokas and something is happening. But gradually this will all become very real. It will begin to look very easy after some time, because just standard Python code. If you want, I can explain any one of these code to you. Python code. If you want, I can explain any one of these code to you. Like for example, here, do you agree that this one line is self explanatory? It is saying go make histogram, do bins of 50. What is the bins is equal to 50 mean? 50 buckets. Exactly, 50. So when you make histogram of a numerical variable, you have to put values into bins because you're counting how many values fall in each bin. You take 50 bins, alpha is this we already talked about, figure size we talked about. So there's no magic to any one of these. Likewise, the pair plot. This is a bit more going on here, but still it's four line. I'll explain it to you. You make pair plots between this, alpha size is 80 80 all of those are self-explanatory then you say that along the diagonal make the histograms right do you see so these histograms are the same as the histograms here then the next thing it says is that what is that now what is the color in the histogram that I give you is the ocean proximity how far you are from the ocean that's the coloration then in the bottom upper diagonal on the upper side of the diagonal the pair should be scatterplot and in the lower part there should be something called a kernel density estimators. Those are sort of, think of it as some, at this moment, KDs will deal with them much better, but think of them as, these are well probability density sort of thing, but think of them as some sort of a heat map at this moment. A very rough idea would be heat map, but we'll talk about this and learn about this in more detail when we do the workshop, the boot camps. Are those numbers, like a table of numbers? In between? No, no, no. These are just the legends. This picture is very dense. See the thing is, we made this picture of size, this size. When you have so many variables, you want to blow up the size of the picture so that everything is well separated and you can see everything. So you would make this picture to be more like 100 by 100 and put it on the wall. Then all the things will show up properly. But it also shows what happens when you have too many variables that you have to, you cannot put everything in a single visualization. This is already beginning to push the limits, isn't it? Imagine that there were 30 variables, but then the pair plot would be horrendous. So what you do is you make it in pieces or you pick a few important ones and you make the pair plots. And there is nothing like you have to see all of them in one place, right? I mean, you could when you're analyzing going deeper, you can always go one by one, right? It's okay. And then make scatter plots. Absolutely. Yeah, you can do that. And now this is the this is these are called beautiful the if they look pretty these are called violent plots they literally give you the box plots but violence a little bit prettier than box plots they give you counts like median household age how many homes and this is why remember that ocean proximity is the categorical right so based on the color so there's value so this household median age you can pretty much see that the median age in this is the ocean Catalina is pretty old homes are there you don't have new homes most likely You don't have new homes. Most likely they don't allow new homes to be built. It's an island. There must have very strict regulations. So this is it. And the same data, I just show you that you can do it as a regular box plot. See with all of these things, and when you present data guys, people underestimate the value of clean visualizations. Visualizations, if you clean it up, it sort of creates a bit of aesthetics. I haven't done much, you can do much better than this, but for what it is, now all of these things that I'm doing box plot, category plot, violin plot, none of them are hard. These are all features there. You have to just go use it. You have to know that those things exist and you have to go use it. Account plot, how many homes are there? So most homes it turns out are less than one hour from the ocean in California, which makes sense. California is a long and narrow state. Most homes are inland. There are some, this is probably the Central Valley, maybe there are, but not that many. Then near the bay, near the ocean, this is it. Now correlation, you realize that any two variables could be correlated. It shows you the degree of correlation now we already saw that in profiling in the pandas profile so i won't go more into that this is the oh by the way here i wanted to show you something quite often you know when you write documents you want to write it in latex people often ask how do i put the output into uh into a research paper that i'm writing something so it's very easy any data frame in pandas all you have to do is do to latex now those of you who know what latex is would find this magic mantra very easy to read if you don't know latex don't worry about it latex is a language in which in the site in the mathematical communities of computer science and math and physics etc most papers are published so this is the syntax for that now the same correlations we show it as a heat map here we're looking what the high correlated are so i'll let you ponder over it age is median age is highly correlated negatively correlated with uh let us say with many things a total number of rooms does that make sense guys median age is negatively correlated with total number of rooms and total number of bedrooms what does it say new houses are smaller no the opposite the bigger the house the small the older the house the smaller the lesser the number of rooms yeah yeah actually if you go to the older house you will see the bedrooms were much larger right so the number of bedrooms were small as a result yeah they yeah but these days if you go to the houses you will see bedrooms are very very i mean they like to because the you know the price is based on number of bedrooms right so they increase the number of bedrooms the number of bedrooms and tiny bedrooms okay guys so that was it so now let's come to regression I thought I taught you guys decision tree and the random forest let's see if we can do that a very simple if you read the river data set you'll find this easy to understand so we need to do some data pre-processing first pre-processing we need to do is that we need to do one hot encoding of the data. What does that mean? One of the predictors was ocean proximity. It's a categorical variable, right? So we need to convert it into a numerical variable. And the way you do that is you take all its values. So, for example, are you within one hour of the ocean? Yes, no. Are you within proximity? Are you inland? Yes, no. Right? within one hour of the ocean yes no are you within proximity are you within are you inland yes no right are you you know so on and so forth are you on an island yes no are you near the bay yes no are you near the ocean yes no so what have you done you burst out you have burst out all of these into This Different columns but different features this is called one hot encoding So you can do one hot encoding of the data. Once you do that The other thing that I'm doing is I'm dropping there In all the rows that don't exist is that a good practice actually you have the luxury to delete or drop rows that are defective only when you have a lot of data here we have a lot of data 20 000 rows are there like 20 plus thousand rows are there but if the data is sparse you don't do that. You do something called imputation. You try to fill in those values and keep those rows and use them anyway. It's like when your car has a nail in the tire, there are only two things you can do. If you're rich, you go and change the tire immediately. Well, if you're like you and me, then I suppose you'll want to take the nail out unless the tire is old you would like to have it patched right so it's like that each of your row of data you would like to patch it up by imputing some value filling in those gaps with some value the imputation is the topic usually these little things or practices we do in the bootcamp, you know, so, but here you just delete it gradually as we make progress and teach you guys. Then we imputed now notice one thing, guys. This data. Do you see skew in the data. Is the data symmetrical, you see a lot of skew in the data guys. Skewed. Yeah, the data has a lot of skew there. One more thing you notice that this particular thing, household median age 50, right, 50 plus. At 50, they have sort of clubbed all data that are 50 or plus into 50 plus category bin. So you have to watch out. It will screw your analysis and you'll see how it screws your analysis after a little bit. I leave this as an exercise for you to deal with this part. One way you can do it is not take the data 50 plus, take it out and so forth, but we'll deal with it this data is skewed is it right skewed or left skewed remember the trick guys if you think of it as the beak of a duck right you're right scale and the beak is looking this way right then duck is looking this way so it is right skewed all of these are right speed some of them are very strongly right skewed. Right? And median house value, the target variable also seems to be right skewed. What can you do about it guys? How can I take care of this skew? Well, you could do the whole power transform and find the right transformation to normalize it. But a cheap way is just take the log of these this one and this one and this one in this one whatever you think needs a thing you can do as a pre-processing you can take a log if you take a log it's a trick and again I'm talking about things that at one point I've taught in depth to you guys but suppose I take the log of it that is the log transform by the way this whole thing most people don't pay attention to and you will see what happens if you don't do this your R squared value will be about six to ten percent less right but if you are careful and you do this, and I invite you to do this guys, comment out this code, run the analysis, see what your model metrics are, performances, and then do this and see your model performance. It is quite interesting actually that most of the notebooks that I saw of California dataset, which have been posted in Kaggle and other places, they did not do this transformations. If you're using, it is always good to do that feature extraction or data preparation carefully. When you do it, you get the extra bit of accuracy, the extra bit of performance, not actually, extra bit of performance you will get so now when you do the transformation do these things look much more normal guys are they better right this cue is gone right so now that it is gone what have I done so far I have massage a pre process the data the data pre-processing after the data pre-processing. After the data pre-processing, then I go and, so what were the things that I did in the pre-processing? I did, let us summarize what all things I did. I mentioned it here, convert the categorical to one hot encoding, drop the missing values, extract the feature space and the target XY, three things then finally I also do the log transform because I look at that so this is the value of drawing those histograms where they're not just one thing you do as a ritual you use it to make some decisions about data and once you do that you now take this and use, does this look totally easy to understand guys? What am I doing? And then I'm going to try a slew of models. First I'll do a simple linear regression. When you do a simple linear regression, do you remember these lines of code guys? This is straight from your river dataset. You just see the linear regression fitted. But now you notice that things are beginning to fall into a repetitive pattern here. It's more about thinking. The code begins to look very easy and familiar. It's exactly the same code. When I run this code, I get a mean squared error, which is fairly low actually. And I get a coefficient of determination, which is 69%. 69% R squared for this data is actually, I would consider fairly impressive. If you had removed the outliers, it would become even better. I leave that as an exercise for you. So one basic thing is that outliers, you know, there's not enough data, not enough information to make a judgment call for them. So you could do one thing. You can remove the outliers and say, I'll make a separate model for outliers, something else, and let me make a better model for the inliners, inline data, data that is not outliers. Let me leave that as an exercise for you guys. So, Asim, where do I see that outlier information in the graph? Oh, it is all here. See, if I look at this graph here, look at the histogram. Do you notice that all these outliers are there in the box in the whiskers plot right so anything beyond the whiskers do you see the this is the box these are the whiskers anything beyond the whiskers are outliers so latitude has outliers well that's not harmful but a total number of rooms total number of rooms has huge outliers right do you see that the main box plot is here and then there is a long tail of outliers total number of rooms total number of bedrooms population they have outliers their households have outliers and let us also go in and income has outliers right so let us go and let us go and verify whether that shows up in the original plot do you notice that this tail soft but it goes to 6000 why does this box plot why does this histogram go all the way to 6000 we can't see it but there must be some one or two outlier values here isn't it in all of these there are some outlier values sitting here like for example total number of rooms 15 right hang on households let's look at the household right in some city block their massive number of households in certain median income some people seem to have a fairly high median income even after you know pretty high median income I'm assuming that this is in thousands or something like that but whatever it is look at the population it goes all the way up to here some blocks are really heavily populated so the data is filled with outliers. Do you see that? Guys, do you see that? It is a clue. Like you ask yourself, why is this histogram going all the way till here? It means that there is data that I don't see. So if you chop off the outliers, guys, your analysis will become better. And I leave that as an exercise for you it's also one of the things people forget to do actually in fact it was just in the break i was looking at the california housing and i was looking at some of the notebooks and what people have done even the more celebrated notebooks uh sorry, the kernels that people have done. By the way, you can learn, I highly encourage you to go and read some of the highly voted kernels that people have created and see what things they did that you missed because you can learn from it. At the same time, you can take pride that you did something more carefully, that a lot of them missed. So, you know, this is the way for you to get a reference of how good you are. These people certainly think they have done a great job, which is why they have posted their notebooks here. Right? So, go check it out, guys. You'll be pleasantly surprised that by the time this workshop ends, you would realize that your notebooks look better than most of the notebooks there at kegel anil do you agree uh yes yeah begins to get better and so forth all right so we are making a linear regression model and we achieve an r square of 64 6 about 70 percent or 69 points let's say give or take at this moment guys remember what did i teach you in ml100 it is not enough to know that you have you're getting a decent r squared or a decent mean squared error what you need to do is what you need to be able to do a residual analysis and see all is well so you begin to see that this is your residual analysis when you look at it first thing you notice on the in the on this side do the errors have a bell shape that do the residuals have a bell shape yes yes yes yeah here it all there Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. the fields this way this is coming from you can ignore that but it is mostly homos uh there is mostly a sense of homoscedasticity except for these outliers are screwing it up they seem to be there more prominent in the lower predicted values than in the high predicted values so if you remove the outliers it would get better so it is not perfect but it is pretty good after all we are using a linear model for a very complex data set what is the relationship between prediction and reality guys this is the relationship between prediction and reality does it make sense is it a positive or negative correlation Is it a positive or negative correlation? Positively correlated, isn't it? Why prediction and reality are positively correlated. They are along the principle diagonal. So you can see the deviation. See a perfection would be if they were along this a gray dotted line and they are along this dotted line. Well, not quite perfection perfection but pretty good right if you're searching for points that can be of high leverage high influence it gives you a sense of high infant points nothing pathological all looks good now this was your linear model guys do you notice that just a good application of a linear model took you this far by the way if you had not done all of those log transformations etc your numbers would not be so good. They would be much worse in the linear model. Try that out. So 69.1. Now you expect, you ask yourself, if I had regularized the data, would I get a better answer? It turns out regularization does what it suppresses overfitting i haven't uh obviously i apologize for those of you have just joined uh overfitting means you know the data has high variance overfitting there is a method to suppress it and you suppress it by different degrees of suppression and see which one is the best and then you whichever is the best you use it ah it has gone to 69.12 well 69.11 versus 69.12 pretty much the same if you look at the coefficients of the model let's look at the first three coefficients uh 0.165.164 do they sort of match the previous one? .165, 164, then 003 and 19, minus 19. So, 003 minus, so you realize that the coefficients are almost all the same. It means that the data does not have, your linear model does not have overfitting, right? And it sort of makes sense. Linear models tend to have underfitting problems, not generally overfitting. So well, that is what it is. If I had made it a polynomial model, so I leave this as exercise guys, see somewhere I taught you polynomial regression. See if you can extend this and make it a polynomial model or be ready that it will be a big computation, because even if you go to quadratic form 11 square is Shubham Tulsiani, Close to 110 combinations, right. So you will have an explosion in the number of variables that will show up. Try it out. See whether it gives you anything. But when you do a polynomial, you must use ridge regression or LASA, whatever it is, to suppress that. Better is actually use elastic net. It's a combination of both. Can I leave that as an exercise, a homework for you guys to try? Let's do that. Then we talk about decision trees now because we learn decision trees the week previous weekend random forest this week let's look at the code for decision trees guys I trust by now this code looks obvious I think I build a regressor I give it the data to fit and then I use the data to predict the labels. So far so good guys. Yeah. Right. These are your predictions. Labels are your predictions. So people use Y hat or labels, whatever you like. Labels is more typically for classifiers. I tend to use Y hat. I don't know here. I wasn't being too careful so i just used the word tables any variable so there we go and what is the r square i get from a decision tree 67.72 is it better or worse than linear regression worse worse actually and look at the complexity of the model this tree built so it is uh illustration of what principle something about lunch that I keep talking about. Anybody can remind, what do I say? Freedom of free lunch. There are no free lunches. NFL, right? Exactly. It's an NFL theorem. No one algorithm is in quotes powerful, more powerful. Every algorithm has its place under the sun. Sometimes one works better, more powerful. Every algorithm has a space under the sun. Sometimes one works better, sometimes others. So don't let anybody tell you that decision trees are more powerful than linear regression or something. Linear regression is too basic. It was a point worth emphasizing so I'm mentioning it to you. You can visualize this, by the way, this is hopeless. The built-in visualization from Python, this Kikit Learn is not terribly illustrative. For visualization of graphs and trees, graph is the best. If you use that that becomes Okay. Oh, I think I removed the old directory. Okay. We won't worry about it. There was a PDF version, a prettier version that I could show up this. I didn't want to run this now. It will take a bit of time. This is our, if I use yellow brick, the same thing. Do I now see a pattern? Does this look much worse than the linear regression plot guys? It does, right? It doesn't look as nice as this. It is actually a little bit worse. But good news is that I still have a more or less normal distribution of errors. Here, you can see that this line is a little bit more off the diagonal than the previous one so let's try a luck with an ensemble method now remember I told you that ensemble methods can be more powerful let's try random forest the forest is just made up of trees again thousand trees 20 will parallelization is 20 it executed in in 5.79 seconds 20 is something you guys can probably run on your laptops if you have multi-core i typically run it with 100 doesn't matter so here we go you go and do the predict again why not this code guys i hope it's beginning to look very easy do you notice that between the decision tree regressor or linear regressor, all I had to do is change this word, the algorithm name. It has become random forest regression. I apply it, get the labels, get the predictions. And what do I get? I get an R square of 83.6. Is it better than linear or worse than linear? Better than both. Better than than both isn't it that was 69 and about 70 percent and this is close to 84 percent so for this problem random forest is certainly the the best of the three algorithms that we have tried isn't it guys this is what people tend to miss they tend not to do the residual part but if you look at it isn't the residual also looking much better now yeah yeah there's far more pattern plasticity the the residuals do have a normal shape normal Gaussian distribution much better so here obviously the random forest is the winner remember that in the river data set the careful hand generated a feature extractions were the winner here there are too many variables and it's harder to do feature extraction but there are feature extractions you can think of so let me tell you and by the way one of the cattle notebooks did it brilliantly some fellow has done he said that one good feature would be or she said the one good feature would be the distance to the metropolis nearest metropolis so distance to barrier or distance to Los Angeles whichever is smaller and he made that a predictor and he came up with a beautiful visualization look up his notebook will be the classic I wish I could have found an open and showed it to you I leave this as exercise for you to find out so you can do a lot of future or further feature extractions right you can and make it out and the other thing you can do is join this data with other data for example you can make it out. And the other thing you can do is join this data with other data. For example, you can join it with educational level, add at any given latitude and longitude or city block. What is the educational level? You would realize that educational level has a huge influence on the home prices. Birds of a feather flock together, right? So you can go on creating more and more features by joining this data with other data sets. And that is where the magic is. Most of the time, if you're given data, you get more data, not by getting more rows of data, but by getting more features into your data set somehow. You manufacture features or you take data that is not there by taking you know view of labor statistics data or something like that economics data weather data something or the other and you bring it in and see how you cool rating go ahead i was just saying school rating school rating yes yes yes excellent school rating is a huge determinant yeah so we can get like the zip code and then based on the zip code we can get the school yeah exactly we can get the overall school rating in California the schools are failing so it's like almost an arms race whichever few school districts are good everybody wants to go there and the house price just skyrockets in my own case I have a illustration I have a house which is much more modern bigger nicer in Vallejo Hills and I have a house here in Fremont as you know proximity matters but also the fact that the school district in Fremont is superior means the house price is practically three times over for a much worse house, actually, much older house. So it's an illustration of that. So anyway, when you visualize, so one of the things I did is a forest is made up of trees. So you might be curious. Let me look into one of the trees. So here it forest is made up of trees. So you might be curious, let me look at into one of the trees. So here it is inside the forest, there are lots of trees they call estimator underscore. So member variables are always hidden with an underscore. Now there's a convention. The output variables have an underscore here at the end. In Skaketlan. So I just took the first tree no reason why i should take the first tree could take any one tree but do you see how complicated it looks guys do you think there is any interpretability here there is so i say what is it exactly swing, one tree means what is it exactly swinging? Oh, look at this. There is this root node, splitting into two nodes, splitting into two. Oh, okay. Oh, they have shown the trees like okay, now I get it. Okay. It is practically indecipherable. Oh, okay. So, that's how it is. Some pruning is needed yes so one of the things I introduce you to today is a very rough measure you may say all right what factors are really important what really which factor is more important than other in determining a house price this is in 1990s let's see if the results agree with our intuition feature Feature importance. Now, by the way, remember guys that this is a very rough and ready measure, right? Usually feature importance is more local phenomenon. In certain regions of the feature space, certain factors may be more important, which is why this feature importance which people tout as a big virtue of decision trees and random forest, obviously it's worth knowing and I'm explaining it to you, but don't take it with a huge grain of salt. Remember that's a very rough approximation. There are better means of feature importance and explainability and those are the Shapley values and making local approximate models and so on and so forth. We'll come to that line in the boot camp of course there's a few attended you know that we covered a whole day to that for Shapley and by the way those are non-trivial things. Shapley got the Nobel Prize in economics for one statement effectively, one big statement. How much value to attribute to a player in a team? If you think of each of these features as player in the team that is together getting a sort of a home value, high home value or whatever, then how much credit should go to each feature that is shapely value we'll talk about all of those of course in the bootcamp or maybe next time see there's a this by now you must have realized that this is a lot to digest are you guys feeling that but we had a long yeah yeah it was very overwhelming today yes so I don't want to bring in even more so i'll leave it at the feature value so at least for me i don't know about others so yeah so let's keep it to that and see does it make sense so the biggest determiner of home value seems to be the median income of people living there right well that doesn't seem to be very useful because obviously expensive homes can be afforded by rich people. Isn't it? The home like our income determines the house we buy right the banks will tell you that you can't spend more than 30% of your income as mortgage payments is the upper bound is a healthy bound. of your income as mortgage payments. It's the upper bound, it's a healthy bound. After that, you get into dangerous territory. So obviously you expect a median income to be very crucially related to the target variable, which is the home value. It determines the home you can afford. The second is, and so people of the, if a house is priced at 1 million, you can ask yourself, what is the mortgage on 1 million, right? And let us say these days interest rate is what, 4%, right? So 40,000, that comes to about $3,200 a month. $3,200 a month means your income has to be a hundred thousand or a hundred thousand plus you know 120,000 of his speaking for it to be 30 percent of you I mean 40 right 40 times 3 is 120 so your income better be 120 to 135 for you to be able to afford that million dollar house minimum isn't it do you see guys how the computation goes so this this sort of makes sense. I mean, I don't know, maybe, by the way, is my explanation right? I made a very rough and ready over the top of my head computation. Hopefully. Okay. That's that. So the second is ocean proximity inland. Are you inland or not? We know that that is true the more inland you are the less the house price would you agree that that's true for California guys broadly then the next factor seems to be latitude very true if you this what does latitude and longitude speak about whether you are in the metropolis or not Bay Area or LA area or not isn't it guys yeah location matters and anybody doing real estate will tell you so what do these things see median income is more of a chicken-and-egg situation expensive homes are afforded by people who can afford them so I would sort of discount that but if you look at these three values ocean proximity latitude longitude what are they saying it is they are speaking to the truism that any real estate agent will tell you so they have saying cliche they say that there are only three factors that matter in a house for a house price. It's location, location, location, and location. Yeah. Right? So those are all the three factors. And you see that said in so many ways out here. Oh, goodness. There is a call. So guys, I need to go attend a meeting now. So I will stop here. This is the last one. The other factors are there, so on and so forth. Median age of the house matters, population, number of rooms matters, and so on and so forth.