 ðŸŽµ So what is information gain in the context of a decision tree? We know this is just a review question to make sure you haven't forgotten from last week it is the differential between the entropy now and the if you split on a region based on the size of the region like size weight proportion rate entropy of those regions if you look at the expectation value of the let's not use complicated words is basically the entropy of this is thick. So you have information gain when the entropy of the two sub regions properly weighed is less than the entropy before you split. Otherwise, it's not worth splitting the information gain is the criteria you use and maximum you split at that point that gives you the maximum information gain. maximum, you split at that point that gives you the maximum information gain. So that was the idea. Weak learners are learners whose predictions are at par with random guesses. And that's the name weak learner. Obviously this is false. A weak learner, see you can't make an ensemble of idiots, right? Because that leads to chaos. There is a theorem, the Condorcet jury theorem says that you need people, when you're taking a jury, the jury should have the ability to be right at least 50% of the time. If it is right less than 50% of the time, then the noise will accumulate. If it is right more than 50% of the time, that is when you get a strong learner by taking an ensemble of weak learners. So that is the whole point. So you can't have a weak bunch of people who are just making clueless guesses and therefore get a get a good idea by taking an ensemble. So that is that's a point. It is not random guessing. They have to be at least right half the time. Next question is the term random in random forest is present to signify that some some trees are assigned to make, so obviously none of this, it was meant to be deliberately confusing. Each tree is given a random, so this is the most enticing one that people end up choosing. meant to be deliberately confusing. Each tree is given a random. So this is the most enticing one that people end up choosing. They think the random comes from the fact that each is given a random subsample of the data set. No, that is true. Random subsample is given. But random subsampling is called bootstrapping. Those are bootstrap samples. And if all you do is random subsamples and build trees, the algorithm would be called bagging trees, not random forest. Bagging trees existed when Ensembl came out, one of the first things people discovered is that let's go and bag, use bootstrap aggregation, bagging of the trees and we'll get good results they did get pretty good results better than individual trees but then the breakthrough that came with random forest is that you also took random sub samples of the feature space at each time you would split in a tree by way, this is a good point for me to correct a mistake I think I made. When I was talking about random forest, I said that each tree not only gets a random sub-sample of the data, a bootstrap sample of the data, it also gets a random sub-sample of the predictors. That is not true for the standard random forest. It's true for something else, but it's not true here. Here what happens is you take a tree, but when it is about to split, you have to choose between which of the axes, which of the features it will split along. You don't let it choose from all the features. You let it choose from only a small set of features, random subsample of features. That is random subspace in other words you limit its knowledge to a random subspace of the feature space that is the where the word random comes from and it is interesting to know that because people by and large confuse they say random comes from the random sampling you might see if a joke when I was first using random forest, in our team there was a new person who had come who didn't know data science and her job was to write documentation on some feature I developed. And it was rather amusing when in a documentation for the work we had done, the statement was, and then we randomly take some data and make a random prediction on it. That sentence stuck with me. It is none of that. So obviously, the person was new and gradually learned. So it almost seems like provocative to say that you made a random prediction. That sounds worse than zero R. That's right. It's sort of what I was like, oh boy, is that what this young people are thinking about my work? All right, the next one, consider a random forest where each tree is just a stump. In other words, it makes only one split, only one feature and it makes a split off. That's it. Just one feature and one split. This would be rather useless since the tree learners cannot learn anything. That was the statement. And it seems very enticing to say yes, but actually no. and it seems very enticing to say yes but actually no this was one of the rules I kept telling you about the one-hour rule that actually it is a surprising observation that in embarrassingly many situations it suffices to build a random forest of stumps and its model performance is almost as good as that of more complex models another advantage is the sheer computational speed of building such a simple random forest. So sometimes start with max features is equal to one. The observation that simple classifiers can do surprisingly well was first made by Robert Seaholt, a professor at New Zealand University, and it is called the 1R classifier. It led to quite a bit of debate. So the effectiveness of random stems is actually an extension of this observation. When 1R was discovered, people were a little bit shocked. There was a bit of debate. Is he really right? And so on and so forth. People tried to prove he's wrong. Today we know it is correct, actually. You can try it. One thing that I would invite you to do, go and so on and so forth people try to prove is wrong and so today we know it is correct actually you can try it one thing that i would invite you to do go back to your california data set set the max features to one see how much accuracy you lose it's a very quick one guys one line change in your in your california data set try that experiment and see, do you really lose a lot of accuracy? Maybe we're doing it, try that out. Given a probability P of the entropy find us this, of course, a negative of P log P. Why negative? Because log of a number between zero and one is negative and a negative of a negative will make it positive. Sir, can I ask, sir, one 1r and 0 are are they synonymous no 0r is to say forget the feature forget all information at all just look at their time and pick the majority sir okay got some example like I give you the example of a doctor quack right comes to a quack and says i'm really sick i don't know what will happen to me the quack will give him a pattern that i can say my son you'll be fine and gives him some fake medicine and sends him home why because most most of the time diseases are self-limiting and people cure themselves so that's zero all right zero r, right? That's zero R. Got it. That's that. Then, this is the matching. I hope you all got it right. Weak learners are learners that predict levels correctly at least half the time. They are not random guesses. An ensemble is a crowd of learners. You can make an ensemble of strong learners also, but generally it's better to start with weak learners. Aggregating for regression simply means taking the average of the predictions from each of the weak learner one predicts three one predicts 3.5 another predicts four you take the average and you say 3.5 is the prediction uncorrelated learners are learners whose predictions are not correlated in other words it shouldn't be that if you look at the predictions for learners, they're correlated. They shouldn't be there. Aggregating for classifiers means taking their voted, the most voted level. Most voted level need not be the majority. So suppose you're classified between a cat, dog, horse, right, and a duck, it may easily be that none of them would be the majority but one of them will get more votes than others. So that's that. For example in the elections in some multi-party democracies, the US is not really a multi-party, it's a bi-party democracy for all purposes. I believe in things like Italy and so forth, where you have multi-party democracy. I think it used to be true in India also, but I don't know if it is still true. But what would happen is a lot of candidates from different parties would stand. The winner would actually not have the majority vote. He would get a lot of votes. So then obviously coalitions would form to achieve majority. But you don't do that here. You just take whichever is the most voted. Bootstrapping is the process of taking random subsamples from a population with replacement. This needs explanation. See, the gold standard for statistical analysis is something so is something called a magic word called IIT independent and identically distributed like if you have samples they should all be taken from the same underlying ground truth and the sample should be independent. You can get those samples, that's the whole idea of bootstrapping. You try to approximate IID. You take random sub samples from a population and you replace it. So in other words, two samples may have something in common. One of the inherent advantages of a decision tree is its robustness against overfitting to the data actually quite false that is as one of its weaknesses it over fits to the data in a random forest one can get a measure of the relative performance yes true you can average over the either the Gini index again or the entropy gain from from the splits along each of the axes and you can therefore tell which is more important than the other. This is again the last function for a forest. We won't go over it, we did it last week. In the context of Baguio-Kuida examples, the individual learners are mutually independent. It must be true in fact. If the learners get correlated, not only should they be independent, if they get correlated, generally things look pretty bad after that. A decision tree is in general a more interpretable model. And by the way, random voice improves the situation of overfitting. It doesn't cure it. Random voice still suffers from impractical problems, many practical problems, but it still has some degree of overfitting. Are we together? So the decision tree is in general more interpretable than a random foist too. You can draw out a decision tree. You can see it in a picture. we saw that in the lab. You can't do that with random forests. You can pick a tree from a random forest and visualize it, but that wouldn't be representative of the forest itself. A decision tree is inherently more powerful than a logistic regression, hardly needs to be told, no, based on the situation, that's a no-freelance theorem. Which of the following are typical uses of a decision tree, both as classifiers and regressors, of course? Bagging is the process of packing decision tree estimators in a small collection of bags. This is useful for random forest. Actually, this is... That gave me a good giggle, little flags. I have a question regarding 13, question number 13. You're asking for one or the another, like typical, the word you typically use. So are we using 50-50 for regression and classifier? See what happens is people who are used, yeah, yeah, it's quite often used. The trouble is that the world is filled with mostly classification problems and fewer classification problems. But when it comes to regression, the proportion of times you can use decision tree is quite high. They use it just as they use it for classification. Decision tree has one advantage, remember. It is a nonlinear and it is somewhat interpretable. You know, you can draw out the decision tree so amongst the interpretable model system of the few models that is nonlinear that can capture nonlinear decision boundaries which is why it is liked a lot by people who tend to use it a lot yeah I would think you know if we if we are doing regressions then why not you just you know linear model or something like that so yeah yeah we'll not capture non-linearities you'll have to go polynomial got it yeah so what does janine the decision tree refer to it refers to of course this great mathematician okay and it's a measure of impurity of the tree is summation over impurities of the tree. Obviously, this doesn't refer to the mathematician, it refers to impurity and Gini the word comes is eponymous with the great Italian statistician and sociologist. In a decision tree, what can measure relative importance is certainly true. The prediction of each of the weak learners should be correct at least half the time. For example, in a binary classifier. That is certainly true. In fact, that is this theorem, the Condorcet theorem. So guys, what I'll do is, because we have limited time for theory, one session a week, I'm not able to cover all the nuances. So some of the nuances, like for example this, I will cover through the quizzes. I'll introduce these things through the quizzes. Let us say that there are a hundred features in a data site P. While building a random forest classifier, you have to decide how many features should each tree in the forest consider for deciding on the split. The answer to that is, in general, the best classifiers will be created if each of the tree considers all the features while splitting nouns false. That will lead to possibly trees getting correlated. You want them to be de-correlated and you don't want that. So for that, take no, generally don't take more than the square root of p. I think that the scikit-learn for the longest time, the default value was all the features which isn't nice. So you should this is one of those things where you shouldn't blindly use a library you should remember to the minimum know the theory and set the parameters carefully. Decision tree is inherently more powerful algorithm than linear and polynomial regression. Once again no certainly not true. We talked about it if your decision boundary happens to be, or your regression plane happens to be a hyperplane that is slanted, decision tree will not in general outperform the simple linear regression, for example. Bagging leads to the creation of a strong learner from an ensemble of weak learners. True. In fact, that is literally the Condorcet's theory up here and then random forest always outperforms logistic regression quite a few of you got this right no it does not once again based on the situation. So guys, that is it. That was the...